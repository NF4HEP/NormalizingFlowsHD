2023-10-02 10:02:28.364764: Importing os...
2023-10-02 10:02:28.364801: Importing sys...
2023-10-02 10:02:28.364807: Importing and initializing argparse...
Visible devices: [1]
2023-10-02 10:02:28.372821: Importing timer from timeit...
2023-10-02 10:02:28.373119: Setting env variables for tf import (only device [1] will be available)...
2023-10-02 10:02:28.373148: Importing numpy...
2023-10-02 10:02:28.829868: Importing pandas...
2023-10-02 10:02:29.609637: Importing shutil...
2023-10-02 10:02:29.609715: Importing subprocess...
2023-10-02 10:02:29.609726: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-02 10:02:38.336406: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-02 10:02:39.964716: Importing textwrap...
2023-10-02 10:02:39.964779: Importing timeit...
2023-10-02 10:02:39.964792: Importing traceback...
2023-10-02 10:02:39.964808: Importing typing...
2023-10-02 10:02:39.964825: Setting tf configs...
2023-10-02 10:02:41.579319: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-02 10:02:47.009301: All modues imported successfully.
Directory ../../results/MAFN_new/ already exists.
Directory ../../results/MAFN_new/run_1/ already exists.
Skipping it.
===========
Run 1/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_2/ already exists.
Skipping it.
===========
Run 2/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_3/ already exists.
Skipping it.
===========
Run 3/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_4/ already exists.
Skipping it.
===========
Run 4/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_5/ already exists.
Skipping it.
===========
Run 5/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_6/ already exists.
Skipping it.
===========
Run 6/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_7/ already exists.
Skipping it.
===========
Run 7/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_8/ already exists.
Skipping it.
===========
Run 8/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_9/ already exists.
Skipping it.
===========
Run 9/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_10/ already exists.
Skipping it.
===========
Run 10/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_11/ already exists.
Skipping it.
===========
Run 11/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_12/ already exists.
Skipping it.
===========
Run 12/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_13/ already exists.
Skipping it.
===========
Run 13/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_14/ already exists.
Skipping it.
===========
Run 14/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_15/ already exists.
Skipping it.
===========
Run 15/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_16/ already exists.
Skipping it.
===========
Run 16/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_17/ already exists.
Skipping it.
===========
Run 17/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_18/ already exists.
Skipping it.
===========
Run 18/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_19/ already exists.
Skipping it.
===========
Run 19/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_20/ already exists.
Skipping it.
===========
Run 20/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_21/ already exists.
Skipping it.
===========
Run 21/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_22/ already exists.
Skipping it.
===========
Run 22/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_23/ already exists.
Skipping it.
===========
Run 23/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_24/ already exists.
Skipping it.
===========
Run 24/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_25/ already exists.
Skipping it.
===========
Run 25/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_26/ already exists.
Skipping it.
===========
Run 26/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_27/ already exists.
Skipping it.
===========
Run 27/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_28/ already exists.
Skipping it.
===========
Run 28/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_29/ already exists.
Skipping it.
===========
Run 29/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_30/ already exists.
Skipping it.
===========
Run 30/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_31/ already exists.
Skipping it.
===========
Run 31/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_32/ already exists.
Skipping it.
===========
Run 32/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_33/ already exists.
Skipping it.
===========
Run 33/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_34/ already exists.
Skipping it.
===========
Run 34/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_35/ already exists.
Skipping it.
===========
Run 35/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_36/ already exists.
Skipping it.
===========
Run 36/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_37/ already exists.
Skipping it.
===========
Run 37/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_38/ already exists.
Skipping it.
===========
Run 38/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_39/ already exists.
Skipping it.
===========
Run 39/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_40/ already exists.
Skipping it.
===========
Run 40/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_41/ already exists.
Skipping it.
===========
Run 41/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_42/ already exists.
Skipping it.
===========
Run 42/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_43/ already exists.
Skipping it.
===========
Run 43/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_44/ already exists.
Skipping it.
===========
Run 44/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_45/ already exists.
Skipping it.
===========
Run 45/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_46/ already exists.
Skipping it.
===========
Run 46/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_47/ already exists.
Skipping it.
===========
Run 47/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_48/ already exists.
Skipping it.
===========
Run 48/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_49/ already exists.
Skipping it.
===========
Run 49/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_50/ already exists.
Skipping it.
===========
Run 50/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_51/ already exists.
Skipping it.
===========
Run 51/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_52/ already exists.
Skipping it.
===========
Run 52/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_53/ already exists.
Skipping it.
===========
Run 53/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_54/ already exists.
Skipping it.
===========
Run 54/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_55/ already exists.
Skipping it.
===========
Run 55/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_56/ already exists.
Skipping it.
===========
Run 56/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_57/ already exists.
Skipping it.
===========
Run 57/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_58/ already exists.
Skipping it.
===========
Run 58/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_59/ already exists.
Skipping it.
===========
Run 59/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_60/ already exists.
Skipping it.
===========
Run 60/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_61/ already exists.
Skipping it.
===========
Run 61/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_62/ already exists.
Skipping it.
===========
Run 62/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_63/ already exists.
Skipping it.
===========
Run 63/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_64/ already exists.
Skipping it.
===========
Run 64/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_65/ already exists.
Skipping it.
===========
Run 65/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_66/ already exists.
Skipping it.
===========
Run 66/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_67/ already exists.
Skipping it.
===========
Run 67/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_68/ already exists.
Skipping it.
===========
Run 68/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_69/ already exists.
Skipping it.
===========
Run 69/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_70/ already exists.
Skipping it.
===========
Run 70/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_71/ already exists.
Skipping it.
===========
Run 71/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_72/ already exists.
Skipping it.
===========
Run 72/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_73/ already exists.
Skipping it.
===========
Run 73/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_74/ already exists.
Skipping it.
===========
Run 74/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_75/ already exists.
Skipping it.
===========
Run 75/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_76/ already exists.
Skipping it.
===========
Run 76/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_77/ already exists.
Skipping it.
===========
Run 77/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_78/ already exists.
Skipping it.
===========
Run 78/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_79/ already exists.
Skipping it.
===========
Run 79/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_80/ already exists.
Skipping it.
===========
Run 80/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_81/ already exists.
Skipping it.
===========
Run 81/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_82/ already exists.
Skipping it.
===========
Run 82/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_83/ already exists.
Skipping it.
===========
Run 83/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_84/ already exists.
Skipping it.
===========
Run 84/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_85/ already exists.
Skipping it.
===========
Run 85/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_86/ already exists.
Skipping it.
===========
Run 86/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_87/ already exists.
Skipping it.
===========
Run 87/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_88/ already exists.
Skipping it.
===========
Run 88/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_89/ already exists.
Skipping it.
===========
Run 89/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_90/ already exists.
Skipping it.
===========
Run 90/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_91/ already exists.
Skipping it.
===========
Run 91/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_92/ already exists.
Skipping it.
===========
Run 92/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_93/ already exists.
Skipping it.
===========
Run 93/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_94/ already exists.
Skipping it.
===========
Run 94/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_95/ already exists.
Skipping it.
===========
Run 95/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_96/ already exists.
Skipping it.
===========
Run 96/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_97/ already exists.
Skipping it.
===========
Run 97/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_98/ already exists.
Skipping it.
===========
Run 98/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_99/ already exists.
Skipping it.
===========
Run 99/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_100/ already exists.
Skipping it.
===========
Run 100/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_101/ already exists.
Skipping it.
===========
Run 101/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_102/ already exists.
Skipping it.
===========
Run 102/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_103/ already exists.
Skipping it.
===========
Run 103/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_104/ already exists.
Skipping it.
===========
Run 104/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_105/ already exists.
Skipping it.
===========
Run 105/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_106/ already exists.
Skipping it.
===========
Run 106/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_107/ already exists.
Skipping it.
===========
Run 107/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_108/ already exists.
Skipping it.
===========
Run 108/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_109/ already exists.
Skipping it.
===========
Run 109/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_110/ already exists.
Skipping it.
===========
Run 110/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_111/ already exists.
Skipping it.
===========
Run 111/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_112/ already exists.
Skipping it.
===========
Run 112/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_113/ already exists.
Skipping it.
===========
Run 113/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_114/ already exists.
Skipping it.
===========
Run 114/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_115/ already exists.
Skipping it.
===========
Run 115/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_116/ already exists.
Skipping it.
===========
Run 116/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_117/ already exists.
Skipping it.
===========
Run 117/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_118/ already exists.
Skipping it.
===========
Run 118/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_119/ already exists.
Skipping it.
===========
Run 119/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_120/ already exists.
Skipping it.
===========
Run 120/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_121/ already exists.
Skipping it.
===========
Run 121/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_122/ already exists.
Skipping it.
===========
Run 122/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_123/ already exists.
Skipping it.
===========
Run 123/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_124/ already exists.
Skipping it.
===========
Run 124/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_125/ already exists.
Skipping it.
===========
Run 125/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_126/ already exists.
Skipping it.
===========
Run 126/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_127/ already exists.
Skipping it.
===========
Run 127/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_128/ already exists.
Skipping it.
===========
Run 128/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_129/ already exists.
Skipping it.
===========
Run 129/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_130/ already exists.
Skipping it.
===========
Run 130/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_131/ already exists.
Skipping it.
===========
Run 131/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_132/ already exists.
Skipping it.
===========
Run 132/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_133/ already exists.
Skipping it.
===========
Run 133/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_134/ already exists.
Skipping it.
===========
Run 134/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_135/ already exists.
Skipping it.
===========
Run 135/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_136/ already exists.
Skipping it.
===========
Run 136/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_137/ already exists.
Skipping it.
===========
Run 137/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_138/ already exists.
Skipping it.
===========
Run 138/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_139/ already exists.
Skipping it.
===========
Run 139/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_140/ already exists.
Skipping it.
===========
Run 140/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_141/ already exists.
Skipping it.
===========
Run 141/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_142/ already exists.
Skipping it.
===========
Run 142/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_143/ already exists.
Skipping it.
===========
Run 143/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_144/ already exists.
Skipping it.
===========
Run 144/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_145/ already exists.
Skipping it.
===========
Run 145/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_146/ already exists.
Skipping it.
===========
Run 146/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_147/ already exists.
Skipping it.
===========
Run 147/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_148/ already exists.
Skipping it.
===========
Run 148/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_149/ already exists.
Skipping it.
===========
Run 149/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_150/ already exists.
Skipping it.
===========
Run 150/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_151/ already exists.
Skipping it.
===========
Run 151/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_152/ already exists.
Skipping it.
===========
Run 152/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_153/ already exists.
Skipping it.
===========
Run 153/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_154/ already exists.
Skipping it.
===========
Run 154/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_155/ already exists.
Skipping it.
===========
Run 155/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_156/ already exists.
Skipping it.
===========
Run 156/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_157/ already exists.
Skipping it.
===========
Run 157/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_158/ already exists.
Skipping it.
===========
Run 158/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_159/ already exists.
Skipping it.
===========
Run 159/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_160/ already exists.
Skipping it.
===========
Run 160/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_161/ already exists.
Skipping it.
===========
Run 161/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_162/ already exists.
Skipping it.
===========
Run 162/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_163/ already exists.
Skipping it.
===========
Run 163/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_164/ already exists.
Skipping it.
===========
Run 164/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_165/ already exists.
Skipping it.
===========
Run 165/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_166/ already exists.
Skipping it.
===========
Run 166/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_167/ already exists.
Skipping it.
===========
Run 167/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_168/ already exists.
Skipping it.
===========
Run 168/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_169/ already exists.
Skipping it.
===========
Run 169/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_170/ already exists.
Skipping it.
===========
Run 170/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_171/ already exists.
Skipping it.
===========
Run 171/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_172/ already exists.
Skipping it.
===========
Run 172/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_173/ already exists.
Skipping it.
===========
Run 173/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_174/ already exists.
Skipping it.
===========
Run 174/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_175/ already exists.
Skipping it.
===========
Run 175/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_176/ already exists.
Skipping it.
===========
Run 176/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_177/ already exists.
Skipping it.
===========
Run 177/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_178/ already exists.
Skipping it.
===========
Run 178/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_179/ already exists.
Skipping it.
===========
Run 179/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_180/ already exists.
Skipping it.
===========
Run 180/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_181/ already exists.
Skipping it.
===========
Run 181/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_182/ already exists.
Skipping it.
===========
Run 182/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_183/ already exists.
Skipping it.
===========
Run 183/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_184/ already exists.
Skipping it.
===========
Run 184/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_185/ already exists.
Skipping it.
===========
Run 185/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_186/ already exists.
Skipping it.
===========
Run 186/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_187/ already exists.
Skipping it.
===========
Run 187/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_188/ already exists.
Skipping it.
===========
Run 188/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_189/ already exists.
Skipping it.
===========
Run 189/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_190/ already exists.
Skipping it.
===========
Run 190/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_191/ already exists.
Skipping it.
===========
Run 191/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_192/ already exists.
Skipping it.
===========
Run 192/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_193/ already exists.
Skipping it.
===========
Run 193/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_194/ already exists.
Skipping it.
===========
Run 194/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_195/ already exists.
Skipping it.
===========
Run 195/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_196/ already exists.
Skipping it.
===========
Run 196/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_197/ already exists.
Skipping it.
===========
Run 197/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_198/ already exists.
Skipping it.
===========
Run 198/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_199/ already exists.
Skipping it.
===========
Run 199/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_200/ already exists.
Skipping it.
===========
Run 200/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_201/ already exists.
Skipping it.
===========
Run 201/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_202/ already exists.
Skipping it.
===========
Run 202/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_203/ already exists.
Skipping it.
===========
Run 203/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_204/ already exists.
Skipping it.
===========
Run 204/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_205/ already exists.
Skipping it.
===========
Run 205/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_206/ already exists.
Skipping it.
===========
Run 206/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_207/ already exists.
Skipping it.
===========
Run 207/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_208/ already exists.
Skipping it.
===========
Run 208/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_209/ already exists.
Skipping it.
===========
Run 209/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_210/ already exists.
Skipping it.
===========
Run 210/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_211/ already exists.
Skipping it.
===========
Run 211/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_212/ already exists.
Skipping it.
===========
Run 212/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_213/ already exists.
Skipping it.
===========
Run 213/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_214/ already exists.
Skipping it.
===========
Run 214/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_215/ already exists.
Skipping it.
===========
Run 215/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_216/ already exists.
Skipping it.
===========
Run 216/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_217/ already exists.
Skipping it.
===========
Run 217/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_218/ already exists.
Skipping it.
===========
Run 218/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_219/ already exists.
Skipping it.
===========
Run 219/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_220/ already exists.
Skipping it.
===========
Run 220/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_221/ already exists.
Skipping it.
===========
Run 221/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_222/ already exists.
Skipping it.
===========
Run 222/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_223/ already exists.
Skipping it.
===========
Run 223/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_224/ already exists.
Skipping it.
===========
Run 224/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_225/ already exists.
Skipping it.
===========
Run 225/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_226/ already exists.
Skipping it.
===========
Run 226/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_227/ already exists.
Skipping it.
===========
Run 227/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_228/ already exists.
Skipping it.
===========
Run 228/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_229/ already exists.
Skipping it.
===========
Run 229/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_230/ already exists.
Skipping it.
===========
Run 230/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_231/ already exists.
Skipping it.
===========
Run 231/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_232/ already exists.
Skipping it.
===========
Run 232/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_233/ already exists.
Skipping it.
===========
Run 233/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_234/ already exists.
Skipping it.
===========
Run 234/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_235/ already exists.
Skipping it.
===========
Run 235/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_236/ already exists.
Skipping it.
===========
Run 236/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_237/ already exists.
Skipping it.
===========
Run 237/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_238/ already exists.
Skipping it.
===========
Run 238/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_239/ already exists.
Skipping it.
===========
Run 239/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_240/ already exists.
Skipping it.
===========
Run 240/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_241/ already exists.
Skipping it.
===========
Run 241/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_242/ already exists.
Skipping it.
===========
Run 242/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_243/ already exists.
Skipping it.
===========
Run 243/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_244/ already exists.
Skipping it.
===========
Run 244/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_245/ already exists.
Skipping it.
===========
Run 245/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_246/ already exists.
Skipping it.
===========
Run 246/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_247/ already exists.
Skipping it.
===========
Run 247/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_248/ already exists.
Skipping it.
===========
Run 248/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_249/ already exists.
Skipping it.
===========
Run 249/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_250/ already exists.
Skipping it.
===========
Run 250/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_251/ already exists.
Skipping it.
===========
Run 251/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_252/ already exists.
Skipping it.
===========
Run 252/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_253/ already exists.
Skipping it.
===========
Run 253/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_254/ already exists.
Skipping it.
===========
Run 254/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_255/ already exists.
Skipping it.
===========
Run 255/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_256/ already exists.
Skipping it.
===========
Run 256/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_257/ already exists.
Skipping it.
===========
Run 257/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_258/ already exists.
Skipping it.
===========
Run 258/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_259/ already exists.
Skipping it.
===========
Run 259/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_260/ already exists.
Skipping it.
===========
Run 260/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_261/ already exists.
Skipping it.
===========
Run 261/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_262/ already exists.
Skipping it.
===========
Run 262/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_263/ already exists.
Skipping it.
===========
Run 263/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_264/ already exists.
Skipping it.
===========
Run 264/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_265/ already exists.
Skipping it.
===========
Run 265/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_266/ already exists.
Skipping it.
===========
Run 266/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_267/ already exists.
Skipping it.
===========
Run 267/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_268/ already exists.
Skipping it.
===========
Run 268/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_269/ already exists.
Skipping it.
===========
Run 269/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_270/ already exists.
Skipping it.
===========
Run 270/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_271/ already exists.
Skipping it.
===========
Run 271/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_272/ already exists.
Skipping it.
===========
Run 272/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_273/ already exists.
Skipping it.
===========
Run 273/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_274/ already exists.
Skipping it.
===========
Run 274/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_275/ already exists.
Skipping it.
===========
Run 275/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_276/ already exists.
Skipping it.
===========
Run 276/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_277/ already exists.
Skipping it.
===========
Run 277/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_278/ already exists.
Skipping it.
===========
Run 278/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_279/ already exists.
Skipping it.
===========
Run 279/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_280/ already exists.
Skipping it.
===========
Run 280/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_281/ already exists.
Skipping it.
===========
Run 281/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_282/ already exists.
Skipping it.
===========
Run 282/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_283/ already exists.
Skipping it.
===========
Run 283/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_284/ already exists.
Skipping it.
===========
Run 284/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_285/ already exists.
Skipping it.
===========
Run 285/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_286/ already exists.
Skipping it.
===========
Run 286/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_287/ already exists.
Skipping it.
===========
Run 287/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_288/ already exists.
Skipping it.
===========
Run 288/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_289/ already exists.
Skipping it.
===========
Run 289/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_290/ already exists.
Skipping it.
===========
Run 290/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_291/ already exists.
Skipping it.
===========
Run 291/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_292/ already exists.
Skipping it.
===========
Run 292/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_293/ already exists.
Skipping it.
===========
Run 293/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_294/ already exists.
Skipping it.
===========
Run 294/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_295/ already exists.
Skipping it.
===========
Run 295/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_296/ already exists.
Skipping it.
===========
Run 296/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_297/ already exists.
Skipping it.
===========
Run 297/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_298/ already exists.
Skipping it.
===========
Run 298/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_299/ already exists.
Skipping it.
===========
Run 299/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_300/ already exists.
Skipping it.
===========
Run 300/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_301/ already exists.
Skipping it.
===========
Run 301/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_302/ already exists.
Skipping it.
===========
Run 302/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_303/ already exists.
Skipping it.
===========
Run 303/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_304/ already exists.
Skipping it.
===========
Run 304/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_305/ already exists.
Skipping it.
===========
Run 305/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_306/ already exists.
Skipping it.
===========
Run 306/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_307/ already exists.
Skipping it.
===========
Run 307/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_308/ already exists.
Skipping it.
===========
Run 308/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_309/ already exists.
Skipping it.
===========
Run 309/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_310/ already exists.
Skipping it.
===========
Run 310/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_311/ already exists.
Skipping it.
===========
Run 311/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_312/ already exists.
Skipping it.
===========
Run 312/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_313/ already exists.
Skipping it.
===========
Run 313/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_314/ already exists.
Skipping it.
===========
Run 314/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_315/ already exists.
Skipping it.
===========
Run 315/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_316/ already exists.
Skipping it.
===========
Run 316/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_317/ already exists.
Skipping it.
===========
Run 317/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_318/ already exists.
Skipping it.
===========
Run 318/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_319/ already exists.
Skipping it.
===========
Run 319/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_320/ already exists.
Skipping it.
===========
Run 320/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_321/ already exists.
Skipping it.
===========
Run 321/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_322/ already exists.
Skipping it.
===========
Run 322/360 already exists. Skipping it.
===========

===========
Generating train data for run 323.
===========
Train data generated in 2.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_323
self.data_kwargs: {'seed': 0}
self.x_data: [[8.210789   4.3589015  5.1949253  ... 2.1186912  8.28666    6.44192   ]
 [5.776202   0.0413513  4.6274076  ... 4.7782993  6.688914   5.570342  ]
 [5.7263374  6.2561994  5.7377024  ... 8.820506   0.58727944 6.515802  ]
 ...
 [5.7884197  0.56533414 4.7867274  ... 4.878848   6.0022035  4.9900055 ]
 [5.9892163  0.222366   4.8566475  ... 4.5662537  6.433208   4.1020565 ]
 [7.8653784  4.6189337  5.157348   ... 2.6189103  7.542449   6.328924  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1000)]            0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  4191520   
 r)                                                              
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fccfbb5c880>
self.optimizer_config: {'class_name': 'Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.legacy.adam.Adam object at 0x7fccfbb05120>
type(optimizer): <class 'keras.optimizers.legacy.adam.Adam'>
Traceback (most recent call last):
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py", line 637, in <module>
    hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py", line 239, in train_function
    NFObject: Trainer.Trainer = Trainer.Trainer(base_distribution = base_dist,
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Trainer.py", line 795, in __init__
    self.optimizer = tf.keras.optimizers.get(self.optimizer_config)
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Trainer.py", line 1177, in optimizer
    raise ValueError("optimizer is not of type tf.keras.optimizers.Optimizer or tf.keras.optimizers.legacy.Optimizer.")
ValueError: optimizer is not of type tf.keras.optimizers.Optimizer or tf.keras.optimizers.legacy.Optimizer.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py", line 729, in <module>
    Utils.save_results_current_run_txt(path_to_results, results_dict)
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py", line 311, in save_results_current_run_txt
    data_string = ','.join(str(value[-1]) for value in dict_copy.values())
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py", line 311, in <genexpr>
    data_string = ','.join(str(value[-1]) for value in dict_copy.values())
IndexError: list index out of range
2023-10-02 10:05:03.774425: Importing os...
2023-10-02 10:05:03.774818: Importing sys...
2023-10-02 10:05:03.774891: Importing and initializing argparse...
Visible devices: [1]
2023-10-02 10:05:03.872786: Importing timer from timeit...
2023-10-02 10:05:03.873468: Setting env variables for tf import (only device [1] will be available)...
2023-10-02 10:05:03.873507: Importing numpy...
2023-10-02 10:05:04.193267: Importing pandas...
2023-10-02 10:05:04.723653: Importing shutil...
2023-10-02 10:05:04.723708: Importing subprocess...
2023-10-02 10:05:04.723717: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-02 10:05:10.444273: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-02 10:05:11.927655: Importing textwrap...
2023-10-02 10:05:11.927676: Importing timeit...
2023-10-02 10:05:11.927683: Importing traceback...
2023-10-02 10:05:11.927688: Importing typing...
2023-10-02 10:05:11.927696: Setting tf configs...
2023-10-02 10:05:12.459590: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-02 10:05:15.728581: All modues imported successfully.
Directory ../../results/MAFN_new/ already exists.
Directory ../../results/MAFN_new/run_1/ already exists.
Skipping it.
===========
Run 1/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_2/ already exists.
Skipping it.
===========
Run 2/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_3/ already exists.
Skipping it.
===========
Run 3/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_4/ already exists.
Skipping it.
===========
Run 4/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_5/ already exists.
Skipping it.
===========
Run 5/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_6/ already exists.
Skipping it.
===========
Run 6/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_7/ already exists.
Skipping it.
===========
Run 7/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_8/ already exists.
Skipping it.
===========
Run 8/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_9/ already exists.
Skipping it.
===========
Run 9/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_10/ already exists.
Skipping it.
===========
Run 10/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_11/ already exists.
Skipping it.
===========
Run 11/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_12/ already exists.
Skipping it.
===========
Run 12/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_13/ already exists.
Skipping it.
===========
Run 13/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_14/ already exists.
Skipping it.
===========
Run 14/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_15/ already exists.
Skipping it.
===========
Run 15/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_16/ already exists.
Skipping it.
===========
Run 16/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_17/ already exists.
Skipping it.
===========
Run 17/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_18/ already exists.
Skipping it.
===========
Run 18/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_19/ already exists.
Skipping it.
===========
Run 19/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_20/ already exists.
Skipping it.
===========
Run 20/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_21/ already exists.
Skipping it.
===========
Run 21/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_22/ already exists.
Skipping it.
===========
Run 22/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_23/ already exists.
Skipping it.
===========
Run 23/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_24/ already exists.
Skipping it.
===========
Run 24/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_25/ already exists.
Skipping it.
===========
Run 25/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_26/ already exists.
Skipping it.
===========
Run 26/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_27/ already exists.
Skipping it.
===========
Run 27/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_28/ already exists.
Skipping it.
===========
Run 28/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_29/ already exists.
Skipping it.
===========
Run 29/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_30/ already exists.
Skipping it.
===========
Run 30/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_31/ already exists.
Skipping it.
===========
Run 31/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_32/ already exists.
Skipping it.
===========
Run 32/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_33/ already exists.
Skipping it.
===========
Run 33/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_34/ already exists.
Skipping it.
===========
Run 34/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_35/ already exists.
Skipping it.
===========
Run 35/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_36/ already exists.
Skipping it.
===========
Run 36/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_37/ already exists.
Skipping it.
===========
Run 37/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_38/ already exists.
Skipping it.
===========
Run 38/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_39/ already exists.
Skipping it.
===========
Run 39/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_40/ already exists.
Skipping it.
===========
Run 40/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_41/ already exists.
Skipping it.
===========
Run 41/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_42/ already exists.
Skipping it.
===========
Run 42/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_43/ already exists.
Skipping it.
===========
Run 43/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_44/ already exists.
Skipping it.
===========
Run 44/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_45/ already exists.
Skipping it.
===========
Run 45/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_46/ already exists.
Skipping it.
===========
Run 46/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_47/ already exists.
Skipping it.
===========
Run 47/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_48/ already exists.
Skipping it.
===========
Run 48/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_49/ already exists.
Skipping it.
===========
Run 49/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_50/ already exists.
Skipping it.
===========
Run 50/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_51/ already exists.
Skipping it.
===========
Run 51/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_52/ already exists.
Skipping it.
===========
Run 52/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_53/ already exists.
Skipping it.
===========
Run 53/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_54/ already exists.
Skipping it.
===========
Run 54/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_55/ already exists.
Skipping it.
===========
Run 55/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_56/ already exists.
Skipping it.
===========
Run 56/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_57/ already exists.
Skipping it.
===========
Run 57/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_58/ already exists.
Skipping it.
===========
Run 58/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_59/ already exists.
Skipping it.
===========
Run 59/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_60/ already exists.
Skipping it.
===========
Run 60/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_61/ already exists.
Skipping it.
===========
Run 61/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_62/ already exists.
Skipping it.
===========
Run 62/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_63/ already exists.
Skipping it.
===========
Run 63/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_64/ already exists.
Skipping it.
===========
Run 64/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_65/ already exists.
Skipping it.
===========
Run 65/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_66/ already exists.
Skipping it.
===========
Run 66/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_67/ already exists.
Skipping it.
===========
Run 67/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_68/ already exists.
Skipping it.
===========
Run 68/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_69/ already exists.
Skipping it.
===========
Run 69/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_70/ already exists.
Skipping it.
===========
Run 70/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_71/ already exists.
Skipping it.
===========
Run 71/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_72/ already exists.
Skipping it.
===========
Run 72/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_73/ already exists.
Skipping it.
===========
Run 73/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_74/ already exists.
Skipping it.
===========
Run 74/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_75/ already exists.
Skipping it.
===========
Run 75/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_76/ already exists.
Skipping it.
===========
Run 76/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_77/ already exists.
Skipping it.
===========
Run 77/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_78/ already exists.
Skipping it.
===========
Run 78/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_79/ already exists.
Skipping it.
===========
Run 79/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_80/ already exists.
Skipping it.
===========
Run 80/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_81/ already exists.
Skipping it.
===========
Run 81/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_82/ already exists.
Skipping it.
===========
Run 82/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_83/ already exists.
Skipping it.
===========
Run 83/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_84/ already exists.
Skipping it.
===========
Run 84/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_85/ already exists.
Skipping it.
===========
Run 85/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_86/ already exists.
Skipping it.
===========
Run 86/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_87/ already exists.
Skipping it.
===========
Run 87/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_88/ already exists.
Skipping it.
===========
Run 88/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_89/ already exists.
Skipping it.
===========
Run 89/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_90/ already exists.
Skipping it.
===========
Run 90/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_91/ already exists.
Skipping it.
===========
Run 91/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_92/ already exists.
Skipping it.
===========
Run 92/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_93/ already exists.
Skipping it.
===========
Run 93/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_94/ already exists.
Skipping it.
===========
Run 94/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_95/ already exists.
Skipping it.
===========
Run 95/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_96/ already exists.
Skipping it.
===========
Run 96/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_97/ already exists.
Skipping it.
===========
Run 97/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_98/ already exists.
Skipping it.
===========
Run 98/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_99/ already exists.
Skipping it.
===========
Run 99/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_100/ already exists.
Skipping it.
===========
Run 100/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_101/ already exists.
Skipping it.
===========
Run 101/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_102/ already exists.
Skipping it.
===========
Run 102/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_103/ already exists.
Skipping it.
===========
Run 103/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_104/ already exists.
Skipping it.
===========
Run 104/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_105/ already exists.
Skipping it.
===========
Run 105/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_106/ already exists.
Skipping it.
===========
Run 106/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_107/ already exists.
Skipping it.
===========
Run 107/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_108/ already exists.
Skipping it.
===========
Run 108/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_109/ already exists.
Skipping it.
===========
Run 109/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_110/ already exists.
Skipping it.
===========
Run 110/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_111/ already exists.
Skipping it.
===========
Run 111/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_112/ already exists.
Skipping it.
===========
Run 112/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_113/ already exists.
Skipping it.
===========
Run 113/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_114/ already exists.
Skipping it.
===========
Run 114/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_115/ already exists.
Skipping it.
===========
Run 115/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_116/ already exists.
Skipping it.
===========
Run 116/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_117/ already exists.
Skipping it.
===========
Run 117/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_118/ already exists.
Skipping it.
===========
Run 118/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_119/ already exists.
Skipping it.
===========
Run 119/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_120/ already exists.
Skipping it.
===========
Run 120/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_121/ already exists.
Skipping it.
===========
Run 121/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_122/ already exists.
Skipping it.
===========
Run 122/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_123/ already exists.
Skipping it.
===========
Run 123/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_124/ already exists.
Skipping it.
===========
Run 124/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_125/ already exists.
Skipping it.
===========
Run 125/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_126/ already exists.
Skipping it.
===========
Run 126/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_127/ already exists.
Skipping it.
===========
Run 127/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_128/ already exists.
Skipping it.
===========
Run 128/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_129/ already exists.
Skipping it.
===========
Run 129/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_130/ already exists.
Skipping it.
===========
Run 130/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_131/ already exists.
Skipping it.
===========
Run 131/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_132/ already exists.
Skipping it.
===========
Run 132/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_133/ already exists.
Skipping it.
===========
Run 133/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_134/ already exists.
Skipping it.
===========
Run 134/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_135/ already exists.
Skipping it.
===========
Run 135/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_136/ already exists.
Skipping it.
===========
Run 136/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_137/ already exists.
Skipping it.
===========
Run 137/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_138/ already exists.
Skipping it.
===========
Run 138/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_139/ already exists.
Skipping it.
===========
Run 139/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_140/ already exists.
Skipping it.
===========
Run 140/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_141/ already exists.
Skipping it.
===========
Run 141/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_142/ already exists.
Skipping it.
===========
Run 142/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_143/ already exists.
Skipping it.
===========
Run 143/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_144/ already exists.
Skipping it.
===========
Run 144/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_145/ already exists.
Skipping it.
===========
Run 145/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_146/ already exists.
Skipping it.
===========
Run 146/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_147/ already exists.
Skipping it.
===========
Run 147/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_148/ already exists.
Skipping it.
===========
Run 148/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_149/ already exists.
Skipping it.
===========
Run 149/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_150/ already exists.
Skipping it.
===========
Run 150/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_151/ already exists.
Skipping it.
===========
Run 151/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_152/ already exists.
Skipping it.
===========
Run 152/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_153/ already exists.
Skipping it.
===========
Run 153/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_154/ already exists.
Skipping it.
===========
Run 154/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_155/ already exists.
Skipping it.
===========
Run 155/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_156/ already exists.
Skipping it.
===========
Run 156/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_157/ already exists.
Skipping it.
===========
Run 157/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_158/ already exists.
Skipping it.
===========
Run 158/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_159/ already exists.
Skipping it.
===========
Run 159/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_160/ already exists.
Skipping it.
===========
Run 160/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_161/ already exists.
Skipping it.
===========
Run 161/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_162/ already exists.
Skipping it.
===========
Run 162/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_163/ already exists.
Skipping it.
===========
Run 163/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_164/ already exists.
Skipping it.
===========
Run 164/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_165/ already exists.
Skipping it.
===========
Run 165/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_166/ already exists.
Skipping it.
===========
Run 166/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_167/ already exists.
Skipping it.
===========
Run 167/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_168/ already exists.
Skipping it.
===========
Run 168/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_169/ already exists.
Skipping it.
===========
Run 169/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_170/ already exists.
Skipping it.
===========
Run 170/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_171/ already exists.
Skipping it.
===========
Run 171/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_172/ already exists.
Skipping it.
===========
Run 172/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_173/ already exists.
Skipping it.
===========
Run 173/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_174/ already exists.
Skipping it.
===========
Run 174/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_175/ already exists.
Skipping it.
===========
Run 175/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_176/ already exists.
Skipping it.
===========
Run 176/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_177/ already exists.
Skipping it.
===========
Run 177/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_178/ already exists.
Skipping it.
===========
Run 178/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_179/ already exists.
Skipping it.
===========
Run 179/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_180/ already exists.
Skipping it.
===========
Run 180/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_181/ already exists.
Skipping it.
===========
Run 181/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_182/ already exists.
Skipping it.
===========
Run 182/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_183/ already exists.
Skipping it.
===========
Run 183/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_184/ already exists.
Skipping it.
===========
Run 184/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_185/ already exists.
Skipping it.
===========
Run 185/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_186/ already exists.
Skipping it.
===========
Run 186/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_187/ already exists.
Skipping it.
===========
Run 187/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_188/ already exists.
Skipping it.
===========
Run 188/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_189/ already exists.
Skipping it.
===========
Run 189/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_190/ already exists.
Skipping it.
===========
Run 190/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_191/ already exists.
Skipping it.
===========
Run 191/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_192/ already exists.
Skipping it.
===========
Run 192/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_193/ already exists.
Skipping it.
===========
Run 193/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_194/ already exists.
Skipping it.
===========
Run 194/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_195/ already exists.
Skipping it.
===========
Run 195/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_196/ already exists.
Skipping it.
===========
Run 196/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_197/ already exists.
Skipping it.
===========
Run 197/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_198/ already exists.
Skipping it.
===========
Run 198/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_199/ already exists.
Skipping it.
===========
Run 199/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_200/ already exists.
Skipping it.
===========
Run 200/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_201/ already exists.
Skipping it.
===========
Run 201/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_202/ already exists.
Skipping it.
===========
Run 202/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_203/ already exists.
Skipping it.
===========
Run 203/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_204/ already exists.
Skipping it.
===========
Run 204/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_205/ already exists.
Skipping it.
===========
Run 205/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_206/ already exists.
Skipping it.
===========
Run 206/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_207/ already exists.
Skipping it.
===========
Run 207/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_208/ already exists.
Skipping it.
===========
Run 208/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_209/ already exists.
Skipping it.
===========
Run 209/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_210/ already exists.
Skipping it.
===========
Run 210/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_211/ already exists.
Skipping it.
===========
Run 211/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_212/ already exists.
Skipping it.
===========
Run 212/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_213/ already exists.
Skipping it.
===========
Run 213/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_214/ already exists.
Skipping it.
===========
Run 214/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_215/ already exists.
Skipping it.
===========
Run 215/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_216/ already exists.
Skipping it.
===========
Run 216/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_217/ already exists.
Skipping it.
===========
Run 217/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_218/ already exists.
Skipping it.
===========
Run 218/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_219/ already exists.
Skipping it.
===========
Run 219/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_220/ already exists.
Skipping it.
===========
Run 220/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_221/ already exists.
Skipping it.
===========
Run 221/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_222/ already exists.
Skipping it.
===========
Run 222/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_223/ already exists.
Skipping it.
===========
Run 223/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_224/ already exists.
Skipping it.
===========
Run 224/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_225/ already exists.
Skipping it.
===========
Run 225/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_226/ already exists.
Skipping it.
===========
Run 226/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_227/ already exists.
Skipping it.
===========
Run 227/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_228/ already exists.
Skipping it.
===========
Run 228/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_229/ already exists.
Skipping it.
===========
Run 229/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_230/ already exists.
Skipping it.
===========
Run 230/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_231/ already exists.
Skipping it.
===========
Run 231/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_232/ already exists.
Skipping it.
===========
Run 232/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_233/ already exists.
Skipping it.
===========
Run 233/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_234/ already exists.
Skipping it.
===========
Run 234/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_235/ already exists.
Skipping it.
===========
Run 235/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_236/ already exists.
Skipping it.
===========
Run 236/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_237/ already exists.
Skipping it.
===========
Run 237/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_238/ already exists.
Skipping it.
===========
Run 238/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_239/ already exists.
Skipping it.
===========
Run 239/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_240/ already exists.
Skipping it.
===========
Run 240/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_241/ already exists.
Skipping it.
===========
Run 241/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_242/ already exists.
Skipping it.
===========
Run 242/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_243/ already exists.
Skipping it.
===========
Run 243/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_244/ already exists.
Skipping it.
===========
Run 244/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_245/ already exists.
Skipping it.
===========
Run 245/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_246/ already exists.
Skipping it.
===========
Run 246/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_247/ already exists.
Skipping it.
===========
Run 247/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_248/ already exists.
Skipping it.
===========
Run 248/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_249/ already exists.
Skipping it.
===========
Run 249/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_250/ already exists.
Skipping it.
===========
Run 250/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_251/ already exists.
Skipping it.
===========
Run 251/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_252/ already exists.
Skipping it.
===========
Run 252/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_253/ already exists.
Skipping it.
===========
Run 253/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_254/ already exists.
Skipping it.
===========
Run 254/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_255/ already exists.
Skipping it.
===========
Run 255/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_256/ already exists.
Skipping it.
===========
Run 256/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_257/ already exists.
Skipping it.
===========
Run 257/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_258/ already exists.
Skipping it.
===========
Run 258/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_259/ already exists.
Skipping it.
===========
Run 259/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_260/ already exists.
Skipping it.
===========
Run 260/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_261/ already exists.
Skipping it.
===========
Run 261/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_262/ already exists.
Skipping it.
===========
Run 262/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_263/ already exists.
Skipping it.
===========
Run 263/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_264/ already exists.
Skipping it.
===========
Run 264/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_265/ already exists.
Skipping it.
===========
Run 265/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_266/ already exists.
Skipping it.
===========
Run 266/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_267/ already exists.
Skipping it.
===========
Run 267/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_268/ already exists.
Skipping it.
===========
Run 268/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_269/ already exists.
Skipping it.
===========
Run 269/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_270/ already exists.
Skipping it.
===========
Run 270/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_271/ already exists.
Skipping it.
===========
Run 271/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_272/ already exists.
Skipping it.
===========
Run 272/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_273/ already exists.
Skipping it.
===========
Run 273/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_274/ already exists.
Skipping it.
===========
Run 274/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_275/ already exists.
Skipping it.
===========
Run 275/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_276/ already exists.
Skipping it.
===========
Run 276/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_277/ already exists.
Skipping it.
===========
Run 277/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_278/ already exists.
Skipping it.
===========
Run 278/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_279/ already exists.
Skipping it.
===========
Run 279/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_280/ already exists.
Skipping it.
===========
Run 280/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_281/ already exists.
Skipping it.
===========
Run 281/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_282/ already exists.
Skipping it.
===========
Run 282/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_283/ already exists.
Skipping it.
===========
Run 283/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_284/ already exists.
Skipping it.
===========
Run 284/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_285/ already exists.
Skipping it.
===========
Run 285/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_286/ already exists.
Skipping it.
===========
Run 286/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_287/ already exists.
Skipping it.
===========
Run 287/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_288/ already exists.
Skipping it.
===========
Run 288/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_289/ already exists.
Skipping it.
===========
Run 289/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_290/ already exists.
Skipping it.
===========
Run 290/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_291/ already exists.
Skipping it.
===========
Run 291/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_292/ already exists.
Skipping it.
===========
Run 292/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_293/ already exists.
Skipping it.
===========
Run 293/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_294/ already exists.
Skipping it.
===========
Run 294/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_295/ already exists.
Skipping it.
===========
Run 295/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_296/ already exists.
Skipping it.
===========
Run 296/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_297/ already exists.
Skipping it.
===========
Run 297/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_298/ already exists.
Skipping it.
===========
Run 298/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_299/ already exists.
Skipping it.
===========
Run 299/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_300/ already exists.
Skipping it.
===========
Run 300/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_301/ already exists.
Skipping it.
===========
Run 301/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_302/ already exists.
Skipping it.
===========
Run 302/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_303/ already exists.
Skipping it.
===========
Run 303/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_304/ already exists.
Skipping it.
===========
Run 304/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_305/ already exists.
Skipping it.
===========
Run 305/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_306/ already exists.
Skipping it.
===========
Run 306/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_307/ already exists.
Skipping it.
===========
Run 307/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_308/ already exists.
Skipping it.
===========
Run 308/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_309/ already exists.
Skipping it.
===========
Run 309/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_310/ already exists.
Skipping it.
===========
Run 310/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_311/ already exists.
Skipping it.
===========
Run 311/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_312/ already exists.
Skipping it.
===========
Run 312/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_313/ already exists.
Skipping it.
===========
Run 313/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_314/ already exists.
Skipping it.
===========
Run 314/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_315/ already exists.
Skipping it.
===========
Run 315/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_316/ already exists.
Skipping it.
===========
Run 316/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_317/ already exists.
Skipping it.
===========
Run 317/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_318/ already exists.
Skipping it.
===========
Run 318/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_319/ already exists.
Skipping it.
===========
Run 319/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_320/ already exists.
Skipping it.
===========
Run 320/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_321/ already exists.
Skipping it.
===========
Run 321/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_322/ already exists.
Skipping it.
===========
Run 322/360 already exists. Skipping it.
===========

===========
Generating train data for run 323.
===========
Train data generated in 2.91 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_323
self.data_kwargs: {'seed': 0}
self.x_data: [[8.210789   4.3589015  5.1949253  ... 2.1186912  8.28666    6.44192   ]
 [5.776202   0.0413513  4.6274076  ... 4.7782993  6.688914   5.570342  ]
 [5.7263374  6.2561994  5.7377024  ... 8.820506   0.58727944 6.515802  ]
 ...
 [5.7884197  0.56533414 4.7867274  ... 4.878848   6.0022035  4.9900055 ]
 [5.9892163  0.222366   4.8566475  ... 4.5662537  6.433208   4.1020565 ]
 [7.8653784  4.6189337  5.157348   ... 2.6189103  7.542449   6.328924  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1000)]            0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  4191520   
 r)                                                              
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fb7987e6410>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb79877f910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb79877f910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb7987ad960>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb798549870>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb79854a020>, <keras.callbacks.ModelCheckpoint object at 0x7fb79854a170>, <keras.callbacks.EarlyStopping object at 0x7fb79854a380>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb79854a3b0>, <keras.callbacks.TerminateOnNaN object at 0x7fb79854a0e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_323/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 323/360 with hyperparameters:
timestamp = 2023-10-02 10:05:27.136087
ndims = 1000
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 8.21078873e+00  4.35890150e+00  5.19492531e+00  2.84795880e+00
  6.23694658e+00  2.48332572e+00  6.01452780e+00  1.49544990e+00
  1.56305087e+00  4.33542109e+00  3.63919592e+00  2.33469009e+00
  7.12970674e-01  4.46248204e-01  1.55621916e-01  3.05101776e+00
  4.20721769e+00  7.49161625e+00  3.96869850e+00  9.05481625e+00
  4.47121114e-01  2.05655861e+00  3.52404070e+00  5.90453529e+00
  8.73787975e+00  3.53096032e+00  2.76253247e+00  6.50554955e-01
  8.03491688e+00  3.65929008e+00  4.93096781e+00  1.00386086e+01
  5.29845119e-01  1.22730446e+00  7.25914860e+00  7.58729315e+00
  7.71210337e+00  5.14168453e+00  9.58227444e+00  4.79759741e+00
  6.05360174e+00 -3.22920084e-03  4.30121565e+00  7.28324509e+00
  2.30840397e+00  9.72805214e+00  7.01551247e+00  3.15825868e+00
  7.77309942e+00  1.27091324e+00  9.88113785e+00  5.76226175e-01
  9.93493366e+00  6.19022250e-01  7.72215176e+00  1.45727849e+00
  2.63241100e+00  7.56394577e+00  4.26764488e+00  1.30306756e+00
  4.47696209e+00  4.10884666e+00  6.25260830e+00  7.17153311e+00
  7.79660606e+00  4.86149597e+00  4.01030636e+00  1.48931491e+00
  8.25993347e+00  6.99169493e+00  4.15396929e+00  5.40362930e+00
  6.69921494e+00  1.80113685e+00 -2.28963286e-01  3.53704309e+00
  5.97519684e+00  5.03495884e+00  5.96093559e+00  8.27530861e+00
  5.11110401e+00  1.71916723e-01  1.00485516e+01  5.39971399e+00
  6.97965956e+00  6.67698717e+00  9.53224850e+00  2.60424829e+00
  6.92656088e+00  2.48836160e+00  1.50950837e+00  3.11806774e+00
  3.62086797e+00  9.48242664e+00  2.35715413e+00  1.47683930e+00
  6.57032585e+00  2.31288910e+00  5.07415724e+00  8.22896194e+00
  2.67307043e+00  4.70531940e+00  5.81045723e+00  3.97613907e+00
  1.00836468e+01  6.80917358e+00  7.91102648e+00  9.83973694e+00
  3.97975469e+00  2.81087971e+00  6.61194921e-01  2.09042239e+00
  1.80242181e+00  2.45311332e+00  7.75794411e+00  5.35263777e+00
  6.36038160e+00  8.03104973e+00  2.23086929e+00  3.26186609e+00
  7.10611820e+00  4.36860800e+00  5.38674593e-01  5.03258133e+00
  8.79059315e+00  4.12663126e+00  2.96597481e+00  9.92143154e+00
  6.63155603e+00  2.29087067e+00  9.67418194e-01  8.54019165e+00
  7.32748413e+00  9.67831326e+00  3.08930302e+00  1.43350685e+00
  7.23957300e+00  2.06572795e+00  1.31994998e+00  2.38906145e+00
  8.15470123e+00  4.92610788e+00  4.60461289e-01  4.69264269e+00
  7.11180496e+00  6.15540504e+00  5.58532810e+00  1.97622848e+00
  8.55810928e+00  5.60682297e+00  6.50732088e+00  9.74595356e+00
  3.70203424e+00  8.65994740e+00  3.66820455e-01 -1.19887441e-01
  5.62859249e+00  3.21332264e+00  2.41353798e+00  2.97626567e+00
  3.33519387e+00  7.60388327e+00  5.94638872e+00  7.90076303e+00
  8.14614868e+00  8.43794918e+00  5.11629820e+00  4.45509529e+00
  8.96594524e+00  6.83408117e+00  5.29020429e-01  5.47313118e+00
  3.56114149e+00  9.86122417e+00 -8.47060025e-01  5.97660685e+00
  7.56903505e+00  3.27315211e+00  9.39664459e+00  9.18616390e+00
  9.16251087e+00  3.12576860e-01  5.81769562e+00  2.36430216e+00
  8.91152668e+00  6.80723286e+00  5.49286032e+00  1.25512540e+00
  6.51518726e+00  6.32031965e+00  9.03301430e+00  5.99218893e+00
  9.68340588e+00  9.75206280e+00  6.48233700e+00  5.02287769e+00
  4.40520668e+00  8.71606636e+00  1.15936518e-01  1.15061617e+00
  1.13465822e+00  7.72926044e+00  3.59013176e+00  6.88610840e+00
  6.62146950e+00  3.82466817e+00  5.06336927e+00  6.39645910e+00
  9.77296638e+00  6.23589993e+00  6.13958120e+00  6.46353066e-01
  7.98168039e+00  5.24744987e+00  8.84934807e+00  7.04309762e-01
  4.59585285e+00  2.57471061e+00  2.76045990e+00  7.58923578e+00
  6.31161690e+00  3.93680930e+00  3.41281295e-03  9.66006947e+00
  9.57943439e+00  2.44953036e+00  7.60993719e-01  5.62422276e+00
  9.45851040e+00  9.12294960e+00  5.39434481e+00  2.33969688e+00
  3.87983751e+00  8.94526768e+00  4.94882011e+00  7.46908426e-01
  3.23506546e+00  2.77883267e+00  1.57590222e+00  5.10829020e+00
  9.52500534e+00  6.17966223e+00  1.10281193e+00  5.18856943e-01
  2.46926522e+00  3.78143835e+00  6.96128178e+00  1.00144138e+01
  3.66667557e+00  5.97449923e+00  9.95713520e+00  7.44061518e+00
  6.02692795e+00  7.26248920e-01  3.43354082e+00  1.11553822e+01
  1.09044552e+00  8.32242012e+00  7.46725512e+00  3.50707126e+00
  4.66837549e+00  6.27135944e+00  7.01602077e+00  4.99376917e+00
  1.22947979e+00  6.21938133e+00  4.57749987e+00  9.84504700e+00
  3.30335212e+00  2.10937715e+00  8.59310150e-01  5.95748758e+00
  7.51813459e+00  1.18187451e+00  3.30510187e+00  6.36148548e+00
  4.50776958e+00  4.50498581e+00  1.05220985e+01  9.05682564e+00
  6.02789402e+00  7.13881588e+00  7.25975084e+00  6.13174152e+00
  2.50386310e+00  6.21227741e+00  6.38945484e+00  8.56712151e+00
  3.63186836e-01  9.32480907e+00  5.54315186e+00 -3.98739636e-01
  3.37429929e+00  7.95210361e+00  5.01406813e+00  9.63213921e+00
  3.99637389e+00  2.97415352e+00  8.29203606e+00  6.00829792e+00
  6.21376610e+00  7.23727989e+00  6.71069527e+00  5.01448536e+00
  9.13507938e-01  6.87569332e+00  9.86827469e+00  4.45279694e+00
  8.80540848e+00  5.50182295e+00  6.56988764e+00  8.22664070e+00
  6.11226082e+00  9.90539372e-01  8.70905399e+00  2.35227823e+00
  6.95939922e+00  4.88893890e+00  4.05508137e+00  1.27094746e+00
  7.41610336e+00  9.57014084e-01  1.03208637e+01  8.77536011e+00
  5.40437126e+00  3.66490221e+00  1.92103791e+00  5.05432844e+00
  6.88084650e+00  2.36994123e+00  4.34519386e+00  1.71688604e+00
  3.39162683e+00  1.85334861e+00  9.56473351e+00  6.55594063e+00
  7.24569368e+00  9.57989931e-01  3.30405354e+00  8.91251564e+00
  9.43958402e-01  2.91835928e+00  6.76908159e+00  6.41434669e+00
  8.02711487e+00  8.37068748e+00  1.76286387e+00  2.83487058e+00
  4.26155806e+00 -1.64624956e-02  5.80787063e-02  2.90215087e+00
  4.82958937e+00  2.67308688e+00  8.74459743e+00  3.35418940e+00
  9.50176620e+00  9.24923992e+00  5.82722712e+00  3.35397267e+00
  3.11244607e+00  7.65114069e+00  2.58552289e+00  3.75996971e+00
  5.17049551e+00  5.17774963e+00  9.58946896e+00  2.46264791e+00
  4.03688717e+00  2.72447777e+00  2.75106120e+00  1.24247396e+00
  2.18880701e+00  8.80450439e+00  2.16815996e+00  4.50869846e+00
  9.47779369e+00  6.92755127e+00  5.21426487e+00  7.63384438e+00
  1.33627713e+00  5.05314064e+00  8.01588058e-01  2.33411765e+00
  2.04597259e+00  3.71735334e+00  3.99399781e+00  9.83961201e+00
  2.91163659e+00  3.73780441e+00  7.86999273e+00  7.62870979e+00
  7.85087109e+00  2.87814641e+00  5.46488953e+00  1.44017196e+00
  2.16978312e+00  4.34651899e+00  7.40160513e+00  5.01842690e+00
  8.79978561e+00  8.01273918e+00  6.23900414e-01  5.34396315e+00
  5.84303570e+00  9.49287987e+00 -2.03253716e-01  5.55819988e+00
  2.56452465e+00  5.52358687e-01  5.80285740e+00  8.33415508e+00
  8.18197632e+00 -8.59678984e-02  3.97874522e+00  4.08348382e-01
  2.77977419e+00  2.01314735e+00  2.23359156e+00  3.15110850e+00
  3.16415262e+00  9.69314194e+00  3.46537042e+00  8.90620518e+00
  1.80000591e+00  6.60689974e+00 -7.20865309e-01  4.78756142e+00
 -1.09752685e-01  9.03241920e+00  8.91885853e+00  6.74625039e-01
  9.64687729e+00  5.28304863e+00  3.44711399e+00  9.08611012e+00
  3.02275133e+00  4.11477327e+00  1.01315606e+00  7.55140066e+00
  7.66734123e-01  4.76552486e+00  6.70099640e+00  7.79399300e+00
  1.63697755e+00  6.94904470e+00  6.56399965e+00  4.96204281e+00
  5.89378309e+00  5.83356380e-01  6.12774992e+00  4.05969334e+00
  8.60876942e+00  3.77930760e+00  5.77260780e+00  4.28877592e+00
  9.17067528e+00  2.55027103e+00  1.33500242e+00  8.23266697e+00
  4.27657557e+00  5.92882109e+00  3.65143597e-01  1.05127277e+01
  9.12497520e+00  4.73808956e+00  1.15415120e+00  4.24439526e+00
  9.80254364e+00  7.13764381e+00  2.21075535e+00  4.46259594e+00
  9.60248947e+00  6.37608194e+00  3.89600515e-01  6.30370569e+00
  2.47006059e+00  9.89494145e-01  4.10520649e+00  5.44608831e+00
  5.12012959e-01  4.04597473e+00  5.92461109e+00  8.09785938e+00
  5.32709646e+00  5.15376234e+00  7.13395691e+00  8.32365990e+00
  4.20349646e+00  6.05286407e+00  6.26060724e+00  6.60055876e-03
  8.09364128e+00  3.49219418e+00  7.10890245e+00  4.91867685e+00
  7.30661583e+00  6.67151976e+00  2.00603342e+00  6.53424406e+00
  8.17309189e+00  2.37493563e+00  7.19742537e+00  6.21698093e+00
  1.02654848e+01  4.02459478e+00  2.00096679e+00  3.60146332e+00
  2.72641587e+00  7.55267096e+00  7.33097744e+00  6.48247480e+00
  1.84161115e+00  8.67273045e+00  1.03891554e+01  5.11711931e+00
  6.14349842e+00  4.63130045e+00  1.04338384e+00  3.16097307e+00
  2.48957181e+00  2.08219433e+00  9.02428818e+00  7.58692598e+00
  3.53382182e+00  2.87967825e+00  9.43597221e+00  1.85656917e+00
  5.57112312e+00  9.00374889e-01  1.43457806e+00  5.70213127e+00
  6.18463993e+00  5.21287870e+00  8.30662155e+00  9.78884792e+00
  8.07082462e+00  4.94385624e+00  9.57595062e+00  4.81561422e-01
  5.93677044e+00  6.03051233e+00  1.27146268e+00  9.59977341e+00
  3.53188229e+00  5.92599010e+00  8.90297794e+00  5.73115826e+00
  8.29036653e-01  6.28442287e+00  4.10012245e+00  8.16020679e+00
  9.57242489e+00  8.73237801e+00  3.82743692e+00  7.33982182e+00
  9.39437389e+00  5.32039928e+00  4.49351358e+00  7.41679072e-01
  7.33104944e+00  8.68616486e+00  2.54391122e+00  2.88136458e+00
  5.05854321e+00  4.12581384e-01  7.20838428e-01  4.39249325e+00
  2.12579203e+00  5.69973421e+00  3.94032979e+00  1.01283722e+01
  9.79717064e+00  2.52009940e+00  8.86107635e+00  8.79512429e-01
  4.84236622e+00  1.72291732e+00  3.82897258e+00  6.94665861e+00
  4.18651247e+00  8.54575062e+00  7.19269276e+00  9.72444630e+00
  5.51988173e+00  8.01771069e+00  3.28205919e+00  3.97813463e+00
  4.18136644e+00  5.16412115e+00  1.75697100e+00  5.15417719e+00
  1.50469279e+00  9.25226021e+00  4.35976595e-01  4.19854736e+00
  3.64054823e+00  1.06782198e-01  8.30288601e+00  7.06305122e+00
  1.87979758e+00  6.35672998e+00  8.16109753e+00  6.51096630e+00
  5.61127961e-01  6.76316595e+00  6.57550716e+00  7.76742983e+00
  5.40051842e+00  4.99695921e+00  1.05247574e+01  7.60159779e+00
  5.60209799e+00  3.34395909e+00  9.19912910e+00  4.68194294e+00
  1.00466835e+00  5.51672554e+00  8.06357193e+00  4.60604143e+00
  7.99039245e-01  6.11217165e+00  9.98776340e+00  7.64210176e+00
  6.11796856e+00  9.89960003e+00  7.96584511e+00  6.26075697e+00
  4.63107634e+00  7.38753414e+00  6.49842548e+00  7.23403335e-01
  7.84123564e+00  3.43258095e+00  5.87750053e+00  8.16937256e+00
  4.84820747e+00  1.59829724e+00  4.18775225e+00  1.30482543e+00
  6.89824677e+00  9.70094967e+00  7.83443594e+00  4.79529858e+00
  6.26568747e+00  3.08977580e+00  5.72115278e+00  2.92952847e+00
  7.45006084e+00  2.38916183e+00  9.71957111e+00  6.54768562e+00
  6.25139415e-01 -2.21325979e-02  1.01162004e+01  1.64097524e+00
  3.30466270e+00  2.32712507e+00  9.82605267e+00  5.42485619e+00
  4.57557726e+00  5.65397930e+00  2.95226574e+00  6.00716829e+00
  4.69566488e+00  1.86493647e+00  5.55787706e+00  4.29599905e+00
  8.97355652e+00  4.49551582e+00  5.21416712e+00  7.87060261e+00
  4.67125940e+00  9.76075363e+00  8.19973755e+00  8.41541100e+00
  9.90537739e+00  2.93996453e+00  5.85271358e+00  7.30432892e+00
  7.22648525e+00  9.17044544e+00  2.83920431e+00  3.20150447e+00
  8.31029034e+00  7.78128242e+00  2.25734663e+00  5.91900253e+00
  7.87056684e+00  3.05571938e+00  3.93086529e+00  5.76315022e+00
  5.30861235e+00  7.55313301e+00 -1.47460684e-01  4.09288597e+00
  1.74171937e+00  4.78674221e+00  7.88887119e+00  7.54741371e-01
  1.54648781e+00  7.28928089e+00  7.57703447e+00  5.61207056e+00
  9.85756111e+00  4.10112953e+00  1.57129598e+00  6.85865021e+00
  6.33887672e+00  6.33726215e+00  5.35512924e+00  8.78589249e+00
  8.93958569e-01 -1.22548640e-01  1.54779208e+00  2.83668208e+00
  5.92615891e+00  5.92532015e+00  2.29801345e+00  4.49938583e+00
  4.36498499e+00  5.37602377e+00  8.03752041e+00  5.66677046e+00
  8.83142376e+00  1.65858328e+00  5.42761660e+00  8.26859760e+00
  1.79626775e+00  4.40248251e+00  5.96399355e+00  2.07387352e+00
  7.58685493e+00  6.19924068e-01  9.63695049e+00  4.80245113e+00
  2.84362292e+00  1.11408031e+00  4.65407324e+00  6.60547924e+00
  1.22691751e+00  3.05578947e+00  1.09281607e+01  6.40130091e+00
  2.10371280e+00  1.27200794e+00  8.89848232e+00  2.80182910e+00
  2.19280529e+00  1.05606747e+01  2.94065785e+00  1.69049454e+00
  5.48146868e+00  4.31994724e+00  9.06744099e+00  1.15592277e+00
 -7.79310703e-01  1.04373407e+00  8.86706924e+00  4.28514481e+00
  1.81781006e+00  5.78969526e+00  5.54994345e+00  2.30405951e+00
  5.54053879e+00  2.74468279e+00  2.27062321e+00  4.07878542e+00
  2.84625888e+00  2.04214787e+00  8.94879723e+00  6.88817441e-01
  2.71495771e+00  3.84625363e+00  2.55016398e+00  2.09636283e+00
  7.99209976e+00  3.07564521e+00  1.11745620e+00  5.30355263e+00
  9.93744850e-01  2.74349833e+00  8.41612911e+00  4.34795761e+00
  6.55439472e+00  6.15104818e+00  5.19303381e-01  6.36637783e+00
  8.49066544e+00  8.24014544e-01  6.90668011e+00  7.08179665e+00
  9.01312637e+00  3.36554408e+00  2.87336349e+00  6.86589861e+00
 -2.01100767e-01  7.31349230e+00  8.66450596e+00  4.29815626e+00
  3.38823771e+00  3.01338983e+00  8.08154404e-01  1.78297210e+00
  9.61242580e+00  8.81327534e+00  2.97019339e+00  8.63016319e+00
  3.02107668e+00  2.72670150e+00  2.13139439e+00  8.94742870e+00
  9.83511925e+00  3.19685555e+00  6.81030130e+00  3.50014639e+00
  1.05297625e+00  4.07330465e+00  5.53509521e+00  1.18256664e+00
  8.23826122e+00  5.53470898e+00  2.24809480e+00  5.40734196e+00
  7.84483671e+00  2.75392199e+00  7.62425852e+00  4.27786589e+00
  9.72611332e+00  8.83753872e+00  3.43726850e+00  1.27960294e-01
  7.29834032e+00  2.04023504e+00  1.30689442e+00  9.04135704e+00
  1.00866823e+01  2.58009624e+00  3.01804805e+00  1.68803823e+00
  4.80738878e+00  8.68211079e+00  6.73749733e+00  8.63989162e+00
  2.21114421e+00  7.04848576e+00  9.34753704e+00  2.76184368e+00
  6.90854836e+00  4.05415773e+00  3.81649780e+00  9.40583229e+00
  4.05083704e+00  8.92635822e-01  3.82358503e+00  4.43289518e+00
  7.26910877e+00  8.82122993e+00  8.22476578e+00  9.33980370e+00
  4.71105146e+00  6.95373154e+00  3.06833911e+00 -4.22824502e-01
  3.65492988e+00  6.86570597e+00  2.05108023e+00  8.74809170e+00
  8.31052685e+00  1.87380075e+00  2.81262565e+00  8.66123104e+00
  3.31760716e+00  3.31471944e+00  3.48811197e+00  7.28613329e+00
 -9.66772735e-02 -7.69796014e-01  6.32836771e+00  3.86802888e+00
  5.99920893e+00  2.40654874e+00  9.12174988e+00  8.34914875e+00
  7.07096219e-01  7.57762194e+00  9.58280849e+00  8.65084171e+00
  6.67687321e+00  1.21811235e+00  5.86707783e+00  3.30112028e+00
  1.01402960e+01  7.17766523e+00  5.74374008e+00  6.99432039e+00
  4.40368319e+00  2.86542940e+00  9.42987251e+00  3.30265784e+00
  3.83230895e-01  4.20491457e+00  8.71614361e+00  7.57816601e+00
  9.77553654e+00  6.46106720e-01  9.44217777e+00  4.46305180e+00
  4.83198547e+00  3.06845570e+00  1.06641746e+00  4.49923372e+00
  7.57343435e+00  5.86748123e-03  1.58664107e+00  5.31323195e-01
  8.55088329e+00  6.48872995e+00  1.54820132e+00  7.71537733e+00
  8.20388412e+00  9.80402565e+00  9.79537868e+00 -8.38677704e-01
  5.94942427e+00  2.88661289e+00  5.26643085e+00  3.18921828e+00
  7.79481411e-01  4.42821693e+00  5.33531380e+00  9.32881355e+00
  2.65765786e-02  3.08747840e+00  8.52437019e+00  6.85948515e+00
  8.21984768e+00  6.24451685e+00  7.30141687e+00  4.90936804e+00
  2.20824790e+00  8.48395348e+00  5.67844057e+00  1.24840343e+00
  8.05260468e+00  9.64081573e+00  3.07099938e+00  3.18240905e+00
  9.12044704e-01  5.96499348e+00  9.19813156e+00  3.36040854e-01
  4.73263264e+00  1.30067658e+00  5.19609153e-01  6.70743704e+00
  8.23411751e+00  3.52812862e+00  8.22581482e+00  1.19572031e+00
  2.63071656e+00  9.79853821e+00  4.98861170e+00  6.00117397e+00
  3.19664389e-01  1.32244027e+00  6.05848253e-01  7.35753870e+00
  1.13696325e+00  8.69066429e+00  3.58892393e+00  7.88068628e+00
  3.04448676e+00  2.65313506e+00  9.56735551e-01  2.75976801e+00
  4.98727465e+00  7.08365965e+00  5.05365181e+00  9.17652988e+00
  5.92350817e+00  1.26345801e+00  3.95679140e+00  7.46081781e+00
  2.64605379e+00  6.97033882e+00  5.93741179e+00  1.01776525e-01
  7.47330189e+00  5.93837404e+00  8.12254488e-01  3.60206747e+00
  3.23938537e+00  8.24193764e+00  3.56294060e+00  9.41426849e+00
  5.64243269e+00  2.11869121e+00  8.28666019e+00  6.44191980e+00]
Epoch 1/1000
2023-10-02 10:07:00.212 
Epoch 1/1000 
	 loss: 1590.0980, MinusLogProbMetric: 1590.0980, val_loss: 628.7557, val_MinusLogProbMetric: 628.7557

Epoch 1: val_loss improved from inf to 628.75568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 94s - loss: 1590.0980 - MinusLogProbMetric: 1590.0980 - val_loss: 628.7557 - val_MinusLogProbMetric: 628.7557 - lr: 0.0010 - 94s/epoch - 479ms/step
Epoch 2/1000
2023-10-02 10:07:17.236 
Epoch 2/1000 
	 loss: 557.1590, MinusLogProbMetric: 557.1590, val_loss: 532.5129, val_MinusLogProbMetric: 532.5129

Epoch 2: val_loss improved from 628.75568 to 532.51288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 16s - loss: 557.1590 - MinusLogProbMetric: 557.1590 - val_loss: 532.5129 - val_MinusLogProbMetric: 532.5129 - lr: 0.0010 - 16s/epoch - 81ms/step
Epoch 3/1000
2023-10-02 10:07:34.955 
Epoch 3/1000 
	 loss: 515.5613, MinusLogProbMetric: 515.5613, val_loss: 499.2704, val_MinusLogProbMetric: 499.2704

Epoch 3: val_loss improved from 532.51288 to 499.27036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 18s - loss: 515.5613 - MinusLogProbMetric: 515.5613 - val_loss: 499.2704 - val_MinusLogProbMetric: 499.2704 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 4/1000
2023-10-02 10:07:54.184 
Epoch 4/1000 
	 loss: 492.4252, MinusLogProbMetric: 492.4252, val_loss: 498.9535, val_MinusLogProbMetric: 498.9535

Epoch 4: val_loss improved from 499.27036 to 498.95352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 492.4252 - MinusLogProbMetric: 492.4252 - val_loss: 498.9535 - val_MinusLogProbMetric: 498.9535 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 5/1000
2023-10-02 10:08:10.007 
Epoch 5/1000 
	 loss: 1743.4878, MinusLogProbMetric: 1743.4878, val_loss: 768.8624, val_MinusLogProbMetric: 768.8624

Epoch 5: val_loss did not improve from 498.95352
196/196 - 15s - loss: 1743.4878 - MinusLogProbMetric: 1743.4878 - val_loss: 768.8624 - val_MinusLogProbMetric: 768.8624 - lr: 0.0010 - 15s/epoch - 77ms/step
Epoch 6/1000
2023-10-02 10:08:27.387 
Epoch 6/1000 
	 loss: 554.7794, MinusLogProbMetric: 554.7794, val_loss: 496.9649, val_MinusLogProbMetric: 496.9649

Epoch 6: val_loss improved from 498.95352 to 496.96490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 18s - loss: 554.7794 - MinusLogProbMetric: 554.7794 - val_loss: 496.9649 - val_MinusLogProbMetric: 496.9649 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 7/1000
2023-10-02 10:08:46.081 
Epoch 7/1000 
	 loss: 485.1065, MinusLogProbMetric: 485.1065, val_loss: 481.1782, val_MinusLogProbMetric: 481.1782

Epoch 7: val_loss improved from 496.96490 to 481.17816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 18s - loss: 485.1065 - MinusLogProbMetric: 485.1065 - val_loss: 481.1782 - val_MinusLogProbMetric: 481.1782 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 8/1000
2023-10-02 10:09:04.442 
Epoch 8/1000 
	 loss: 470.2102, MinusLogProbMetric: 470.2102, val_loss: 473.6053, val_MinusLogProbMetric: 473.6053

Epoch 8: val_loss improved from 481.17816 to 473.60526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 470.2102 - MinusLogProbMetric: 470.2102 - val_loss: 473.6053 - val_MinusLogProbMetric: 473.6053 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 9/1000
2023-10-02 10:09:21.640 
Epoch 9/1000 
	 loss: 461.8693, MinusLogProbMetric: 461.8693, val_loss: 458.9319, val_MinusLogProbMetric: 458.9319

Epoch 9: val_loss improved from 473.60526 to 458.93188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 17s - loss: 461.8693 - MinusLogProbMetric: 461.8693 - val_loss: 458.9319 - val_MinusLogProbMetric: 458.9319 - lr: 0.0010 - 17s/epoch - 86ms/step
Epoch 10/1000
2023-10-02 10:09:38.818 
Epoch 10/1000 
	 loss: 455.5573, MinusLogProbMetric: 455.5573, val_loss: 454.3623, val_MinusLogProbMetric: 454.3623

Epoch 10: val_loss improved from 458.93188 to 454.36227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 17s - loss: 455.5573 - MinusLogProbMetric: 455.5573 - val_loss: 454.3623 - val_MinusLogProbMetric: 454.3623 - lr: 0.0010 - 17s/epoch - 87ms/step
Epoch 11/1000
2023-10-02 10:09:55.998 
Epoch 11/1000 
	 loss: 451.6592, MinusLogProbMetric: 451.6592, val_loss: 449.0128, val_MinusLogProbMetric: 449.0128

Epoch 11: val_loss improved from 454.36227 to 449.01276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 18s - loss: 451.6592 - MinusLogProbMetric: 451.6592 - val_loss: 449.0128 - val_MinusLogProbMetric: 449.0128 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 12/1000
2023-10-02 10:10:11.912 
Epoch 12/1000 
	 loss: 446.8886, MinusLogProbMetric: 446.8886, val_loss: 447.1643, val_MinusLogProbMetric: 447.1643

Epoch 12: val_loss improved from 449.01276 to 447.16431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 15s - loss: 446.8886 - MinusLogProbMetric: 446.8886 - val_loss: 447.1643 - val_MinusLogProbMetric: 447.1643 - lr: 0.0010 - 15s/epoch - 77ms/step
Epoch 13/1000
2023-10-02 10:10:29.523 
Epoch 13/1000 
	 loss: 443.4205, MinusLogProbMetric: 443.4205, val_loss: 458.5480, val_MinusLogProbMetric: 458.5480

Epoch 13: val_loss did not improve from 447.16431
196/196 - 17s - loss: 443.4205 - MinusLogProbMetric: 443.4205 - val_loss: 458.5480 - val_MinusLogProbMetric: 458.5480 - lr: 0.0010 - 17s/epoch - 89ms/step
Epoch 14/1000
2023-10-02 10:10:44.972 
Epoch 14/1000 
	 loss: 442.4505, MinusLogProbMetric: 442.4505, val_loss: 450.7384, val_MinusLogProbMetric: 450.7384

Epoch 14: val_loss did not improve from 447.16431
196/196 - 15s - loss: 442.4505 - MinusLogProbMetric: 442.4505 - val_loss: 450.7384 - val_MinusLogProbMetric: 450.7384 - lr: 0.0010 - 15s/epoch - 79ms/step
Epoch 15/1000
2023-10-02 10:11:04.346 
Epoch 15/1000 
	 loss: 438.9535, MinusLogProbMetric: 438.9535, val_loss: 450.3412, val_MinusLogProbMetric: 450.3412

Epoch 15: val_loss did not improve from 447.16431
196/196 - 19s - loss: 438.9535 - MinusLogProbMetric: 438.9535 - val_loss: 450.3412 - val_MinusLogProbMetric: 450.3412 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 16/1000
2023-10-02 10:11:21.714 
Epoch 16/1000 
	 loss: 437.4919, MinusLogProbMetric: 437.4919, val_loss: 435.0089, val_MinusLogProbMetric: 435.0089

Epoch 16: val_loss improved from 447.16431 to 435.00888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 18s - loss: 437.4919 - MinusLogProbMetric: 437.4919 - val_loss: 435.0089 - val_MinusLogProbMetric: 435.0089 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 17/1000
2023-10-02 10:11:40.962 
Epoch 17/1000 
	 loss: 435.3841, MinusLogProbMetric: 435.3841, val_loss: 449.4154, val_MinusLogProbMetric: 449.4154

Epoch 17: val_loss did not improve from 435.00888
196/196 - 19s - loss: 435.3841 - MinusLogProbMetric: 435.3841 - val_loss: 449.4154 - val_MinusLogProbMetric: 449.4154 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 18/1000
2023-10-02 10:11:59.113 
Epoch 18/1000 
	 loss: 434.2753, MinusLogProbMetric: 434.2753, val_loss: 434.1644, val_MinusLogProbMetric: 434.1644

Epoch 18: val_loss improved from 435.00888 to 434.16440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 434.2753 - MinusLogProbMetric: 434.2753 - val_loss: 434.1644 - val_MinusLogProbMetric: 434.1644 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 19/1000
2023-10-02 10:12:17.028 
Epoch 19/1000 
	 loss: 432.3000, MinusLogProbMetric: 432.3000, val_loss: 438.3545, val_MinusLogProbMetric: 438.3545

Epoch 19: val_loss did not improve from 434.16440
196/196 - 17s - loss: 432.3000 - MinusLogProbMetric: 432.3000 - val_loss: 438.3545 - val_MinusLogProbMetric: 438.3545 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 20/1000
2023-10-02 10:12:36.276 
Epoch 20/1000 
	 loss: 431.0922, MinusLogProbMetric: 431.0922, val_loss: 427.9261, val_MinusLogProbMetric: 427.9261

Epoch 20: val_loss improved from 434.16440 to 427.92615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 431.0922 - MinusLogProbMetric: 431.0922 - val_loss: 427.9261 - val_MinusLogProbMetric: 427.9261 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 21/1000
2023-10-02 10:12:55.329 
Epoch 21/1000 
	 loss: 430.1325, MinusLogProbMetric: 430.1325, val_loss: 428.5366, val_MinusLogProbMetric: 428.5366

Epoch 21: val_loss did not improve from 427.92615
196/196 - 18s - loss: 430.1325 - MinusLogProbMetric: 430.1325 - val_loss: 428.5366 - val_MinusLogProbMetric: 428.5366 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 22/1000
2023-10-02 10:13:14.280 
Epoch 22/1000 
	 loss: 429.0372, MinusLogProbMetric: 429.0372, val_loss: 437.3152, val_MinusLogProbMetric: 437.3152

Epoch 22: val_loss did not improve from 427.92615
196/196 - 19s - loss: 429.0372 - MinusLogProbMetric: 429.0372 - val_loss: 437.3152 - val_MinusLogProbMetric: 437.3152 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 23/1000
2023-10-02 10:13:33.601 
Epoch 23/1000 
	 loss: 429.3576, MinusLogProbMetric: 429.3576, val_loss: 426.9287, val_MinusLogProbMetric: 426.9287

Epoch 23: val_loss improved from 427.92615 to 426.92874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 429.3576 - MinusLogProbMetric: 429.3576 - val_loss: 426.9287 - val_MinusLogProbMetric: 426.9287 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 24/1000
2023-10-02 10:13:53.645 
Epoch 24/1000 
	 loss: 427.1277, MinusLogProbMetric: 427.1277, val_loss: 425.7613, val_MinusLogProbMetric: 425.7613

Epoch 24: val_loss improved from 426.92874 to 425.76132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 427.1277 - MinusLogProbMetric: 427.1277 - val_loss: 425.7613 - val_MinusLogProbMetric: 425.7613 - lr: 0.0010 - 20s/epoch - 103ms/step
Epoch 25/1000
2023-10-02 10:14:12.015 
Epoch 25/1000 
	 loss: 426.9908, MinusLogProbMetric: 426.9908, val_loss: 430.7603, val_MinusLogProbMetric: 430.7603

Epoch 25: val_loss did not improve from 425.76132
196/196 - 17s - loss: 426.9908 - MinusLogProbMetric: 426.9908 - val_loss: 430.7603 - val_MinusLogProbMetric: 430.7603 - lr: 0.0010 - 17s/epoch - 89ms/step
Epoch 26/1000
2023-10-02 10:14:32.347 
Epoch 26/1000 
	 loss: 426.5666, MinusLogProbMetric: 426.5666, val_loss: 428.8052, val_MinusLogProbMetric: 428.8052

Epoch 26: val_loss did not improve from 425.76132
196/196 - 20s - loss: 426.5666 - MinusLogProbMetric: 426.5666 - val_loss: 428.8052 - val_MinusLogProbMetric: 428.8052 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 27/1000
2023-10-02 10:14:53.606 
Epoch 27/1000 
	 loss: 424.9377, MinusLogProbMetric: 424.9377, val_loss: 426.4133, val_MinusLogProbMetric: 426.4133

Epoch 27: val_loss did not improve from 425.76132
196/196 - 21s - loss: 424.9377 - MinusLogProbMetric: 424.9377 - val_loss: 426.4133 - val_MinusLogProbMetric: 426.4133 - lr: 0.0010 - 21s/epoch - 108ms/step
Epoch 28/1000
2023-10-02 10:15:13.933 
Epoch 28/1000 
	 loss: 424.2233, MinusLogProbMetric: 424.2233, val_loss: 424.7584, val_MinusLogProbMetric: 424.7584

Epoch 28: val_loss improved from 425.76132 to 424.75839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 21s - loss: 424.2233 - MinusLogProbMetric: 424.2233 - val_loss: 424.7584 - val_MinusLogProbMetric: 424.7584 - lr: 0.0010 - 21s/epoch - 109ms/step
Epoch 29/1000
2023-10-02 10:15:36.012 
Epoch 29/1000 
	 loss: 424.2333, MinusLogProbMetric: 424.2333, val_loss: 422.5077, val_MinusLogProbMetric: 422.5077

Epoch 29: val_loss improved from 424.75839 to 422.50772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 23s - loss: 424.2333 - MinusLogProbMetric: 424.2333 - val_loss: 422.5077 - val_MinusLogProbMetric: 422.5077 - lr: 0.0010 - 23s/epoch - 116ms/step
Epoch 30/1000
2023-10-02 10:15:57.003 
Epoch 30/1000 
	 loss: 423.4168, MinusLogProbMetric: 423.4168, val_loss: 422.8662, val_MinusLogProbMetric: 422.8662

Epoch 30: val_loss did not improve from 422.50772
196/196 - 19s - loss: 423.4168 - MinusLogProbMetric: 423.4168 - val_loss: 422.8662 - val_MinusLogProbMetric: 422.8662 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 31/1000
2023-10-02 10:16:17.490 
Epoch 31/1000 
	 loss: 422.0282, MinusLogProbMetric: 422.0282, val_loss: 427.7948, val_MinusLogProbMetric: 427.7948

Epoch 31: val_loss did not improve from 422.50772
196/196 - 20s - loss: 422.0282 - MinusLogProbMetric: 422.0282 - val_loss: 427.7948 - val_MinusLogProbMetric: 427.7948 - lr: 0.0010 - 20s/epoch - 105ms/step
Epoch 32/1000
2023-10-02 10:16:38.481 
Epoch 32/1000 
	 loss: 422.5808, MinusLogProbMetric: 422.5808, val_loss: 427.6744, val_MinusLogProbMetric: 427.6744

Epoch 32: val_loss did not improve from 422.50772
196/196 - 21s - loss: 422.5808 - MinusLogProbMetric: 422.5808 - val_loss: 427.6744 - val_MinusLogProbMetric: 427.6744 - lr: 0.0010 - 21s/epoch - 107ms/step
Epoch 33/1000
2023-10-02 10:16:58.304 
Epoch 33/1000 
	 loss: 421.6918, MinusLogProbMetric: 421.6918, val_loss: 419.9489, val_MinusLogProbMetric: 419.9489

Epoch 33: val_loss improved from 422.50772 to 419.94885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 21s - loss: 421.6918 - MinusLogProbMetric: 421.6918 - val_loss: 419.9489 - val_MinusLogProbMetric: 419.9489 - lr: 0.0010 - 21s/epoch - 105ms/step
Epoch 34/1000
2023-10-02 10:17:18.397 
Epoch 34/1000 
	 loss: 421.0680, MinusLogProbMetric: 421.0680, val_loss: 424.6969, val_MinusLogProbMetric: 424.6969

Epoch 34: val_loss did not improve from 419.94885
196/196 - 19s - loss: 421.0680 - MinusLogProbMetric: 421.0680 - val_loss: 424.6969 - val_MinusLogProbMetric: 424.6969 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 35/1000
2023-10-02 10:17:37.849 
Epoch 35/1000 
	 loss: 452.3869, MinusLogProbMetric: 452.3869, val_loss: 435.2258, val_MinusLogProbMetric: 435.2258

Epoch 35: val_loss did not improve from 419.94885
196/196 - 19s - loss: 452.3869 - MinusLogProbMetric: 452.3869 - val_loss: 435.2258 - val_MinusLogProbMetric: 435.2258 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 36/1000
2023-10-02 10:17:57.552 
Epoch 36/1000 
	 loss: 428.4272, MinusLogProbMetric: 428.4272, val_loss: 424.1833, val_MinusLogProbMetric: 424.1833

Epoch 36: val_loss did not improve from 419.94885
196/196 - 20s - loss: 428.4272 - MinusLogProbMetric: 428.4272 - val_loss: 424.1833 - val_MinusLogProbMetric: 424.1833 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 37/1000
2023-10-02 10:18:19.234 
Epoch 37/1000 
	 loss: 423.0156, MinusLogProbMetric: 423.0156, val_loss: 421.3222, val_MinusLogProbMetric: 421.3222

Epoch 37: val_loss did not improve from 419.94885
196/196 - 22s - loss: 423.0156 - MinusLogProbMetric: 423.0156 - val_loss: 421.3222 - val_MinusLogProbMetric: 421.3222 - lr: 0.0010 - 22s/epoch - 111ms/step
Epoch 38/1000
2023-10-02 10:18:37.320 
Epoch 38/1000 
	 loss: 421.4226, MinusLogProbMetric: 421.4226, val_loss: 423.2783, val_MinusLogProbMetric: 423.2783

Epoch 38: val_loss did not improve from 419.94885
196/196 - 18s - loss: 421.4226 - MinusLogProbMetric: 421.4226 - val_loss: 423.2783 - val_MinusLogProbMetric: 423.2783 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 39/1000
2023-10-02 10:18:54.462 
Epoch 39/1000 
	 loss: 420.6920, MinusLogProbMetric: 420.6920, val_loss: 422.0742, val_MinusLogProbMetric: 422.0742

Epoch 39: val_loss did not improve from 419.94885
196/196 - 17s - loss: 420.6920 - MinusLogProbMetric: 420.6920 - val_loss: 422.0742 - val_MinusLogProbMetric: 422.0742 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 40/1000
2023-10-02 10:19:13.038 
Epoch 40/1000 
	 loss: 419.0291, MinusLogProbMetric: 419.0291, val_loss: 418.8230, val_MinusLogProbMetric: 418.8230

Epoch 40: val_loss improved from 419.94885 to 418.82297, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 419.0291 - MinusLogProbMetric: 419.0291 - val_loss: 418.8230 - val_MinusLogProbMetric: 418.8230 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 41/1000
2023-10-02 10:19:32.835 
Epoch 41/1000 
	 loss: 418.6408, MinusLogProbMetric: 418.6408, val_loss: 420.3973, val_MinusLogProbMetric: 420.3973

Epoch 41: val_loss did not improve from 418.82297
196/196 - 19s - loss: 418.6408 - MinusLogProbMetric: 418.6408 - val_loss: 420.3973 - val_MinusLogProbMetric: 420.3973 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 42/1000
2023-10-02 10:19:51.379 
Epoch 42/1000 
	 loss: 418.7478, MinusLogProbMetric: 418.7478, val_loss: 419.9451, val_MinusLogProbMetric: 419.9451

Epoch 42: val_loss did not improve from 418.82297
196/196 - 19s - loss: 418.7478 - MinusLogProbMetric: 418.7478 - val_loss: 419.9451 - val_MinusLogProbMetric: 419.9451 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 43/1000
2023-10-02 10:20:10.556 
Epoch 43/1000 
	 loss: 417.9722, MinusLogProbMetric: 417.9722, val_loss: 418.3482, val_MinusLogProbMetric: 418.3482

Epoch 43: val_loss improved from 418.82297 to 418.34824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 417.9722 - MinusLogProbMetric: 417.9722 - val_loss: 418.3482 - val_MinusLogProbMetric: 418.3482 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 44/1000
2023-10-02 10:20:29.749 
Epoch 44/1000 
	 loss: 417.7825, MinusLogProbMetric: 417.7825, val_loss: 416.6042, val_MinusLogProbMetric: 416.6042

Epoch 44: val_loss improved from 418.34824 to 416.60419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 417.7825 - MinusLogProbMetric: 417.7825 - val_loss: 416.6042 - val_MinusLogProbMetric: 416.6042 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 45/1000
2023-10-02 10:20:49.500 
Epoch 45/1000 
	 loss: 417.1518, MinusLogProbMetric: 417.1518, val_loss: 416.2024, val_MinusLogProbMetric: 416.2024

Epoch 45: val_loss improved from 416.60419 to 416.20242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 417.1518 - MinusLogProbMetric: 417.1518 - val_loss: 416.2024 - val_MinusLogProbMetric: 416.2024 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 46/1000
2023-10-02 10:21:07.805 
Epoch 46/1000 
	 loss: 417.2559, MinusLogProbMetric: 417.2559, val_loss: 420.7426, val_MinusLogProbMetric: 420.7426

Epoch 46: val_loss did not improve from 416.20242
196/196 - 18s - loss: 417.2559 - MinusLogProbMetric: 417.2559 - val_loss: 420.7426 - val_MinusLogProbMetric: 420.7426 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 47/1000
2023-10-02 10:21:27.825 
Epoch 47/1000 
	 loss: 416.8521, MinusLogProbMetric: 416.8521, val_loss: 438.1693, val_MinusLogProbMetric: 438.1693

Epoch 47: val_loss did not improve from 416.20242
196/196 - 20s - loss: 416.8521 - MinusLogProbMetric: 416.8521 - val_loss: 438.1693 - val_MinusLogProbMetric: 438.1693 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 48/1000
2023-10-02 10:21:46.170 
Epoch 48/1000 
	 loss: 416.2900, MinusLogProbMetric: 416.2900, val_loss: 416.5727, val_MinusLogProbMetric: 416.5727

Epoch 48: val_loss did not improve from 416.20242
196/196 - 18s - loss: 416.2900 - MinusLogProbMetric: 416.2900 - val_loss: 416.5727 - val_MinusLogProbMetric: 416.5727 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 49/1000
2023-10-02 10:22:05.547 
Epoch 49/1000 
	 loss: 416.2459, MinusLogProbMetric: 416.2459, val_loss: 417.5833, val_MinusLogProbMetric: 417.5833

Epoch 49: val_loss did not improve from 416.20242
196/196 - 19s - loss: 416.2459 - MinusLogProbMetric: 416.2459 - val_loss: 417.5833 - val_MinusLogProbMetric: 417.5833 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 50/1000
2023-10-02 10:22:25.052 
Epoch 50/1000 
	 loss: 415.5846, MinusLogProbMetric: 415.5846, val_loss: 432.3086, val_MinusLogProbMetric: 432.3086

Epoch 50: val_loss did not improve from 416.20242
196/196 - 19s - loss: 415.5846 - MinusLogProbMetric: 415.5846 - val_loss: 432.3086 - val_MinusLogProbMetric: 432.3086 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 51/1000
2023-10-02 10:22:43.207 
Epoch 51/1000 
	 loss: 416.0052, MinusLogProbMetric: 416.0052, val_loss: 426.3535, val_MinusLogProbMetric: 426.3535

Epoch 51: val_loss did not improve from 416.20242
196/196 - 18s - loss: 416.0052 - MinusLogProbMetric: 416.0052 - val_loss: 426.3535 - val_MinusLogProbMetric: 426.3535 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 52/1000
2023-10-02 10:23:02.162 
Epoch 52/1000 
	 loss: 415.1097, MinusLogProbMetric: 415.1097, val_loss: 416.0421, val_MinusLogProbMetric: 416.0421

Epoch 52: val_loss improved from 416.20242 to 416.04205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 415.1097 - MinusLogProbMetric: 415.1097 - val_loss: 416.0421 - val_MinusLogProbMetric: 416.0421 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 53/1000
2023-10-02 10:23:21.614 
Epoch 53/1000 
	 loss: 415.0244, MinusLogProbMetric: 415.0244, val_loss: 421.8750, val_MinusLogProbMetric: 421.8750

Epoch 53: val_loss did not improve from 416.04205
196/196 - 18s - loss: 415.0244 - MinusLogProbMetric: 415.0244 - val_loss: 421.8750 - val_MinusLogProbMetric: 421.8750 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 54/1000
2023-10-02 10:23:40.135 
Epoch 54/1000 
	 loss: 414.7974, MinusLogProbMetric: 414.7974, val_loss: 424.1329, val_MinusLogProbMetric: 424.1329

Epoch 54: val_loss did not improve from 416.04205
196/196 - 18s - loss: 414.7974 - MinusLogProbMetric: 414.7974 - val_loss: 424.1329 - val_MinusLogProbMetric: 424.1329 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 55/1000
2023-10-02 10:23:58.517 
Epoch 55/1000 
	 loss: 414.9364, MinusLogProbMetric: 414.9364, val_loss: 413.1417, val_MinusLogProbMetric: 413.1417

Epoch 55: val_loss improved from 416.04205 to 413.14169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 414.9364 - MinusLogProbMetric: 414.9364 - val_loss: 413.1417 - val_MinusLogProbMetric: 413.1417 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 56/1000
2023-10-02 10:24:18.044 
Epoch 56/1000 
	 loss: 414.1051, MinusLogProbMetric: 414.1051, val_loss: 413.0898, val_MinusLogProbMetric: 413.0898

Epoch 56: val_loss improved from 413.14169 to 413.08981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 414.1051 - MinusLogProbMetric: 414.1051 - val_loss: 413.0898 - val_MinusLogProbMetric: 413.0898 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 57/1000
2023-10-02 10:24:36.584 
Epoch 57/1000 
	 loss: 414.7876, MinusLogProbMetric: 414.7876, val_loss: 413.5854, val_MinusLogProbMetric: 413.5854

Epoch 57: val_loss did not improve from 413.08981
196/196 - 18s - loss: 414.7876 - MinusLogProbMetric: 414.7876 - val_loss: 413.5854 - val_MinusLogProbMetric: 413.5854 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 58/1000
2023-10-02 10:24:54.736 
Epoch 58/1000 
	 loss: 414.5287, MinusLogProbMetric: 414.5287, val_loss: 414.5410, val_MinusLogProbMetric: 414.5410

Epoch 58: val_loss did not improve from 413.08981
196/196 - 18s - loss: 414.5287 - MinusLogProbMetric: 414.5287 - val_loss: 414.5410 - val_MinusLogProbMetric: 414.5410 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 59/1000
2023-10-02 10:25:15.690 
Epoch 59/1000 
	 loss: 414.6721, MinusLogProbMetric: 414.6721, val_loss: 414.3779, val_MinusLogProbMetric: 414.3779

Epoch 59: val_loss did not improve from 413.08981
196/196 - 21s - loss: 414.6721 - MinusLogProbMetric: 414.6721 - val_loss: 414.3779 - val_MinusLogProbMetric: 414.3779 - lr: 0.0010 - 21s/epoch - 107ms/step
Epoch 60/1000
2023-10-02 10:25:34.013 
Epoch 60/1000 
	 loss: 413.6598, MinusLogProbMetric: 413.6598, val_loss: 417.7544, val_MinusLogProbMetric: 417.7544

Epoch 60: val_loss did not improve from 413.08981
196/196 - 18s - loss: 413.6598 - MinusLogProbMetric: 413.6598 - val_loss: 417.7544 - val_MinusLogProbMetric: 417.7544 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 61/1000
2023-10-02 10:25:56.251 
Epoch 61/1000 
	 loss: 412.9731, MinusLogProbMetric: 412.9731, val_loss: 413.1710, val_MinusLogProbMetric: 413.1710

Epoch 61: val_loss did not improve from 413.08981
196/196 - 22s - loss: 412.9731 - MinusLogProbMetric: 412.9731 - val_loss: 413.1710 - val_MinusLogProbMetric: 413.1710 - lr: 0.0010 - 22s/epoch - 114ms/step
Epoch 62/1000
2023-10-02 10:26:16.122 
Epoch 62/1000 
	 loss: 414.2839, MinusLogProbMetric: 414.2839, val_loss: 413.3393, val_MinusLogProbMetric: 413.3393

Epoch 62: val_loss did not improve from 413.08981
196/196 - 20s - loss: 414.2839 - MinusLogProbMetric: 414.2839 - val_loss: 413.3393 - val_MinusLogProbMetric: 413.3393 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 63/1000
2023-10-02 10:26:34.844 
Epoch 63/1000 
	 loss: 413.1310, MinusLogProbMetric: 413.1310, val_loss: 412.4540, val_MinusLogProbMetric: 412.4540

Epoch 63: val_loss improved from 413.08981 to 412.45401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 413.1310 - MinusLogProbMetric: 413.1310 - val_loss: 412.4540 - val_MinusLogProbMetric: 412.4540 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 64/1000
2023-10-02 10:26:54.203 
Epoch 64/1000 
	 loss: 412.7925, MinusLogProbMetric: 412.7925, val_loss: 414.6648, val_MinusLogProbMetric: 414.6648

Epoch 64: val_loss did not improve from 412.45401
196/196 - 19s - loss: 412.7925 - MinusLogProbMetric: 412.7925 - val_loss: 414.6648 - val_MinusLogProbMetric: 414.6648 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 65/1000
2023-10-02 10:27:14.202 
Epoch 65/1000 
	 loss: 412.6253, MinusLogProbMetric: 412.6253, val_loss: 413.6460, val_MinusLogProbMetric: 413.6460

Epoch 65: val_loss did not improve from 412.45401
196/196 - 20s - loss: 412.6253 - MinusLogProbMetric: 412.6253 - val_loss: 413.6460 - val_MinusLogProbMetric: 413.6460 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 66/1000
2023-10-02 10:27:32.448 
Epoch 66/1000 
	 loss: 412.0024, MinusLogProbMetric: 412.0024, val_loss: 414.1956, val_MinusLogProbMetric: 414.1956

Epoch 66: val_loss did not improve from 412.45401
196/196 - 18s - loss: 412.0024 - MinusLogProbMetric: 412.0024 - val_loss: 414.1956 - val_MinusLogProbMetric: 414.1956 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 67/1000
2023-10-02 10:27:48.735 
Epoch 67/1000 
	 loss: 412.5355, MinusLogProbMetric: 412.5355, val_loss: 411.4800, val_MinusLogProbMetric: 411.4800

Epoch 67: val_loss improved from 412.45401 to 411.48001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 17s - loss: 412.5355 - MinusLogProbMetric: 412.5355 - val_loss: 411.4800 - val_MinusLogProbMetric: 411.4800 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 68/1000
2023-10-02 10:28:06.592 
Epoch 68/1000 
	 loss: 412.2125, MinusLogProbMetric: 412.2125, val_loss: 410.4604, val_MinusLogProbMetric: 410.4604

Epoch 68: val_loss improved from 411.48001 to 410.46036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 18s - loss: 412.2125 - MinusLogProbMetric: 412.2125 - val_loss: 410.4604 - val_MinusLogProbMetric: 410.4604 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 69/1000
2023-10-02 10:28:29.058 
Epoch 69/1000 
	 loss: 411.8914, MinusLogProbMetric: 411.8914, val_loss: 410.8684, val_MinusLogProbMetric: 410.8684

Epoch 69: val_loss did not improve from 410.46036
196/196 - 21s - loss: 411.8914 - MinusLogProbMetric: 411.8914 - val_loss: 410.8684 - val_MinusLogProbMetric: 410.8684 - lr: 0.0010 - 21s/epoch - 109ms/step
Epoch 70/1000
2023-10-02 10:28:48.361 
Epoch 70/1000 
	 loss: 412.1547, MinusLogProbMetric: 412.1547, val_loss: 412.1381, val_MinusLogProbMetric: 412.1381

Epoch 70: val_loss did not improve from 410.46036
196/196 - 19s - loss: 412.1547 - MinusLogProbMetric: 412.1547 - val_loss: 412.1381 - val_MinusLogProbMetric: 412.1381 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 71/1000
2023-10-02 10:29:08.148 
Epoch 71/1000 
	 loss: 411.6884, MinusLogProbMetric: 411.6884, val_loss: 419.6773, val_MinusLogProbMetric: 419.6773

Epoch 71: val_loss did not improve from 410.46036
196/196 - 20s - loss: 411.6884 - MinusLogProbMetric: 411.6884 - val_loss: 419.6773 - val_MinusLogProbMetric: 419.6773 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 72/1000
2023-10-02 10:29:25.688 
Epoch 72/1000 
	 loss: 411.6812, MinusLogProbMetric: 411.6812, val_loss: 412.6495, val_MinusLogProbMetric: 412.6495

Epoch 72: val_loss did not improve from 410.46036
196/196 - 18s - loss: 411.6812 - MinusLogProbMetric: 411.6812 - val_loss: 412.6495 - val_MinusLogProbMetric: 412.6495 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 73/1000
2023-10-02 10:29:43.811 
Epoch 73/1000 
	 loss: 410.6792, MinusLogProbMetric: 410.6792, val_loss: 410.8661, val_MinusLogProbMetric: 410.8661

Epoch 73: val_loss did not improve from 410.46036
196/196 - 18s - loss: 410.6792 - MinusLogProbMetric: 410.6792 - val_loss: 410.8661 - val_MinusLogProbMetric: 410.8661 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 74/1000
2023-10-02 10:30:03.350 
Epoch 74/1000 
	 loss: 411.5642, MinusLogProbMetric: 411.5642, val_loss: 410.8197, val_MinusLogProbMetric: 410.8197

Epoch 74: val_loss did not improve from 410.46036
196/196 - 20s - loss: 411.5642 - MinusLogProbMetric: 411.5642 - val_loss: 410.8197 - val_MinusLogProbMetric: 410.8197 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 75/1000
2023-10-02 10:30:22.192 
Epoch 75/1000 
	 loss: 410.5964, MinusLogProbMetric: 410.5964, val_loss: 414.1552, val_MinusLogProbMetric: 414.1552

Epoch 75: val_loss did not improve from 410.46036
196/196 - 19s - loss: 410.5964 - MinusLogProbMetric: 410.5964 - val_loss: 414.1552 - val_MinusLogProbMetric: 414.1552 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 76/1000
2023-10-02 10:30:40.967 
Epoch 76/1000 
	 loss: 411.6089, MinusLogProbMetric: 411.6089, val_loss: 410.3269, val_MinusLogProbMetric: 410.3269

Epoch 76: val_loss improved from 410.46036 to 410.32693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 411.6089 - MinusLogProbMetric: 411.6089 - val_loss: 410.3269 - val_MinusLogProbMetric: 410.3269 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 77/1000
2023-10-02 10:31:00.134 
Epoch 77/1000 
	 loss: 410.6031, MinusLogProbMetric: 410.6031, val_loss: 411.3093, val_MinusLogProbMetric: 411.3093

Epoch 77: val_loss did not improve from 410.32693
196/196 - 18s - loss: 410.6031 - MinusLogProbMetric: 410.6031 - val_loss: 411.3093 - val_MinusLogProbMetric: 411.3093 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 78/1000
2023-10-02 10:31:19.672 
Epoch 78/1000 
	 loss: 410.4676, MinusLogProbMetric: 410.4676, val_loss: 413.1987, val_MinusLogProbMetric: 413.1987

Epoch 78: val_loss did not improve from 410.32693
196/196 - 20s - loss: 410.4676 - MinusLogProbMetric: 410.4676 - val_loss: 413.1987 - val_MinusLogProbMetric: 413.1987 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 79/1000
2023-10-02 10:31:39.350 
Epoch 79/1000 
	 loss: 410.6459, MinusLogProbMetric: 410.6459, val_loss: 409.8323, val_MinusLogProbMetric: 409.8323

Epoch 79: val_loss improved from 410.32693 to 409.83228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 410.6459 - MinusLogProbMetric: 410.6459 - val_loss: 409.8323 - val_MinusLogProbMetric: 409.8323 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 80/1000
2023-10-02 10:31:57.425 
Epoch 80/1000 
	 loss: 411.1024, MinusLogProbMetric: 411.1024, val_loss: 440.3815, val_MinusLogProbMetric: 440.3815

Epoch 80: val_loss did not improve from 409.83228
196/196 - 17s - loss: 411.1024 - MinusLogProbMetric: 411.1024 - val_loss: 440.3815 - val_MinusLogProbMetric: 440.3815 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 81/1000
2023-10-02 10:32:16.770 
Epoch 81/1000 
	 loss: 410.6648, MinusLogProbMetric: 410.6648, val_loss: 408.5386, val_MinusLogProbMetric: 408.5386

Epoch 81: val_loss improved from 409.83228 to 408.53864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 410.6648 - MinusLogProbMetric: 410.6648 - val_loss: 408.5386 - val_MinusLogProbMetric: 408.5386 - lr: 0.0010 - 20s/epoch - 103ms/step
Epoch 82/1000
2023-10-02 10:32:37.054 
Epoch 82/1000 
	 loss: 410.2804, MinusLogProbMetric: 410.2804, val_loss: 419.9379, val_MinusLogProbMetric: 419.9379

Epoch 82: val_loss did not improve from 408.53864
196/196 - 19s - loss: 410.2804 - MinusLogProbMetric: 410.2804 - val_loss: 419.9379 - val_MinusLogProbMetric: 419.9379 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 83/1000
2023-10-02 10:32:56.325 
Epoch 83/1000 
	 loss: 410.7963, MinusLogProbMetric: 410.7963, val_loss: 410.6832, val_MinusLogProbMetric: 410.6832

Epoch 83: val_loss did not improve from 408.53864
196/196 - 19s - loss: 410.7963 - MinusLogProbMetric: 410.7963 - val_loss: 410.6832 - val_MinusLogProbMetric: 410.6832 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 84/1000
2023-10-02 10:33:14.534 
Epoch 84/1000 
	 loss: 409.2448, MinusLogProbMetric: 409.2448, val_loss: 410.8900, val_MinusLogProbMetric: 410.8900

Epoch 84: val_loss did not improve from 408.53864
196/196 - 18s - loss: 409.2448 - MinusLogProbMetric: 409.2448 - val_loss: 410.8900 - val_MinusLogProbMetric: 410.8900 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 85/1000
2023-10-02 10:33:32.622 
Epoch 85/1000 
	 loss: 409.6136, MinusLogProbMetric: 409.6136, val_loss: 411.6131, val_MinusLogProbMetric: 411.6131

Epoch 85: val_loss did not improve from 408.53864
196/196 - 18s - loss: 409.6136 - MinusLogProbMetric: 409.6136 - val_loss: 411.6131 - val_MinusLogProbMetric: 411.6131 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 86/1000
2023-10-02 10:33:50.987 
Epoch 86/1000 
	 loss: 409.7522, MinusLogProbMetric: 409.7522, val_loss: 412.4052, val_MinusLogProbMetric: 412.4052

Epoch 86: val_loss did not improve from 408.53864
196/196 - 18s - loss: 409.7522 - MinusLogProbMetric: 409.7522 - val_loss: 412.4052 - val_MinusLogProbMetric: 412.4052 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 87/1000
2023-10-02 10:34:10.531 
Epoch 87/1000 
	 loss: 409.7053, MinusLogProbMetric: 409.7053, val_loss: 415.0877, val_MinusLogProbMetric: 415.0877

Epoch 87: val_loss did not improve from 408.53864
196/196 - 20s - loss: 409.7053 - MinusLogProbMetric: 409.7053 - val_loss: 415.0877 - val_MinusLogProbMetric: 415.0877 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 88/1000
2023-10-02 10:34:31.092 
Epoch 88/1000 
	 loss: 409.4803, MinusLogProbMetric: 409.4803, val_loss: 419.0890, val_MinusLogProbMetric: 419.0890

Epoch 88: val_loss did not improve from 408.53864
196/196 - 21s - loss: 409.4803 - MinusLogProbMetric: 409.4803 - val_loss: 419.0890 - val_MinusLogProbMetric: 419.0890 - lr: 0.0010 - 21s/epoch - 105ms/step
Epoch 89/1000
2023-10-02 10:34:51.234 
Epoch 89/1000 
	 loss: 410.0998, MinusLogProbMetric: 410.0998, val_loss: 409.0779, val_MinusLogProbMetric: 409.0779

Epoch 89: val_loss did not improve from 408.53864
196/196 - 20s - loss: 410.0998 - MinusLogProbMetric: 410.0998 - val_loss: 409.0779 - val_MinusLogProbMetric: 409.0779 - lr: 0.0010 - 20s/epoch - 103ms/step
Epoch 90/1000
2023-10-02 10:35:10.242 
Epoch 90/1000 
	 loss: 409.0108, MinusLogProbMetric: 409.0108, val_loss: 407.7473, val_MinusLogProbMetric: 407.7473

Epoch 90: val_loss improved from 408.53864 to 407.74725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 409.0108 - MinusLogProbMetric: 409.0108 - val_loss: 407.7473 - val_MinusLogProbMetric: 407.7473 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 91/1000
2023-10-02 10:35:31.887 
Epoch 91/1000 
	 loss: 408.8655, MinusLogProbMetric: 408.8655, val_loss: 409.1352, val_MinusLogProbMetric: 409.1352

Epoch 91: val_loss did not improve from 407.74725
196/196 - 21s - loss: 408.8655 - MinusLogProbMetric: 408.8655 - val_loss: 409.1352 - val_MinusLogProbMetric: 409.1352 - lr: 0.0010 - 21s/epoch - 106ms/step
Epoch 92/1000
2023-10-02 10:35:51.215 
Epoch 92/1000 
	 loss: 889.3127, MinusLogProbMetric: 889.3127, val_loss: 556.8448, val_MinusLogProbMetric: 556.8448

Epoch 92: val_loss did not improve from 407.74725
196/196 - 19s - loss: 889.3127 - MinusLogProbMetric: 889.3127 - val_loss: 556.8448 - val_MinusLogProbMetric: 556.8448 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 93/1000
2023-10-02 10:36:10.333 
Epoch 93/1000 
	 loss: 493.8514, MinusLogProbMetric: 493.8514, val_loss: 469.6231, val_MinusLogProbMetric: 469.6231

Epoch 93: val_loss did not improve from 407.74725
196/196 - 19s - loss: 493.8514 - MinusLogProbMetric: 493.8514 - val_loss: 469.6231 - val_MinusLogProbMetric: 469.6231 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 94/1000
2023-10-02 10:36:32.207 
Epoch 94/1000 
	 loss: 452.0108, MinusLogProbMetric: 452.0108, val_loss: 445.3598, val_MinusLogProbMetric: 445.3598

Epoch 94: val_loss did not improve from 407.74725
196/196 - 22s - loss: 452.0108 - MinusLogProbMetric: 452.0108 - val_loss: 445.3598 - val_MinusLogProbMetric: 445.3598 - lr: 0.0010 - 22s/epoch - 111ms/step
Epoch 95/1000
2023-10-02 10:36:51.569 
Epoch 95/1000 
	 loss: 438.9800, MinusLogProbMetric: 438.9800, val_loss: 438.8529, val_MinusLogProbMetric: 438.8529

Epoch 95: val_loss did not improve from 407.74725
196/196 - 19s - loss: 438.9800 - MinusLogProbMetric: 438.9800 - val_loss: 438.8529 - val_MinusLogProbMetric: 438.8529 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 96/1000
2023-10-02 10:37:11.902 
Epoch 96/1000 
	 loss: 431.9119, MinusLogProbMetric: 431.9119, val_loss: 429.8272, val_MinusLogProbMetric: 429.8272

Epoch 96: val_loss did not improve from 407.74725
196/196 - 20s - loss: 431.9119 - MinusLogProbMetric: 431.9119 - val_loss: 429.8272 - val_MinusLogProbMetric: 429.8272 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 97/1000
2023-10-02 10:37:31.441 
Epoch 97/1000 
	 loss: 427.8335, MinusLogProbMetric: 427.8335, val_loss: 427.4911, val_MinusLogProbMetric: 427.4911

Epoch 97: val_loss did not improve from 407.74725
196/196 - 20s - loss: 427.8335 - MinusLogProbMetric: 427.8335 - val_loss: 427.4911 - val_MinusLogProbMetric: 427.4911 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 98/1000
2023-10-02 10:37:49.655 
Epoch 98/1000 
	 loss: 425.4981, MinusLogProbMetric: 425.4981, val_loss: 423.2324, val_MinusLogProbMetric: 423.2324

Epoch 98: val_loss did not improve from 407.74725
196/196 - 18s - loss: 425.4981 - MinusLogProbMetric: 425.4981 - val_loss: 423.2324 - val_MinusLogProbMetric: 423.2324 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 99/1000
2023-10-02 10:38:08.005 
Epoch 99/1000 
	 loss: 422.4583, MinusLogProbMetric: 422.4583, val_loss: 419.7358, val_MinusLogProbMetric: 419.7358

Epoch 99: val_loss did not improve from 407.74725
196/196 - 18s - loss: 422.4583 - MinusLogProbMetric: 422.4583 - val_loss: 419.7358 - val_MinusLogProbMetric: 419.7358 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 100/1000
2023-10-02 10:38:27.817 
Epoch 100/1000 
	 loss: 419.4122, MinusLogProbMetric: 419.4122, val_loss: 419.3927, val_MinusLogProbMetric: 419.3927

Epoch 100: val_loss did not improve from 407.74725
196/196 - 20s - loss: 419.4122 - MinusLogProbMetric: 419.4122 - val_loss: 419.3927 - val_MinusLogProbMetric: 419.3927 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 101/1000
2023-10-02 10:38:48.916 
Epoch 101/1000 
	 loss: 418.5113, MinusLogProbMetric: 418.5113, val_loss: 418.7785, val_MinusLogProbMetric: 418.7785

Epoch 101: val_loss did not improve from 407.74725
196/196 - 21s - loss: 418.5113 - MinusLogProbMetric: 418.5113 - val_loss: 418.7785 - val_MinusLogProbMetric: 418.7785 - lr: 0.0010 - 21s/epoch - 107ms/step
Epoch 102/1000
2023-10-02 10:39:09.201 
Epoch 102/1000 
	 loss: 417.7443, MinusLogProbMetric: 417.7443, val_loss: 428.0350, val_MinusLogProbMetric: 428.0350

Epoch 102: val_loss did not improve from 407.74725
196/196 - 20s - loss: 417.7443 - MinusLogProbMetric: 417.7443 - val_loss: 428.0350 - val_MinusLogProbMetric: 428.0350 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 103/1000
2023-10-02 10:39:28.753 
Epoch 103/1000 
	 loss: 416.3168, MinusLogProbMetric: 416.3168, val_loss: 417.5224, val_MinusLogProbMetric: 417.5224

Epoch 103: val_loss did not improve from 407.74725
196/196 - 20s - loss: 416.3168 - MinusLogProbMetric: 416.3168 - val_loss: 417.5224 - val_MinusLogProbMetric: 417.5224 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 104/1000
2023-10-02 10:39:46.772 
Epoch 104/1000 
	 loss: 414.9213, MinusLogProbMetric: 414.9213, val_loss: 414.3769, val_MinusLogProbMetric: 414.3769

Epoch 104: val_loss did not improve from 407.74725
196/196 - 18s - loss: 414.9213 - MinusLogProbMetric: 414.9213 - val_loss: 414.3769 - val_MinusLogProbMetric: 414.3769 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 105/1000
2023-10-02 10:40:04.732 
Epoch 105/1000 
	 loss: 414.8461, MinusLogProbMetric: 414.8461, val_loss: 414.0477, val_MinusLogProbMetric: 414.0477

Epoch 105: val_loss did not improve from 407.74725
196/196 - 18s - loss: 414.8461 - MinusLogProbMetric: 414.8461 - val_loss: 414.0477 - val_MinusLogProbMetric: 414.0477 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 106/1000
2023-10-02 10:40:24.481 
Epoch 106/1000 
	 loss: 413.4034, MinusLogProbMetric: 413.4034, val_loss: 414.7942, val_MinusLogProbMetric: 414.7942

Epoch 106: val_loss did not improve from 407.74725
196/196 - 20s - loss: 413.4034 - MinusLogProbMetric: 413.4034 - val_loss: 414.7942 - val_MinusLogProbMetric: 414.7942 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 107/1000
2023-10-02 10:40:43.458 
Epoch 107/1000 
	 loss: 412.8925, MinusLogProbMetric: 412.8925, val_loss: 412.8317, val_MinusLogProbMetric: 412.8317

Epoch 107: val_loss did not improve from 407.74725
196/196 - 19s - loss: 412.8925 - MinusLogProbMetric: 412.8925 - val_loss: 412.8317 - val_MinusLogProbMetric: 412.8317 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 108/1000
2023-10-02 10:41:00.601 
Epoch 108/1000 
	 loss: 413.0573, MinusLogProbMetric: 413.0573, val_loss: 427.2660, val_MinusLogProbMetric: 427.2660

Epoch 108: val_loss did not improve from 407.74725
196/196 - 17s - loss: 413.0573 - MinusLogProbMetric: 413.0573 - val_loss: 427.2660 - val_MinusLogProbMetric: 427.2660 - lr: 0.0010 - 17s/epoch - 87ms/step
Epoch 109/1000
2023-10-02 10:41:18.246 
Epoch 109/1000 
	 loss: 411.9932, MinusLogProbMetric: 411.9932, val_loss: 430.4343, val_MinusLogProbMetric: 430.4343

Epoch 109: val_loss did not improve from 407.74725
196/196 - 18s - loss: 411.9932 - MinusLogProbMetric: 411.9932 - val_loss: 430.4343 - val_MinusLogProbMetric: 430.4343 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 110/1000
2023-10-02 10:41:37.719 
Epoch 110/1000 
	 loss: 412.4749, MinusLogProbMetric: 412.4749, val_loss: 412.4225, val_MinusLogProbMetric: 412.4225

Epoch 110: val_loss did not improve from 407.74725
196/196 - 19s - loss: 412.4749 - MinusLogProbMetric: 412.4749 - val_loss: 412.4225 - val_MinusLogProbMetric: 412.4225 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 111/1000
2023-10-02 10:41:56.491 
Epoch 111/1000 
	 loss: 410.9315, MinusLogProbMetric: 410.9315, val_loss: 413.0383, val_MinusLogProbMetric: 413.0383

Epoch 111: val_loss did not improve from 407.74725
196/196 - 19s - loss: 410.9315 - MinusLogProbMetric: 410.9315 - val_loss: 413.0383 - val_MinusLogProbMetric: 413.0383 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 112/1000
2023-10-02 10:42:17.906 
Epoch 112/1000 
	 loss: 410.6673, MinusLogProbMetric: 410.6673, val_loss: 410.8757, val_MinusLogProbMetric: 410.8757

Epoch 112: val_loss did not improve from 407.74725
196/196 - 21s - loss: 410.6673 - MinusLogProbMetric: 410.6673 - val_loss: 410.8757 - val_MinusLogProbMetric: 410.8757 - lr: 0.0010 - 21s/epoch - 109ms/step
Epoch 113/1000
2023-10-02 10:42:37.320 
Epoch 113/1000 
	 loss: 410.4189, MinusLogProbMetric: 410.4189, val_loss: 410.6007, val_MinusLogProbMetric: 410.6007

Epoch 113: val_loss did not improve from 407.74725
196/196 - 19s - loss: 410.4189 - MinusLogProbMetric: 410.4189 - val_loss: 410.6007 - val_MinusLogProbMetric: 410.6007 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 114/1000
2023-10-02 10:42:55.553 
Epoch 114/1000 
	 loss: 410.2133, MinusLogProbMetric: 410.2133, val_loss: 411.3987, val_MinusLogProbMetric: 411.3987

Epoch 114: val_loss did not improve from 407.74725
196/196 - 18s - loss: 410.2133 - MinusLogProbMetric: 410.2133 - val_loss: 411.3987 - val_MinusLogProbMetric: 411.3987 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 115/1000
2023-10-02 10:43:13.186 
Epoch 115/1000 
	 loss: 409.9733, MinusLogProbMetric: 409.9733, val_loss: 409.4629, val_MinusLogProbMetric: 409.4629

Epoch 115: val_loss did not improve from 407.74725
196/196 - 18s - loss: 409.9733 - MinusLogProbMetric: 409.9733 - val_loss: 409.4629 - val_MinusLogProbMetric: 409.4629 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 116/1000
2023-10-02 10:43:30.992 
Epoch 116/1000 
	 loss: 409.3560, MinusLogProbMetric: 409.3560, val_loss: 409.4800, val_MinusLogProbMetric: 409.4800

Epoch 116: val_loss did not improve from 407.74725
196/196 - 18s - loss: 409.3560 - MinusLogProbMetric: 409.3560 - val_loss: 409.4800 - val_MinusLogProbMetric: 409.4800 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 117/1000
2023-10-02 10:43:51.443 
Epoch 117/1000 
	 loss: 409.4814, MinusLogProbMetric: 409.4814, val_loss: 409.6921, val_MinusLogProbMetric: 409.6921

Epoch 117: val_loss did not improve from 407.74725
196/196 - 20s - loss: 409.4814 - MinusLogProbMetric: 409.4814 - val_loss: 409.6921 - val_MinusLogProbMetric: 409.6921 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 118/1000
2023-10-02 10:44:09.420 
Epoch 118/1000 
	 loss: 410.2167, MinusLogProbMetric: 410.2167, val_loss: 415.5383, val_MinusLogProbMetric: 415.5383

Epoch 118: val_loss did not improve from 407.74725
196/196 - 18s - loss: 410.2167 - MinusLogProbMetric: 410.2167 - val_loss: 415.5383 - val_MinusLogProbMetric: 415.5383 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 119/1000
2023-10-02 10:44:27.653 
Epoch 119/1000 
	 loss: 408.7040, MinusLogProbMetric: 408.7040, val_loss: 409.4294, val_MinusLogProbMetric: 409.4294

Epoch 119: val_loss did not improve from 407.74725
196/196 - 18s - loss: 408.7040 - MinusLogProbMetric: 408.7040 - val_loss: 409.4294 - val_MinusLogProbMetric: 409.4294 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 120/1000
2023-10-02 10:44:47.591 
Epoch 120/1000 
	 loss: 408.6487, MinusLogProbMetric: 408.6487, val_loss: 410.3893, val_MinusLogProbMetric: 410.3893

Epoch 120: val_loss did not improve from 407.74725
196/196 - 20s - loss: 408.6487 - MinusLogProbMetric: 408.6487 - val_loss: 410.3893 - val_MinusLogProbMetric: 410.3893 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 121/1000
2023-10-02 10:45:07.440 
Epoch 121/1000 
	 loss: 408.3056, MinusLogProbMetric: 408.3056, val_loss: 410.8801, val_MinusLogProbMetric: 410.8801

Epoch 121: val_loss did not improve from 407.74725
196/196 - 20s - loss: 408.3056 - MinusLogProbMetric: 408.3056 - val_loss: 410.8801 - val_MinusLogProbMetric: 410.8801 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 122/1000
2023-10-02 10:45:27.299 
Epoch 122/1000 
	 loss: 408.5892, MinusLogProbMetric: 408.5892, val_loss: 407.3469, val_MinusLogProbMetric: 407.3469

Epoch 122: val_loss improved from 407.74725 to 407.34686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 21s - loss: 408.5892 - MinusLogProbMetric: 408.5892 - val_loss: 407.3469 - val_MinusLogProbMetric: 407.3469 - lr: 0.0010 - 21s/epoch - 105ms/step
Epoch 123/1000
2023-10-02 10:45:47.182 
Epoch 123/1000 
	 loss: 408.9290, MinusLogProbMetric: 408.9290, val_loss: 406.3835, val_MinusLogProbMetric: 406.3835

Epoch 123: val_loss improved from 407.34686 to 406.38351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 408.9290 - MinusLogProbMetric: 408.9290 - val_loss: 406.3835 - val_MinusLogProbMetric: 406.3835 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 124/1000
2023-10-02 10:46:04.552 
Epoch 124/1000 
	 loss: 408.1430, MinusLogProbMetric: 408.1430, val_loss: 409.5445, val_MinusLogProbMetric: 409.5445

Epoch 124: val_loss did not improve from 406.38351
196/196 - 17s - loss: 408.1430 - MinusLogProbMetric: 408.1430 - val_loss: 409.5445 - val_MinusLogProbMetric: 409.5445 - lr: 0.0010 - 17s/epoch - 86ms/step
Epoch 125/1000
2023-10-02 10:46:23.399 
Epoch 125/1000 
	 loss: 409.6218, MinusLogProbMetric: 409.6218, val_loss: 410.2987, val_MinusLogProbMetric: 410.2987

Epoch 125: val_loss did not improve from 406.38351
196/196 - 19s - loss: 409.6218 - MinusLogProbMetric: 409.6218 - val_loss: 410.2987 - val_MinusLogProbMetric: 410.2987 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 126/1000
2023-10-02 10:46:42.979 
Epoch 126/1000 
	 loss: 407.3505, MinusLogProbMetric: 407.3505, val_loss: 406.3214, val_MinusLogProbMetric: 406.3214

Epoch 126: val_loss improved from 406.38351 to 406.32144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 407.3505 - MinusLogProbMetric: 407.3505 - val_loss: 406.3214 - val_MinusLogProbMetric: 406.3214 - lr: 0.0010 - 20s/epoch - 103ms/step
Epoch 127/1000
2023-10-02 10:47:02.551 
Epoch 127/1000 
	 loss: 406.8641, MinusLogProbMetric: 406.8641, val_loss: 406.9337, val_MinusLogProbMetric: 406.9337

Epoch 127: val_loss did not improve from 406.32144
196/196 - 19s - loss: 406.8641 - MinusLogProbMetric: 406.8641 - val_loss: 406.9337 - val_MinusLogProbMetric: 406.9337 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 128/1000
2023-10-02 10:47:22.567 
Epoch 128/1000 
	 loss: 407.1433, MinusLogProbMetric: 407.1433, val_loss: 406.9773, val_MinusLogProbMetric: 406.9773

Epoch 128: val_loss did not improve from 406.32144
196/196 - 20s - loss: 407.1433 - MinusLogProbMetric: 407.1433 - val_loss: 406.9773 - val_MinusLogProbMetric: 406.9773 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 129/1000
2023-10-02 10:47:39.885 
Epoch 129/1000 
	 loss: 406.5579, MinusLogProbMetric: 406.5579, val_loss: 407.6607, val_MinusLogProbMetric: 407.6607

Epoch 129: val_loss did not improve from 406.32144
196/196 - 17s - loss: 406.5579 - MinusLogProbMetric: 406.5579 - val_loss: 407.6607 - val_MinusLogProbMetric: 407.6607 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 130/1000
2023-10-02 10:47:59.098 
Epoch 130/1000 
	 loss: 407.3341, MinusLogProbMetric: 407.3341, val_loss: 405.2659, val_MinusLogProbMetric: 405.2659

Epoch 130: val_loss improved from 406.32144 to 405.26590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 407.3341 - MinusLogProbMetric: 407.3341 - val_loss: 405.2659 - val_MinusLogProbMetric: 405.2659 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 131/1000
2023-10-02 10:48:18.017 
Epoch 131/1000 
	 loss: 406.4346, MinusLogProbMetric: 406.4346, val_loss: 410.4932, val_MinusLogProbMetric: 410.4932

Epoch 131: val_loss did not improve from 405.26590
196/196 - 18s - loss: 406.4346 - MinusLogProbMetric: 406.4346 - val_loss: 410.4932 - val_MinusLogProbMetric: 410.4932 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 132/1000
2023-10-02 10:48:37.624 
Epoch 132/1000 
	 loss: 407.0511, MinusLogProbMetric: 407.0511, val_loss: 405.5839, val_MinusLogProbMetric: 405.5839

Epoch 132: val_loss did not improve from 405.26590
196/196 - 20s - loss: 407.0511 - MinusLogProbMetric: 407.0511 - val_loss: 405.5839 - val_MinusLogProbMetric: 405.5839 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 133/1000
2023-10-02 10:48:56.832 
Epoch 133/1000 
	 loss: 408.7088, MinusLogProbMetric: 408.7088, val_loss: 406.0190, val_MinusLogProbMetric: 406.0190

Epoch 133: val_loss did not improve from 405.26590
196/196 - 19s - loss: 408.7088 - MinusLogProbMetric: 408.7088 - val_loss: 406.0190 - val_MinusLogProbMetric: 406.0190 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 134/1000
2023-10-02 10:49:16.158 
Epoch 134/1000 
	 loss: 405.7602, MinusLogProbMetric: 405.7602, val_loss: 406.4795, val_MinusLogProbMetric: 406.4795

Epoch 134: val_loss did not improve from 405.26590
196/196 - 19s - loss: 405.7602 - MinusLogProbMetric: 405.7602 - val_loss: 406.4795 - val_MinusLogProbMetric: 406.4795 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 135/1000
2023-10-02 10:49:34.200 
Epoch 135/1000 
	 loss: 405.9554, MinusLogProbMetric: 405.9554, val_loss: 404.8197, val_MinusLogProbMetric: 404.8197

Epoch 135: val_loss improved from 405.26590 to 404.81970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 405.9554 - MinusLogProbMetric: 405.9554 - val_loss: 404.8197 - val_MinusLogProbMetric: 404.8197 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 136/1000
2023-10-02 10:49:53.729 
Epoch 136/1000 
	 loss: 406.1572, MinusLogProbMetric: 406.1572, val_loss: 404.7648, val_MinusLogProbMetric: 404.7648

Epoch 136: val_loss improved from 404.81970 to 404.76477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 406.1572 - MinusLogProbMetric: 406.1572 - val_loss: 404.7648 - val_MinusLogProbMetric: 404.7648 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 137/1000
2023-10-02 10:50:13.933 
Epoch 137/1000 
	 loss: 405.7155, MinusLogProbMetric: 405.7155, val_loss: 405.3910, val_MinusLogProbMetric: 405.3910

Epoch 137: val_loss did not improve from 404.76477
196/196 - 20s - loss: 405.7155 - MinusLogProbMetric: 405.7155 - val_loss: 405.3910 - val_MinusLogProbMetric: 405.3910 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 138/1000
2023-10-02 10:50:32.866 
Epoch 138/1000 
	 loss: 405.9775, MinusLogProbMetric: 405.9775, val_loss: 406.6725, val_MinusLogProbMetric: 406.6725

Epoch 138: val_loss did not improve from 404.76477
196/196 - 19s - loss: 405.9775 - MinusLogProbMetric: 405.9775 - val_loss: 406.6725 - val_MinusLogProbMetric: 406.6725 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 139/1000
2023-10-02 10:50:50.712 
Epoch 139/1000 
	 loss: 406.1710, MinusLogProbMetric: 406.1710, val_loss: 407.7845, val_MinusLogProbMetric: 407.7845

Epoch 139: val_loss did not improve from 404.76477
196/196 - 18s - loss: 406.1710 - MinusLogProbMetric: 406.1710 - val_loss: 407.7845 - val_MinusLogProbMetric: 407.7845 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 140/1000
2023-10-02 10:51:09.048 
Epoch 140/1000 
	 loss: 405.8715, MinusLogProbMetric: 405.8715, val_loss: 405.4700, val_MinusLogProbMetric: 405.4700

Epoch 140: val_loss did not improve from 404.76477
196/196 - 18s - loss: 405.8715 - MinusLogProbMetric: 405.8715 - val_loss: 405.4700 - val_MinusLogProbMetric: 405.4700 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 141/1000
2023-10-02 10:51:27.879 
Epoch 141/1000 
	 loss: 405.8278, MinusLogProbMetric: 405.8278, val_loss: 406.4364, val_MinusLogProbMetric: 406.4364

Epoch 141: val_loss did not improve from 404.76477
196/196 - 19s - loss: 405.8278 - MinusLogProbMetric: 405.8278 - val_loss: 406.4364 - val_MinusLogProbMetric: 406.4364 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 142/1000
2023-10-02 10:51:46.239 
Epoch 142/1000 
	 loss: 405.0331, MinusLogProbMetric: 405.0331, val_loss: 403.7671, val_MinusLogProbMetric: 403.7671

Epoch 142: val_loss improved from 404.76477 to 403.76706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 405.0331 - MinusLogProbMetric: 405.0331 - val_loss: 403.7671 - val_MinusLogProbMetric: 403.7671 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 143/1000
2023-10-02 10:52:05.687 
Epoch 143/1000 
	 loss: 405.6684, MinusLogProbMetric: 405.6684, val_loss: 405.1748, val_MinusLogProbMetric: 405.1748

Epoch 143: val_loss did not improve from 403.76706
196/196 - 19s - loss: 405.6684 - MinusLogProbMetric: 405.6684 - val_loss: 405.1748 - val_MinusLogProbMetric: 405.1748 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 144/1000
2023-10-02 10:52:22.936 
Epoch 144/1000 
	 loss: 405.4042, MinusLogProbMetric: 405.4042, val_loss: 405.5603, val_MinusLogProbMetric: 405.5603

Epoch 144: val_loss did not improve from 403.76706
196/196 - 17s - loss: 405.4042 - MinusLogProbMetric: 405.4042 - val_loss: 405.5603 - val_MinusLogProbMetric: 405.5603 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 145/1000
2023-10-02 10:52:40.784 
Epoch 145/1000 
	 loss: 404.6855, MinusLogProbMetric: 404.6855, val_loss: 403.6433, val_MinusLogProbMetric: 403.6433

Epoch 145: val_loss improved from 403.76706 to 403.64328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 18s - loss: 404.6855 - MinusLogProbMetric: 404.6855 - val_loss: 403.6433 - val_MinusLogProbMetric: 403.6433 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 146/1000
2023-10-02 10:53:01.304 
Epoch 146/1000 
	 loss: 404.8871, MinusLogProbMetric: 404.8871, val_loss: 403.4464, val_MinusLogProbMetric: 403.4464

Epoch 146: val_loss improved from 403.64328 to 403.44644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 20s - loss: 404.8871 - MinusLogProbMetric: 404.8871 - val_loss: 403.4464 - val_MinusLogProbMetric: 403.4464 - lr: 0.0010 - 20s/epoch - 104ms/step
Epoch 147/1000
2023-10-02 10:53:18.808 
Epoch 147/1000 
	 loss: 404.3342, MinusLogProbMetric: 404.3342, val_loss: 409.3941, val_MinusLogProbMetric: 409.3941

Epoch 147: val_loss did not improve from 403.44644
196/196 - 17s - loss: 404.3342 - MinusLogProbMetric: 404.3342 - val_loss: 409.3941 - val_MinusLogProbMetric: 409.3941 - lr: 0.0010 - 17s/epoch - 88ms/step
Epoch 148/1000
2023-10-02 10:53:36.952 
Epoch 148/1000 
	 loss: 404.3714, MinusLogProbMetric: 404.3714, val_loss: 405.0878, val_MinusLogProbMetric: 405.0878

Epoch 148: val_loss did not improve from 403.44644
196/196 - 18s - loss: 404.3714 - MinusLogProbMetric: 404.3714 - val_loss: 405.0878 - val_MinusLogProbMetric: 405.0878 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 149/1000
2023-10-02 10:53:56.397 
Epoch 149/1000 
	 loss: 404.2905, MinusLogProbMetric: 404.2905, val_loss: 404.5407, val_MinusLogProbMetric: 404.5407

Epoch 149: val_loss did not improve from 403.44644
196/196 - 21s - loss: 404.2905 - MinusLogProbMetric: 404.2905 - val_loss: 404.5407 - val_MinusLogProbMetric: 404.5407 - lr: 0.0010 - 21s/epoch - 105ms/step
Epoch 150/1000
2023-10-02 10:54:16.292 
Epoch 150/1000 
	 loss: 405.6746, MinusLogProbMetric: 405.6746, val_loss: 403.7165, val_MinusLogProbMetric: 403.7165

Epoch 150: val_loss did not improve from 403.44644
196/196 - 19s - loss: 405.6746 - MinusLogProbMetric: 405.6746 - val_loss: 403.7165 - val_MinusLogProbMetric: 403.7165 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 151/1000
2023-10-02 10:54:34.691 
Epoch 151/1000 
	 loss: 404.4862, MinusLogProbMetric: 404.4862, val_loss: 406.8425, val_MinusLogProbMetric: 406.8425

Epoch 151: val_loss did not improve from 403.44644
196/196 - 18s - loss: 404.4862 - MinusLogProbMetric: 404.4862 - val_loss: 406.8425 - val_MinusLogProbMetric: 406.8425 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 152/1000
2023-10-02 10:54:53.495 
Epoch 152/1000 
	 loss: 403.6110, MinusLogProbMetric: 403.6110, val_loss: 402.5997, val_MinusLogProbMetric: 402.5997

Epoch 152: val_loss improved from 403.44644 to 402.59970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 19s - loss: 403.6110 - MinusLogProbMetric: 403.6110 - val_loss: 402.5997 - val_MinusLogProbMetric: 402.5997 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 153/1000
2023-10-02 10:55:12.537 
Epoch 153/1000 
	 loss: 404.9130, MinusLogProbMetric: 404.9130, val_loss: 408.9275, val_MinusLogProbMetric: 408.9275

Epoch 153: val_loss did not improve from 402.59970
196/196 - 19s - loss: 404.9130 - MinusLogProbMetric: 404.9130 - val_loss: 408.9275 - val_MinusLogProbMetric: 408.9275 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 154/1000
2023-10-02 10:55:30.617 
Epoch 154/1000 
	 loss: 1139.6772, MinusLogProbMetric: 1139.6772, val_loss: 806.2604, val_MinusLogProbMetric: 806.2604

Epoch 154: val_loss did not improve from 402.59970
196/196 - 18s - loss: 1139.6772 - MinusLogProbMetric: 1139.6772 - val_loss: 806.2604 - val_MinusLogProbMetric: 806.2604 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 155/1000
2023-10-02 10:55:49.209 
Epoch 155/1000 
	 loss: 592.7845, MinusLogProbMetric: 592.7845, val_loss: 510.0903, val_MinusLogProbMetric: 510.0903

Epoch 155: val_loss did not improve from 402.59970
196/196 - 19s - loss: 592.7845 - MinusLogProbMetric: 592.7845 - val_loss: 510.0903 - val_MinusLogProbMetric: 510.0903 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 156/1000
2023-10-02 10:56:07.353 
Epoch 156/1000 
	 loss: 489.7834, MinusLogProbMetric: 489.7834, val_loss: 478.4497, val_MinusLogProbMetric: 478.4497

Epoch 156: val_loss did not improve from 402.59970
196/196 - 18s - loss: 489.7834 - MinusLogProbMetric: 489.7834 - val_loss: 478.4497 - val_MinusLogProbMetric: 478.4497 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 157/1000
2023-10-02 10:56:26.136 
Epoch 157/1000 
	 loss: 467.1263, MinusLogProbMetric: 467.1263, val_loss: 461.1973, val_MinusLogProbMetric: 461.1973

Epoch 157: val_loss did not improve from 402.59970
196/196 - 19s - loss: 467.1263 - MinusLogProbMetric: 467.1263 - val_loss: 461.1973 - val_MinusLogProbMetric: 461.1973 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 158/1000
2023-10-02 10:56:43.019 
Epoch 158/1000 
	 loss: 453.3593, MinusLogProbMetric: 453.3593, val_loss: 448.0184, val_MinusLogProbMetric: 448.0184

Epoch 158: val_loss did not improve from 402.59970
196/196 - 17s - loss: 453.3593 - MinusLogProbMetric: 453.3593 - val_loss: 448.0184 - val_MinusLogProbMetric: 448.0184 - lr: 0.0010 - 17s/epoch - 86ms/step
Epoch 159/1000
2023-10-02 10:57:01.441 
Epoch 159/1000 
	 loss: 445.2022, MinusLogProbMetric: 445.2022, val_loss: 442.0763, val_MinusLogProbMetric: 442.0763

Epoch 159: val_loss did not improve from 402.59970
196/196 - 18s - loss: 445.2022 - MinusLogProbMetric: 445.2022 - val_loss: 442.0763 - val_MinusLogProbMetric: 442.0763 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 160/1000
2023-10-02 10:57:19.372 
Epoch 160/1000 
	 loss: 438.5858, MinusLogProbMetric: 438.5858, val_loss: 438.2587, val_MinusLogProbMetric: 438.2587

Epoch 160: val_loss did not improve from 402.59970
196/196 - 18s - loss: 438.5858 - MinusLogProbMetric: 438.5858 - val_loss: 438.2587 - val_MinusLogProbMetric: 438.2587 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 161/1000
2023-10-02 10:57:38.440 
Epoch 161/1000 
	 loss: 435.0540, MinusLogProbMetric: 435.0540, val_loss: 432.7716, val_MinusLogProbMetric: 432.7716

Epoch 161: val_loss did not improve from 402.59970
196/196 - 19s - loss: 435.0540 - MinusLogProbMetric: 435.0540 - val_loss: 432.7716 - val_MinusLogProbMetric: 432.7716 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 162/1000
2023-10-02 10:57:57.383 
Epoch 162/1000 
	 loss: 430.9895, MinusLogProbMetric: 430.9895, val_loss: 429.4039, val_MinusLogProbMetric: 429.4039

Epoch 162: val_loss did not improve from 402.59970
196/196 - 19s - loss: 430.9895 - MinusLogProbMetric: 430.9895 - val_loss: 429.4039 - val_MinusLogProbMetric: 429.4039 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 163/1000
2023-10-02 10:58:17.304 
Epoch 163/1000 
	 loss: 428.7028, MinusLogProbMetric: 428.7028, val_loss: 431.6232, val_MinusLogProbMetric: 431.6232

Epoch 163: val_loss did not improve from 402.59970
196/196 - 20s - loss: 428.7028 - MinusLogProbMetric: 428.7028 - val_loss: 431.6232 - val_MinusLogProbMetric: 431.6232 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 164/1000
2023-10-02 10:58:35.492 
Epoch 164/1000 
	 loss: 425.8721, MinusLogProbMetric: 425.8721, val_loss: 424.2798, val_MinusLogProbMetric: 424.2798

Epoch 164: val_loss did not improve from 402.59970
196/196 - 18s - loss: 425.8721 - MinusLogProbMetric: 425.8721 - val_loss: 424.2798 - val_MinusLogProbMetric: 424.2798 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 165/1000
2023-10-02 10:58:51.635 
Epoch 165/1000 
	 loss: 423.6691, MinusLogProbMetric: 423.6691, val_loss: 426.4853, val_MinusLogProbMetric: 426.4853

Epoch 165: val_loss did not improve from 402.59970
196/196 - 16s - loss: 423.6691 - MinusLogProbMetric: 423.6691 - val_loss: 426.4853 - val_MinusLogProbMetric: 426.4853 - lr: 0.0010 - 16s/epoch - 82ms/step
Epoch 166/1000
2023-10-02 10:59:10.054 
Epoch 166/1000 
	 loss: 421.9298, MinusLogProbMetric: 421.9298, val_loss: 425.1681, val_MinusLogProbMetric: 425.1681

Epoch 166: val_loss did not improve from 402.59970
196/196 - 18s - loss: 421.9298 - MinusLogProbMetric: 421.9298 - val_loss: 425.1681 - val_MinusLogProbMetric: 425.1681 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 167/1000
2023-10-02 10:59:27.713 
Epoch 167/1000 
	 loss: 420.4265, MinusLogProbMetric: 420.4265, val_loss: 421.9472, val_MinusLogProbMetric: 421.9472

Epoch 167: val_loss did not improve from 402.59970
196/196 - 18s - loss: 420.4265 - MinusLogProbMetric: 420.4265 - val_loss: 421.9472 - val_MinusLogProbMetric: 421.9472 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 168/1000
2023-10-02 10:59:45.802 
Epoch 168/1000 
	 loss: 419.4580, MinusLogProbMetric: 419.4580, val_loss: 420.3313, val_MinusLogProbMetric: 420.3313

Epoch 168: val_loss did not improve from 402.59970
196/196 - 18s - loss: 419.4580 - MinusLogProbMetric: 419.4580 - val_loss: 420.3313 - val_MinusLogProbMetric: 420.3313 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 169/1000
2023-10-02 11:00:04.156 
Epoch 169/1000 
	 loss: 418.1142, MinusLogProbMetric: 418.1142, val_loss: 418.3112, val_MinusLogProbMetric: 418.3112

Epoch 169: val_loss did not improve from 402.59970
196/196 - 18s - loss: 418.1142 - MinusLogProbMetric: 418.1142 - val_loss: 418.3112 - val_MinusLogProbMetric: 418.3112 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 170/1000
2023-10-02 11:00:22.692 
Epoch 170/1000 
	 loss: 417.3544, MinusLogProbMetric: 417.3544, val_loss: 416.5733, val_MinusLogProbMetric: 416.5733

Epoch 170: val_loss did not improve from 402.59970
196/196 - 19s - loss: 417.3544 - MinusLogProbMetric: 417.3544 - val_loss: 416.5733 - val_MinusLogProbMetric: 416.5733 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 171/1000
2023-10-02 11:00:38.852 
Epoch 171/1000 
	 loss: 416.1812, MinusLogProbMetric: 416.1812, val_loss: 416.3355, val_MinusLogProbMetric: 416.3355

Epoch 171: val_loss did not improve from 402.59970
196/196 - 16s - loss: 416.1812 - MinusLogProbMetric: 416.1812 - val_loss: 416.3355 - val_MinusLogProbMetric: 416.3355 - lr: 0.0010 - 16s/epoch - 82ms/step
Epoch 172/1000
2023-10-02 11:00:57.382 
Epoch 172/1000 
	 loss: 415.2260, MinusLogProbMetric: 415.2260, val_loss: 422.3035, val_MinusLogProbMetric: 422.3035

Epoch 172: val_loss did not improve from 402.59970
196/196 - 19s - loss: 415.2260 - MinusLogProbMetric: 415.2260 - val_loss: 422.3035 - val_MinusLogProbMetric: 422.3035 - lr: 0.0010 - 19s/epoch - 94ms/step
Epoch 173/1000
2023-10-02 11:01:16.466 
Epoch 173/1000 
	 loss: 414.6511, MinusLogProbMetric: 414.6511, val_loss: 413.5534, val_MinusLogProbMetric: 413.5534

Epoch 173: val_loss did not improve from 402.59970
196/196 - 19s - loss: 414.6511 - MinusLogProbMetric: 414.6511 - val_loss: 413.5534 - val_MinusLogProbMetric: 413.5534 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 174/1000
2023-10-02 11:01:35.560 
Epoch 174/1000 
	 loss: 414.3498, MinusLogProbMetric: 414.3498, val_loss: 414.7634, val_MinusLogProbMetric: 414.7634

Epoch 174: val_loss did not improve from 402.59970
196/196 - 19s - loss: 414.3498 - MinusLogProbMetric: 414.3498 - val_loss: 414.7634 - val_MinusLogProbMetric: 414.7634 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 175/1000
2023-10-02 11:01:53.364 
Epoch 175/1000 
	 loss: 413.8028, MinusLogProbMetric: 413.8028, val_loss: 412.4475, val_MinusLogProbMetric: 412.4475

Epoch 175: val_loss did not improve from 402.59970
196/196 - 18s - loss: 413.8028 - MinusLogProbMetric: 413.8028 - val_loss: 412.4475 - val_MinusLogProbMetric: 412.4475 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 176/1000
2023-10-02 11:02:13.094 
Epoch 176/1000 
	 loss: 412.4043, MinusLogProbMetric: 412.4043, val_loss: 414.5171, val_MinusLogProbMetric: 414.5171

Epoch 176: val_loss did not improve from 402.59970
196/196 - 20s - loss: 412.4043 - MinusLogProbMetric: 412.4043 - val_loss: 414.5171 - val_MinusLogProbMetric: 414.5171 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 177/1000
2023-10-02 11:02:31.696 
Epoch 177/1000 
	 loss: 411.9497, MinusLogProbMetric: 411.9497, val_loss: 412.4672, val_MinusLogProbMetric: 412.4672

Epoch 177: val_loss did not improve from 402.59970
196/196 - 19s - loss: 411.9497 - MinusLogProbMetric: 411.9497 - val_loss: 412.4672 - val_MinusLogProbMetric: 412.4672 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 178/1000
2023-10-02 11:02:51.561 
Epoch 178/1000 
	 loss: 412.2811, MinusLogProbMetric: 412.2811, val_loss: 410.6024, val_MinusLogProbMetric: 410.6024

Epoch 178: val_loss did not improve from 402.59970
196/196 - 20s - loss: 412.2811 - MinusLogProbMetric: 412.2811 - val_loss: 410.6024 - val_MinusLogProbMetric: 410.6024 - lr: 0.0010 - 20s/epoch - 101ms/step
Epoch 179/1000
2023-10-02 11:03:10.830 
Epoch 179/1000 
	 loss: 411.1977, MinusLogProbMetric: 411.1977, val_loss: 411.3295, val_MinusLogProbMetric: 411.3295

Epoch 179: val_loss did not improve from 402.59970
196/196 - 19s - loss: 411.1977 - MinusLogProbMetric: 411.1977 - val_loss: 411.3295 - val_MinusLogProbMetric: 411.3295 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 180/1000
2023-10-02 11:03:29.468 
Epoch 180/1000 
	 loss: 410.9524, MinusLogProbMetric: 410.9524, val_loss: 410.8247, val_MinusLogProbMetric: 410.8247

Epoch 180: val_loss did not improve from 402.59970
196/196 - 19s - loss: 410.9524 - MinusLogProbMetric: 410.9524 - val_loss: 410.8247 - val_MinusLogProbMetric: 410.8247 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 181/1000
2023-10-02 11:03:47.593 
Epoch 181/1000 
	 loss: 410.0814, MinusLogProbMetric: 410.0814, val_loss: 420.5215, val_MinusLogProbMetric: 420.5215

Epoch 181: val_loss did not improve from 402.59970
196/196 - 18s - loss: 410.0814 - MinusLogProbMetric: 410.0814 - val_loss: 420.5215 - val_MinusLogProbMetric: 420.5215 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 182/1000
2023-10-02 11:04:06.885 
Epoch 182/1000 
	 loss: 410.2030, MinusLogProbMetric: 410.2030, val_loss: 409.0271, val_MinusLogProbMetric: 409.0271

Epoch 182: val_loss did not improve from 402.59970
196/196 - 19s - loss: 410.2030 - MinusLogProbMetric: 410.2030 - val_loss: 409.0271 - val_MinusLogProbMetric: 409.0271 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 183/1000
2023-10-02 11:04:21.343 
Epoch 183/1000 
	 loss: 409.8252, MinusLogProbMetric: 409.8252, val_loss: 409.6653, val_MinusLogProbMetric: 409.6653

Epoch 183: val_loss did not improve from 402.59970
196/196 - 14s - loss: 409.8252 - MinusLogProbMetric: 409.8252 - val_loss: 409.6653 - val_MinusLogProbMetric: 409.6653 - lr: 0.0010 - 14s/epoch - 74ms/step
Epoch 184/1000
2023-10-02 11:04:39.653 
Epoch 184/1000 
	 loss: 410.2441, MinusLogProbMetric: 410.2441, val_loss: 409.9384, val_MinusLogProbMetric: 409.9384

Epoch 184: val_loss did not improve from 402.59970
196/196 - 18s - loss: 410.2441 - MinusLogProbMetric: 410.2441 - val_loss: 409.9384 - val_MinusLogProbMetric: 409.9384 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 185/1000
2023-10-02 11:04:59.104 
Epoch 185/1000 
	 loss: 409.0999, MinusLogProbMetric: 409.0999, val_loss: 408.7726, val_MinusLogProbMetric: 408.7726

Epoch 185: val_loss did not improve from 402.59970
196/196 - 19s - loss: 409.0999 - MinusLogProbMetric: 409.0999 - val_loss: 408.7726 - val_MinusLogProbMetric: 408.7726 - lr: 0.0010 - 19s/epoch - 99ms/step
Epoch 186/1000
2023-10-02 11:05:18.318 
Epoch 186/1000 
	 loss: 408.4528, MinusLogProbMetric: 408.4528, val_loss: 407.6432, val_MinusLogProbMetric: 407.6432

Epoch 186: val_loss did not improve from 402.59970
196/196 - 19s - loss: 408.4528 - MinusLogProbMetric: 408.4528 - val_loss: 407.6432 - val_MinusLogProbMetric: 407.6432 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 187/1000
2023-10-02 11:05:36.812 
Epoch 187/1000 
	 loss: 408.4387, MinusLogProbMetric: 408.4387, val_loss: 410.8894, val_MinusLogProbMetric: 410.8894

Epoch 187: val_loss did not improve from 402.59970
196/196 - 18s - loss: 408.4387 - MinusLogProbMetric: 408.4387 - val_loss: 410.8894 - val_MinusLogProbMetric: 410.8894 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 188/1000
2023-10-02 11:05:53.888 
Epoch 188/1000 
	 loss: 408.3092, MinusLogProbMetric: 408.3092, val_loss: 420.4166, val_MinusLogProbMetric: 420.4166

Epoch 188: val_loss did not improve from 402.59970
196/196 - 17s - loss: 408.3092 - MinusLogProbMetric: 408.3092 - val_loss: 420.4166 - val_MinusLogProbMetric: 420.4166 - lr: 0.0010 - 17s/epoch - 87ms/step
Epoch 189/1000
2023-10-02 11:06:13.488 
Epoch 189/1000 
	 loss: 409.0393, MinusLogProbMetric: 409.0393, val_loss: 408.5348, val_MinusLogProbMetric: 408.5348

Epoch 189: val_loss did not improve from 402.59970
196/196 - 20s - loss: 409.0393 - MinusLogProbMetric: 409.0393 - val_loss: 408.5348 - val_MinusLogProbMetric: 408.5348 - lr: 0.0010 - 20s/epoch - 100ms/step
Epoch 190/1000
2023-10-02 11:06:29.598 
Epoch 190/1000 
	 loss: 407.5058, MinusLogProbMetric: 407.5058, val_loss: 408.0137, val_MinusLogProbMetric: 408.0137

Epoch 190: val_loss did not improve from 402.59970
196/196 - 16s - loss: 407.5058 - MinusLogProbMetric: 407.5058 - val_loss: 408.0137 - val_MinusLogProbMetric: 408.0137 - lr: 0.0010 - 16s/epoch - 82ms/step
Epoch 191/1000
2023-10-02 11:06:48.123 
Epoch 191/1000 
	 loss: 1425.0431, MinusLogProbMetric: 1425.0431, val_loss: 614.3776, val_MinusLogProbMetric: 614.3776

Epoch 191: val_loss did not improve from 402.59970
196/196 - 19s - loss: 1425.0431 - MinusLogProbMetric: 1425.0431 - val_loss: 614.3776 - val_MinusLogProbMetric: 614.3776 - lr: 0.0010 - 19s/epoch - 94ms/step
Epoch 192/1000
2023-10-02 11:07:06.003 
Epoch 192/1000 
	 loss: 542.3543, MinusLogProbMetric: 542.3543, val_loss: 502.8298, val_MinusLogProbMetric: 502.8298

Epoch 192: val_loss did not improve from 402.59970
196/196 - 18s - loss: 542.3543 - MinusLogProbMetric: 542.3543 - val_loss: 502.8298 - val_MinusLogProbMetric: 502.8298 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 193/1000
2023-10-02 11:07:24.370 
Epoch 193/1000 
	 loss: 488.1156, MinusLogProbMetric: 488.1156, val_loss: 482.4620, val_MinusLogProbMetric: 482.4620

Epoch 193: val_loss did not improve from 402.59970
196/196 - 18s - loss: 488.1156 - MinusLogProbMetric: 488.1156 - val_loss: 482.4620 - val_MinusLogProbMetric: 482.4620 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 194/1000
2023-10-02 11:07:42.214 
Epoch 194/1000 
	 loss: 470.2212, MinusLogProbMetric: 470.2212, val_loss: 462.6920, val_MinusLogProbMetric: 462.6920

Epoch 194: val_loss did not improve from 402.59970
196/196 - 18s - loss: 470.2212 - MinusLogProbMetric: 470.2212 - val_loss: 462.6920 - val_MinusLogProbMetric: 462.6920 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 195/1000
2023-10-02 11:07:58.528 
Epoch 195/1000 
	 loss: 458.6436, MinusLogProbMetric: 458.6436, val_loss: 454.1752, val_MinusLogProbMetric: 454.1752

Epoch 195: val_loss did not improve from 402.59970
196/196 - 16s - loss: 458.6436 - MinusLogProbMetric: 458.6436 - val_loss: 454.1752 - val_MinusLogProbMetric: 454.1752 - lr: 0.0010 - 16s/epoch - 83ms/step
Epoch 196/1000
2023-10-02 11:08:17.430 
Epoch 196/1000 
	 loss: 451.7056, MinusLogProbMetric: 451.7056, val_loss: 449.8131, val_MinusLogProbMetric: 449.8131

Epoch 196: val_loss did not improve from 402.59970
196/196 - 19s - loss: 451.7056 - MinusLogProbMetric: 451.7056 - val_loss: 449.8131 - val_MinusLogProbMetric: 449.8131 - lr: 0.0010 - 19s/epoch - 96ms/step
Epoch 197/1000
2023-10-02 11:08:34.959 
Epoch 197/1000 
	 loss: 446.2034, MinusLogProbMetric: 446.2034, val_loss: 444.0288, val_MinusLogProbMetric: 444.0288

Epoch 197: val_loss did not improve from 402.59970
196/196 - 18s - loss: 446.2034 - MinusLogProbMetric: 446.2034 - val_loss: 444.0288 - val_MinusLogProbMetric: 444.0288 - lr: 0.0010 - 18s/epoch - 89ms/step
Epoch 198/1000
2023-10-02 11:08:53.437 
Epoch 198/1000 
	 loss: 442.8092, MinusLogProbMetric: 442.8092, val_loss: 439.8679, val_MinusLogProbMetric: 439.8679

Epoch 198: val_loss did not improve from 402.59970
196/196 - 18s - loss: 442.8092 - MinusLogProbMetric: 442.8092 - val_loss: 439.8679 - val_MinusLogProbMetric: 439.8679 - lr: 0.0010 - 18s/epoch - 94ms/step
Epoch 199/1000
2023-10-02 11:09:11.088 
Epoch 199/1000 
	 loss: 438.6636, MinusLogProbMetric: 438.6636, val_loss: 439.4743, val_MinusLogProbMetric: 439.4743

Epoch 199: val_loss did not improve from 402.59970
196/196 - 18s - loss: 438.6636 - MinusLogProbMetric: 438.6636 - val_loss: 439.4743 - val_MinusLogProbMetric: 439.4743 - lr: 0.0010 - 18s/epoch - 90ms/step
Epoch 200/1000
2023-10-02 11:09:29.669 
Epoch 200/1000 
	 loss: 436.0121, MinusLogProbMetric: 436.0121, val_loss: 434.3521, val_MinusLogProbMetric: 434.3521

Epoch 200: val_loss did not improve from 402.59970
196/196 - 19s - loss: 436.0121 - MinusLogProbMetric: 436.0121 - val_loss: 434.3521 - val_MinusLogProbMetric: 434.3521 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 201/1000
2023-10-02 11:09:47.175 
Epoch 201/1000 
	 loss: 434.3629, MinusLogProbMetric: 434.3629, val_loss: 434.6893, val_MinusLogProbMetric: 434.6893

Epoch 201: val_loss did not improve from 402.59970
196/196 - 18s - loss: 434.3629 - MinusLogProbMetric: 434.3629 - val_loss: 434.6893 - val_MinusLogProbMetric: 434.6893 - lr: 0.0010 - 18s/epoch - 89ms/step
Epoch 202/1000
2023-10-02 11:10:06.499 
Epoch 202/1000 
	 loss: 432.4096, MinusLogProbMetric: 432.4096, val_loss: 432.1008, val_MinusLogProbMetric: 432.1008

Epoch 202: val_loss did not improve from 402.59970
196/196 - 19s - loss: 432.4096 - MinusLogProbMetric: 432.4096 - val_loss: 432.1008 - val_MinusLogProbMetric: 432.1008 - lr: 0.0010 - 19s/epoch - 98ms/step
Epoch 203/1000
2023-10-02 11:10:25.384 
Epoch 203/1000 
	 loss: 425.6900, MinusLogProbMetric: 425.6900, val_loss: 425.8007, val_MinusLogProbMetric: 425.8007

Epoch 203: val_loss did not improve from 402.59970
196/196 - 19s - loss: 425.6900 - MinusLogProbMetric: 425.6900 - val_loss: 425.8007 - val_MinusLogProbMetric: 425.8007 - lr: 5.0000e-04 - 19s/epoch - 96ms/step
Epoch 204/1000
2023-10-02 11:10:43.720 
Epoch 204/1000 
	 loss: 424.7228, MinusLogProbMetric: 424.7228, val_loss: 425.0529, val_MinusLogProbMetric: 425.0529

Epoch 204: val_loss did not improve from 402.59970
196/196 - 18s - loss: 424.7228 - MinusLogProbMetric: 424.7228 - val_loss: 425.0529 - val_MinusLogProbMetric: 425.0529 - lr: 5.0000e-04 - 18s/epoch - 94ms/step
Epoch 205/1000
2023-10-02 11:11:01.807 
Epoch 205/1000 
	 loss: 423.8746, MinusLogProbMetric: 423.8746, val_loss: 425.2727, val_MinusLogProbMetric: 425.2727

Epoch 205: val_loss did not improve from 402.59970
196/196 - 18s - loss: 423.8746 - MinusLogProbMetric: 423.8746 - val_loss: 425.2727 - val_MinusLogProbMetric: 425.2727 - lr: 5.0000e-04 - 18s/epoch - 92ms/step
Epoch 206/1000
2023-10-02 11:11:18.330 
Epoch 206/1000 
	 loss: 423.3524, MinusLogProbMetric: 423.3524, val_loss: 423.2516, val_MinusLogProbMetric: 423.2516

Epoch 206: val_loss did not improve from 402.59970
196/196 - 16s - loss: 423.3524 - MinusLogProbMetric: 423.3524 - val_loss: 423.2516 - val_MinusLogProbMetric: 423.2516 - lr: 5.0000e-04 - 16s/epoch - 84ms/step
Epoch 207/1000
2023-10-02 11:11:36.321 
Epoch 207/1000 
	 loss: 422.5480, MinusLogProbMetric: 422.5480, val_loss: 423.0546, val_MinusLogProbMetric: 423.0546

Epoch 207: val_loss did not improve from 402.59970
196/196 - 18s - loss: 422.5480 - MinusLogProbMetric: 422.5480 - val_loss: 423.0546 - val_MinusLogProbMetric: 423.0546 - lr: 5.0000e-04 - 18s/epoch - 92ms/step
Epoch 208/1000
2023-10-02 11:11:54.809 
Epoch 208/1000 
	 loss: 422.0003, MinusLogProbMetric: 422.0003, val_loss: 421.8780, val_MinusLogProbMetric: 421.8780

Epoch 208: val_loss did not improve from 402.59970
196/196 - 18s - loss: 422.0003 - MinusLogProbMetric: 422.0003 - val_loss: 421.8780 - val_MinusLogProbMetric: 421.8780 - lr: 5.0000e-04 - 18s/epoch - 94ms/step
Epoch 209/1000
2023-10-02 11:12:12.651 
Epoch 209/1000 
	 loss: 421.1352, MinusLogProbMetric: 421.1352, val_loss: 423.0686, val_MinusLogProbMetric: 423.0686

Epoch 209: val_loss did not improve from 402.59970
196/196 - 18s - loss: 421.1352 - MinusLogProbMetric: 421.1352 - val_loss: 423.0686 - val_MinusLogProbMetric: 423.0686 - lr: 5.0000e-04 - 18s/epoch - 91ms/step
Epoch 210/1000
2023-10-02 11:12:30.753 
Epoch 210/1000 
	 loss: 420.9607, MinusLogProbMetric: 420.9607, val_loss: 422.0039, val_MinusLogProbMetric: 422.0039

Epoch 210: val_loss did not improve from 402.59970
196/196 - 18s - loss: 420.9607 - MinusLogProbMetric: 420.9607 - val_loss: 422.0039 - val_MinusLogProbMetric: 422.0039 - lr: 5.0000e-04 - 18s/epoch - 92ms/step
Epoch 211/1000
2023-10-02 11:12:49.638 
Epoch 211/1000 
	 loss: 420.3763, MinusLogProbMetric: 420.3763, val_loss: 421.8712, val_MinusLogProbMetric: 421.8712

Epoch 211: val_loss did not improve from 402.59970
196/196 - 19s - loss: 420.3763 - MinusLogProbMetric: 420.3763 - val_loss: 421.8712 - val_MinusLogProbMetric: 421.8712 - lr: 5.0000e-04 - 19s/epoch - 96ms/step
Epoch 212/1000
2023-10-02 11:13:07.363 
Epoch 212/1000 
	 loss: 419.5190, MinusLogProbMetric: 419.5190, val_loss: 420.8957, val_MinusLogProbMetric: 420.8957

Epoch 212: val_loss did not improve from 402.59970
196/196 - 18s - loss: 419.5190 - MinusLogProbMetric: 419.5190 - val_loss: 420.8957 - val_MinusLogProbMetric: 420.8957 - lr: 5.0000e-04 - 18s/epoch - 90ms/step
Epoch 213/1000
2023-10-02 11:13:24.552 
Epoch 213/1000 
	 loss: 419.2485, MinusLogProbMetric: 419.2485, val_loss: 420.4131, val_MinusLogProbMetric: 420.4131

Epoch 213: val_loss did not improve from 402.59970
196/196 - 17s - loss: 419.2485 - MinusLogProbMetric: 419.2485 - val_loss: 420.4131 - val_MinusLogProbMetric: 420.4131 - lr: 5.0000e-04 - 17s/epoch - 88ms/step
Epoch 214/1000
2023-10-02 11:13:43.211 
Epoch 214/1000 
	 loss: 418.6061, MinusLogProbMetric: 418.6061, val_loss: 419.0927, val_MinusLogProbMetric: 419.0927

Epoch 214: val_loss did not improve from 402.59970
196/196 - 19s - loss: 418.6061 - MinusLogProbMetric: 418.6061 - val_loss: 419.0927 - val_MinusLogProbMetric: 419.0927 - lr: 5.0000e-04 - 19s/epoch - 95ms/step
Epoch 215/1000
2023-10-02 11:13:59.868 
Epoch 215/1000 
	 loss: 418.3557, MinusLogProbMetric: 418.3557, val_loss: 419.0780, val_MinusLogProbMetric: 419.0780

Epoch 215: val_loss did not improve from 402.59970
196/196 - 17s - loss: 418.3557 - MinusLogProbMetric: 418.3557 - val_loss: 419.0780 - val_MinusLogProbMetric: 419.0780 - lr: 5.0000e-04 - 17s/epoch - 85ms/step
Epoch 216/1000
2023-10-02 11:14:18.673 
Epoch 216/1000 
	 loss: 417.7088, MinusLogProbMetric: 417.7088, val_loss: 420.9809, val_MinusLogProbMetric: 420.9809

Epoch 216: val_loss did not improve from 402.59970
196/196 - 19s - loss: 417.7088 - MinusLogProbMetric: 417.7088 - val_loss: 420.9809 - val_MinusLogProbMetric: 420.9809 - lr: 5.0000e-04 - 19s/epoch - 96ms/step
Epoch 217/1000
2023-10-02 11:14:37.410 
Epoch 217/1000 
	 loss: 417.3664, MinusLogProbMetric: 417.3664, val_loss: 417.3273, val_MinusLogProbMetric: 417.3273

Epoch 217: val_loss did not improve from 402.59970
196/196 - 19s - loss: 417.3664 - MinusLogProbMetric: 417.3664 - val_loss: 417.3273 - val_MinusLogProbMetric: 417.3273 - lr: 5.0000e-04 - 19s/epoch - 96ms/step
Epoch 218/1000
2023-10-02 11:14:55.353 
Epoch 218/1000 
	 loss: 416.9118, MinusLogProbMetric: 416.9118, val_loss: 417.2264, val_MinusLogProbMetric: 417.2264

Epoch 218: val_loss did not improve from 402.59970
196/196 - 18s - loss: 416.9118 - MinusLogProbMetric: 416.9118 - val_loss: 417.2264 - val_MinusLogProbMetric: 417.2264 - lr: 5.0000e-04 - 18s/epoch - 92ms/step
Epoch 219/1000
2023-10-02 11:15:13.292 
Epoch 219/1000 
	 loss: 416.8309, MinusLogProbMetric: 416.8309, val_loss: 417.6173, val_MinusLogProbMetric: 417.6173

Epoch 219: val_loss did not improve from 402.59970
196/196 - 18s - loss: 416.8309 - MinusLogProbMetric: 416.8309 - val_loss: 417.6173 - val_MinusLogProbMetric: 417.6173 - lr: 5.0000e-04 - 18s/epoch - 91ms/step
Epoch 220/1000
2023-10-02 11:15:29.926 
Epoch 220/1000 
	 loss: 416.0378, MinusLogProbMetric: 416.0378, val_loss: 416.4685, val_MinusLogProbMetric: 416.4685

Epoch 220: val_loss did not improve from 402.59970
196/196 - 17s - loss: 416.0378 - MinusLogProbMetric: 416.0378 - val_loss: 416.4685 - val_MinusLogProbMetric: 416.4685 - lr: 5.0000e-04 - 17s/epoch - 85ms/step
Epoch 221/1000
2023-10-02 11:15:47.809 
Epoch 221/1000 
	 loss: 416.0166, MinusLogProbMetric: 416.0166, val_loss: 415.6023, val_MinusLogProbMetric: 415.6023

Epoch 221: val_loss did not improve from 402.59970
196/196 - 18s - loss: 416.0166 - MinusLogProbMetric: 416.0166 - val_loss: 415.6023 - val_MinusLogProbMetric: 415.6023 - lr: 5.0000e-04 - 18s/epoch - 91ms/step
Epoch 222/1000
2023-10-02 11:16:06.157 
Epoch 222/1000 
	 loss: 415.2344, MinusLogProbMetric: 415.2344, val_loss: 415.5219, val_MinusLogProbMetric: 415.5219

Epoch 222: val_loss did not improve from 402.59970
196/196 - 18s - loss: 415.2344 - MinusLogProbMetric: 415.2344 - val_loss: 415.5219 - val_MinusLogProbMetric: 415.5219 - lr: 5.0000e-04 - 18s/epoch - 93ms/step
Epoch 223/1000
2023-10-02 11:16:24.952 
Epoch 223/1000 
	 loss: 415.4438, MinusLogProbMetric: 415.4438, val_loss: 416.2102, val_MinusLogProbMetric: 416.2102

Epoch 223: val_loss did not improve from 402.59970
196/196 - 19s - loss: 415.4438 - MinusLogProbMetric: 415.4438 - val_loss: 416.2102 - val_MinusLogProbMetric: 416.2102 - lr: 5.0000e-04 - 19s/epoch - 96ms/step
Epoch 224/1000
2023-10-02 11:16:43.673 
Epoch 224/1000 
	 loss: 414.6903, MinusLogProbMetric: 414.6903, val_loss: 415.8797, val_MinusLogProbMetric: 415.8797

Epoch 224: val_loss did not improve from 402.59970
196/196 - 19s - loss: 414.6903 - MinusLogProbMetric: 414.6903 - val_loss: 415.8797 - val_MinusLogProbMetric: 415.8797 - lr: 5.0000e-04 - 19s/epoch - 95ms/step
Epoch 225/1000
2023-10-02 11:17:01.692 
Epoch 225/1000 
	 loss: 415.0122, MinusLogProbMetric: 415.0122, val_loss: 415.6257, val_MinusLogProbMetric: 415.6257

Epoch 225: val_loss did not improve from 402.59970
196/196 - 18s - loss: 415.0122 - MinusLogProbMetric: 415.0122 - val_loss: 415.6257 - val_MinusLogProbMetric: 415.6257 - lr: 5.0000e-04 - 18s/epoch - 92ms/step
Epoch 226/1000
2023-10-02 11:17:20.604 
Epoch 226/1000 
	 loss: 413.9342, MinusLogProbMetric: 413.9342, val_loss: 415.1887, val_MinusLogProbMetric: 415.1887

Epoch 226: val_loss did not improve from 402.59970
196/196 - 19s - loss: 413.9342 - MinusLogProbMetric: 413.9342 - val_loss: 415.1887 - val_MinusLogProbMetric: 415.1887 - lr: 5.0000e-04 - 19s/epoch - 96ms/step
Epoch 227/1000
2023-10-02 11:17:38.271 
Epoch 227/1000 
	 loss: 414.1937, MinusLogProbMetric: 414.1937, val_loss: 414.6240, val_MinusLogProbMetric: 414.6240

Epoch 227: val_loss did not improve from 402.59970
196/196 - 18s - loss: 414.1937 - MinusLogProbMetric: 414.1937 - val_loss: 414.6240 - val_MinusLogProbMetric: 414.6240 - lr: 5.0000e-04 - 18s/epoch - 90ms/step
Epoch 228/1000
2023-10-02 11:17:55.881 
Epoch 228/1000 
	 loss: 413.7430, MinusLogProbMetric: 413.7430, val_loss: 414.2233, val_MinusLogProbMetric: 414.2233

Epoch 228: val_loss did not improve from 402.59970
196/196 - 18s - loss: 413.7430 - MinusLogProbMetric: 413.7430 - val_loss: 414.2233 - val_MinusLogProbMetric: 414.2233 - lr: 5.0000e-04 - 18s/epoch - 90ms/step
Epoch 229/1000
2023-10-02 11:18:14.901 
Epoch 229/1000 
	 loss: 413.4663, MinusLogProbMetric: 413.4663, val_loss: 413.0128, val_MinusLogProbMetric: 413.0128

Epoch 229: val_loss did not improve from 402.59970
196/196 - 19s - loss: 413.4663 - MinusLogProbMetric: 413.4663 - val_loss: 413.0128 - val_MinusLogProbMetric: 413.0128 - lr: 5.0000e-04 - 19s/epoch - 97ms/step
Epoch 230/1000
2023-10-02 11:18:32.741 
Epoch 230/1000 
	 loss: 413.0156, MinusLogProbMetric: 413.0156, val_loss: 413.4111, val_MinusLogProbMetric: 413.4111

Epoch 230: val_loss did not improve from 402.59970
196/196 - 18s - loss: 413.0156 - MinusLogProbMetric: 413.0156 - val_loss: 413.4111 - val_MinusLogProbMetric: 413.4111 - lr: 5.0000e-04 - 18s/epoch - 91ms/step
Epoch 231/1000
2023-10-02 11:18:49.603 
Epoch 231/1000 
	 loss: 412.8921, MinusLogProbMetric: 412.8921, val_loss: 413.8204, val_MinusLogProbMetric: 413.8204

Epoch 231: val_loss did not improve from 402.59970
196/196 - 17s - loss: 412.8921 - MinusLogProbMetric: 412.8921 - val_loss: 413.8204 - val_MinusLogProbMetric: 413.8204 - lr: 5.0000e-04 - 17s/epoch - 86ms/step
Epoch 232/1000
2023-10-02 11:19:09.452 
Epoch 232/1000 
	 loss: 412.6300, MinusLogProbMetric: 412.6300, val_loss: 415.8450, val_MinusLogProbMetric: 415.8450

Epoch 232: val_loss did not improve from 402.59970
196/196 - 20s - loss: 412.6300 - MinusLogProbMetric: 412.6300 - val_loss: 415.8450 - val_MinusLogProbMetric: 415.8450 - lr: 5.0000e-04 - 20s/epoch - 101ms/step
Epoch 233/1000
2023-10-02 11:19:26.752 
Epoch 233/1000 
	 loss: 412.4713, MinusLogProbMetric: 412.4713, val_loss: 412.3756, val_MinusLogProbMetric: 412.3756

Epoch 233: val_loss did not improve from 402.59970
196/196 - 17s - loss: 412.4713 - MinusLogProbMetric: 412.4713 - val_loss: 412.3756 - val_MinusLogProbMetric: 412.3756 - lr: 5.0000e-04 - 17s/epoch - 88ms/step
Epoch 234/1000
2023-10-02 11:19:44.532 
Epoch 234/1000 
	 loss: 412.8028, MinusLogProbMetric: 412.8028, val_loss: 412.6203, val_MinusLogProbMetric: 412.6203

Epoch 234: val_loss did not improve from 402.59970
196/196 - 18s - loss: 412.8028 - MinusLogProbMetric: 412.8028 - val_loss: 412.6203 - val_MinusLogProbMetric: 412.6203 - lr: 5.0000e-04 - 18s/epoch - 91ms/step
Epoch 235/1000
2023-10-02 11:20:04.778 
Epoch 235/1000 
	 loss: 411.4850, MinusLogProbMetric: 411.4850, val_loss: 412.3275, val_MinusLogProbMetric: 412.3275

Epoch 235: val_loss did not improve from 402.59970
196/196 - 20s - loss: 411.4850 - MinusLogProbMetric: 411.4850 - val_loss: 412.3275 - val_MinusLogProbMetric: 412.3275 - lr: 5.0000e-04 - 20s/epoch - 103ms/step
Epoch 236/1000
2023-10-02 11:20:23.884 
Epoch 236/1000 
	 loss: 411.7178, MinusLogProbMetric: 411.7178, val_loss: 414.2514, val_MinusLogProbMetric: 414.2514

Epoch 236: val_loss did not improve from 402.59970
196/196 - 19s - loss: 411.7178 - MinusLogProbMetric: 411.7178 - val_loss: 414.2514 - val_MinusLogProbMetric: 414.2514 - lr: 5.0000e-04 - 19s/epoch - 97ms/step
Epoch 237/1000
2023-10-02 11:20:43.540 
Epoch 237/1000 
	 loss: 411.7527, MinusLogProbMetric: 411.7527, val_loss: 411.7837, val_MinusLogProbMetric: 411.7837

Epoch 237: val_loss did not improve from 402.59970
196/196 - 20s - loss: 411.7527 - MinusLogProbMetric: 411.7527 - val_loss: 411.7837 - val_MinusLogProbMetric: 411.7837 - lr: 5.0000e-04 - 20s/epoch - 100ms/step
Epoch 238/1000
2023-10-02 11:21:01.590 
Epoch 238/1000 
	 loss: 411.3309, MinusLogProbMetric: 411.3309, val_loss: 416.1274, val_MinusLogProbMetric: 416.1274

Epoch 238: val_loss did not improve from 402.59970
196/196 - 18s - loss: 411.3309 - MinusLogProbMetric: 411.3309 - val_loss: 416.1274 - val_MinusLogProbMetric: 416.1274 - lr: 5.0000e-04 - 18s/epoch - 92ms/step
Epoch 239/1000
2023-10-02 11:21:23.449 
Epoch 239/1000 
	 loss: 411.2971, MinusLogProbMetric: 411.2971, val_loss: 413.7484, val_MinusLogProbMetric: 413.7484

Epoch 239: val_loss did not improve from 402.59970
196/196 - 22s - loss: 411.2971 - MinusLogProbMetric: 411.2971 - val_loss: 413.7484 - val_MinusLogProbMetric: 413.7484 - lr: 5.0000e-04 - 22s/epoch - 111ms/step
Epoch 240/1000
2023-10-02 11:21:42.023 
Epoch 240/1000 
	 loss: 411.4050, MinusLogProbMetric: 411.4050, val_loss: 412.1381, val_MinusLogProbMetric: 412.1381

Epoch 240: val_loss did not improve from 402.59970
196/196 - 19s - loss: 411.4050 - MinusLogProbMetric: 411.4050 - val_loss: 412.1381 - val_MinusLogProbMetric: 412.1381 - lr: 5.0000e-04 - 19s/epoch - 95ms/step
Epoch 241/1000
2023-10-02 11:22:00.947 
Epoch 241/1000 
	 loss: 410.2921, MinusLogProbMetric: 410.2921, val_loss: 410.3858, val_MinusLogProbMetric: 410.3858

Epoch 241: val_loss did not improve from 402.59970
196/196 - 19s - loss: 410.2921 - MinusLogProbMetric: 410.2921 - val_loss: 410.3858 - val_MinusLogProbMetric: 410.3858 - lr: 5.0000e-04 - 19s/epoch - 96ms/step
Epoch 242/1000
2023-10-02 11:22:20.090 
Epoch 242/1000 
	 loss: 410.4009, MinusLogProbMetric: 410.4009, val_loss: 414.8285, val_MinusLogProbMetric: 414.8285

Epoch 242: val_loss did not improve from 402.59970
196/196 - 19s - loss: 410.4009 - MinusLogProbMetric: 410.4009 - val_loss: 414.8285 - val_MinusLogProbMetric: 414.8285 - lr: 5.0000e-04 - 19s/epoch - 98ms/step
Epoch 243/1000
2023-10-02 11:22:35.190 
Epoch 243/1000 
	 loss: 410.5260, MinusLogProbMetric: 410.5260, val_loss: 412.5784, val_MinusLogProbMetric: 412.5784

Epoch 243: val_loss did not improve from 402.59970
196/196 - 15s - loss: 410.5260 - MinusLogProbMetric: 410.5260 - val_loss: 412.5784 - val_MinusLogProbMetric: 412.5784 - lr: 5.0000e-04 - 15s/epoch - 77ms/step
Epoch 244/1000
2023-10-02 11:22:50.377 
Epoch 244/1000 
	 loss: 410.0760, MinusLogProbMetric: 410.0760, val_loss: 409.9024, val_MinusLogProbMetric: 409.9024

Epoch 244: val_loss did not improve from 402.59970
196/196 - 15s - loss: 410.0760 - MinusLogProbMetric: 410.0760 - val_loss: 409.9024 - val_MinusLogProbMetric: 409.9024 - lr: 5.0000e-04 - 15s/epoch - 77ms/step
Epoch 245/1000
2023-10-02 11:23:07.274 
Epoch 245/1000 
	 loss: 410.1353, MinusLogProbMetric: 410.1353, val_loss: 410.1849, val_MinusLogProbMetric: 410.1849

Epoch 245: val_loss did not improve from 402.59970
196/196 - 17s - loss: 410.1353 - MinusLogProbMetric: 410.1353 - val_loss: 410.1849 - val_MinusLogProbMetric: 410.1849 - lr: 5.0000e-04 - 17s/epoch - 86ms/step
Epoch 246/1000
2023-10-02 11:23:25.762 
Epoch 246/1000 
	 loss: 409.9126, MinusLogProbMetric: 409.9126, val_loss: 410.1801, val_MinusLogProbMetric: 410.1801

Epoch 246: val_loss did not improve from 402.59970
196/196 - 18s - loss: 409.9126 - MinusLogProbMetric: 409.9126 - val_loss: 410.1801 - val_MinusLogProbMetric: 410.1801 - lr: 5.0000e-04 - 18s/epoch - 94ms/step
Epoch 247/1000
2023-10-02 11:23:43.072 
Epoch 247/1000 
	 loss: 409.4305, MinusLogProbMetric: 409.4305, val_loss: 409.3494, val_MinusLogProbMetric: 409.3494

Epoch 247: val_loss did not improve from 402.59970
196/196 - 17s - loss: 409.4305 - MinusLogProbMetric: 409.4305 - val_loss: 409.3494 - val_MinusLogProbMetric: 409.3494 - lr: 5.0000e-04 - 17s/epoch - 88ms/step
Epoch 248/1000
2023-10-02 11:24:01.366 
Epoch 248/1000 
	 loss: 409.4845, MinusLogProbMetric: 409.4845, val_loss: 411.6792, val_MinusLogProbMetric: 411.6792

Epoch 248: val_loss did not improve from 402.59970
196/196 - 18s - loss: 409.4845 - MinusLogProbMetric: 409.4845 - val_loss: 411.6792 - val_MinusLogProbMetric: 411.6792 - lr: 5.0000e-04 - 18s/epoch - 93ms/step
Epoch 249/1000
2023-10-02 11:24:19.512 
Epoch 249/1000 
	 loss: 409.4911, MinusLogProbMetric: 409.4911, val_loss: 411.3299, val_MinusLogProbMetric: 411.3299

Epoch 249: val_loss did not improve from 402.59970
196/196 - 18s - loss: 409.4911 - MinusLogProbMetric: 409.4911 - val_loss: 411.3299 - val_MinusLogProbMetric: 411.3299 - lr: 5.0000e-04 - 18s/epoch - 93ms/step
Epoch 250/1000
2023-10-02 11:24:38.989 
Epoch 250/1000 
	 loss: 408.6520, MinusLogProbMetric: 408.6520, val_loss: 411.5620, val_MinusLogProbMetric: 411.5620

Epoch 250: val_loss did not improve from 402.59970
196/196 - 19s - loss: 408.6520 - MinusLogProbMetric: 408.6520 - val_loss: 411.5620 - val_MinusLogProbMetric: 411.5620 - lr: 5.0000e-04 - 19s/epoch - 99ms/step
Epoch 251/1000
2023-10-02 11:24:56.043 
Epoch 251/1000 
	 loss: 409.1707, MinusLogProbMetric: 409.1707, val_loss: 409.8032, val_MinusLogProbMetric: 409.8032

Epoch 251: val_loss did not improve from 402.59970
196/196 - 17s - loss: 409.1707 - MinusLogProbMetric: 409.1707 - val_loss: 409.8032 - val_MinusLogProbMetric: 409.8032 - lr: 5.0000e-04 - 17s/epoch - 87ms/step
Epoch 252/1000
2023-10-02 11:25:15.195 
Epoch 252/1000 
	 loss: 408.8934, MinusLogProbMetric: 408.8934, val_loss: 410.4685, val_MinusLogProbMetric: 410.4685

Epoch 252: val_loss did not improve from 402.59970
Restoring model weights from the end of the best epoch: 152.
196/196 - 19s - loss: 408.8934 - MinusLogProbMetric: 408.8934 - val_loss: 410.4685 - val_MinusLogProbMetric: 410.4685 - lr: 5.0000e-04 - 19s/epoch - 99ms/step
Epoch 252: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 4774.293399699964 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 4743.44030448806 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 4535.939234866062 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 4541.057046058937 seconds.
Training succeeded with seed 0.
Model trained in 4788.31 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 18785.39 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 18785.89 s.
===========
Run 323/360 done in 23582.37 s.
===========

Directory ../../results/MAFN_new/run_324/ already exists.
Skipping it.
===========
Run 324/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_325/ already exists.
Skipping it.
===========
Run 325/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_326/ already exists.
Skipping it.
===========
Run 326/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_327/ already exists.
Skipping it.
===========
Run 327/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_328/ already exists.
Skipping it.
===========
Run 328/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_329/ already exists.
Skipping it.
===========
Run 329/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_330/ already exists.
Skipping it.
===========
Run 330/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_331/ already exists.
Skipping it.
===========
Run 331/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_332/ already exists.
Skipping it.
===========
Run 332/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_333/ already exists.
Skipping it.
===========
Run 333/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_334/ already exists.
Skipping it.
===========
Run 334/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_335/ already exists.
Skipping it.
===========
Run 335/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_336/ already exists.
Skipping it.
===========
Run 336/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_337/ already exists.
Skipping it.
===========
Run 337/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_338/ already exists.
Skipping it.
===========
Run 338/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_339/ already exists.
Skipping it.
===========
Run 339/360 already exists. Skipping it.
===========

===========
Generating train data for run 340.
===========
Train data generated in 0.45 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_340/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.1938453 ,  0.533144  ,  4.8263674 , ...,  4.486572  ,
         6.023426  ,  7.1273623 ],
       [ 7.585662  ,  4.7953405 ,  5.2079363 , ...,  2.9872682 ,
         8.320559  ,  6.9911246 ],
       [ 5.966249  , -0.30219892,  4.8852797 , ...,  4.988506  ,
         6.203924  ,  5.582959  ],
       ...,
       [ 7.967994  ,  4.477745  ,  5.2694697 , ...,  2.879139  ,
         7.399321  ,  6.960496  ],
       [ 6.5566854 ,  0.02881634,  4.848876  , ...,  5.0976205 ,
         6.4114294 ,  3.5750432 ],
       [ 8.147113  ,  4.715337  ,  5.2998085 , ...,  4.052641  ,
         8.0347185 ,  6.6445417 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_340/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_340
self.data_kwargs: {'seed': 520}
self.x_data: [[6.0386963  0.30078053 4.582872   ... 4.9010735  5.585139   5.0534396 ]
 [7.9469204  4.5061865  5.2455854  ... 2.9358454  8.744124   6.9367347 ]
 [8.143589   4.4823375  5.2392807  ... 3.0099165  8.623259   7.0103106 ]
 ...
 [8.140467   4.6205688  5.2538304  ... 5.1522036  8.523501   6.461891  ]
 [5.550299   8.4923     5.931979   ... 9.995682   1.7941611  6.8577833 ]
 [7.7436986  5.02466    5.1777496  ... 2.148604   8.593714   7.418009  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7fbb1a19beb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb460bb310>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb460bb310>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb1a19b1c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb22eec970>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb10afceb0>, <keras.callbacks.ModelCheckpoint object at 0x7fbb4615dff0>, <keras.callbacks.EarlyStopping object at 0x7fbb460f9510>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbb4615f070>, <keras.callbacks.TerminateOnNaN object at 0x7fbb10afe170>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.1938453 ,  0.533144  ,  4.8263674 , ...,  4.486572  ,
         6.023426  ,  7.1273623 ],
       [ 7.585662  ,  4.7953405 ,  5.2079363 , ...,  2.9872682 ,
         8.320559  ,  6.9911246 ],
       [ 5.966249  , -0.30219892,  4.8852797 , ...,  4.988506  ,
         6.203924  ,  5.582959  ],
       ...,
       [ 7.967994  ,  4.477745  ,  5.2694697 , ...,  2.879139  ,
         7.399321  ,  6.960496  ],
       [ 6.5566854 ,  0.02881634,  4.848876  , ...,  5.0976205 ,
         6.4114294 ,  3.5750432 ],
       [ 8.147113  ,  4.715337  ,  5.2998085 , ...,  4.052641  ,
         8.0347185 ,  6.6445417 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_340/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 340/360 with hyperparameters:
timestamp = 2023-10-02 16:38:23.860954
ndims = 1000
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.0386963   0.30078053  4.582872    7.656263    0.50402063  8.927606
  5.0806236   0.7069437   3.2612228   9.222077    5.8550296   0.7950525
  3.7572262   5.421181   -0.42896372  5.3715286   5.0170646   4.5275445
  6.268279    8.342493    2.4881694   7.7397876   7.198324   10.337882
  7.1367445   7.992043    3.640705    1.6190335   9.687809   -0.10210621
  4.3673606   0.5702325   9.0355015   9.576009    7.180074    2.9750605
  2.7969558   2.6438603   1.315014    5.2577987   1.5748067   3.07832
  6.905322    7.2643003   0.31143492  3.629692    6.198628    9.935606
  4.738879    9.644313    2.6127305  -0.2701767   4.573748    2.6033232
  8.31127     2.847469    3.1219404   8.036686    8.350981    5.1923842
  7.0816703   3.7841296   0.439943    0.31411228  5.105436    0.16452146
  0.37245148  8.840347    7.522216    2.485342   -0.04555227  9.480491
  3.3716009   2.2168918   5.8693147   1.2792552   2.5388308   9.7780695
  6.3509502   5.2145343   4.668051    7.835037    0.81844634  9.829676
  8.602084    6.4478574   1.8822466   8.708197    9.9343      5.2983065
  4.648863    1.7545191   7.5778584   7.992651    1.2489895   7.932429
  0.19842413 10.670638    6.7138567   5.5415473   2.8593335   8.939356
  1.8772883   8.274395    9.729145    7.7069526   4.83206     0.20179847
  4.5409455   4.72331     2.8598065   4.388705    3.542864    1.3279399
  4.2783537   2.6017168   5.7597165   0.7641455   9.5386305   9.114207
  5.246056    8.810347    9.548426    1.1960151   4.535228    9.936641
 10.11391     6.0469294   1.2320046   5.771403    0.03692677  1.7631103
  9.768336    4.6296864   6.3364687   2.3349156   9.236544    8.197755
  3.2024207   7.716959    2.077003    4.211735    2.1907134   7.0612254
  1.2628212   4.2821627   1.9289484   4.29324     4.816094    7.6030316
  0.34979546  7.054235    2.5239222   2.5584276   7.064572    9.337272
  4.1277614   8.632163    1.1907344   7.2465253   7.3629594   1.4210125
  3.8094866   8.687733    1.3656402   0.28611335  5.895154    4.691248
  7.848699    8.486627    2.7608519   4.164383    5.0772963   6.1176133
 10.264451    6.3248825   3.0036852   2.4301364   9.323013    1.1239512
  6.9529996   8.971375    6.7585683   4.1878514   2.5465474   3.7158139
  7.954454    6.121273    8.889866    8.739019   10.887082    9.803803
  2.4961972   3.7658646   4.353828    5.831876    6.706101    2.2424636
  0.4813192   4.0677247   2.7481353   8.85868     8.81974     8.779158
  6.5635395   3.1331716   5.4378457   8.115037    0.15877736  7.7527685
 -0.23714772  9.2626095   7.987624    0.7620828   6.501852    3.2839687
  1.7200415   1.1543069   7.17581     5.3241725   9.85193    10.548408
 10.603701    2.2370188   9.374007    3.4676795   4.4167695   7.9351892
  2.2672663   5.607824    2.2299185   0.88661635  1.2501802   4.0727944
 -0.12534815  4.24075     6.864432    4.1316953   0.98898524  1.0468978
  2.6923618   1.5284702   9.68549     2.6858997   8.196742    5.9775615
  6.107674    3.8373623   9.739576    3.8176546   7.881323    5.9619036
  4.809063    9.994005    6.06644     0.60921174  5.0061116   8.33161
  7.0547867   8.514869    2.0673354   7.7308583   4.0995107   5.0039496
 10.610853    9.848376    8.120186    4.446167    8.818996    6.304151
  3.6133528   0.34783545  8.203926    4.955636    8.081435    7.584287
  7.8737516   3.389564    7.996908    6.0786796   4.5982037   7.0935416
  0.92889404  1.281023    2.9661057   9.792753    1.4344769   3.2898872
  1.1154623   3.3197548   4.133613    9.714192    7.8477564   3.915498
 -0.54802597  8.3167515   6.381558    1.8454669  11.718771    1.0781447
  8.230542    2.993074    4.4022226   7.81114     6.1109514   8.33177
  6.2205343   0.9577046   9.167438    7.7908244   7.120584    1.3328398
  3.1939583   1.6910427   8.12919     6.4541216   7.9364567  10.26773
  7.809719   10.104616    6.0293574   0.40289783 -0.560338    3.7847402
  7.9600444   9.035484   10.840271    1.0387208  -0.19058862  8.854946
  3.0963922   5.0594854  10.279567    5.074058    6.6726894   3.9883325
  7.652588    4.496253    3.0499647   2.3702488   2.5340867   3.782253
  6.6244855   9.641262    7.137413    6.8682413   1.8626316   4.317209
  4.4707828   6.841525    2.992224    2.4876325   3.3212593   7.333737
  5.75945     0.32461804  0.8335988   3.4026284   8.618828    6.705712
  6.8725452   7.33386     2.8872874   5.3803415   5.1434884   6.246975
  9.339241    0.6288371   8.414895    6.5827723   5.714417    7.4871626
  9.453507    0.7600125   1.494343    7.271453   -0.10680145  4.284081
  3.0645723   2.7818801   6.7562222   0.9355258   8.759254    4.4485765
  4.9456024   5.2843823   6.5932875   3.4578905   0.32452983  3.0972865
  0.45848465  5.198651    2.197647    9.560289    9.731855    6.5005336
  6.619211   10.057005    8.876658    8.295638    3.1194568   3.6742027
  2.26406     5.3983374   9.887169    9.133878    1.5052447   1.7451565
  8.9965925   5.319445    2.527405    6.8142905   6.4233      1.2479438
  4.035429    0.24714679  2.1173966   1.830858    1.9078584   7.0054536
  0.5009528   0.8397763   2.0496254   2.9811964   2.9665575   3.3201675
  7.2900653   9.622518    4.901422    7.8189707   3.9295015   0.91512156
  4.572243    3.2736297   4.8408017   7.065958    4.651118    9.0410385
  8.961053    5.5870433   5.2621417   1.1291223   3.93055     6.9391193
  3.508282    0.4460107   9.179407    7.387717    4.650465    8.17948
  7.53271     5.0031796   8.804058    9.204221    3.6460009   7.006398
  5.8495746   5.197357    9.277365    2.9547234  10.355085    8.981658
  5.367272    8.041138    7.377086    7.5142937   4.0544214   2.2740157
  6.749342    8.733181   -1.1557603   6.3426285   4.84659     8.766928
  2.8006268   4.1529183   4.987336    0.13959682  6.5040164   8.188027
  1.676772    2.4030275   0.5443996   9.24266     5.4919004   9.14114
  5.4362607   7.7436833   3.4114141   2.927477    3.843325    6.4828835
  2.9296427   9.1742115   8.056144   -0.09604371  2.0134284   1.6318083
  0.04543668  2.690591    4.990765    8.81346     2.1004107   4.4873705
 10.513251    7.137761    7.6034775   5.209668    9.811113    5.6359825
  0.19526497  2.564422    4.0089235   6.4994254   7.695936    4.1304693
  7.2958503   8.454953    2.967984    9.869264    7.2374787   8.593143
  5.7742076   7.5300508   7.2095394   7.823381    2.118999    5.039945
  4.053479    9.185997    0.9152442  10.862353    8.841984    0.75618994
  4.901044    7.2554336   7.1853967   9.084257    8.29529     4.220908
  5.8150444   7.7635503   1.2401824   1.3507668   4.54757     3.457343
  7.9834795   5.028612    3.85299     8.574433    8.163936   10.616821
  8.4081      4.1708117   1.2379287   8.800616    1.4772482   4.080577
  8.428701   10.380288    6.389518    6.8510385   0.61914694  4.0696154
  6.8948746   3.383478    0.4705223   3.009568    2.6594605   3.048335
  5.988357    7.1357174   3.261515    4.0008183   0.27625528 -0.3875654
  2.6110508   6.7222776   4.8376966   7.503794    6.6424117   8.686342
  7.1706657   9.419308    7.926278    6.2358265   6.0555067  -0.70361733
 10.064413    1.4912657   8.07811     7.900392    4.8439436   2.0139859
  1.0714337   7.2137423   4.526915    6.6356516   9.933811    1.5130765
  9.312532    3.7438834   3.550579    9.596389    9.815127    1.7642053
  7.4918814  -0.49520278  6.4749107   9.756271    7.145049    4.039104
  3.7851322   4.082335    6.1268287   8.183756    9.32123     0.28198594
  2.5236871  -0.31258178  2.2504802   3.5232162   4.9025984   6.3538814
  7.8291597   9.178961   10.243786    7.067563    8.285971    9.500681
  3.5682902   8.450219    0.17376387  0.216589    2.7681222   4.1601896
  6.1601834   1.706507    6.643297    7.9561872   1.7723906   4.5958123
  5.858561    4.6678243   8.187405    9.715012    8.1015      0.45350024
  7.7912116   0.77989995  2.3104167   1.3162912   1.205177    5.248102
  6.7170506   5.558635    9.548942    2.291232    7.7552695   8.649344
  5.677332    4.394188    6.64837     4.7241607   5.719767    3.967581
  4.259147    7.17209     1.0130833   1.7905484   8.393191    7.4030704
  2.735002    2.5851526   9.099808    7.8557224   6.1393533   2.1640992
  0.7774042   4.113809    6.8772874   9.341514    7.624882   -0.15643695
  2.3431437   3.3053102   1.6893227   7.5518737   7.47631     5.387754
  9.710713    1.6661869   8.289583    1.1974262   3.0808344   7.3471923
  0.8228001   5.6459312   3.6627483   7.2736874   8.484636    7.5182805
  6.5065517   0.4700985   2.234543    5.2005877   3.5432315   2.8302953
  2.0971265   9.56212     3.0628858   4.7415     -0.44130558  9.013574
  1.085037    3.311532    2.5385356   8.806095    2.0905962   7.8503737
  8.578108    9.528165    0.590898    1.9645467   7.9985986   1.753687
  3.172571    1.6611466   1.8709267   4.7864184   1.3145969   3.1479964
  4.083562    0.2390917   3.327033    0.9105044   6.7123947   7.861191
  7.02552     0.75634694  5.9596577   8.067755    2.4828937   7.6143017
  8.580062    7.3855762  -0.04546678  0.0738206   1.8115419   2.7274795
  6.6129613   3.8759396   6.692868    3.7261245   1.3076189   3.1666129
  9.223293    9.462051    2.376804    7.5547466   2.749257    8.444417
  1.3476957   6.958887    0.12667698  9.694718    6.407499    8.479325
  5.627874    4.950144    6.2193155   5.3255057   4.9456944   8.290707
  0.8348843   2.2261314   4.1954646   3.6872017   0.73160475  8.504263
  7.791428    5.3747725   9.8119135   0.7039877   7.6769605   2.5379546
  3.694837    2.045665    7.2654924   0.12175488  7.247275    7.029465
  0.6658832   8.352133    5.5662146   9.030525    7.8712974   5.6255913
  8.1955      3.5710793   4.832326    5.828453   10.745017    1.85231
 -0.526502    3.3114655   4.8024893   0.62556016  4.9190855   9.197764
  6.527681    8.61912     9.540909    9.433219    7.9817605   5.9839487
  4.9048758   0.6134869   5.948723   10.432518    8.046861    3.3048022
  1.5137587   7.991438    7.427192    9.765905    8.476495    5.9867887
  1.0616344   5.6400924   2.9184952   5.1216874   2.580817    8.646928
  7.5732      4.096987    6.089647    4.3319507   8.435398    2.1281555
  7.02788     9.129615    1.7530978   1.3687971   7.042738    7.6120076
  4.263667    6.845998    3.1399002   4.1320887   1.0887251   3.7039056
  2.7662733   3.7930636   9.0424595   1.3231362   3.5029469   6.702694
  9.477917    0.9764585   9.146437    0.77568066  5.0711017   4.2204366
  1.9013925   3.6039436   4.875466    8.647586    9.651502    6.2052207
  8.608539    6.7567143   3.158669    1.8811914   4.3497324   5.161394
  0.6098187   1.152159    6.051886    1.3697395   8.290108    2.6722713
  2.2818537   0.9584924   9.105338    1.0756454   1.5344902   8.241914
  2.561514    6.2516336   7.197662    6.6100316   8.612244    7.1160245
  1.4579626   5.4742584   6.604126    8.5800705   0.06284574  7.271172
  9.004775    7.700049    5.588264    4.5855756   4.7795944   7.2477436
  8.905804    6.081058    9.249524    7.4357276   0.8587049   4.7164693
  2.8504999   3.989924    3.8770769   2.9917765   5.714236    6.905015
  2.6859546   1.846874    0.41534102  4.812243    1.8113589  -0.9216505
  9.0558815   4.663535    1.0859932   8.355898    7.844323    4.0918174
  0.6433816   1.5224574   4.468763    8.928121    1.204919    8.635436
 -0.03399917  4.6824536   6.482672    0.5008905   5.682656    8.837162
  2.072514    7.1392884   6.1446853   2.1056101   0.07073235  0.26507834
  5.238253    9.371503    6.251286    7.188611    5.0308127   6.007279
  1.024926    8.809044    7.712478   10.019146    2.5657964   3.195103
  0.9103755   9.777725    7.5711966   3.4351816   5.4496226   4.112291
  1.660826    1.7369623   9.639072    2.935317    9.930944    0.73487484
  5.733989    1.3034487   0.502199    5.5747876   1.0674012   8.124411
  7.0287094   2.9280264   9.545342    7.30834     2.6214435   7.561452
  7.846946    1.3374016   3.65282     3.05842     6.171471    8.9460125
  1.6092017   8.305956   -0.07000089  6.6192155   4.523222    7.941383
  7.4171886   4.9010735   5.585139    5.0534396 ]
Epoch 1/1000
2023-10-02 16:39:01.015 
Epoch 1/1000 
	 loss: 1447.9357, MinusLogProbMetric: 1447.9357, val_loss: 711.7483, val_MinusLogProbMetric: 711.7483

Epoch 1: val_loss improved from inf to 711.74829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 1447.9357 - MinusLogProbMetric: 1447.9357 - val_loss: 711.7483 - val_MinusLogProbMetric: 711.7483 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 16:39:03.333 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 340203156925317120.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 711.74829
196/196 - 2s - loss: nan - MinusLogProbMetric: 340203156925317120.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 2s/epoch - 8ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 340.
===========
Train data generated in 0.68 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_340/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.1938453 ,  0.533144  ,  4.8263674 , ...,  4.486572  ,
         6.023426  ,  7.1273623 ],
       [ 7.585662  ,  4.7953405 ,  5.2079363 , ...,  2.9872682 ,
         8.320559  ,  6.9911246 ],
       [ 5.966249  , -0.30219892,  4.8852797 , ...,  4.988506  ,
         6.203924  ,  5.582959  ],
       ...,
       [ 7.967994  ,  4.477745  ,  5.2694697 , ...,  2.879139  ,
         7.399321  ,  6.960496  ],
       [ 6.5566854 ,  0.02881634,  4.848876  , ...,  5.0976205 ,
         6.4114294 ,  3.5750432 ],
       [ 8.147113  ,  4.715337  ,  5.2998085 , ...,  4.052641  ,
         8.0347185 ,  6.6445417 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_340/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_340
self.data_kwargs: {'seed': 520}
self.x_data: [[6.0386963  0.30078053 4.582872   ... 4.9010735  5.585139   5.0534396 ]
 [7.9469204  4.5061865  5.2455854  ... 2.9358454  8.744124   6.9367347 ]
 [8.143589   4.4823375  5.2392807  ... 3.0099165  8.623259   7.0103106 ]
 ...
 [8.140467   4.6205688  5.2538304  ... 5.1522036  8.523501   6.461891  ]
 [5.550299   8.4923     5.931979   ... 9.995682   1.7941611  6.8577833 ]
 [7.7436986  5.02466    5.1777496  ... 2.148604   8.593714   7.418009  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7fbb2c1cfee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb2c172d70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb2c172d70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb349c1b70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb4623b700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb461d3eb0>, <keras.callbacks.ModelCheckpoint object at 0x7fbb461d3f70>, <keras.callbacks.EarlyStopping object at 0x7fbb461d3e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbb461d3e50>, <keras.callbacks.TerminateOnNaN object at 0x7fbb46208220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.1938453 ,  0.533144  ,  4.8263674 , ...,  4.486572  ,
         6.023426  ,  7.1273623 ],
       [ 7.585662  ,  4.7953405 ,  5.2079363 , ...,  2.9872682 ,
         8.320559  ,  6.9911246 ],
       [ 5.966249  , -0.30219892,  4.8852797 , ...,  4.988506  ,
         6.203924  ,  5.582959  ],
       ...,
       [ 7.967994  ,  4.477745  ,  5.2694697 , ...,  2.879139  ,
         7.399321  ,  6.960496  ],
       [ 6.5566854 ,  0.02881634,  4.848876  , ...,  5.0976205 ,
         6.4114294 ,  3.5750432 ],
       [ 8.147113  ,  4.715337  ,  5.2998085 , ...,  4.052641  ,
         8.0347185 ,  6.6445417 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 340/360 with hyperparameters:
timestamp = 2023-10-02 16:39:06.176793
ndims = 1000
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.0386963   0.30078053  4.582872    7.656263    0.50402063  8.927606
  5.0806236   0.7069437   3.2612228   9.222077    5.8550296   0.7950525
  3.7572262   5.421181   -0.42896372  5.3715286   5.0170646   4.5275445
  6.268279    8.342493    2.4881694   7.7397876   7.198324   10.337882
  7.1367445   7.992043    3.640705    1.6190335   9.687809   -0.10210621
  4.3673606   0.5702325   9.0355015   9.576009    7.180074    2.9750605
  2.7969558   2.6438603   1.315014    5.2577987   1.5748067   3.07832
  6.905322    7.2643003   0.31143492  3.629692    6.198628    9.935606
  4.738879    9.644313    2.6127305  -0.2701767   4.573748    2.6033232
  8.31127     2.847469    3.1219404   8.036686    8.350981    5.1923842
  7.0816703   3.7841296   0.439943    0.31411228  5.105436    0.16452146
  0.37245148  8.840347    7.522216    2.485342   -0.04555227  9.480491
  3.3716009   2.2168918   5.8693147   1.2792552   2.5388308   9.7780695
  6.3509502   5.2145343   4.668051    7.835037    0.81844634  9.829676
  8.602084    6.4478574   1.8822466   8.708197    9.9343      5.2983065
  4.648863    1.7545191   7.5778584   7.992651    1.2489895   7.932429
  0.19842413 10.670638    6.7138567   5.5415473   2.8593335   8.939356
  1.8772883   8.274395    9.729145    7.7069526   4.83206     0.20179847
  4.5409455   4.72331     2.8598065   4.388705    3.542864    1.3279399
  4.2783537   2.6017168   5.7597165   0.7641455   9.5386305   9.114207
  5.246056    8.810347    9.548426    1.1960151   4.535228    9.936641
 10.11391     6.0469294   1.2320046   5.771403    0.03692677  1.7631103
  9.768336    4.6296864   6.3364687   2.3349156   9.236544    8.197755
  3.2024207   7.716959    2.077003    4.211735    2.1907134   7.0612254
  1.2628212   4.2821627   1.9289484   4.29324     4.816094    7.6030316
  0.34979546  7.054235    2.5239222   2.5584276   7.064572    9.337272
  4.1277614   8.632163    1.1907344   7.2465253   7.3629594   1.4210125
  3.8094866   8.687733    1.3656402   0.28611335  5.895154    4.691248
  7.848699    8.486627    2.7608519   4.164383    5.0772963   6.1176133
 10.264451    6.3248825   3.0036852   2.4301364   9.323013    1.1239512
  6.9529996   8.971375    6.7585683   4.1878514   2.5465474   3.7158139
  7.954454    6.121273    8.889866    8.739019   10.887082    9.803803
  2.4961972   3.7658646   4.353828    5.831876    6.706101    2.2424636
  0.4813192   4.0677247   2.7481353   8.85868     8.81974     8.779158
  6.5635395   3.1331716   5.4378457   8.115037    0.15877736  7.7527685
 -0.23714772  9.2626095   7.987624    0.7620828   6.501852    3.2839687
  1.7200415   1.1543069   7.17581     5.3241725   9.85193    10.548408
 10.603701    2.2370188   9.374007    3.4676795   4.4167695   7.9351892
  2.2672663   5.607824    2.2299185   0.88661635  1.2501802   4.0727944
 -0.12534815  4.24075     6.864432    4.1316953   0.98898524  1.0468978
  2.6923618   1.5284702   9.68549     2.6858997   8.196742    5.9775615
  6.107674    3.8373623   9.739576    3.8176546   7.881323    5.9619036
  4.809063    9.994005    6.06644     0.60921174  5.0061116   8.33161
  7.0547867   8.514869    2.0673354   7.7308583   4.0995107   5.0039496
 10.610853    9.848376    8.120186    4.446167    8.818996    6.304151
  3.6133528   0.34783545  8.203926    4.955636    8.081435    7.584287
  7.8737516   3.389564    7.996908    6.0786796   4.5982037   7.0935416
  0.92889404  1.281023    2.9661057   9.792753    1.4344769   3.2898872
  1.1154623   3.3197548   4.133613    9.714192    7.8477564   3.915498
 -0.54802597  8.3167515   6.381558    1.8454669  11.718771    1.0781447
  8.230542    2.993074    4.4022226   7.81114     6.1109514   8.33177
  6.2205343   0.9577046   9.167438    7.7908244   7.120584    1.3328398
  3.1939583   1.6910427   8.12919     6.4541216   7.9364567  10.26773
  7.809719   10.104616    6.0293574   0.40289783 -0.560338    3.7847402
  7.9600444   9.035484   10.840271    1.0387208  -0.19058862  8.854946
  3.0963922   5.0594854  10.279567    5.074058    6.6726894   3.9883325
  7.652588    4.496253    3.0499647   2.3702488   2.5340867   3.782253
  6.6244855   9.641262    7.137413    6.8682413   1.8626316   4.317209
  4.4707828   6.841525    2.992224    2.4876325   3.3212593   7.333737
  5.75945     0.32461804  0.8335988   3.4026284   8.618828    6.705712
  6.8725452   7.33386     2.8872874   5.3803415   5.1434884   6.246975
  9.339241    0.6288371   8.414895    6.5827723   5.714417    7.4871626
  9.453507    0.7600125   1.494343    7.271453   -0.10680145  4.284081
  3.0645723   2.7818801   6.7562222   0.9355258   8.759254    4.4485765
  4.9456024   5.2843823   6.5932875   3.4578905   0.32452983  3.0972865
  0.45848465  5.198651    2.197647    9.560289    9.731855    6.5005336
  6.619211   10.057005    8.876658    8.295638    3.1194568   3.6742027
  2.26406     5.3983374   9.887169    9.133878    1.5052447   1.7451565
  8.9965925   5.319445    2.527405    6.8142905   6.4233      1.2479438
  4.035429    0.24714679  2.1173966   1.830858    1.9078584   7.0054536
  0.5009528   0.8397763   2.0496254   2.9811964   2.9665575   3.3201675
  7.2900653   9.622518    4.901422    7.8189707   3.9295015   0.91512156
  4.572243    3.2736297   4.8408017   7.065958    4.651118    9.0410385
  8.961053    5.5870433   5.2621417   1.1291223   3.93055     6.9391193
  3.508282    0.4460107   9.179407    7.387717    4.650465    8.17948
  7.53271     5.0031796   8.804058    9.204221    3.6460009   7.006398
  5.8495746   5.197357    9.277365    2.9547234  10.355085    8.981658
  5.367272    8.041138    7.377086    7.5142937   4.0544214   2.2740157
  6.749342    8.733181   -1.1557603   6.3426285   4.84659     8.766928
  2.8006268   4.1529183   4.987336    0.13959682  6.5040164   8.188027
  1.676772    2.4030275   0.5443996   9.24266     5.4919004   9.14114
  5.4362607   7.7436833   3.4114141   2.927477    3.843325    6.4828835
  2.9296427   9.1742115   8.056144   -0.09604371  2.0134284   1.6318083
  0.04543668  2.690591    4.990765    8.81346     2.1004107   4.4873705
 10.513251    7.137761    7.6034775   5.209668    9.811113    5.6359825
  0.19526497  2.564422    4.0089235   6.4994254   7.695936    4.1304693
  7.2958503   8.454953    2.967984    9.869264    7.2374787   8.593143
  5.7742076   7.5300508   7.2095394   7.823381    2.118999    5.039945
  4.053479    9.185997    0.9152442  10.862353    8.841984    0.75618994
  4.901044    7.2554336   7.1853967   9.084257    8.29529     4.220908
  5.8150444   7.7635503   1.2401824   1.3507668   4.54757     3.457343
  7.9834795   5.028612    3.85299     8.574433    8.163936   10.616821
  8.4081      4.1708117   1.2379287   8.800616    1.4772482   4.080577
  8.428701   10.380288    6.389518    6.8510385   0.61914694  4.0696154
  6.8948746   3.383478    0.4705223   3.009568    2.6594605   3.048335
  5.988357    7.1357174   3.261515    4.0008183   0.27625528 -0.3875654
  2.6110508   6.7222776   4.8376966   7.503794    6.6424117   8.686342
  7.1706657   9.419308    7.926278    6.2358265   6.0555067  -0.70361733
 10.064413    1.4912657   8.07811     7.900392    4.8439436   2.0139859
  1.0714337   7.2137423   4.526915    6.6356516   9.933811    1.5130765
  9.312532    3.7438834   3.550579    9.596389    9.815127    1.7642053
  7.4918814  -0.49520278  6.4749107   9.756271    7.145049    4.039104
  3.7851322   4.082335    6.1268287   8.183756    9.32123     0.28198594
  2.5236871  -0.31258178  2.2504802   3.5232162   4.9025984   6.3538814
  7.8291597   9.178961   10.243786    7.067563    8.285971    9.500681
  3.5682902   8.450219    0.17376387  0.216589    2.7681222   4.1601896
  6.1601834   1.706507    6.643297    7.9561872   1.7723906   4.5958123
  5.858561    4.6678243   8.187405    9.715012    8.1015      0.45350024
  7.7912116   0.77989995  2.3104167   1.3162912   1.205177    5.248102
  6.7170506   5.558635    9.548942    2.291232    7.7552695   8.649344
  5.677332    4.394188    6.64837     4.7241607   5.719767    3.967581
  4.259147    7.17209     1.0130833   1.7905484   8.393191    7.4030704
  2.735002    2.5851526   9.099808    7.8557224   6.1393533   2.1640992
  0.7774042   4.113809    6.8772874   9.341514    7.624882   -0.15643695
  2.3431437   3.3053102   1.6893227   7.5518737   7.47631     5.387754
  9.710713    1.6661869   8.289583    1.1974262   3.0808344   7.3471923
  0.8228001   5.6459312   3.6627483   7.2736874   8.484636    7.5182805
  6.5065517   0.4700985   2.234543    5.2005877   3.5432315   2.8302953
  2.0971265   9.56212     3.0628858   4.7415     -0.44130558  9.013574
  1.085037    3.311532    2.5385356   8.806095    2.0905962   7.8503737
  8.578108    9.528165    0.590898    1.9645467   7.9985986   1.753687
  3.172571    1.6611466   1.8709267   4.7864184   1.3145969   3.1479964
  4.083562    0.2390917   3.327033    0.9105044   6.7123947   7.861191
  7.02552     0.75634694  5.9596577   8.067755    2.4828937   7.6143017
  8.580062    7.3855762  -0.04546678  0.0738206   1.8115419   2.7274795
  6.6129613   3.8759396   6.692868    3.7261245   1.3076189   3.1666129
  9.223293    9.462051    2.376804    7.5547466   2.749257    8.444417
  1.3476957   6.958887    0.12667698  9.694718    6.407499    8.479325
  5.627874    4.950144    6.2193155   5.3255057   4.9456944   8.290707
  0.8348843   2.2261314   4.1954646   3.6872017   0.73160475  8.504263
  7.791428    5.3747725   9.8119135   0.7039877   7.6769605   2.5379546
  3.694837    2.045665    7.2654924   0.12175488  7.247275    7.029465
  0.6658832   8.352133    5.5662146   9.030525    7.8712974   5.6255913
  8.1955      3.5710793   4.832326    5.828453   10.745017    1.85231
 -0.526502    3.3114655   4.8024893   0.62556016  4.9190855   9.197764
  6.527681    8.61912     9.540909    9.433219    7.9817605   5.9839487
  4.9048758   0.6134869   5.948723   10.432518    8.046861    3.3048022
  1.5137587   7.991438    7.427192    9.765905    8.476495    5.9867887
  1.0616344   5.6400924   2.9184952   5.1216874   2.580817    8.646928
  7.5732      4.096987    6.089647    4.3319507   8.435398    2.1281555
  7.02788     9.129615    1.7530978   1.3687971   7.042738    7.6120076
  4.263667    6.845998    3.1399002   4.1320887   1.0887251   3.7039056
  2.7662733   3.7930636   9.0424595   1.3231362   3.5029469   6.702694
  9.477917    0.9764585   9.146437    0.77568066  5.0711017   4.2204366
  1.9013925   3.6039436   4.875466    8.647586    9.651502    6.2052207
  8.608539    6.7567143   3.158669    1.8811914   4.3497324   5.161394
  0.6098187   1.152159    6.051886    1.3697395   8.290108    2.6722713
  2.2818537   0.9584924   9.105338    1.0756454   1.5344902   8.241914
  2.561514    6.2516336   7.197662    6.6100316   8.612244    7.1160245
  1.4579626   5.4742584   6.604126    8.5800705   0.06284574  7.271172
  9.004775    7.700049    5.588264    4.5855756   4.7795944   7.2477436
  8.905804    6.081058    9.249524    7.4357276   0.8587049   4.7164693
  2.8504999   3.989924    3.8770769   2.9917765   5.714236    6.905015
  2.6859546   1.846874    0.41534102  4.812243    1.8113589  -0.9216505
  9.0558815   4.663535    1.0859932   8.355898    7.844323    4.0918174
  0.6433816   1.5224574   4.468763    8.928121    1.204919    8.635436
 -0.03399917  4.6824536   6.482672    0.5008905   5.682656    8.837162
  2.072514    7.1392884   6.1446853   2.1056101   0.07073235  0.26507834
  5.238253    9.371503    6.251286    7.188611    5.0308127   6.007279
  1.024926    8.809044    7.712478   10.019146    2.5657964   3.195103
  0.9103755   9.777725    7.5711966   3.4351816   5.4496226   4.112291
  1.660826    1.7369623   9.639072    2.935317    9.930944    0.73487484
  5.733989    1.3034487   0.502199    5.5747876   1.0674012   8.124411
  7.0287094   2.9280264   9.545342    7.30834     2.6214435   7.561452
  7.846946    1.3374016   3.65282     3.05842     6.171471    8.9460125
  1.6092017   8.305956   -0.07000089  6.6192155   4.523222    7.941383
  7.4171886   4.9010735   5.585139    5.0534396 ]
Epoch 1/1000
2023-10-02 16:39:43.496 
Epoch 1/1000 
	 loss: 806.8372, MinusLogProbMetric: 806.8372, val_loss: 546.4225, val_MinusLogProbMetric: 546.4225

Epoch 1: val_loss improved from inf to 546.42255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 806.8372 - MinusLogProbMetric: 806.8372 - val_loss: 546.4225 - val_MinusLogProbMetric: 546.4225 - lr: 3.3333e-04 - 38s/epoch - 191ms/step
Epoch 2/1000
2023-10-02 16:39:54.561 
Epoch 2/1000 
	 loss: 530.0914, MinusLogProbMetric: 530.0914, val_loss: 518.3823, val_MinusLogProbMetric: 518.3823

Epoch 2: val_loss improved from 546.42255 to 518.38232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 530.0914 - MinusLogProbMetric: 530.0914 - val_loss: 518.3823 - val_MinusLogProbMetric: 518.3823 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 3/1000
2023-10-02 16:40:05.488 
Epoch 3/1000 
	 loss: 507.4824, MinusLogProbMetric: 507.4824, val_loss: 499.4232, val_MinusLogProbMetric: 499.4232

Epoch 3: val_loss improved from 518.38232 to 499.42319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 507.4824 - MinusLogProbMetric: 507.4824 - val_loss: 499.4232 - val_MinusLogProbMetric: 499.4232 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 4/1000
2023-10-02 16:40:16.228 
Epoch 4/1000 
	 loss: 491.7440, MinusLogProbMetric: 491.7440, val_loss: 491.0204, val_MinusLogProbMetric: 491.0204

Epoch 4: val_loss improved from 499.42319 to 491.02039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 491.7440 - MinusLogProbMetric: 491.7440 - val_loss: 491.0204 - val_MinusLogProbMetric: 491.0204 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 5/1000
2023-10-02 16:40:27.078 
Epoch 5/1000 
	 loss: 480.0898, MinusLogProbMetric: 480.0898, val_loss: 476.3973, val_MinusLogProbMetric: 476.3973

Epoch 5: val_loss improved from 491.02039 to 476.39728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 480.0898 - MinusLogProbMetric: 480.0898 - val_loss: 476.3973 - val_MinusLogProbMetric: 476.3973 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 6/1000
2023-10-02 16:40:37.994 
Epoch 6/1000 
	 loss: 470.8200, MinusLogProbMetric: 470.8200, val_loss: 467.7334, val_MinusLogProbMetric: 467.7334

Epoch 6: val_loss improved from 476.39728 to 467.73340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 470.8200 - MinusLogProbMetric: 470.8200 - val_loss: 467.7334 - val_MinusLogProbMetric: 467.7334 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 7/1000
2023-10-02 16:40:48.753 
Epoch 7/1000 
	 loss: 463.9480, MinusLogProbMetric: 463.9480, val_loss: 461.4374, val_MinusLogProbMetric: 461.4374

Epoch 7: val_loss improved from 467.73340 to 461.43738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 463.9480 - MinusLogProbMetric: 463.9480 - val_loss: 461.4374 - val_MinusLogProbMetric: 461.4374 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 8/1000
2023-10-02 16:40:59.582 
Epoch 8/1000 
	 loss: 458.0554, MinusLogProbMetric: 458.0554, val_loss: 457.2517, val_MinusLogProbMetric: 457.2517

Epoch 8: val_loss improved from 461.43738 to 457.25168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 458.0554 - MinusLogProbMetric: 458.0554 - val_loss: 457.2517 - val_MinusLogProbMetric: 457.2517 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 9/1000
2023-10-02 16:41:10.471 
Epoch 9/1000 
	 loss: 453.8916, MinusLogProbMetric: 453.8916, val_loss: 453.0792, val_MinusLogProbMetric: 453.0792

Epoch 9: val_loss improved from 457.25168 to 453.07916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 453.8916 - MinusLogProbMetric: 453.8916 - val_loss: 453.0792 - val_MinusLogProbMetric: 453.0792 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 10/1000
2023-10-02 16:41:21.363 
Epoch 10/1000 
	 loss: 449.7281, MinusLogProbMetric: 449.7281, val_loss: 451.8439, val_MinusLogProbMetric: 451.8439

Epoch 10: val_loss improved from 453.07916 to 451.84390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 449.7281 - MinusLogProbMetric: 449.7281 - val_loss: 451.8439 - val_MinusLogProbMetric: 451.8439 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 11/1000
2023-10-02 16:41:32.292 
Epoch 11/1000 
	 loss: 446.7896, MinusLogProbMetric: 446.7896, val_loss: 446.9169, val_MinusLogProbMetric: 446.9169

Epoch 11: val_loss improved from 451.84390 to 446.91687, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 446.7896 - MinusLogProbMetric: 446.7896 - val_loss: 446.9169 - val_MinusLogProbMetric: 446.9169 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 12/1000
2023-10-02 16:41:43.041 
Epoch 12/1000 
	 loss: 443.6195, MinusLogProbMetric: 443.6195, val_loss: 441.5690, val_MinusLogProbMetric: 441.5690

Epoch 12: val_loss improved from 446.91687 to 441.56897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 443.6195 - MinusLogProbMetric: 443.6195 - val_loss: 441.5690 - val_MinusLogProbMetric: 441.5690 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 13/1000
2023-10-02 16:41:53.781 
Epoch 13/1000 
	 loss: 440.3840, MinusLogProbMetric: 440.3840, val_loss: 441.8072, val_MinusLogProbMetric: 441.8072

Epoch 13: val_loss did not improve from 441.56897
196/196 - 10s - loss: 440.3840 - MinusLogProbMetric: 440.3840 - val_loss: 441.8072 - val_MinusLogProbMetric: 441.8072 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 14/1000
2023-10-02 16:42:04.240 
Epoch 14/1000 
	 loss: 439.5542, MinusLogProbMetric: 439.5542, val_loss: 439.2930, val_MinusLogProbMetric: 439.2930

Epoch 14: val_loss improved from 441.56897 to 439.29300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 439.5542 - MinusLogProbMetric: 439.5542 - val_loss: 439.2930 - val_MinusLogProbMetric: 439.2930 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 15/1000
2023-10-02 16:42:15.037 
Epoch 15/1000 
	 loss: 439.5670, MinusLogProbMetric: 439.5670, val_loss: 435.7445, val_MinusLogProbMetric: 435.7445

Epoch 15: val_loss improved from 439.29300 to 435.74451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 439.5670 - MinusLogProbMetric: 439.5670 - val_loss: 435.7445 - val_MinusLogProbMetric: 435.7445 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 16/1000
2023-10-02 16:42:26.487 
Epoch 16/1000 
	 loss: 434.6680, MinusLogProbMetric: 434.6680, val_loss: 444.1426, val_MinusLogProbMetric: 444.1426

Epoch 16: val_loss did not improve from 435.74451
196/196 - 11s - loss: 434.6680 - MinusLogProbMetric: 434.6680 - val_loss: 444.1426 - val_MinusLogProbMetric: 444.1426 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 17/1000
2023-10-02 16:42:37.005 
Epoch 17/1000 
	 loss: 433.4538, MinusLogProbMetric: 433.4538, val_loss: 434.3801, val_MinusLogProbMetric: 434.3801

Epoch 17: val_loss improved from 435.74451 to 434.38013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 433.4538 - MinusLogProbMetric: 433.4538 - val_loss: 434.3801 - val_MinusLogProbMetric: 434.3801 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 18/1000
2023-10-02 16:42:47.829 
Epoch 18/1000 
	 loss: 432.2787, MinusLogProbMetric: 432.2787, val_loss: 434.2334, val_MinusLogProbMetric: 434.2334

Epoch 18: val_loss improved from 434.38013 to 434.23340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 432.2787 - MinusLogProbMetric: 432.2787 - val_loss: 434.2334 - val_MinusLogProbMetric: 434.2334 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 19/1000
2023-10-02 16:42:58.509 
Epoch 19/1000 
	 loss: 430.6393, MinusLogProbMetric: 430.6393, val_loss: 434.9710, val_MinusLogProbMetric: 434.9710

Epoch 19: val_loss did not improve from 434.23340
196/196 - 10s - loss: 430.6393 - MinusLogProbMetric: 430.6393 - val_loss: 434.9710 - val_MinusLogProbMetric: 434.9710 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 20/1000
2023-10-02 16:43:08.827 
Epoch 20/1000 
	 loss: 429.6670, MinusLogProbMetric: 429.6670, val_loss: 429.1823, val_MinusLogProbMetric: 429.1823

Epoch 20: val_loss improved from 434.23340 to 429.18234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 429.6670 - MinusLogProbMetric: 429.6670 - val_loss: 429.1823 - val_MinusLogProbMetric: 429.1823 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 21/1000
2023-10-02 16:43:19.801 
Epoch 21/1000 
	 loss: 427.7974, MinusLogProbMetric: 427.7974, val_loss: 428.6594, val_MinusLogProbMetric: 428.6594

Epoch 21: val_loss improved from 429.18234 to 428.65939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 427.7974 - MinusLogProbMetric: 427.7974 - val_loss: 428.6594 - val_MinusLogProbMetric: 428.6594 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 22/1000
2023-10-02 16:43:30.434 
Epoch 22/1000 
	 loss: 427.8404, MinusLogProbMetric: 427.8404, val_loss: 430.0319, val_MinusLogProbMetric: 430.0319

Epoch 22: val_loss did not improve from 428.65939
196/196 - 10s - loss: 427.8404 - MinusLogProbMetric: 427.8404 - val_loss: 430.0319 - val_MinusLogProbMetric: 430.0319 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 23/1000
2023-10-02 16:43:40.748 
Epoch 23/1000 
	 loss: 427.3387, MinusLogProbMetric: 427.3387, val_loss: 427.6288, val_MinusLogProbMetric: 427.6288

Epoch 23: val_loss improved from 428.65939 to 427.62881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 427.3387 - MinusLogProbMetric: 427.3387 - val_loss: 427.6288 - val_MinusLogProbMetric: 427.6288 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 24/1000
2023-10-02 16:43:51.169 
Epoch 24/1000 
	 loss: 424.8831, MinusLogProbMetric: 424.8831, val_loss: 425.5698, val_MinusLogProbMetric: 425.5698

Epoch 24: val_loss improved from 427.62881 to 425.56979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 10s - loss: 424.8831 - MinusLogProbMetric: 424.8831 - val_loss: 425.5698 - val_MinusLogProbMetric: 425.5698 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 25/1000
2023-10-02 16:44:01.890 
Epoch 25/1000 
	 loss: 424.8248, MinusLogProbMetric: 424.8248, val_loss: 425.4934, val_MinusLogProbMetric: 425.4934

Epoch 25: val_loss improved from 425.56979 to 425.49338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 424.8248 - MinusLogProbMetric: 424.8248 - val_loss: 425.4934 - val_MinusLogProbMetric: 425.4934 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 26/1000
2023-10-02 16:44:13.283 
Epoch 26/1000 
	 loss: 424.0795, MinusLogProbMetric: 424.0795, val_loss: 426.0431, val_MinusLogProbMetric: 426.0431

Epoch 26: val_loss did not improve from 425.49338
196/196 - 11s - loss: 424.0795 - MinusLogProbMetric: 424.0795 - val_loss: 426.0431 - val_MinusLogProbMetric: 426.0431 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 27/1000
2023-10-02 16:44:24.261 
Epoch 27/1000 
	 loss: 423.0411, MinusLogProbMetric: 423.0411, val_loss: 424.6929, val_MinusLogProbMetric: 424.6929

Epoch 27: val_loss improved from 425.49338 to 424.69293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 423.0411 - MinusLogProbMetric: 423.0411 - val_loss: 424.6929 - val_MinusLogProbMetric: 424.6929 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 28/1000
2023-10-02 16:44:35.016 
Epoch 28/1000 
	 loss: 422.3630, MinusLogProbMetric: 422.3630, val_loss: 422.6239, val_MinusLogProbMetric: 422.6239

Epoch 28: val_loss improved from 424.69293 to 422.62393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 422.3630 - MinusLogProbMetric: 422.3630 - val_loss: 422.6239 - val_MinusLogProbMetric: 422.6239 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 29/1000
2023-10-02 16:44:46.088 
Epoch 29/1000 
	 loss: 422.9292, MinusLogProbMetric: 422.9292, val_loss: 420.9907, val_MinusLogProbMetric: 420.9907

Epoch 29: val_loss improved from 422.62393 to 420.99072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 422.9292 - MinusLogProbMetric: 422.9292 - val_loss: 420.9907 - val_MinusLogProbMetric: 420.9907 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 30/1000
2023-10-02 16:44:56.830 
Epoch 30/1000 
	 loss: 422.0657, MinusLogProbMetric: 422.0657, val_loss: 431.8781, val_MinusLogProbMetric: 431.8781

Epoch 30: val_loss did not improve from 420.99072
196/196 - 10s - loss: 422.0657 - MinusLogProbMetric: 422.0657 - val_loss: 431.8781 - val_MinusLogProbMetric: 431.8781 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 31/1000
2023-10-02 16:45:07.074 
Epoch 31/1000 
	 loss: 420.8784, MinusLogProbMetric: 420.8784, val_loss: 432.3361, val_MinusLogProbMetric: 432.3361

Epoch 31: val_loss did not improve from 420.99072
196/196 - 10s - loss: 420.8784 - MinusLogProbMetric: 420.8784 - val_loss: 432.3361 - val_MinusLogProbMetric: 432.3361 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 32/1000
2023-10-02 16:45:17.595 
Epoch 32/1000 
	 loss: 420.7206, MinusLogProbMetric: 420.7206, val_loss: 419.2875, val_MinusLogProbMetric: 419.2875

Epoch 32: val_loss improved from 420.99072 to 419.28748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 420.7206 - MinusLogProbMetric: 420.7206 - val_loss: 419.2875 - val_MinusLogProbMetric: 419.2875 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 33/1000
2023-10-02 16:45:28.474 
Epoch 33/1000 
	 loss: 420.1899, MinusLogProbMetric: 420.1899, val_loss: 418.5292, val_MinusLogProbMetric: 418.5292

Epoch 33: val_loss improved from 419.28748 to 418.52921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 420.1899 - MinusLogProbMetric: 420.1899 - val_loss: 418.5292 - val_MinusLogProbMetric: 418.5292 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 34/1000
2023-10-02 16:45:39.212 
Epoch 34/1000 
	 loss: 419.2278, MinusLogProbMetric: 419.2278, val_loss: 419.0674, val_MinusLogProbMetric: 419.0674

Epoch 34: val_loss did not improve from 418.52921
196/196 - 10s - loss: 419.2278 - MinusLogProbMetric: 419.2278 - val_loss: 419.0674 - val_MinusLogProbMetric: 419.0674 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 35/1000
2023-10-02 16:45:49.588 
Epoch 35/1000 
	 loss: 418.4445, MinusLogProbMetric: 418.4445, val_loss: 422.3401, val_MinusLogProbMetric: 422.3401

Epoch 35: val_loss did not improve from 418.52921
196/196 - 10s - loss: 418.4445 - MinusLogProbMetric: 418.4445 - val_loss: 422.3401 - val_MinusLogProbMetric: 422.3401 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 36/1000
2023-10-02 16:46:00.073 
Epoch 36/1000 
	 loss: 418.7457, MinusLogProbMetric: 418.7457, val_loss: 418.5140, val_MinusLogProbMetric: 418.5140

Epoch 36: val_loss improved from 418.52921 to 418.51404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 418.7457 - MinusLogProbMetric: 418.7457 - val_loss: 418.5140 - val_MinusLogProbMetric: 418.5140 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 37/1000
2023-10-02 16:46:10.849 
Epoch 37/1000 
	 loss: 418.3324, MinusLogProbMetric: 418.3324, val_loss: 424.0000, val_MinusLogProbMetric: 424.0000

Epoch 37: val_loss did not improve from 418.51404
196/196 - 10s - loss: 418.3324 - MinusLogProbMetric: 418.3324 - val_loss: 424.0000 - val_MinusLogProbMetric: 424.0000 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 38/1000
2023-10-02 16:46:21.281 
Epoch 38/1000 
	 loss: 417.2670, MinusLogProbMetric: 417.2670, val_loss: 442.0208, val_MinusLogProbMetric: 442.0208

Epoch 38: val_loss did not improve from 418.51404
196/196 - 10s - loss: 417.2670 - MinusLogProbMetric: 417.2670 - val_loss: 442.0208 - val_MinusLogProbMetric: 442.0208 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 39/1000
2023-10-02 16:46:31.719 
Epoch 39/1000 
	 loss: 417.4059, MinusLogProbMetric: 417.4059, val_loss: 416.7166, val_MinusLogProbMetric: 416.7166

Epoch 39: val_loss improved from 418.51404 to 416.71655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 417.4059 - MinusLogProbMetric: 417.4059 - val_loss: 416.7166 - val_MinusLogProbMetric: 416.7166 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 40/1000
2023-10-02 16:46:42.405 
Epoch 40/1000 
	 loss: 417.4581, MinusLogProbMetric: 417.4581, val_loss: 421.4510, val_MinusLogProbMetric: 421.4510

Epoch 40: val_loss did not improve from 416.71655
196/196 - 10s - loss: 417.4581 - MinusLogProbMetric: 417.4581 - val_loss: 421.4510 - val_MinusLogProbMetric: 421.4510 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 41/1000
2023-10-02 16:46:53.008 
Epoch 41/1000 
	 loss: 416.6434, MinusLogProbMetric: 416.6434, val_loss: 422.2101, val_MinusLogProbMetric: 422.2101

Epoch 41: val_loss did not improve from 416.71655
196/196 - 11s - loss: 416.6434 - MinusLogProbMetric: 416.6434 - val_loss: 422.2101 - val_MinusLogProbMetric: 422.2101 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 42/1000
2023-10-02 16:47:03.505 
Epoch 42/1000 
	 loss: 416.5427, MinusLogProbMetric: 416.5427, val_loss: 414.1662, val_MinusLogProbMetric: 414.1662

Epoch 42: val_loss improved from 416.71655 to 414.16620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 416.5427 - MinusLogProbMetric: 416.5427 - val_loss: 414.1662 - val_MinusLogProbMetric: 414.1662 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 43/1000
2023-10-02 16:47:14.225 
Epoch 43/1000 
	 loss: 415.5969, MinusLogProbMetric: 415.5969, val_loss: 423.1351, val_MinusLogProbMetric: 423.1351

Epoch 43: val_loss did not improve from 414.16620
196/196 - 10s - loss: 415.5969 - MinusLogProbMetric: 415.5969 - val_loss: 423.1351 - val_MinusLogProbMetric: 423.1351 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 44/1000
2023-10-02 16:47:24.510 
Epoch 44/1000 
	 loss: 415.4919, MinusLogProbMetric: 415.4919, val_loss: 415.2412, val_MinusLogProbMetric: 415.2412

Epoch 44: val_loss did not improve from 414.16620
196/196 - 10s - loss: 415.4919 - MinusLogProbMetric: 415.4919 - val_loss: 415.2412 - val_MinusLogProbMetric: 415.2412 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 45/1000
2023-10-02 16:47:35.112 
Epoch 45/1000 
	 loss: 415.0445, MinusLogProbMetric: 415.0445, val_loss: 415.8447, val_MinusLogProbMetric: 415.8447

Epoch 45: val_loss did not improve from 414.16620
196/196 - 11s - loss: 415.0445 - MinusLogProbMetric: 415.0445 - val_loss: 415.8447 - val_MinusLogProbMetric: 415.8447 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 46/1000
2023-10-02 16:47:45.929 
Epoch 46/1000 
	 loss: 414.8911, MinusLogProbMetric: 414.8911, val_loss: 413.9044, val_MinusLogProbMetric: 413.9044

Epoch 46: val_loss improved from 414.16620 to 413.90442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 414.8911 - MinusLogProbMetric: 414.8911 - val_loss: 413.9044 - val_MinusLogProbMetric: 413.9044 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 47/1000
2023-10-02 16:47:57.097 
Epoch 47/1000 
	 loss: 414.2767, MinusLogProbMetric: 414.2767, val_loss: 413.0749, val_MinusLogProbMetric: 413.0749

Epoch 47: val_loss improved from 413.90442 to 413.07492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 414.2767 - MinusLogProbMetric: 414.2767 - val_loss: 413.0749 - val_MinusLogProbMetric: 413.0749 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 48/1000
2023-10-02 16:48:07.862 
Epoch 48/1000 
	 loss: 414.8027, MinusLogProbMetric: 414.8027, val_loss: 413.0979, val_MinusLogProbMetric: 413.0979

Epoch 48: val_loss did not improve from 413.07492
196/196 - 10s - loss: 414.8027 - MinusLogProbMetric: 414.8027 - val_loss: 413.0979 - val_MinusLogProbMetric: 413.0979 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 49/1000
2023-10-02 16:48:18.314 
Epoch 49/1000 
	 loss: 413.3650, MinusLogProbMetric: 413.3650, val_loss: 412.2146, val_MinusLogProbMetric: 412.2146

Epoch 49: val_loss improved from 413.07492 to 412.21457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 413.3650 - MinusLogProbMetric: 413.3650 - val_loss: 412.2146 - val_MinusLogProbMetric: 412.2146 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 50/1000
2023-10-02 16:48:29.114 
Epoch 50/1000 
	 loss: 413.1628, MinusLogProbMetric: 413.1628, val_loss: 416.6893, val_MinusLogProbMetric: 416.6893

Epoch 50: val_loss did not improve from 412.21457
196/196 - 10s - loss: 413.1628 - MinusLogProbMetric: 413.1628 - val_loss: 416.6893 - val_MinusLogProbMetric: 416.6893 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 51/1000
2023-10-02 16:48:39.417 
Epoch 51/1000 
	 loss: 413.6378, MinusLogProbMetric: 413.6378, val_loss: 416.2867, val_MinusLogProbMetric: 416.2867

Epoch 51: val_loss did not improve from 412.21457
196/196 - 10s - loss: 413.6378 - MinusLogProbMetric: 413.6378 - val_loss: 416.2867 - val_MinusLogProbMetric: 416.2867 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 52/1000
2023-10-02 16:48:49.827 
Epoch 52/1000 
	 loss: 412.8728, MinusLogProbMetric: 412.8728, val_loss: 412.8800, val_MinusLogProbMetric: 412.8800

Epoch 52: val_loss did not improve from 412.21457
196/196 - 10s - loss: 412.8728 - MinusLogProbMetric: 412.8728 - val_loss: 412.8800 - val_MinusLogProbMetric: 412.8800 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 53/1000
2023-10-02 16:49:00.196 
Epoch 53/1000 
	 loss: 412.6305, MinusLogProbMetric: 412.6305, val_loss: 412.5927, val_MinusLogProbMetric: 412.5927

Epoch 53: val_loss did not improve from 412.21457
196/196 - 10s - loss: 412.6305 - MinusLogProbMetric: 412.6305 - val_loss: 412.5927 - val_MinusLogProbMetric: 412.5927 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 54/1000
2023-10-02 16:49:10.661 
Epoch 54/1000 
	 loss: 412.9827, MinusLogProbMetric: 412.9827, val_loss: 412.9508, val_MinusLogProbMetric: 412.9508

Epoch 54: val_loss did not improve from 412.21457
196/196 - 10s - loss: 412.9827 - MinusLogProbMetric: 412.9827 - val_loss: 412.9508 - val_MinusLogProbMetric: 412.9508 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 55/1000
2023-10-02 16:49:20.963 
Epoch 55/1000 
	 loss: 412.1010, MinusLogProbMetric: 412.1010, val_loss: 412.9721, val_MinusLogProbMetric: 412.9721

Epoch 55: val_loss did not improve from 412.21457
196/196 - 10s - loss: 412.1010 - MinusLogProbMetric: 412.1010 - val_loss: 412.9721 - val_MinusLogProbMetric: 412.9721 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 56/1000
2023-10-02 16:49:31.535 
Epoch 56/1000 
	 loss: 412.1962, MinusLogProbMetric: 412.1962, val_loss: 414.7439, val_MinusLogProbMetric: 414.7439

Epoch 56: val_loss did not improve from 412.21457
196/196 - 11s - loss: 412.1962 - MinusLogProbMetric: 412.1962 - val_loss: 414.7439 - val_MinusLogProbMetric: 414.7439 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 57/1000
2023-10-02 16:49:42.144 
Epoch 57/1000 
	 loss: 410.9733, MinusLogProbMetric: 410.9733, val_loss: 411.5107, val_MinusLogProbMetric: 411.5107

Epoch 57: val_loss improved from 412.21457 to 411.51068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 410.9733 - MinusLogProbMetric: 410.9733 - val_loss: 411.5107 - val_MinusLogProbMetric: 411.5107 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 58/1000
2023-10-02 16:49:52.842 
Epoch 58/1000 
	 loss: 411.1567, MinusLogProbMetric: 411.1567, val_loss: 415.3748, val_MinusLogProbMetric: 415.3748

Epoch 58: val_loss did not improve from 411.51068
196/196 - 10s - loss: 411.1567 - MinusLogProbMetric: 411.1567 - val_loss: 415.3748 - val_MinusLogProbMetric: 415.3748 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 59/1000
2023-10-02 16:50:03.297 
Epoch 59/1000 
	 loss: 411.4830, MinusLogProbMetric: 411.4830, val_loss: 412.0070, val_MinusLogProbMetric: 412.0070

Epoch 59: val_loss did not improve from 411.51068
196/196 - 10s - loss: 411.4830 - MinusLogProbMetric: 411.4830 - val_loss: 412.0070 - val_MinusLogProbMetric: 412.0070 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 60/1000
2023-10-02 16:50:13.548 
Epoch 60/1000 
	 loss: 411.2914, MinusLogProbMetric: 411.2914, val_loss: 412.2922, val_MinusLogProbMetric: 412.2922

Epoch 60: val_loss did not improve from 411.51068
196/196 - 10s - loss: 411.2914 - MinusLogProbMetric: 411.2914 - val_loss: 412.2922 - val_MinusLogProbMetric: 412.2922 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 61/1000
2023-10-02 16:50:23.968 
Epoch 61/1000 
	 loss: 410.9205, MinusLogProbMetric: 410.9205, val_loss: 418.5066, val_MinusLogProbMetric: 418.5066

Epoch 61: val_loss did not improve from 411.51068
196/196 - 10s - loss: 410.9205 - MinusLogProbMetric: 410.9205 - val_loss: 418.5066 - val_MinusLogProbMetric: 418.5066 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 62/1000
2023-10-02 16:50:34.740 
Epoch 62/1000 
	 loss: 410.9080, MinusLogProbMetric: 410.9080, val_loss: 414.0897, val_MinusLogProbMetric: 414.0897

Epoch 62: val_loss did not improve from 411.51068
196/196 - 11s - loss: 410.9080 - MinusLogProbMetric: 410.9080 - val_loss: 414.0897 - val_MinusLogProbMetric: 414.0897 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 63/1000
2023-10-02 16:50:45.906 
Epoch 63/1000 
	 loss: 410.0721, MinusLogProbMetric: 410.0721, val_loss: 410.6679, val_MinusLogProbMetric: 410.6679

Epoch 63: val_loss improved from 411.51068 to 410.66794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 410.0721 - MinusLogProbMetric: 410.0721 - val_loss: 410.6679 - val_MinusLogProbMetric: 410.6679 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 64/1000
2023-10-02 16:50:56.912 
Epoch 64/1000 
	 loss: 409.8370, MinusLogProbMetric: 409.8370, val_loss: 410.5769, val_MinusLogProbMetric: 410.5769

Epoch 64: val_loss improved from 410.66794 to 410.57690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 409.8370 - MinusLogProbMetric: 409.8370 - val_loss: 410.5769 - val_MinusLogProbMetric: 410.5769 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 65/1000
2023-10-02 16:51:07.683 
Epoch 65/1000 
	 loss: 409.8670, MinusLogProbMetric: 409.8670, val_loss: 417.2226, val_MinusLogProbMetric: 417.2226

Epoch 65: val_loss did not improve from 410.57690
196/196 - 10s - loss: 409.8670 - MinusLogProbMetric: 409.8670 - val_loss: 417.2226 - val_MinusLogProbMetric: 417.2226 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 66/1000
2023-10-02 16:51:18.266 
Epoch 66/1000 
	 loss: 410.5394, MinusLogProbMetric: 410.5394, val_loss: 412.1026, val_MinusLogProbMetric: 412.1026

Epoch 66: val_loss did not improve from 410.57690
196/196 - 11s - loss: 410.5394 - MinusLogProbMetric: 410.5394 - val_loss: 412.1026 - val_MinusLogProbMetric: 412.1026 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 67/1000
2023-10-02 16:51:28.728 
Epoch 67/1000 
	 loss: 409.7598, MinusLogProbMetric: 409.7598, val_loss: 410.6167, val_MinusLogProbMetric: 410.6167

Epoch 67: val_loss did not improve from 410.57690
196/196 - 10s - loss: 409.7598 - MinusLogProbMetric: 409.7598 - val_loss: 410.6167 - val_MinusLogProbMetric: 410.6167 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 68/1000
2023-10-02 16:51:39.154 
Epoch 68/1000 
	 loss: 409.5614, MinusLogProbMetric: 409.5614, val_loss: 411.1037, val_MinusLogProbMetric: 411.1037

Epoch 68: val_loss did not improve from 410.57690
196/196 - 10s - loss: 409.5614 - MinusLogProbMetric: 409.5614 - val_loss: 411.1037 - val_MinusLogProbMetric: 411.1037 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 69/1000
2023-10-02 16:51:49.542 
Epoch 69/1000 
	 loss: 408.9310, MinusLogProbMetric: 408.9310, val_loss: 409.0411, val_MinusLogProbMetric: 409.0411

Epoch 69: val_loss improved from 410.57690 to 409.04111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 408.9310 - MinusLogProbMetric: 408.9310 - val_loss: 409.0411 - val_MinusLogProbMetric: 409.0411 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 70/1000
2023-10-02 16:52:00.312 
Epoch 70/1000 
	 loss: 409.8404, MinusLogProbMetric: 409.8404, val_loss: 409.1479, val_MinusLogProbMetric: 409.1479

Epoch 70: val_loss did not improve from 409.04111
196/196 - 10s - loss: 409.8404 - MinusLogProbMetric: 409.8404 - val_loss: 409.1479 - val_MinusLogProbMetric: 409.1479 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 71/1000
2023-10-02 16:52:10.705 
Epoch 71/1000 
	 loss: 408.4238, MinusLogProbMetric: 408.4238, val_loss: 415.5713, val_MinusLogProbMetric: 415.5713

Epoch 71: val_loss did not improve from 409.04111
196/196 - 10s - loss: 408.4238 - MinusLogProbMetric: 408.4238 - val_loss: 415.5713 - val_MinusLogProbMetric: 415.5713 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 72/1000
2023-10-02 16:52:21.044 
Epoch 72/1000 
	 loss: 409.6794, MinusLogProbMetric: 409.6794, val_loss: 413.7378, val_MinusLogProbMetric: 413.7378

Epoch 72: val_loss did not improve from 409.04111
196/196 - 10s - loss: 409.6794 - MinusLogProbMetric: 409.6794 - val_loss: 413.7378 - val_MinusLogProbMetric: 413.7378 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 73/1000
2023-10-02 16:52:31.505 
Epoch 73/1000 
	 loss: 408.2702, MinusLogProbMetric: 408.2702, val_loss: 416.0723, val_MinusLogProbMetric: 416.0723

Epoch 73: val_loss did not improve from 409.04111
196/196 - 10s - loss: 408.2702 - MinusLogProbMetric: 408.2702 - val_loss: 416.0723 - val_MinusLogProbMetric: 416.0723 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 74/1000
2023-10-02 16:52:41.860 
Epoch 74/1000 
	 loss: 408.9376, MinusLogProbMetric: 408.9376, val_loss: 410.4975, val_MinusLogProbMetric: 410.4975

Epoch 74: val_loss did not improve from 409.04111
196/196 - 10s - loss: 408.9376 - MinusLogProbMetric: 408.9376 - val_loss: 410.4975 - val_MinusLogProbMetric: 410.4975 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 75/1000
2023-10-02 16:52:52.344 
Epoch 75/1000 
	 loss: 413.3275, MinusLogProbMetric: 413.3275, val_loss: 410.2238, val_MinusLogProbMetric: 410.2238

Epoch 75: val_loss did not improve from 409.04111
196/196 - 10s - loss: 413.3275 - MinusLogProbMetric: 413.3275 - val_loss: 410.2238 - val_MinusLogProbMetric: 410.2238 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 76/1000
2023-10-02 16:53:02.795 
Epoch 76/1000 
	 loss: 407.9516, MinusLogProbMetric: 407.9516, val_loss: 409.6512, val_MinusLogProbMetric: 409.6512

Epoch 76: val_loss did not improve from 409.04111
196/196 - 10s - loss: 407.9516 - MinusLogProbMetric: 407.9516 - val_loss: 409.6512 - val_MinusLogProbMetric: 409.6512 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 77/1000
2023-10-02 16:53:13.238 
Epoch 77/1000 
	 loss: 407.9685, MinusLogProbMetric: 407.9685, val_loss: 411.0095, val_MinusLogProbMetric: 411.0095

Epoch 77: val_loss did not improve from 409.04111
196/196 - 10s - loss: 407.9685 - MinusLogProbMetric: 407.9685 - val_loss: 411.0095 - val_MinusLogProbMetric: 411.0095 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 78/1000
2023-10-02 16:53:23.637 
Epoch 78/1000 
	 loss: 407.1652, MinusLogProbMetric: 407.1652, val_loss: 408.3996, val_MinusLogProbMetric: 408.3996

Epoch 78: val_loss improved from 409.04111 to 408.39960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 407.1652 - MinusLogProbMetric: 407.1652 - val_loss: 408.3996 - val_MinusLogProbMetric: 408.3996 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 79/1000
2023-10-02 16:53:34.236 
Epoch 79/1000 
	 loss: 408.4276, MinusLogProbMetric: 408.4276, val_loss: 408.7989, val_MinusLogProbMetric: 408.7989

Epoch 79: val_loss did not improve from 408.39960
196/196 - 10s - loss: 408.4276 - MinusLogProbMetric: 408.4276 - val_loss: 408.7989 - val_MinusLogProbMetric: 408.7989 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 80/1000
2023-10-02 16:53:44.688 
Epoch 80/1000 
	 loss: 407.1564, MinusLogProbMetric: 407.1564, val_loss: 409.0276, val_MinusLogProbMetric: 409.0276

Epoch 80: val_loss did not improve from 408.39960
196/196 - 10s - loss: 407.1564 - MinusLogProbMetric: 407.1564 - val_loss: 409.0276 - val_MinusLogProbMetric: 409.0276 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 81/1000
2023-10-02 16:53:55.041 
Epoch 81/1000 
	 loss: 407.3929, MinusLogProbMetric: 407.3929, val_loss: 409.3185, val_MinusLogProbMetric: 409.3185

Epoch 81: val_loss did not improve from 408.39960
196/196 - 10s - loss: 407.3929 - MinusLogProbMetric: 407.3929 - val_loss: 409.3185 - val_MinusLogProbMetric: 409.3185 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 82/1000
2023-10-02 16:54:05.404 
Epoch 82/1000 
	 loss: 407.1389, MinusLogProbMetric: 407.1389, val_loss: 408.2674, val_MinusLogProbMetric: 408.2674

Epoch 82: val_loss improved from 408.39960 to 408.26736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 407.1389 - MinusLogProbMetric: 407.1389 - val_loss: 408.2674 - val_MinusLogProbMetric: 408.2674 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 83/1000
2023-10-02 16:54:16.123 
Epoch 83/1000 
	 loss: 407.7903, MinusLogProbMetric: 407.7903, val_loss: 408.5794, val_MinusLogProbMetric: 408.5794

Epoch 83: val_loss did not improve from 408.26736
196/196 - 10s - loss: 407.7903 - MinusLogProbMetric: 407.7903 - val_loss: 408.5794 - val_MinusLogProbMetric: 408.5794 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 84/1000
2023-10-02 16:54:26.463 
Epoch 84/1000 
	 loss: 406.7385, MinusLogProbMetric: 406.7385, val_loss: 411.4189, val_MinusLogProbMetric: 411.4189

Epoch 84: val_loss did not improve from 408.26736
196/196 - 10s - loss: 406.7385 - MinusLogProbMetric: 406.7385 - val_loss: 411.4189 - val_MinusLogProbMetric: 411.4189 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 85/1000
2023-10-02 16:54:36.666 
Epoch 85/1000 
	 loss: 406.6455, MinusLogProbMetric: 406.6455, val_loss: 421.8639, val_MinusLogProbMetric: 421.8639

Epoch 85: val_loss did not improve from 408.26736
196/196 - 10s - loss: 406.6455 - MinusLogProbMetric: 406.6455 - val_loss: 421.8639 - val_MinusLogProbMetric: 421.8639 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 86/1000
2023-10-02 16:54:47.047 
Epoch 86/1000 
	 loss: 406.7550, MinusLogProbMetric: 406.7550, val_loss: 410.1199, val_MinusLogProbMetric: 410.1199

Epoch 86: val_loss did not improve from 408.26736
196/196 - 10s - loss: 406.7550 - MinusLogProbMetric: 406.7550 - val_loss: 410.1199 - val_MinusLogProbMetric: 410.1199 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 87/1000
2023-10-02 16:54:57.496 
Epoch 87/1000 
	 loss: 407.1387, MinusLogProbMetric: 407.1387, val_loss: 407.2743, val_MinusLogProbMetric: 407.2743

Epoch 87: val_loss improved from 408.26736 to 407.27429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 407.1387 - MinusLogProbMetric: 407.1387 - val_loss: 407.2743 - val_MinusLogProbMetric: 407.2743 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 88/1000
2023-10-02 16:55:08.432 
Epoch 88/1000 
	 loss: 407.1763, MinusLogProbMetric: 407.1763, val_loss: 408.0046, val_MinusLogProbMetric: 408.0046

Epoch 88: val_loss did not improve from 407.27429
196/196 - 11s - loss: 407.1763 - MinusLogProbMetric: 407.1763 - val_loss: 408.0046 - val_MinusLogProbMetric: 408.0046 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 89/1000
2023-10-02 16:55:18.872 
Epoch 89/1000 
	 loss: 406.5141, MinusLogProbMetric: 406.5141, val_loss: 409.4673, val_MinusLogProbMetric: 409.4673

Epoch 89: val_loss did not improve from 407.27429
196/196 - 10s - loss: 406.5141 - MinusLogProbMetric: 406.5141 - val_loss: 409.4673 - val_MinusLogProbMetric: 409.4673 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 90/1000
2023-10-02 16:55:29.296 
Epoch 90/1000 
	 loss: 406.4160, MinusLogProbMetric: 406.4160, val_loss: 407.8270, val_MinusLogProbMetric: 407.8270

Epoch 90: val_loss did not improve from 407.27429
196/196 - 10s - loss: 406.4160 - MinusLogProbMetric: 406.4160 - val_loss: 407.8270 - val_MinusLogProbMetric: 407.8270 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 91/1000
2023-10-02 16:55:39.769 
Epoch 91/1000 
	 loss: 405.9944, MinusLogProbMetric: 405.9944, val_loss: 408.3953, val_MinusLogProbMetric: 408.3953

Epoch 91: val_loss did not improve from 407.27429
196/196 - 10s - loss: 405.9944 - MinusLogProbMetric: 405.9944 - val_loss: 408.3953 - val_MinusLogProbMetric: 408.3953 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 92/1000
2023-10-02 16:55:50.196 
Epoch 92/1000 
	 loss: 405.7641, MinusLogProbMetric: 405.7641, val_loss: 409.7128, val_MinusLogProbMetric: 409.7128

Epoch 92: val_loss did not improve from 407.27429
196/196 - 10s - loss: 405.7641 - MinusLogProbMetric: 405.7641 - val_loss: 409.7128 - val_MinusLogProbMetric: 409.7128 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 93/1000
2023-10-02 16:56:00.548 
Epoch 93/1000 
	 loss: 405.5050, MinusLogProbMetric: 405.5050, val_loss: 407.8874, val_MinusLogProbMetric: 407.8874

Epoch 93: val_loss did not improve from 407.27429
196/196 - 10s - loss: 405.5050 - MinusLogProbMetric: 405.5050 - val_loss: 407.8874 - val_MinusLogProbMetric: 407.8874 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-10-02 16:56:10.867 
Epoch 94/1000 
	 loss: 406.7559, MinusLogProbMetric: 406.7559, val_loss: 408.1250, val_MinusLogProbMetric: 408.1250

Epoch 94: val_loss did not improve from 407.27429
196/196 - 10s - loss: 406.7559 - MinusLogProbMetric: 406.7559 - val_loss: 408.1250 - val_MinusLogProbMetric: 408.1250 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 95/1000
2023-10-02 16:56:21.351 
Epoch 95/1000 
	 loss: 406.5121, MinusLogProbMetric: 406.5121, val_loss: 409.3803, val_MinusLogProbMetric: 409.3803

Epoch 95: val_loss did not improve from 407.27429
196/196 - 10s - loss: 406.5121 - MinusLogProbMetric: 406.5121 - val_loss: 409.3803 - val_MinusLogProbMetric: 409.3803 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 96/1000
2023-10-02 16:56:31.893 
Epoch 96/1000 
	 loss: 405.5415, MinusLogProbMetric: 405.5415, val_loss: 407.9494, val_MinusLogProbMetric: 407.9494

Epoch 96: val_loss did not improve from 407.27429
196/196 - 11s - loss: 405.5415 - MinusLogProbMetric: 405.5415 - val_loss: 407.9494 - val_MinusLogProbMetric: 407.9494 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 97/1000
2023-10-02 16:56:42.351 
Epoch 97/1000 
	 loss: 405.7980, MinusLogProbMetric: 405.7980, val_loss: 407.2962, val_MinusLogProbMetric: 407.2962

Epoch 97: val_loss did not improve from 407.27429
196/196 - 10s - loss: 405.7980 - MinusLogProbMetric: 405.7980 - val_loss: 407.2962 - val_MinusLogProbMetric: 407.2962 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 98/1000
2023-10-02 16:56:52.694 
Epoch 98/1000 
	 loss: 405.7598, MinusLogProbMetric: 405.7598, val_loss: 409.3141, val_MinusLogProbMetric: 409.3141

Epoch 98: val_loss did not improve from 407.27429
196/196 - 10s - loss: 405.7598 - MinusLogProbMetric: 405.7598 - val_loss: 409.3141 - val_MinusLogProbMetric: 409.3141 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 99/1000
2023-10-02 16:57:03.945 
Epoch 99/1000 
	 loss: 404.8893, MinusLogProbMetric: 404.8893, val_loss: 405.9200, val_MinusLogProbMetric: 405.9200

Epoch 99: val_loss improved from 407.27429 to 405.91995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 404.8893 - MinusLogProbMetric: 404.8893 - val_loss: 405.9200 - val_MinusLogProbMetric: 405.9200 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 100/1000
2023-10-02 16:57:15.236 
Epoch 100/1000 
	 loss: 405.1419, MinusLogProbMetric: 405.1419, val_loss: 406.6687, val_MinusLogProbMetric: 406.6687

Epoch 100: val_loss did not improve from 405.91995
196/196 - 11s - loss: 405.1419 - MinusLogProbMetric: 405.1419 - val_loss: 406.6687 - val_MinusLogProbMetric: 406.6687 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 101/1000
2023-10-02 16:57:25.755 
Epoch 101/1000 
	 loss: 404.9006, MinusLogProbMetric: 404.9006, val_loss: 406.4925, val_MinusLogProbMetric: 406.4925

Epoch 101: val_loss did not improve from 405.91995
196/196 - 11s - loss: 404.9006 - MinusLogProbMetric: 404.9006 - val_loss: 406.4925 - val_MinusLogProbMetric: 406.4925 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 102/1000
2023-10-02 16:57:36.229 
Epoch 102/1000 
	 loss: 404.7933, MinusLogProbMetric: 404.7933, val_loss: 411.4323, val_MinusLogProbMetric: 411.4323

Epoch 102: val_loss did not improve from 405.91995
196/196 - 10s - loss: 404.7933 - MinusLogProbMetric: 404.7933 - val_loss: 411.4323 - val_MinusLogProbMetric: 411.4323 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 103/1000
2023-10-02 16:57:46.695 
Epoch 103/1000 
	 loss: 404.9322, MinusLogProbMetric: 404.9322, val_loss: 407.2415, val_MinusLogProbMetric: 407.2415

Epoch 103: val_loss did not improve from 405.91995
196/196 - 10s - loss: 404.9322 - MinusLogProbMetric: 404.9322 - val_loss: 407.2415 - val_MinusLogProbMetric: 407.2415 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 104/1000
2023-10-02 16:57:57.185 
Epoch 104/1000 
	 loss: 404.4832, MinusLogProbMetric: 404.4832, val_loss: 422.5820, val_MinusLogProbMetric: 422.5820

Epoch 104: val_loss did not improve from 405.91995
196/196 - 10s - loss: 404.4832 - MinusLogProbMetric: 404.4832 - val_loss: 422.5820 - val_MinusLogProbMetric: 422.5820 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 105/1000
2023-10-02 16:58:07.634 
Epoch 105/1000 
	 loss: 405.5382, MinusLogProbMetric: 405.5382, val_loss: 405.8060, val_MinusLogProbMetric: 405.8060

Epoch 105: val_loss improved from 405.91995 to 405.80603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 405.5382 - MinusLogProbMetric: 405.5382 - val_loss: 405.8060 - val_MinusLogProbMetric: 405.8060 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 106/1000
2023-10-02 16:58:18.320 
Epoch 106/1000 
	 loss: 404.1841, MinusLogProbMetric: 404.1841, val_loss: 408.6222, val_MinusLogProbMetric: 408.6222

Epoch 106: val_loss did not improve from 405.80603
196/196 - 10s - loss: 404.1841 - MinusLogProbMetric: 404.1841 - val_loss: 408.6222 - val_MinusLogProbMetric: 408.6222 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 107/1000
2023-10-02 16:58:28.786 
Epoch 107/1000 
	 loss: 404.4940, MinusLogProbMetric: 404.4940, val_loss: 407.2111, val_MinusLogProbMetric: 407.2111

Epoch 107: val_loss did not improve from 405.80603
196/196 - 10s - loss: 404.4940 - MinusLogProbMetric: 404.4940 - val_loss: 407.2111 - val_MinusLogProbMetric: 407.2111 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 108/1000
2023-10-02 16:58:39.311 
Epoch 108/1000 
	 loss: 404.8123, MinusLogProbMetric: 404.8123, val_loss: 405.0526, val_MinusLogProbMetric: 405.0526

Epoch 108: val_loss improved from 405.80603 to 405.05264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 404.8123 - MinusLogProbMetric: 404.8123 - val_loss: 405.0526 - val_MinusLogProbMetric: 405.0526 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 109/1000
2023-10-02 16:58:50.097 
Epoch 109/1000 
	 loss: 404.2758, MinusLogProbMetric: 404.2758, val_loss: 406.2980, val_MinusLogProbMetric: 406.2980

Epoch 109: val_loss did not improve from 405.05264
196/196 - 10s - loss: 404.2758 - MinusLogProbMetric: 404.2758 - val_loss: 406.2980 - val_MinusLogProbMetric: 406.2980 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 110/1000
2023-10-02 16:59:00.532 
Epoch 110/1000 
	 loss: 405.3791, MinusLogProbMetric: 405.3791, val_loss: 406.0179, val_MinusLogProbMetric: 406.0179

Epoch 110: val_loss did not improve from 405.05264
196/196 - 10s - loss: 405.3791 - MinusLogProbMetric: 405.3791 - val_loss: 406.0179 - val_MinusLogProbMetric: 406.0179 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 111/1000
2023-10-02 16:59:11.010 
Epoch 111/1000 
	 loss: 404.1998, MinusLogProbMetric: 404.1998, val_loss: 406.1401, val_MinusLogProbMetric: 406.1401

Epoch 111: val_loss did not improve from 405.05264
196/196 - 10s - loss: 404.1998 - MinusLogProbMetric: 404.1998 - val_loss: 406.1401 - val_MinusLogProbMetric: 406.1401 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 112/1000
2023-10-02 16:59:21.480 
Epoch 112/1000 
	 loss: 403.8746, MinusLogProbMetric: 403.8746, val_loss: 408.6630, val_MinusLogProbMetric: 408.6630

Epoch 112: val_loss did not improve from 405.05264
196/196 - 10s - loss: 403.8746 - MinusLogProbMetric: 403.8746 - val_loss: 408.6630 - val_MinusLogProbMetric: 408.6630 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 113/1000
2023-10-02 16:59:31.957 
Epoch 113/1000 
	 loss: 406.8969, MinusLogProbMetric: 406.8969, val_loss: 405.9910, val_MinusLogProbMetric: 405.9910

Epoch 113: val_loss did not improve from 405.05264
196/196 - 10s - loss: 406.8969 - MinusLogProbMetric: 406.8969 - val_loss: 405.9910 - val_MinusLogProbMetric: 405.9910 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 114/1000
2023-10-02 16:59:42.538 
Epoch 114/1000 
	 loss: 403.8255, MinusLogProbMetric: 403.8255, val_loss: 405.0036, val_MinusLogProbMetric: 405.0036

Epoch 114: val_loss improved from 405.05264 to 405.00363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 403.8255 - MinusLogProbMetric: 403.8255 - val_loss: 405.0036 - val_MinusLogProbMetric: 405.0036 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 115/1000
2023-10-02 16:59:53.527 
Epoch 115/1000 
	 loss: 403.5759, MinusLogProbMetric: 403.5759, val_loss: 410.2966, val_MinusLogProbMetric: 410.2966

Epoch 115: val_loss did not improve from 405.00363
196/196 - 10s - loss: 403.5759 - MinusLogProbMetric: 403.5759 - val_loss: 410.2966 - val_MinusLogProbMetric: 410.2966 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 116/1000
2023-10-02 17:00:03.978 
Epoch 116/1000 
	 loss: 404.0611, MinusLogProbMetric: 404.0611, val_loss: 406.5447, val_MinusLogProbMetric: 406.5447

Epoch 116: val_loss did not improve from 405.00363
196/196 - 10s - loss: 404.0611 - MinusLogProbMetric: 404.0611 - val_loss: 406.5447 - val_MinusLogProbMetric: 406.5447 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 117/1000
2023-10-02 17:00:14.430 
Epoch 117/1000 
	 loss: 403.7677, MinusLogProbMetric: 403.7677, val_loss: 406.2615, val_MinusLogProbMetric: 406.2615

Epoch 117: val_loss did not improve from 405.00363
196/196 - 10s - loss: 403.7677 - MinusLogProbMetric: 403.7677 - val_loss: 406.2615 - val_MinusLogProbMetric: 406.2615 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 118/1000
2023-10-02 17:00:25.046 
Epoch 118/1000 
	 loss: 403.2454, MinusLogProbMetric: 403.2454, val_loss: 405.0480, val_MinusLogProbMetric: 405.0480

Epoch 118: val_loss did not improve from 405.00363
196/196 - 11s - loss: 403.2454 - MinusLogProbMetric: 403.2454 - val_loss: 405.0480 - val_MinusLogProbMetric: 405.0480 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 119/1000
2023-10-02 17:00:35.459 
Epoch 119/1000 
	 loss: 403.9528, MinusLogProbMetric: 403.9528, val_loss: 405.0904, val_MinusLogProbMetric: 405.0904

Epoch 119: val_loss did not improve from 405.00363
196/196 - 10s - loss: 403.9528 - MinusLogProbMetric: 403.9528 - val_loss: 405.0904 - val_MinusLogProbMetric: 405.0904 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 120/1000
2023-10-02 17:00:45.955 
Epoch 120/1000 
	 loss: 403.2690, MinusLogProbMetric: 403.2690, val_loss: 404.5729, val_MinusLogProbMetric: 404.5729

Epoch 120: val_loss improved from 405.00363 to 404.57294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 403.2690 - MinusLogProbMetric: 403.2690 - val_loss: 404.5729 - val_MinusLogProbMetric: 404.5729 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 121/1000
2023-10-02 17:00:56.692 
Epoch 121/1000 
	 loss: 403.3614, MinusLogProbMetric: 403.3614, val_loss: 408.6313, val_MinusLogProbMetric: 408.6313

Epoch 121: val_loss did not improve from 404.57294
196/196 - 10s - loss: 403.3614 - MinusLogProbMetric: 403.3614 - val_loss: 408.6313 - val_MinusLogProbMetric: 408.6313 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 122/1000
2023-10-02 17:01:07.124 
Epoch 122/1000 
	 loss: 403.0092, MinusLogProbMetric: 403.0092, val_loss: 407.2056, val_MinusLogProbMetric: 407.2056

Epoch 122: val_loss did not improve from 404.57294
196/196 - 10s - loss: 403.0092 - MinusLogProbMetric: 403.0092 - val_loss: 407.2056 - val_MinusLogProbMetric: 407.2056 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 123/1000
2023-10-02 17:01:17.614 
Epoch 123/1000 
	 loss: 404.4640, MinusLogProbMetric: 404.4640, val_loss: 404.5338, val_MinusLogProbMetric: 404.5338

Epoch 123: val_loss improved from 404.57294 to 404.53384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 404.4640 - MinusLogProbMetric: 404.4640 - val_loss: 404.5338 - val_MinusLogProbMetric: 404.5338 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 124/1000
2023-10-02 17:01:28.341 
Epoch 124/1000 
	 loss: 403.3576, MinusLogProbMetric: 403.3576, val_loss: 405.2586, val_MinusLogProbMetric: 405.2586

Epoch 124: val_loss did not improve from 404.53384
196/196 - 10s - loss: 403.3576 - MinusLogProbMetric: 403.3576 - val_loss: 405.2586 - val_MinusLogProbMetric: 405.2586 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 125/1000
2023-10-02 17:01:38.852 
Epoch 125/1000 
	 loss: 402.5423, MinusLogProbMetric: 402.5423, val_loss: 408.0989, val_MinusLogProbMetric: 408.0989

Epoch 125: val_loss did not improve from 404.53384
196/196 - 11s - loss: 402.5423 - MinusLogProbMetric: 402.5423 - val_loss: 408.0989 - val_MinusLogProbMetric: 408.0989 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 126/1000
2023-10-02 17:01:49.310 
Epoch 126/1000 
	 loss: 403.1093, MinusLogProbMetric: 403.1093, val_loss: 404.8417, val_MinusLogProbMetric: 404.8417

Epoch 126: val_loss did not improve from 404.53384
196/196 - 10s - loss: 403.1093 - MinusLogProbMetric: 403.1093 - val_loss: 404.8417 - val_MinusLogProbMetric: 404.8417 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 127/1000
2023-10-02 17:01:59.641 
Epoch 127/1000 
	 loss: 402.6860, MinusLogProbMetric: 402.6860, val_loss: 403.3799, val_MinusLogProbMetric: 403.3799

Epoch 127: val_loss improved from 404.53384 to 403.37985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 402.6860 - MinusLogProbMetric: 402.6860 - val_loss: 403.3799 - val_MinusLogProbMetric: 403.3799 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 128/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 114: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-02 17:02:06.627 
Epoch 128/1000 
	 loss: nan, MinusLogProbMetric: 588758948773888.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 128: val_loss did not improve from 403.37985
196/196 - 7s - loss: nan - MinusLogProbMetric: 588758948773888.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 7s/epoch - 34ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 340.
===========
Train data generated in 0.64 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_340/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.1938453 ,  0.533144  ,  4.8263674 , ...,  4.486572  ,
         6.023426  ,  7.1273623 ],
       [ 7.585662  ,  4.7953405 ,  5.2079363 , ...,  2.9872682 ,
         8.320559  ,  6.9911246 ],
       [ 5.966249  , -0.30219892,  4.8852797 , ...,  4.988506  ,
         6.203924  ,  5.582959  ],
       ...,
       [ 7.967994  ,  4.477745  ,  5.2694697 , ...,  2.879139  ,
         7.399321  ,  6.960496  ],
       [ 6.5566854 ,  0.02881634,  4.848876  , ...,  5.0976205 ,
         6.4114294 ,  3.5750432 ],
       [ 8.147113  ,  4.715337  ,  5.2998085 , ...,  4.052641  ,
         8.0347185 ,  6.6445417 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_340/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_340
self.data_kwargs: {'seed': 520}
self.x_data: [[6.0386963  0.30078053 4.582872   ... 4.9010735  5.585139   5.0534396 ]
 [7.9469204  4.5061865  5.2455854  ... 2.9358454  8.744124   6.9367347 ]
 [8.143589   4.4823375  5.2392807  ... 3.0099165  8.623259   7.0103106 ]
 ...
 [8.140467   4.6205688  5.2538304  ... 5.1522036  8.523501   6.461891  ]
 [5.550299   8.4923     5.931979   ... 9.995682   1.7941611  6.8577833 ]
 [7.7436986  5.02466    5.1777496  ... 2.148604   8.593714   7.418009  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7fbb109fe140>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb19260430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb19260430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb1a19bac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb462e2b90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb462e3070>, <keras.callbacks.ModelCheckpoint object at 0x7fbb462e3130>, <keras.callbacks.EarlyStopping object at 0x7fbb462e33a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbb462e33d0>, <keras.callbacks.TerminateOnNaN object at 0x7fbb462e3010>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.1938453 ,  0.533144  ,  4.8263674 , ...,  4.486572  ,
         6.023426  ,  7.1273623 ],
       [ 7.585662  ,  4.7953405 ,  5.2079363 , ...,  2.9872682 ,
         8.320559  ,  6.9911246 ],
       [ 5.966249  , -0.30219892,  4.8852797 , ...,  4.988506  ,
         6.203924  ,  5.582959  ],
       ...,
       [ 7.967994  ,  4.477745  ,  5.2694697 , ...,  2.879139  ,
         7.399321  ,  6.960496  ],
       [ 6.5566854 ,  0.02881634,  4.848876  , ...,  5.0976205 ,
         6.4114294 ,  3.5750432 ],
       [ 8.147113  ,  4.715337  ,  5.2998085 , ...,  4.052641  ,
         8.0347185 ,  6.6445417 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 340/360 with hyperparameters:
timestamp = 2023-10-02 17:02:09.679598
ndims = 1000
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.0386963   0.30078053  4.582872    7.656263    0.50402063  8.927606
  5.0806236   0.7069437   3.2612228   9.222077    5.8550296   0.7950525
  3.7572262   5.421181   -0.42896372  5.3715286   5.0170646   4.5275445
  6.268279    8.342493    2.4881694   7.7397876   7.198324   10.337882
  7.1367445   7.992043    3.640705    1.6190335   9.687809   -0.10210621
  4.3673606   0.5702325   9.0355015   9.576009    7.180074    2.9750605
  2.7969558   2.6438603   1.315014    5.2577987   1.5748067   3.07832
  6.905322    7.2643003   0.31143492  3.629692    6.198628    9.935606
  4.738879    9.644313    2.6127305  -0.2701767   4.573748    2.6033232
  8.31127     2.847469    3.1219404   8.036686    8.350981    5.1923842
  7.0816703   3.7841296   0.439943    0.31411228  5.105436    0.16452146
  0.37245148  8.840347    7.522216    2.485342   -0.04555227  9.480491
  3.3716009   2.2168918   5.8693147   1.2792552   2.5388308   9.7780695
  6.3509502   5.2145343   4.668051    7.835037    0.81844634  9.829676
  8.602084    6.4478574   1.8822466   8.708197    9.9343      5.2983065
  4.648863    1.7545191   7.5778584   7.992651    1.2489895   7.932429
  0.19842413 10.670638    6.7138567   5.5415473   2.8593335   8.939356
  1.8772883   8.274395    9.729145    7.7069526   4.83206     0.20179847
  4.5409455   4.72331     2.8598065   4.388705    3.542864    1.3279399
  4.2783537   2.6017168   5.7597165   0.7641455   9.5386305   9.114207
  5.246056    8.810347    9.548426    1.1960151   4.535228    9.936641
 10.11391     6.0469294   1.2320046   5.771403    0.03692677  1.7631103
  9.768336    4.6296864   6.3364687   2.3349156   9.236544    8.197755
  3.2024207   7.716959    2.077003    4.211735    2.1907134   7.0612254
  1.2628212   4.2821627   1.9289484   4.29324     4.816094    7.6030316
  0.34979546  7.054235    2.5239222   2.5584276   7.064572    9.337272
  4.1277614   8.632163    1.1907344   7.2465253   7.3629594   1.4210125
  3.8094866   8.687733    1.3656402   0.28611335  5.895154    4.691248
  7.848699    8.486627    2.7608519   4.164383    5.0772963   6.1176133
 10.264451    6.3248825   3.0036852   2.4301364   9.323013    1.1239512
  6.9529996   8.971375    6.7585683   4.1878514   2.5465474   3.7158139
  7.954454    6.121273    8.889866    8.739019   10.887082    9.803803
  2.4961972   3.7658646   4.353828    5.831876    6.706101    2.2424636
  0.4813192   4.0677247   2.7481353   8.85868     8.81974     8.779158
  6.5635395   3.1331716   5.4378457   8.115037    0.15877736  7.7527685
 -0.23714772  9.2626095   7.987624    0.7620828   6.501852    3.2839687
  1.7200415   1.1543069   7.17581     5.3241725   9.85193    10.548408
 10.603701    2.2370188   9.374007    3.4676795   4.4167695   7.9351892
  2.2672663   5.607824    2.2299185   0.88661635  1.2501802   4.0727944
 -0.12534815  4.24075     6.864432    4.1316953   0.98898524  1.0468978
  2.6923618   1.5284702   9.68549     2.6858997   8.196742    5.9775615
  6.107674    3.8373623   9.739576    3.8176546   7.881323    5.9619036
  4.809063    9.994005    6.06644     0.60921174  5.0061116   8.33161
  7.0547867   8.514869    2.0673354   7.7308583   4.0995107   5.0039496
 10.610853    9.848376    8.120186    4.446167    8.818996    6.304151
  3.6133528   0.34783545  8.203926    4.955636    8.081435    7.584287
  7.8737516   3.389564    7.996908    6.0786796   4.5982037   7.0935416
  0.92889404  1.281023    2.9661057   9.792753    1.4344769   3.2898872
  1.1154623   3.3197548   4.133613    9.714192    7.8477564   3.915498
 -0.54802597  8.3167515   6.381558    1.8454669  11.718771    1.0781447
  8.230542    2.993074    4.4022226   7.81114     6.1109514   8.33177
  6.2205343   0.9577046   9.167438    7.7908244   7.120584    1.3328398
  3.1939583   1.6910427   8.12919     6.4541216   7.9364567  10.26773
  7.809719   10.104616    6.0293574   0.40289783 -0.560338    3.7847402
  7.9600444   9.035484   10.840271    1.0387208  -0.19058862  8.854946
  3.0963922   5.0594854  10.279567    5.074058    6.6726894   3.9883325
  7.652588    4.496253    3.0499647   2.3702488   2.5340867   3.782253
  6.6244855   9.641262    7.137413    6.8682413   1.8626316   4.317209
  4.4707828   6.841525    2.992224    2.4876325   3.3212593   7.333737
  5.75945     0.32461804  0.8335988   3.4026284   8.618828    6.705712
  6.8725452   7.33386     2.8872874   5.3803415   5.1434884   6.246975
  9.339241    0.6288371   8.414895    6.5827723   5.714417    7.4871626
  9.453507    0.7600125   1.494343    7.271453   -0.10680145  4.284081
  3.0645723   2.7818801   6.7562222   0.9355258   8.759254    4.4485765
  4.9456024   5.2843823   6.5932875   3.4578905   0.32452983  3.0972865
  0.45848465  5.198651    2.197647    9.560289    9.731855    6.5005336
  6.619211   10.057005    8.876658    8.295638    3.1194568   3.6742027
  2.26406     5.3983374   9.887169    9.133878    1.5052447   1.7451565
  8.9965925   5.319445    2.527405    6.8142905   6.4233      1.2479438
  4.035429    0.24714679  2.1173966   1.830858    1.9078584   7.0054536
  0.5009528   0.8397763   2.0496254   2.9811964   2.9665575   3.3201675
  7.2900653   9.622518    4.901422    7.8189707   3.9295015   0.91512156
  4.572243    3.2736297   4.8408017   7.065958    4.651118    9.0410385
  8.961053    5.5870433   5.2621417   1.1291223   3.93055     6.9391193
  3.508282    0.4460107   9.179407    7.387717    4.650465    8.17948
  7.53271     5.0031796   8.804058    9.204221    3.6460009   7.006398
  5.8495746   5.197357    9.277365    2.9547234  10.355085    8.981658
  5.367272    8.041138    7.377086    7.5142937   4.0544214   2.2740157
  6.749342    8.733181   -1.1557603   6.3426285   4.84659     8.766928
  2.8006268   4.1529183   4.987336    0.13959682  6.5040164   8.188027
  1.676772    2.4030275   0.5443996   9.24266     5.4919004   9.14114
  5.4362607   7.7436833   3.4114141   2.927477    3.843325    6.4828835
  2.9296427   9.1742115   8.056144   -0.09604371  2.0134284   1.6318083
  0.04543668  2.690591    4.990765    8.81346     2.1004107   4.4873705
 10.513251    7.137761    7.6034775   5.209668    9.811113    5.6359825
  0.19526497  2.564422    4.0089235   6.4994254   7.695936    4.1304693
  7.2958503   8.454953    2.967984    9.869264    7.2374787   8.593143
  5.7742076   7.5300508   7.2095394   7.823381    2.118999    5.039945
  4.053479    9.185997    0.9152442  10.862353    8.841984    0.75618994
  4.901044    7.2554336   7.1853967   9.084257    8.29529     4.220908
  5.8150444   7.7635503   1.2401824   1.3507668   4.54757     3.457343
  7.9834795   5.028612    3.85299     8.574433    8.163936   10.616821
  8.4081      4.1708117   1.2379287   8.800616    1.4772482   4.080577
  8.428701   10.380288    6.389518    6.8510385   0.61914694  4.0696154
  6.8948746   3.383478    0.4705223   3.009568    2.6594605   3.048335
  5.988357    7.1357174   3.261515    4.0008183   0.27625528 -0.3875654
  2.6110508   6.7222776   4.8376966   7.503794    6.6424117   8.686342
  7.1706657   9.419308    7.926278    6.2358265   6.0555067  -0.70361733
 10.064413    1.4912657   8.07811     7.900392    4.8439436   2.0139859
  1.0714337   7.2137423   4.526915    6.6356516   9.933811    1.5130765
  9.312532    3.7438834   3.550579    9.596389    9.815127    1.7642053
  7.4918814  -0.49520278  6.4749107   9.756271    7.145049    4.039104
  3.7851322   4.082335    6.1268287   8.183756    9.32123     0.28198594
  2.5236871  -0.31258178  2.2504802   3.5232162   4.9025984   6.3538814
  7.8291597   9.178961   10.243786    7.067563    8.285971    9.500681
  3.5682902   8.450219    0.17376387  0.216589    2.7681222   4.1601896
  6.1601834   1.706507    6.643297    7.9561872   1.7723906   4.5958123
  5.858561    4.6678243   8.187405    9.715012    8.1015      0.45350024
  7.7912116   0.77989995  2.3104167   1.3162912   1.205177    5.248102
  6.7170506   5.558635    9.548942    2.291232    7.7552695   8.649344
  5.677332    4.394188    6.64837     4.7241607   5.719767    3.967581
  4.259147    7.17209     1.0130833   1.7905484   8.393191    7.4030704
  2.735002    2.5851526   9.099808    7.8557224   6.1393533   2.1640992
  0.7774042   4.113809    6.8772874   9.341514    7.624882   -0.15643695
  2.3431437   3.3053102   1.6893227   7.5518737   7.47631     5.387754
  9.710713    1.6661869   8.289583    1.1974262   3.0808344   7.3471923
  0.8228001   5.6459312   3.6627483   7.2736874   8.484636    7.5182805
  6.5065517   0.4700985   2.234543    5.2005877   3.5432315   2.8302953
  2.0971265   9.56212     3.0628858   4.7415     -0.44130558  9.013574
  1.085037    3.311532    2.5385356   8.806095    2.0905962   7.8503737
  8.578108    9.528165    0.590898    1.9645467   7.9985986   1.753687
  3.172571    1.6611466   1.8709267   4.7864184   1.3145969   3.1479964
  4.083562    0.2390917   3.327033    0.9105044   6.7123947   7.861191
  7.02552     0.75634694  5.9596577   8.067755    2.4828937   7.6143017
  8.580062    7.3855762  -0.04546678  0.0738206   1.8115419   2.7274795
  6.6129613   3.8759396   6.692868    3.7261245   1.3076189   3.1666129
  9.223293    9.462051    2.376804    7.5547466   2.749257    8.444417
  1.3476957   6.958887    0.12667698  9.694718    6.407499    8.479325
  5.627874    4.950144    6.2193155   5.3255057   4.9456944   8.290707
  0.8348843   2.2261314   4.1954646   3.6872017   0.73160475  8.504263
  7.791428    5.3747725   9.8119135   0.7039877   7.6769605   2.5379546
  3.694837    2.045665    7.2654924   0.12175488  7.247275    7.029465
  0.6658832   8.352133    5.5662146   9.030525    7.8712974   5.6255913
  8.1955      3.5710793   4.832326    5.828453   10.745017    1.85231
 -0.526502    3.3114655   4.8024893   0.62556016  4.9190855   9.197764
  6.527681    8.61912     9.540909    9.433219    7.9817605   5.9839487
  4.9048758   0.6134869   5.948723   10.432518    8.046861    3.3048022
  1.5137587   7.991438    7.427192    9.765905    8.476495    5.9867887
  1.0616344   5.6400924   2.9184952   5.1216874   2.580817    8.646928
  7.5732      4.096987    6.089647    4.3319507   8.435398    2.1281555
  7.02788     9.129615    1.7530978   1.3687971   7.042738    7.6120076
  4.263667    6.845998    3.1399002   4.1320887   1.0887251   3.7039056
  2.7662733   3.7930636   9.0424595   1.3231362   3.5029469   6.702694
  9.477917    0.9764585   9.146437    0.77568066  5.0711017   4.2204366
  1.9013925   3.6039436   4.875466    8.647586    9.651502    6.2052207
  8.608539    6.7567143   3.158669    1.8811914   4.3497324   5.161394
  0.6098187   1.152159    6.051886    1.3697395   8.290108    2.6722713
  2.2818537   0.9584924   9.105338    1.0756454   1.5344902   8.241914
  2.561514    6.2516336   7.197662    6.6100316   8.612244    7.1160245
  1.4579626   5.4742584   6.604126    8.5800705   0.06284574  7.271172
  9.004775    7.700049    5.588264    4.5855756   4.7795944   7.2477436
  8.905804    6.081058    9.249524    7.4357276   0.8587049   4.7164693
  2.8504999   3.989924    3.8770769   2.9917765   5.714236    6.905015
  2.6859546   1.846874    0.41534102  4.812243    1.8113589  -0.9216505
  9.0558815   4.663535    1.0859932   8.355898    7.844323    4.0918174
  0.6433816   1.5224574   4.468763    8.928121    1.204919    8.635436
 -0.03399917  4.6824536   6.482672    0.5008905   5.682656    8.837162
  2.072514    7.1392884   6.1446853   2.1056101   0.07073235  0.26507834
  5.238253    9.371503    6.251286    7.188611    5.0308127   6.007279
  1.024926    8.809044    7.712478   10.019146    2.5657964   3.195103
  0.9103755   9.777725    7.5711966   3.4351816   5.4496226   4.112291
  1.660826    1.7369623   9.639072    2.935317    9.930944    0.73487484
  5.733989    1.3034487   0.502199    5.5747876   1.0674012   8.124411
  7.0287094   2.9280264   9.545342    7.30834     2.6214435   7.561452
  7.846946    1.3374016   3.65282     3.05842     6.171471    8.9460125
  1.6092017   8.305956   -0.07000089  6.6192155   4.523222    7.941383
  7.4171886   4.9010735   5.585139    5.0534396 ]
Epoch 1/1000
2023-10-02 17:02:45.144 
Epoch 1/1000 
	 loss: 427.5744, MinusLogProbMetric: 427.5744, val_loss: 401.2196, val_MinusLogProbMetric: 401.2196

Epoch 1: val_loss improved from inf to 401.21957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 36s - loss: 427.5744 - MinusLogProbMetric: 427.5744 - val_loss: 401.2196 - val_MinusLogProbMetric: 401.2196 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 2/1000
2023-10-02 17:02:55.852 
Epoch 2/1000 
	 loss: 397.9464, MinusLogProbMetric: 397.9464, val_loss: 400.7593, val_MinusLogProbMetric: 400.7593

Epoch 2: val_loss improved from 401.21957 to 400.75934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 397.9464 - MinusLogProbMetric: 397.9464 - val_loss: 400.7593 - val_MinusLogProbMetric: 400.7593 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 3/1000
2023-10-02 17:03:06.522 
Epoch 3/1000 
	 loss: 398.0366, MinusLogProbMetric: 398.0366, val_loss: 402.0090, val_MinusLogProbMetric: 402.0090

Epoch 3: val_loss did not improve from 400.75934
196/196 - 10s - loss: 398.0366 - MinusLogProbMetric: 398.0366 - val_loss: 402.0090 - val_MinusLogProbMetric: 402.0090 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 4/1000
2023-10-02 17:03:16.906 
Epoch 4/1000 
	 loss: 398.3493, MinusLogProbMetric: 398.3493, val_loss: 402.0650, val_MinusLogProbMetric: 402.0650

Epoch 4: val_loss did not improve from 400.75934
196/196 - 10s - loss: 398.3493 - MinusLogProbMetric: 398.3493 - val_loss: 402.0650 - val_MinusLogProbMetric: 402.0650 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 5/1000
2023-10-02 17:03:27.837 
Epoch 5/1000 
	 loss: 398.0761, MinusLogProbMetric: 398.0761, val_loss: 400.9003, val_MinusLogProbMetric: 400.9003

Epoch 5: val_loss did not improve from 400.75934
196/196 - 11s - loss: 398.0761 - MinusLogProbMetric: 398.0761 - val_loss: 400.9003 - val_MinusLogProbMetric: 400.9003 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 6/1000
2023-10-02 17:03:38.716 
Epoch 6/1000 
	 loss: 398.2986, MinusLogProbMetric: 398.2986, val_loss: 401.1960, val_MinusLogProbMetric: 401.1960

Epoch 6: val_loss did not improve from 400.75934
196/196 - 11s - loss: 398.2986 - MinusLogProbMetric: 398.2986 - val_loss: 401.1960 - val_MinusLogProbMetric: 401.1960 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 7/1000
2023-10-02 17:03:49.112 
Epoch 7/1000 
	 loss: 398.4642, MinusLogProbMetric: 398.4642, val_loss: 400.5429, val_MinusLogProbMetric: 400.5429

Epoch 7: val_loss improved from 400.75934 to 400.54294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 398.4642 - MinusLogProbMetric: 398.4642 - val_loss: 400.5429 - val_MinusLogProbMetric: 400.5429 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 8/1000
2023-10-02 17:03:59.964 
Epoch 8/1000 
	 loss: 397.9949, MinusLogProbMetric: 397.9949, val_loss: 400.5211, val_MinusLogProbMetric: 400.5211

Epoch 8: val_loss improved from 400.54294 to 400.52109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 397.9949 - MinusLogProbMetric: 397.9949 - val_loss: 400.5211 - val_MinusLogProbMetric: 400.5211 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 9/1000
2023-10-02 17:04:10.677 
Epoch 9/1000 
	 loss: 398.1116, MinusLogProbMetric: 398.1116, val_loss: 401.1837, val_MinusLogProbMetric: 401.1837

Epoch 9: val_loss did not improve from 400.52109
196/196 - 10s - loss: 398.1116 - MinusLogProbMetric: 398.1116 - val_loss: 401.1837 - val_MinusLogProbMetric: 401.1837 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 10/1000
2023-10-02 17:04:21.171 
Epoch 10/1000 
	 loss: 397.9171, MinusLogProbMetric: 397.9171, val_loss: 400.8842, val_MinusLogProbMetric: 400.8842

Epoch 10: val_loss did not improve from 400.52109
196/196 - 10s - loss: 397.9171 - MinusLogProbMetric: 397.9171 - val_loss: 400.8842 - val_MinusLogProbMetric: 400.8842 - lr: 1.1111e-04 - 10s/epoch - 54ms/step
Epoch 11/1000
2023-10-02 17:04:31.660 
Epoch 11/1000 
	 loss: 397.9296, MinusLogProbMetric: 397.9296, val_loss: 400.7427, val_MinusLogProbMetric: 400.7427

Epoch 11: val_loss did not improve from 400.52109
196/196 - 10s - loss: 397.9296 - MinusLogProbMetric: 397.9296 - val_loss: 400.7427 - val_MinusLogProbMetric: 400.7427 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 12/1000
2023-10-02 17:04:42.186 
Epoch 12/1000 
	 loss: 398.1787, MinusLogProbMetric: 398.1787, val_loss: 402.1448, val_MinusLogProbMetric: 402.1448

Epoch 12: val_loss did not improve from 400.52109
196/196 - 11s - loss: 398.1787 - MinusLogProbMetric: 398.1787 - val_loss: 402.1448 - val_MinusLogProbMetric: 402.1448 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 13/1000
2023-10-02 17:04:52.677 
Epoch 13/1000 
	 loss: 397.4511, MinusLogProbMetric: 397.4511, val_loss: 404.5731, val_MinusLogProbMetric: 404.5731

Epoch 13: val_loss did not improve from 400.52109
196/196 - 10s - loss: 397.4511 - MinusLogProbMetric: 397.4511 - val_loss: 404.5731 - val_MinusLogProbMetric: 404.5731 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 14/1000
2023-10-02 17:05:03.103 
Epoch 14/1000 
	 loss: 398.6885, MinusLogProbMetric: 398.6885, val_loss: 402.5054, val_MinusLogProbMetric: 402.5054

Epoch 14: val_loss did not improve from 400.52109
196/196 - 10s - loss: 398.6885 - MinusLogProbMetric: 398.6885 - val_loss: 402.5054 - val_MinusLogProbMetric: 402.5054 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 15/1000
2023-10-02 17:05:13.513 
Epoch 15/1000 
	 loss: 397.1949, MinusLogProbMetric: 397.1949, val_loss: 402.3190, val_MinusLogProbMetric: 402.3190

Epoch 15: val_loss did not improve from 400.52109
196/196 - 10s - loss: 397.1949 - MinusLogProbMetric: 397.1949 - val_loss: 402.3190 - val_MinusLogProbMetric: 402.3190 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 16/1000
2023-10-02 17:05:24.035 
Epoch 16/1000 
	 loss: 397.5980, MinusLogProbMetric: 397.5980, val_loss: 405.8434, val_MinusLogProbMetric: 405.8434

Epoch 16: val_loss did not improve from 400.52109
196/196 - 11s - loss: 397.5980 - MinusLogProbMetric: 397.5980 - val_loss: 405.8434 - val_MinusLogProbMetric: 405.8434 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 17/1000
2023-10-02 17:05:34.469 
Epoch 17/1000 
	 loss: 397.5206, MinusLogProbMetric: 397.5206, val_loss: 402.1085, val_MinusLogProbMetric: 402.1085

Epoch 17: val_loss did not improve from 400.52109
196/196 - 10s - loss: 397.5206 - MinusLogProbMetric: 397.5206 - val_loss: 402.1085 - val_MinusLogProbMetric: 402.1085 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 18/1000
2023-10-02 17:05:45.052 
Epoch 18/1000 
	 loss: 397.2413, MinusLogProbMetric: 397.2413, val_loss: 399.9591, val_MinusLogProbMetric: 399.9591

Epoch 18: val_loss improved from 400.52109 to 399.95914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 397.2413 - MinusLogProbMetric: 397.2413 - val_loss: 399.9591 - val_MinusLogProbMetric: 399.9591 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 19/1000
2023-10-02 17:05:55.820 
Epoch 19/1000 
	 loss: 397.4246, MinusLogProbMetric: 397.4246, val_loss: 400.7899, val_MinusLogProbMetric: 400.7899

Epoch 19: val_loss did not improve from 399.95914
196/196 - 10s - loss: 397.4246 - MinusLogProbMetric: 397.4246 - val_loss: 400.7899 - val_MinusLogProbMetric: 400.7899 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 20/1000
2023-10-02 17:06:06.056 
Epoch 20/1000 
	 loss: 397.2258, MinusLogProbMetric: 397.2258, val_loss: 400.6150, val_MinusLogProbMetric: 400.6150

Epoch 20: val_loss did not improve from 399.95914
196/196 - 10s - loss: 397.2258 - MinusLogProbMetric: 397.2258 - val_loss: 400.6150 - val_MinusLogProbMetric: 400.6150 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 21/1000
2023-10-02 17:06:16.604 
Epoch 21/1000 
	 loss: 397.3426, MinusLogProbMetric: 397.3426, val_loss: 402.8856, val_MinusLogProbMetric: 402.8856

Epoch 21: val_loss did not improve from 399.95914
196/196 - 11s - loss: 397.3426 - MinusLogProbMetric: 397.3426 - val_loss: 402.8856 - val_MinusLogProbMetric: 402.8856 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 22/1000
2023-10-02 17:06:26.965 
Epoch 22/1000 
	 loss: 397.0838, MinusLogProbMetric: 397.0838, val_loss: 400.4574, val_MinusLogProbMetric: 400.4574

Epoch 22: val_loss did not improve from 399.95914
196/196 - 10s - loss: 397.0838 - MinusLogProbMetric: 397.0838 - val_loss: 400.4574 - val_MinusLogProbMetric: 400.4574 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 23/1000
2023-10-02 17:06:37.625 
Epoch 23/1000 
	 loss: 397.6022, MinusLogProbMetric: 397.6022, val_loss: 399.7114, val_MinusLogProbMetric: 399.7114

Epoch 23: val_loss improved from 399.95914 to 399.71140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 397.6022 - MinusLogProbMetric: 397.6022 - val_loss: 399.7114 - val_MinusLogProbMetric: 399.7114 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 24/1000
2023-10-02 17:06:48.383 
Epoch 24/1000 
	 loss: 397.1133, MinusLogProbMetric: 397.1133, val_loss: 399.5577, val_MinusLogProbMetric: 399.5577

Epoch 24: val_loss improved from 399.71140 to 399.55768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 397.1133 - MinusLogProbMetric: 397.1133 - val_loss: 399.5577 - val_MinusLogProbMetric: 399.5577 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 25/1000
2023-10-02 17:06:59.178 
Epoch 25/1000 
	 loss: 397.0509, MinusLogProbMetric: 397.0509, val_loss: 405.6939, val_MinusLogProbMetric: 405.6939

Epoch 25: val_loss did not improve from 399.55768
196/196 - 10s - loss: 397.0509 - MinusLogProbMetric: 397.0509 - val_loss: 405.6939 - val_MinusLogProbMetric: 405.6939 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 26/1000
2023-10-02 17:07:09.595 
Epoch 26/1000 
	 loss: 396.7229, MinusLogProbMetric: 396.7229, val_loss: 399.9972, val_MinusLogProbMetric: 399.9972

Epoch 26: val_loss did not improve from 399.55768
196/196 - 10s - loss: 396.7229 - MinusLogProbMetric: 396.7229 - val_loss: 399.9972 - val_MinusLogProbMetric: 399.9972 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 27/1000
2023-10-02 17:07:20.018 
Epoch 27/1000 
	 loss: 397.2307, MinusLogProbMetric: 397.2307, val_loss: 399.8289, val_MinusLogProbMetric: 399.8289

Epoch 27: val_loss did not improve from 399.55768
196/196 - 10s - loss: 397.2307 - MinusLogProbMetric: 397.2307 - val_loss: 399.8289 - val_MinusLogProbMetric: 399.8289 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 28/1000
2023-10-02 17:07:30.601 
Epoch 28/1000 
	 loss: 396.6280, MinusLogProbMetric: 396.6280, val_loss: 403.5522, val_MinusLogProbMetric: 403.5522

Epoch 28: val_loss did not improve from 399.55768
196/196 - 11s - loss: 396.6280 - MinusLogProbMetric: 396.6280 - val_loss: 403.5522 - val_MinusLogProbMetric: 403.5522 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 29/1000
2023-10-02 17:07:41.076 
Epoch 29/1000 
	 loss: 397.1391, MinusLogProbMetric: 397.1391, val_loss: 403.6900, val_MinusLogProbMetric: 403.6900

Epoch 29: val_loss did not improve from 399.55768
196/196 - 10s - loss: 397.1391 - MinusLogProbMetric: 397.1391 - val_loss: 403.6900 - val_MinusLogProbMetric: 403.6900 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 30/1000
2023-10-02 17:07:51.471 
Epoch 30/1000 
	 loss: 396.5184, MinusLogProbMetric: 396.5184, val_loss: 399.2454, val_MinusLogProbMetric: 399.2454

Epoch 30: val_loss improved from 399.55768 to 399.24536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 396.5184 - MinusLogProbMetric: 396.5184 - val_loss: 399.2454 - val_MinusLogProbMetric: 399.2454 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 31/1000
2023-10-02 17:08:02.107 
Epoch 31/1000 
	 loss: 396.6505, MinusLogProbMetric: 396.6505, val_loss: 399.5509, val_MinusLogProbMetric: 399.5509

Epoch 31: val_loss did not improve from 399.24536
196/196 - 10s - loss: 396.6505 - MinusLogProbMetric: 396.6505 - val_loss: 399.5509 - val_MinusLogProbMetric: 399.5509 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 32/1000
2023-10-02 17:08:12.611 
Epoch 32/1000 
	 loss: 396.7075, MinusLogProbMetric: 396.7075, val_loss: 399.4903, val_MinusLogProbMetric: 399.4903

Epoch 32: val_loss did not improve from 399.24536
196/196 - 10s - loss: 396.7075 - MinusLogProbMetric: 396.7075 - val_loss: 399.4903 - val_MinusLogProbMetric: 399.4903 - lr: 1.1111e-04 - 10s/epoch - 54ms/step
Epoch 33/1000
2023-10-02 17:08:23.109 
Epoch 33/1000 
	 loss: 396.6030, MinusLogProbMetric: 396.6030, val_loss: 401.4265, val_MinusLogProbMetric: 401.4265

Epoch 33: val_loss did not improve from 399.24536
196/196 - 10s - loss: 396.6030 - MinusLogProbMetric: 396.6030 - val_loss: 401.4265 - val_MinusLogProbMetric: 401.4265 - lr: 1.1111e-04 - 10s/epoch - 54ms/step
Epoch 34/1000
2023-10-02 17:08:33.557 
Epoch 34/1000 
	 loss: 396.3810, MinusLogProbMetric: 396.3810, val_loss: 401.2238, val_MinusLogProbMetric: 401.2238

Epoch 34: val_loss did not improve from 399.24536
196/196 - 10s - loss: 396.3810 - MinusLogProbMetric: 396.3810 - val_loss: 401.2238 - val_MinusLogProbMetric: 401.2238 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 35/1000
2023-10-02 17:08:44.133 
Epoch 35/1000 
	 loss: 396.7971, MinusLogProbMetric: 396.7971, val_loss: 401.2055, val_MinusLogProbMetric: 401.2055

Epoch 35: val_loss did not improve from 399.24536
196/196 - 11s - loss: 396.7971 - MinusLogProbMetric: 396.7971 - val_loss: 401.2055 - val_MinusLogProbMetric: 401.2055 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 36/1000
2023-10-02 17:08:54.557 
Epoch 36/1000 
	 loss: 396.1720, MinusLogProbMetric: 396.1720, val_loss: 398.8221, val_MinusLogProbMetric: 398.8221

Epoch 36: val_loss improved from 399.24536 to 398.82205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 396.1720 - MinusLogProbMetric: 396.1720 - val_loss: 398.8221 - val_MinusLogProbMetric: 398.8221 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 37/1000
2023-10-02 17:09:05.333 
Epoch 37/1000 
	 loss: 396.2191, MinusLogProbMetric: 396.2191, val_loss: 400.4993, val_MinusLogProbMetric: 400.4993

Epoch 37: val_loss did not improve from 398.82205
196/196 - 10s - loss: 396.2191 - MinusLogProbMetric: 396.2191 - val_loss: 400.4993 - val_MinusLogProbMetric: 400.4993 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 38/1000
2023-10-02 17:09:15.547 
Epoch 38/1000 
	 loss: 396.3727, MinusLogProbMetric: 396.3727, val_loss: 401.3749, val_MinusLogProbMetric: 401.3749

Epoch 38: val_loss did not improve from 398.82205
196/196 - 10s - loss: 396.3727 - MinusLogProbMetric: 396.3727 - val_loss: 401.3749 - val_MinusLogProbMetric: 401.3749 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 39/1000
2023-10-02 17:09:25.947 
Epoch 39/1000 
	 loss: 396.3140, MinusLogProbMetric: 396.3140, val_loss: 399.1910, val_MinusLogProbMetric: 399.1910

Epoch 39: val_loss did not improve from 398.82205
196/196 - 10s - loss: 396.3140 - MinusLogProbMetric: 396.3140 - val_loss: 399.1910 - val_MinusLogProbMetric: 399.1910 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 40/1000
2023-10-02 17:09:36.367 
Epoch 40/1000 
	 loss: 396.5542, MinusLogProbMetric: 396.5542, val_loss: 399.0566, val_MinusLogProbMetric: 399.0566

Epoch 40: val_loss did not improve from 398.82205
196/196 - 10s - loss: 396.5542 - MinusLogProbMetric: 396.5542 - val_loss: 399.0566 - val_MinusLogProbMetric: 399.0566 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 41/1000
2023-10-02 17:09:47.183 
Epoch 41/1000 
	 loss: 395.9407, MinusLogProbMetric: 395.9407, val_loss: 399.5484, val_MinusLogProbMetric: 399.5484

Epoch 41: val_loss did not improve from 398.82205
196/196 - 11s - loss: 395.9407 - MinusLogProbMetric: 395.9407 - val_loss: 399.5484 - val_MinusLogProbMetric: 399.5484 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 42/1000
2023-10-02 17:09:57.734 
Epoch 42/1000 
	 loss: 396.1823, MinusLogProbMetric: 396.1823, val_loss: 398.9574, val_MinusLogProbMetric: 398.9574

Epoch 42: val_loss did not improve from 398.82205
196/196 - 11s - loss: 396.1823 - MinusLogProbMetric: 396.1823 - val_loss: 398.9574 - val_MinusLogProbMetric: 398.9574 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 43/1000
2023-10-02 17:10:08.193 
Epoch 43/1000 
	 loss: 396.0416, MinusLogProbMetric: 396.0416, val_loss: 399.1399, val_MinusLogProbMetric: 399.1399

Epoch 43: val_loss did not improve from 398.82205
196/196 - 10s - loss: 396.0416 - MinusLogProbMetric: 396.0416 - val_loss: 399.1399 - val_MinusLogProbMetric: 399.1399 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 44/1000
2023-10-02 17:10:18.500 
Epoch 44/1000 
	 loss: 396.0913, MinusLogProbMetric: 396.0913, val_loss: 399.4560, val_MinusLogProbMetric: 399.4560

Epoch 44: val_loss did not improve from 398.82205
196/196 - 10s - loss: 396.0913 - MinusLogProbMetric: 396.0913 - val_loss: 399.4560 - val_MinusLogProbMetric: 399.4560 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 45/1000
2023-10-02 17:10:29.215 
Epoch 45/1000 
	 loss: 396.2006, MinusLogProbMetric: 396.2006, val_loss: 400.8075, val_MinusLogProbMetric: 400.8075

Epoch 45: val_loss did not improve from 398.82205
196/196 - 11s - loss: 396.2006 - MinusLogProbMetric: 396.2006 - val_loss: 400.8075 - val_MinusLogProbMetric: 400.8075 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 46/1000
2023-10-02 17:10:39.816 
Epoch 46/1000 
	 loss: 396.2196, MinusLogProbMetric: 396.2196, val_loss: 400.7804, val_MinusLogProbMetric: 400.7804

Epoch 46: val_loss did not improve from 398.82205
196/196 - 11s - loss: 396.2196 - MinusLogProbMetric: 396.2196 - val_loss: 400.7804 - val_MinusLogProbMetric: 400.7804 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 47/1000
2023-10-02 17:10:50.332 
Epoch 47/1000 
	 loss: 395.9905, MinusLogProbMetric: 395.9905, val_loss: 398.9387, val_MinusLogProbMetric: 398.9387

Epoch 47: val_loss did not improve from 398.82205
196/196 - 11s - loss: 395.9905 - MinusLogProbMetric: 395.9905 - val_loss: 398.9387 - val_MinusLogProbMetric: 398.9387 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 48/1000
2023-10-02 17:11:00.673 
Epoch 48/1000 
	 loss: 395.8951, MinusLogProbMetric: 395.8951, val_loss: 400.6882, val_MinusLogProbMetric: 400.6882

Epoch 48: val_loss did not improve from 398.82205
196/196 - 10s - loss: 395.8951 - MinusLogProbMetric: 395.8951 - val_loss: 400.6882 - val_MinusLogProbMetric: 400.6882 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 49/1000
2023-10-02 17:11:10.807 
Epoch 49/1000 
	 loss: 395.9710, MinusLogProbMetric: 395.9710, val_loss: 399.9731, val_MinusLogProbMetric: 399.9731

Epoch 49: val_loss did not improve from 398.82205
196/196 - 10s - loss: 395.9710 - MinusLogProbMetric: 395.9710 - val_loss: 399.9731 - val_MinusLogProbMetric: 399.9731 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 50/1000
2023-10-02 17:11:21.376 
Epoch 50/1000 
	 loss: 395.6158, MinusLogProbMetric: 395.6158, val_loss: 399.8027, val_MinusLogProbMetric: 399.8027

Epoch 50: val_loss did not improve from 398.82205
196/196 - 11s - loss: 395.6158 - MinusLogProbMetric: 395.6158 - val_loss: 399.8027 - val_MinusLogProbMetric: 399.8027 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 51/1000
2023-10-02 17:11:31.970 
Epoch 51/1000 
	 loss: 395.7036, MinusLogProbMetric: 395.7036, val_loss: 399.4404, val_MinusLogProbMetric: 399.4404

Epoch 51: val_loss did not improve from 398.82205
196/196 - 11s - loss: 395.7036 - MinusLogProbMetric: 395.7036 - val_loss: 399.4404 - val_MinusLogProbMetric: 399.4404 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 52/1000
2023-10-02 17:11:42.496 
Epoch 52/1000 
	 loss: 396.1635, MinusLogProbMetric: 396.1635, val_loss: 399.1494, val_MinusLogProbMetric: 399.1494

Epoch 52: val_loss did not improve from 398.82205
196/196 - 11s - loss: 396.1635 - MinusLogProbMetric: 396.1635 - val_loss: 399.1494 - val_MinusLogProbMetric: 399.1494 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 53/1000
2023-10-02 17:11:52.942 
Epoch 53/1000 
	 loss: 395.2079, MinusLogProbMetric: 395.2079, val_loss: 403.6183, val_MinusLogProbMetric: 403.6183

Epoch 53: val_loss did not improve from 398.82205
196/196 - 10s - loss: 395.2079 - MinusLogProbMetric: 395.2079 - val_loss: 403.6183 - val_MinusLogProbMetric: 403.6183 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 54/1000
2023-10-02 17:12:03.372 
Epoch 54/1000 
	 loss: 395.9384, MinusLogProbMetric: 395.9384, val_loss: 398.7336, val_MinusLogProbMetric: 398.7336

Epoch 54: val_loss improved from 398.82205 to 398.73358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 395.9384 - MinusLogProbMetric: 395.9384 - val_loss: 398.7336 - val_MinusLogProbMetric: 398.7336 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 55/1000
2023-10-02 17:12:14.158 
Epoch 55/1000 
	 loss: 395.5429, MinusLogProbMetric: 395.5429, val_loss: 400.2546, val_MinusLogProbMetric: 400.2546

Epoch 55: val_loss did not improve from 398.73358
196/196 - 10s - loss: 395.5429 - MinusLogProbMetric: 395.5429 - val_loss: 400.2546 - val_MinusLogProbMetric: 400.2546 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 56/1000
2023-10-02 17:12:24.754 
Epoch 56/1000 
	 loss: 395.5811, MinusLogProbMetric: 395.5811, val_loss: 399.8430, val_MinusLogProbMetric: 399.8430

Epoch 56: val_loss did not improve from 398.73358
196/196 - 11s - loss: 395.5811 - MinusLogProbMetric: 395.5811 - val_loss: 399.8430 - val_MinusLogProbMetric: 399.8430 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 57/1000
2023-10-02 17:12:35.207 
Epoch 57/1000 
	 loss: 395.5130, MinusLogProbMetric: 395.5130, val_loss: 400.5087, val_MinusLogProbMetric: 400.5087

Epoch 57: val_loss did not improve from 398.73358
196/196 - 10s - loss: 395.5130 - MinusLogProbMetric: 395.5130 - val_loss: 400.5087 - val_MinusLogProbMetric: 400.5087 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 58/1000
2023-10-02 17:12:45.764 
Epoch 58/1000 
	 loss: 395.5675, MinusLogProbMetric: 395.5675, val_loss: 399.0146, val_MinusLogProbMetric: 399.0146

Epoch 58: val_loss did not improve from 398.73358
196/196 - 11s - loss: 395.5675 - MinusLogProbMetric: 395.5675 - val_loss: 399.0146 - val_MinusLogProbMetric: 399.0146 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 59/1000
2023-10-02 17:12:56.269 
Epoch 59/1000 
	 loss: 395.3373, MinusLogProbMetric: 395.3373, val_loss: 400.0153, val_MinusLogProbMetric: 400.0153

Epoch 59: val_loss did not improve from 398.73358
196/196 - 10s - loss: 395.3373 - MinusLogProbMetric: 395.3373 - val_loss: 400.0153 - val_MinusLogProbMetric: 400.0153 - lr: 1.1111e-04 - 10s/epoch - 54ms/step
Epoch 60/1000
2023-10-02 17:13:06.674 
Epoch 60/1000 
	 loss: 395.1716, MinusLogProbMetric: 395.1716, val_loss: 401.9436, val_MinusLogProbMetric: 401.9436

Epoch 60: val_loss did not improve from 398.73358
196/196 - 10s - loss: 395.1716 - MinusLogProbMetric: 395.1716 - val_loss: 401.9436 - val_MinusLogProbMetric: 401.9436 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 61/1000
2023-10-02 17:13:17.210 
Epoch 61/1000 
	 loss: 395.4292, MinusLogProbMetric: 395.4292, val_loss: 401.0574, val_MinusLogProbMetric: 401.0574

Epoch 61: val_loss did not improve from 398.73358
196/196 - 11s - loss: 395.4292 - MinusLogProbMetric: 395.4292 - val_loss: 401.0574 - val_MinusLogProbMetric: 401.0574 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 62/1000
2023-10-02 17:13:27.637 
Epoch 62/1000 
	 loss: 395.3591, MinusLogProbMetric: 395.3591, val_loss: 399.4193, val_MinusLogProbMetric: 399.4193

Epoch 62: val_loss did not improve from 398.73358
196/196 - 10s - loss: 395.3591 - MinusLogProbMetric: 395.3591 - val_loss: 399.4193 - val_MinusLogProbMetric: 399.4193 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 63/1000
2023-10-02 17:13:38.065 
Epoch 63/1000 
	 loss: 395.2420, MinusLogProbMetric: 395.2420, val_loss: 400.5980, val_MinusLogProbMetric: 400.5980

Epoch 63: val_loss did not improve from 398.73358
196/196 - 10s - loss: 395.2420 - MinusLogProbMetric: 395.2420 - val_loss: 400.5980 - val_MinusLogProbMetric: 400.5980 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 64/1000
2023-10-02 17:13:48.506 
Epoch 64/1000 
	 loss: 395.2594, MinusLogProbMetric: 395.2594, val_loss: 399.1082, val_MinusLogProbMetric: 399.1082

Epoch 64: val_loss did not improve from 398.73358
196/196 - 10s - loss: 395.2594 - MinusLogProbMetric: 395.2594 - val_loss: 399.1082 - val_MinusLogProbMetric: 399.1082 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 65/1000
2023-10-02 17:13:59.102 
Epoch 65/1000 
	 loss: 395.0564, MinusLogProbMetric: 395.0564, val_loss: 399.0031, val_MinusLogProbMetric: 399.0031

Epoch 65: val_loss did not improve from 398.73358
196/196 - 11s - loss: 395.0564 - MinusLogProbMetric: 395.0564 - val_loss: 399.0031 - val_MinusLogProbMetric: 399.0031 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 66/1000
2023-10-02 17:14:09.600 
Epoch 66/1000 
	 loss: 395.0998, MinusLogProbMetric: 395.0998, val_loss: 398.6038, val_MinusLogProbMetric: 398.6038

Epoch 66: val_loss improved from 398.73358 to 398.60382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 395.0998 - MinusLogProbMetric: 395.0998 - val_loss: 398.6038 - val_MinusLogProbMetric: 398.6038 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 67/1000
2023-10-02 17:14:20.595 
Epoch 67/1000 
	 loss: 395.7030, MinusLogProbMetric: 395.7030, val_loss: 398.6687, val_MinusLogProbMetric: 398.6687

Epoch 67: val_loss did not improve from 398.60382
196/196 - 11s - loss: 395.7030 - MinusLogProbMetric: 395.7030 - val_loss: 398.6687 - val_MinusLogProbMetric: 398.6687 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 68/1000
2023-10-02 17:14:31.029 
Epoch 68/1000 
	 loss: 394.8399, MinusLogProbMetric: 394.8399, val_loss: 401.0378, val_MinusLogProbMetric: 401.0378

Epoch 68: val_loss did not improve from 398.60382
196/196 - 10s - loss: 394.8399 - MinusLogProbMetric: 394.8399 - val_loss: 401.0378 - val_MinusLogProbMetric: 401.0378 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 69/1000
2023-10-02 17:14:41.556 
Epoch 69/1000 
	 loss: 395.1963, MinusLogProbMetric: 395.1963, val_loss: 400.6007, val_MinusLogProbMetric: 400.6007

Epoch 69: val_loss did not improve from 398.60382
196/196 - 11s - loss: 395.1963 - MinusLogProbMetric: 395.1963 - val_loss: 400.6007 - val_MinusLogProbMetric: 400.6007 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 70/1000
2023-10-02 17:14:52.284 
Epoch 70/1000 
	 loss: 394.9236, MinusLogProbMetric: 394.9236, val_loss: 398.1133, val_MinusLogProbMetric: 398.1133

Epoch 70: val_loss improved from 398.60382 to 398.11328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 394.9236 - MinusLogProbMetric: 394.9236 - val_loss: 398.1133 - val_MinusLogProbMetric: 398.1133 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 71/1000
2023-10-02 17:15:03.144 
Epoch 71/1000 
	 loss: 395.0282, MinusLogProbMetric: 395.0282, val_loss: 398.9372, val_MinusLogProbMetric: 398.9372

Epoch 71: val_loss did not improve from 398.11328
196/196 - 10s - loss: 395.0282 - MinusLogProbMetric: 395.0282 - val_loss: 398.9372 - val_MinusLogProbMetric: 398.9372 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 72/1000
2023-10-02 17:15:13.545 
Epoch 72/1000 
	 loss: 395.0005, MinusLogProbMetric: 395.0005, val_loss: 398.8683, val_MinusLogProbMetric: 398.8683

Epoch 72: val_loss did not improve from 398.11328
196/196 - 10s - loss: 395.0005 - MinusLogProbMetric: 395.0005 - val_loss: 398.8683 - val_MinusLogProbMetric: 398.8683 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 73/1000
2023-10-02 17:15:23.930 
Epoch 73/1000 
	 loss: 395.1774, MinusLogProbMetric: 395.1774, val_loss: 399.0415, val_MinusLogProbMetric: 399.0415

Epoch 73: val_loss did not improve from 398.11328
196/196 - 10s - loss: 395.1774 - MinusLogProbMetric: 395.1774 - val_loss: 399.0415 - val_MinusLogProbMetric: 399.0415 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 74/1000
2023-10-02 17:15:34.341 
Epoch 74/1000 
	 loss: 394.6345, MinusLogProbMetric: 394.6345, val_loss: 398.7691, val_MinusLogProbMetric: 398.7691

Epoch 74: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.6345 - MinusLogProbMetric: 394.6345 - val_loss: 398.7691 - val_MinusLogProbMetric: 398.7691 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 75/1000
2023-10-02 17:15:44.762 
Epoch 75/1000 
	 loss: 394.5937, MinusLogProbMetric: 394.5937, val_loss: 399.0686, val_MinusLogProbMetric: 399.0686

Epoch 75: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.5937 - MinusLogProbMetric: 394.5937 - val_loss: 399.0686 - val_MinusLogProbMetric: 399.0686 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 76/1000
2023-10-02 17:15:55.195 
Epoch 76/1000 
	 loss: 394.8580, MinusLogProbMetric: 394.8580, val_loss: 398.9293, val_MinusLogProbMetric: 398.9293

Epoch 76: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.8580 - MinusLogProbMetric: 394.8580 - val_loss: 398.9293 - val_MinusLogProbMetric: 398.9293 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 77/1000
2023-10-02 17:16:05.663 
Epoch 77/1000 
	 loss: 394.8850, MinusLogProbMetric: 394.8850, val_loss: 398.5219, val_MinusLogProbMetric: 398.5219

Epoch 77: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.8850 - MinusLogProbMetric: 394.8850 - val_loss: 398.5219 - val_MinusLogProbMetric: 398.5219 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 78/1000
2023-10-02 17:16:16.299 
Epoch 78/1000 
	 loss: 394.8539, MinusLogProbMetric: 394.8539, val_loss: 400.3997, val_MinusLogProbMetric: 400.3997

Epoch 78: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.8539 - MinusLogProbMetric: 394.8539 - val_loss: 400.3997 - val_MinusLogProbMetric: 400.3997 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 79/1000
2023-10-02 17:16:26.899 
Epoch 79/1000 
	 loss: 394.7145, MinusLogProbMetric: 394.7145, val_loss: 398.6622, val_MinusLogProbMetric: 398.6622

Epoch 79: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.7145 - MinusLogProbMetric: 394.7145 - val_loss: 398.6622 - val_MinusLogProbMetric: 398.6622 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 80/1000
2023-10-02 17:16:37.516 
Epoch 80/1000 
	 loss: 394.5825, MinusLogProbMetric: 394.5825, val_loss: 398.8259, val_MinusLogProbMetric: 398.8259

Epoch 80: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.5825 - MinusLogProbMetric: 394.5825 - val_loss: 398.8259 - val_MinusLogProbMetric: 398.8259 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 81/1000
2023-10-02 17:16:48.069 
Epoch 81/1000 
	 loss: 394.4982, MinusLogProbMetric: 394.4982, val_loss: 400.2452, val_MinusLogProbMetric: 400.2452

Epoch 81: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.4982 - MinusLogProbMetric: 394.4982 - val_loss: 400.2452 - val_MinusLogProbMetric: 400.2452 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 82/1000
2023-10-02 17:16:58.453 
Epoch 82/1000 
	 loss: 394.7947, MinusLogProbMetric: 394.7947, val_loss: 398.8368, val_MinusLogProbMetric: 398.8368

Epoch 82: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.7947 - MinusLogProbMetric: 394.7947 - val_loss: 398.8368 - val_MinusLogProbMetric: 398.8368 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 83/1000
2023-10-02 17:17:08.982 
Epoch 83/1000 
	 loss: 394.5813, MinusLogProbMetric: 394.5813, val_loss: 400.0234, val_MinusLogProbMetric: 400.0234

Epoch 83: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.5813 - MinusLogProbMetric: 394.5813 - val_loss: 400.0234 - val_MinusLogProbMetric: 400.0234 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 84/1000
2023-10-02 17:17:19.330 
Epoch 84/1000 
	 loss: 394.3933, MinusLogProbMetric: 394.3933, val_loss: 400.3991, val_MinusLogProbMetric: 400.3991

Epoch 84: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.3933 - MinusLogProbMetric: 394.3933 - val_loss: 400.3991 - val_MinusLogProbMetric: 400.3991 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 85/1000
2023-10-02 17:17:30.029 
Epoch 85/1000 
	 loss: 395.0592, MinusLogProbMetric: 395.0592, val_loss: 398.3831, val_MinusLogProbMetric: 398.3831

Epoch 85: val_loss did not improve from 398.11328
196/196 - 11s - loss: 395.0592 - MinusLogProbMetric: 395.0592 - val_loss: 398.3831 - val_MinusLogProbMetric: 398.3831 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 86/1000
2023-10-02 17:17:40.458 
Epoch 86/1000 
	 loss: 394.5288, MinusLogProbMetric: 394.5288, val_loss: 399.5955, val_MinusLogProbMetric: 399.5955

Epoch 86: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.5288 - MinusLogProbMetric: 394.5288 - val_loss: 399.5955 - val_MinusLogProbMetric: 399.5955 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 87/1000
2023-10-02 17:17:50.921 
Epoch 87/1000 
	 loss: 394.4737, MinusLogProbMetric: 394.4737, val_loss: 398.9361, val_MinusLogProbMetric: 398.9361

Epoch 87: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.4737 - MinusLogProbMetric: 394.4737 - val_loss: 398.9361 - val_MinusLogProbMetric: 398.9361 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 88/1000
2023-10-02 17:18:01.440 
Epoch 88/1000 
	 loss: 394.2010, MinusLogProbMetric: 394.2010, val_loss: 402.6384, val_MinusLogProbMetric: 402.6384

Epoch 88: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.2010 - MinusLogProbMetric: 394.2010 - val_loss: 402.6384 - val_MinusLogProbMetric: 402.6384 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 89/1000
2023-10-02 17:18:12.202 
Epoch 89/1000 
	 loss: 394.2774, MinusLogProbMetric: 394.2774, val_loss: 399.2048, val_MinusLogProbMetric: 399.2048

Epoch 89: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.2774 - MinusLogProbMetric: 394.2774 - val_loss: 399.2048 - val_MinusLogProbMetric: 399.2048 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 90/1000
2023-10-02 17:18:22.769 
Epoch 90/1000 
	 loss: 394.7673, MinusLogProbMetric: 394.7673, val_loss: 398.5253, val_MinusLogProbMetric: 398.5253

Epoch 90: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.7673 - MinusLogProbMetric: 394.7673 - val_loss: 398.5253 - val_MinusLogProbMetric: 398.5253 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 91/1000
2023-10-02 17:18:33.206 
Epoch 91/1000 
	 loss: 394.3791, MinusLogProbMetric: 394.3791, val_loss: 399.4635, val_MinusLogProbMetric: 399.4635

Epoch 91: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.3791 - MinusLogProbMetric: 394.3791 - val_loss: 399.4635 - val_MinusLogProbMetric: 399.4635 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 92/1000
2023-10-02 17:18:43.921 
Epoch 92/1000 
	 loss: 394.0305, MinusLogProbMetric: 394.0305, val_loss: 399.8130, val_MinusLogProbMetric: 399.8130

Epoch 92: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.0305 - MinusLogProbMetric: 394.0305 - val_loss: 399.8130 - val_MinusLogProbMetric: 399.8130 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 93/1000
2023-10-02 17:18:54.397 
Epoch 93/1000 
	 loss: 394.4758, MinusLogProbMetric: 394.4758, val_loss: 400.2106, val_MinusLogProbMetric: 400.2106

Epoch 93: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.4758 - MinusLogProbMetric: 394.4758 - val_loss: 400.2106 - val_MinusLogProbMetric: 400.2106 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-10-02 17:19:04.881 
Epoch 94/1000 
	 loss: 394.1687, MinusLogProbMetric: 394.1687, val_loss: 398.4088, val_MinusLogProbMetric: 398.4088

Epoch 94: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.1687 - MinusLogProbMetric: 394.1687 - val_loss: 398.4088 - val_MinusLogProbMetric: 398.4088 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 95/1000
2023-10-02 17:19:15.337 
Epoch 95/1000 
	 loss: 394.1792, MinusLogProbMetric: 394.1792, val_loss: 398.9474, val_MinusLogProbMetric: 398.9474

Epoch 95: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.1792 - MinusLogProbMetric: 394.1792 - val_loss: 398.9474 - val_MinusLogProbMetric: 398.9474 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 96/1000
2023-10-02 17:19:25.945 
Epoch 96/1000 
	 loss: 394.0121, MinusLogProbMetric: 394.0121, val_loss: 400.3572, val_MinusLogProbMetric: 400.3572

Epoch 96: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.0121 - MinusLogProbMetric: 394.0121 - val_loss: 400.3572 - val_MinusLogProbMetric: 400.3572 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 97/1000
2023-10-02 17:19:36.493 
Epoch 97/1000 
	 loss: 394.1746, MinusLogProbMetric: 394.1746, val_loss: 399.2900, val_MinusLogProbMetric: 399.2900

Epoch 97: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.1746 - MinusLogProbMetric: 394.1746 - val_loss: 399.2900 - val_MinusLogProbMetric: 399.2900 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 98/1000
2023-10-02 17:19:46.895 
Epoch 98/1000 
	 loss: 394.2141, MinusLogProbMetric: 394.2141, val_loss: 398.4563, val_MinusLogProbMetric: 398.4563

Epoch 98: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.2141 - MinusLogProbMetric: 394.2141 - val_loss: 398.4563 - val_MinusLogProbMetric: 398.4563 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 99/1000
2023-10-02 17:19:57.627 
Epoch 99/1000 
	 loss: 393.9879, MinusLogProbMetric: 393.9879, val_loss: 398.7921, val_MinusLogProbMetric: 398.7921

Epoch 99: val_loss did not improve from 398.11328
196/196 - 11s - loss: 393.9879 - MinusLogProbMetric: 393.9879 - val_loss: 398.7921 - val_MinusLogProbMetric: 398.7921 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 100/1000
2023-10-02 17:20:08.118 
Epoch 100/1000 
	 loss: 394.1982, MinusLogProbMetric: 394.1982, val_loss: 398.6900, val_MinusLogProbMetric: 398.6900

Epoch 100: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.1982 - MinusLogProbMetric: 394.1982 - val_loss: 398.6900 - val_MinusLogProbMetric: 398.6900 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 101/1000
2023-10-02 17:20:18.721 
Epoch 101/1000 
	 loss: 393.8490, MinusLogProbMetric: 393.8490, val_loss: 398.7807, val_MinusLogProbMetric: 398.7807

Epoch 101: val_loss did not improve from 398.11328
196/196 - 11s - loss: 393.8490 - MinusLogProbMetric: 393.8490 - val_loss: 398.7807 - val_MinusLogProbMetric: 398.7807 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 102/1000
2023-10-02 17:20:29.189 
Epoch 102/1000 
	 loss: 394.2357, MinusLogProbMetric: 394.2357, val_loss: 399.7623, val_MinusLogProbMetric: 399.7623

Epoch 102: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.2357 - MinusLogProbMetric: 394.2357 - val_loss: 399.7623 - val_MinusLogProbMetric: 399.7623 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 103/1000
2023-10-02 17:20:39.937 
Epoch 103/1000 
	 loss: 393.9235, MinusLogProbMetric: 393.9235, val_loss: 399.1514, val_MinusLogProbMetric: 399.1514

Epoch 103: val_loss did not improve from 398.11328
196/196 - 11s - loss: 393.9235 - MinusLogProbMetric: 393.9235 - val_loss: 399.1514 - val_MinusLogProbMetric: 399.1514 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 104/1000
2023-10-02 17:20:50.474 
Epoch 104/1000 
	 loss: 394.3258, MinusLogProbMetric: 394.3258, val_loss: 399.9965, val_MinusLogProbMetric: 399.9965

Epoch 104: val_loss did not improve from 398.11328
196/196 - 11s - loss: 394.3258 - MinusLogProbMetric: 394.3258 - val_loss: 399.9965 - val_MinusLogProbMetric: 399.9965 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 105/1000
2023-10-02 17:21:00.995 
Epoch 105/1000 
	 loss: 393.7686, MinusLogProbMetric: 393.7686, val_loss: 405.3486, val_MinusLogProbMetric: 405.3486

Epoch 105: val_loss did not improve from 398.11328
196/196 - 11s - loss: 393.7686 - MinusLogProbMetric: 393.7686 - val_loss: 405.3486 - val_MinusLogProbMetric: 405.3486 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 106/1000
2023-10-02 17:21:11.452 
Epoch 106/1000 
	 loss: 394.5620, MinusLogProbMetric: 394.5620, val_loss: 405.4422, val_MinusLogProbMetric: 405.4422

Epoch 106: val_loss did not improve from 398.11328
196/196 - 10s - loss: 394.5620 - MinusLogProbMetric: 394.5620 - val_loss: 405.4422 - val_MinusLogProbMetric: 405.4422 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 107/1000
2023-10-02 17:21:22.017 
Epoch 107/1000 
	 loss: 393.9151, MinusLogProbMetric: 393.9151, val_loss: 398.4335, val_MinusLogProbMetric: 398.4335

Epoch 107: val_loss did not improve from 398.11328
196/196 - 11s - loss: 393.9151 - MinusLogProbMetric: 393.9151 - val_loss: 398.4335 - val_MinusLogProbMetric: 398.4335 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 108/1000
2023-10-02 17:21:32.452 
Epoch 108/1000 
	 loss: 393.5923, MinusLogProbMetric: 393.5923, val_loss: 401.1694, val_MinusLogProbMetric: 401.1694

Epoch 108: val_loss did not improve from 398.11328
196/196 - 10s - loss: 393.5923 - MinusLogProbMetric: 393.5923 - val_loss: 401.1694 - val_MinusLogProbMetric: 401.1694 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 109/1000
2023-10-02 17:21:42.884 
Epoch 109/1000 
	 loss: 393.6691, MinusLogProbMetric: 393.6691, val_loss: 400.6412, val_MinusLogProbMetric: 400.6412

Epoch 109: val_loss did not improve from 398.11328
196/196 - 10s - loss: 393.6691 - MinusLogProbMetric: 393.6691 - val_loss: 400.6412 - val_MinusLogProbMetric: 400.6412 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 110/1000
2023-10-02 17:21:53.544 
Epoch 110/1000 
	 loss: 393.5869, MinusLogProbMetric: 393.5869, val_loss: 399.0781, val_MinusLogProbMetric: 399.0781

Epoch 110: val_loss did not improve from 398.11328
196/196 - 11s - loss: 393.5869 - MinusLogProbMetric: 393.5869 - val_loss: 399.0781 - val_MinusLogProbMetric: 399.0781 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 111/1000
2023-10-02 17:22:03.976 
Epoch 111/1000 
	 loss: 393.7090, MinusLogProbMetric: 393.7090, val_loss: 399.8007, val_MinusLogProbMetric: 399.8007

Epoch 111: val_loss did not improve from 398.11328
196/196 - 10s - loss: 393.7090 - MinusLogProbMetric: 393.7090 - val_loss: 399.8007 - val_MinusLogProbMetric: 399.8007 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 112/1000
2023-10-02 17:22:14.351 
Epoch 112/1000 
	 loss: 393.6766, MinusLogProbMetric: 393.6766, val_loss: 399.6366, val_MinusLogProbMetric: 399.6366

Epoch 112: val_loss did not improve from 398.11328
196/196 - 10s - loss: 393.6766 - MinusLogProbMetric: 393.6766 - val_loss: 399.6366 - val_MinusLogProbMetric: 399.6366 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 113/1000
2023-10-02 17:22:24.922 
Epoch 113/1000 
	 loss: 393.6760, MinusLogProbMetric: 393.6760, val_loss: 412.9817, val_MinusLogProbMetric: 412.9817

Epoch 113: val_loss did not improve from 398.11328
196/196 - 11s - loss: 393.6760 - MinusLogProbMetric: 393.6760 - val_loss: 412.9817 - val_MinusLogProbMetric: 412.9817 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 114/1000
2023-10-02 17:22:35.671 
Epoch 114/1000 
	 loss: 393.9682, MinusLogProbMetric: 393.9682, val_loss: 397.7329, val_MinusLogProbMetric: 397.7329

Epoch 114: val_loss improved from 398.11328 to 397.73288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 393.9682 - MinusLogProbMetric: 393.9682 - val_loss: 397.7329 - val_MinusLogProbMetric: 397.7329 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 115/1000
2023-10-02 17:22:46.527 
Epoch 115/1000 
	 loss: 393.6495, MinusLogProbMetric: 393.6495, val_loss: 399.3746, val_MinusLogProbMetric: 399.3746

Epoch 115: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.6495 - MinusLogProbMetric: 393.6495 - val_loss: 399.3746 - val_MinusLogProbMetric: 399.3746 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 116/1000
2023-10-02 17:22:57.125 
Epoch 116/1000 
	 loss: 393.5367, MinusLogProbMetric: 393.5367, val_loss: 399.0032, val_MinusLogProbMetric: 399.0032

Epoch 116: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.5367 - MinusLogProbMetric: 393.5367 - val_loss: 399.0032 - val_MinusLogProbMetric: 399.0032 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 117/1000
2023-10-02 17:23:07.608 
Epoch 117/1000 
	 loss: 393.4958, MinusLogProbMetric: 393.4958, val_loss: 399.8811, val_MinusLogProbMetric: 399.8811

Epoch 117: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.4958 - MinusLogProbMetric: 393.4958 - val_loss: 399.8811 - val_MinusLogProbMetric: 399.8811 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 118/1000
2023-10-02 17:23:18.517 
Epoch 118/1000 
	 loss: 393.6300, MinusLogProbMetric: 393.6300, val_loss: 402.6618, val_MinusLogProbMetric: 402.6618

Epoch 118: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.6300 - MinusLogProbMetric: 393.6300 - val_loss: 402.6618 - val_MinusLogProbMetric: 402.6618 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 119/1000
2023-10-02 17:23:29.044 
Epoch 119/1000 
	 loss: 393.4700, MinusLogProbMetric: 393.4700, val_loss: 398.5462, val_MinusLogProbMetric: 398.5462

Epoch 119: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.4700 - MinusLogProbMetric: 393.4700 - val_loss: 398.5462 - val_MinusLogProbMetric: 398.5462 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 120/1000
2023-10-02 17:23:39.700 
Epoch 120/1000 
	 loss: 393.5919, MinusLogProbMetric: 393.5919, val_loss: 398.8121, val_MinusLogProbMetric: 398.8121

Epoch 120: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.5919 - MinusLogProbMetric: 393.5919 - val_loss: 398.8121 - val_MinusLogProbMetric: 398.8121 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 121/1000
2023-10-02 17:23:50.487 
Epoch 121/1000 
	 loss: 393.4920, MinusLogProbMetric: 393.4920, val_loss: 397.9725, val_MinusLogProbMetric: 397.9725

Epoch 121: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.4920 - MinusLogProbMetric: 393.4920 - val_loss: 397.9725 - val_MinusLogProbMetric: 397.9725 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 122/1000
2023-10-02 17:24:00.914 
Epoch 122/1000 
	 loss: 393.3185, MinusLogProbMetric: 393.3185, val_loss: 406.1432, val_MinusLogProbMetric: 406.1432

Epoch 122: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.3185 - MinusLogProbMetric: 393.3185 - val_loss: 406.1432 - val_MinusLogProbMetric: 406.1432 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 123/1000
2023-10-02 17:24:11.297 
Epoch 123/1000 
	 loss: 393.5424, MinusLogProbMetric: 393.5424, val_loss: 398.9220, val_MinusLogProbMetric: 398.9220

Epoch 123: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.5424 - MinusLogProbMetric: 393.5424 - val_loss: 398.9220 - val_MinusLogProbMetric: 398.9220 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 124/1000
2023-10-02 17:24:21.771 
Epoch 124/1000 
	 loss: 393.1694, MinusLogProbMetric: 393.1694, val_loss: 401.4893, val_MinusLogProbMetric: 401.4893

Epoch 124: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.1694 - MinusLogProbMetric: 393.1694 - val_loss: 401.4893 - val_MinusLogProbMetric: 401.4893 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 125/1000
2023-10-02 17:24:32.532 
Epoch 125/1000 
	 loss: 393.2971, MinusLogProbMetric: 393.2971, val_loss: 398.1004, val_MinusLogProbMetric: 398.1004

Epoch 125: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.2971 - MinusLogProbMetric: 393.2971 - val_loss: 398.1004 - val_MinusLogProbMetric: 398.1004 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 126/1000
2023-10-02 17:24:42.987 
Epoch 126/1000 
	 loss: 393.2482, MinusLogProbMetric: 393.2482, val_loss: 400.2401, val_MinusLogProbMetric: 400.2401

Epoch 126: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.2482 - MinusLogProbMetric: 393.2482 - val_loss: 400.2401 - val_MinusLogProbMetric: 400.2401 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 127/1000
2023-10-02 17:24:53.484 
Epoch 127/1000 
	 loss: 393.4228, MinusLogProbMetric: 393.4228, val_loss: 398.6213, val_MinusLogProbMetric: 398.6213

Epoch 127: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.4228 - MinusLogProbMetric: 393.4228 - val_loss: 398.6213 - val_MinusLogProbMetric: 398.6213 - lr: 1.1111e-04 - 10s/epoch - 54ms/step
Epoch 128/1000
2023-10-02 17:25:04.047 
Epoch 128/1000 
	 loss: 393.3855, MinusLogProbMetric: 393.3855, val_loss: 406.8202, val_MinusLogProbMetric: 406.8202

Epoch 128: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.3855 - MinusLogProbMetric: 393.3855 - val_loss: 406.8202 - val_MinusLogProbMetric: 406.8202 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 129/1000
2023-10-02 17:25:14.635 
Epoch 129/1000 
	 loss: 393.4501, MinusLogProbMetric: 393.4501, val_loss: 407.3965, val_MinusLogProbMetric: 407.3965

Epoch 129: val_loss did not improve from 397.73288
196/196 - 11s - loss: 393.4501 - MinusLogProbMetric: 393.4501 - val_loss: 407.3965 - val_MinusLogProbMetric: 407.3965 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 130/1000
2023-10-02 17:25:25.100 
Epoch 130/1000 
	 loss: 393.1618, MinusLogProbMetric: 393.1618, val_loss: 400.0213, val_MinusLogProbMetric: 400.0213

Epoch 130: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.1618 - MinusLogProbMetric: 393.1618 - val_loss: 400.0213 - val_MinusLogProbMetric: 400.0213 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 131/1000
2023-10-02 17:25:35.423 
Epoch 131/1000 
	 loss: 393.1944, MinusLogProbMetric: 393.1944, val_loss: 399.1089, val_MinusLogProbMetric: 399.1089

Epoch 131: val_loss did not improve from 397.73288
196/196 - 10s - loss: 393.1944 - MinusLogProbMetric: 393.1944 - val_loss: 399.1089 - val_MinusLogProbMetric: 399.1089 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 132/1000
2023-10-02 17:25:46.026 
Epoch 132/1000 
	 loss: 393.2517, MinusLogProbMetric: 393.2517, val_loss: 397.6894, val_MinusLogProbMetric: 397.6894

Epoch 132: val_loss improved from 397.73288 to 397.68936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 393.2517 - MinusLogProbMetric: 393.2517 - val_loss: 397.6894 - val_MinusLogProbMetric: 397.6894 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 133/1000
2023-10-02 17:25:56.659 
Epoch 133/1000 
	 loss: 393.2590, MinusLogProbMetric: 393.2590, val_loss: 398.9841, val_MinusLogProbMetric: 398.9841

Epoch 133: val_loss did not improve from 397.68936
196/196 - 10s - loss: 393.2590 - MinusLogProbMetric: 393.2590 - val_loss: 398.9841 - val_MinusLogProbMetric: 398.9841 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 134/1000
2023-10-02 17:26:07.092 
Epoch 134/1000 
	 loss: 393.6722, MinusLogProbMetric: 393.6722, val_loss: 397.8335, val_MinusLogProbMetric: 397.8335

Epoch 134: val_loss did not improve from 397.68936
196/196 - 10s - loss: 393.6722 - MinusLogProbMetric: 393.6722 - val_loss: 397.8335 - val_MinusLogProbMetric: 397.8335 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 135/1000
2023-10-02 17:26:17.499 
Epoch 135/1000 
	 loss: 393.0763, MinusLogProbMetric: 393.0763, val_loss: 399.9367, val_MinusLogProbMetric: 399.9367

Epoch 135: val_loss did not improve from 397.68936
196/196 - 10s - loss: 393.0763 - MinusLogProbMetric: 393.0763 - val_loss: 399.9367 - val_MinusLogProbMetric: 399.9367 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 136/1000
2023-10-02 17:26:28.212 
Epoch 136/1000 
	 loss: 393.1619, MinusLogProbMetric: 393.1619, val_loss: 398.6705, val_MinusLogProbMetric: 398.6705

Epoch 136: val_loss did not improve from 397.68936
196/196 - 11s - loss: 393.1619 - MinusLogProbMetric: 393.1619 - val_loss: 398.6705 - val_MinusLogProbMetric: 398.6705 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 137/1000
2023-10-02 17:26:38.749 
Epoch 137/1000 
	 loss: 392.7980, MinusLogProbMetric: 392.7980, val_loss: 399.1070, val_MinusLogProbMetric: 399.1070

Epoch 137: val_loss did not improve from 397.68936
196/196 - 11s - loss: 392.7980 - MinusLogProbMetric: 392.7980 - val_loss: 399.1070 - val_MinusLogProbMetric: 399.1070 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 138/1000
2023-10-02 17:26:49.238 
Epoch 138/1000 
	 loss: 393.1310, MinusLogProbMetric: 393.1310, val_loss: 399.3517, val_MinusLogProbMetric: 399.3517

Epoch 138: val_loss did not improve from 397.68936
196/196 - 10s - loss: 393.1310 - MinusLogProbMetric: 393.1310 - val_loss: 399.3517 - val_MinusLogProbMetric: 399.3517 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 139/1000
2023-10-02 17:26:59.934 
Epoch 139/1000 
	 loss: 393.0547, MinusLogProbMetric: 393.0547, val_loss: 398.6466, val_MinusLogProbMetric: 398.6466

Epoch 139: val_loss did not improve from 397.68936
196/196 - 11s - loss: 393.0547 - MinusLogProbMetric: 393.0547 - val_loss: 398.6466 - val_MinusLogProbMetric: 398.6466 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 140/1000
2023-10-02 17:27:10.512 
Epoch 140/1000 
	 loss: 393.5549, MinusLogProbMetric: 393.5549, val_loss: 398.6290, val_MinusLogProbMetric: 398.6290

Epoch 140: val_loss did not improve from 397.68936
196/196 - 11s - loss: 393.5549 - MinusLogProbMetric: 393.5549 - val_loss: 398.6290 - val_MinusLogProbMetric: 398.6290 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 141/1000
2023-10-02 17:27:20.830 
Epoch 141/1000 
	 loss: 392.7360, MinusLogProbMetric: 392.7360, val_loss: 397.8634, val_MinusLogProbMetric: 397.8634

Epoch 141: val_loss did not improve from 397.68936
196/196 - 10s - loss: 392.7360 - MinusLogProbMetric: 392.7360 - val_loss: 397.8634 - val_MinusLogProbMetric: 397.8634 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 142/1000
2023-10-02 17:27:31.304 
Epoch 142/1000 
	 loss: 392.8532, MinusLogProbMetric: 392.8532, val_loss: 398.6302, val_MinusLogProbMetric: 398.6302

Epoch 142: val_loss did not improve from 397.68936
196/196 - 10s - loss: 392.8532 - MinusLogProbMetric: 392.8532 - val_loss: 398.6302 - val_MinusLogProbMetric: 398.6302 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 143/1000
2023-10-02 17:27:42.066 
Epoch 143/1000 
	 loss: 393.0414, MinusLogProbMetric: 393.0414, val_loss: 398.9926, val_MinusLogProbMetric: 398.9926

Epoch 143: val_loss did not improve from 397.68936
196/196 - 11s - loss: 393.0414 - MinusLogProbMetric: 393.0414 - val_loss: 398.9926 - val_MinusLogProbMetric: 398.9926 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 144/1000
2023-10-02 17:27:51.569 
Epoch 144/1000 
	 loss: 392.7074, MinusLogProbMetric: 392.7074, val_loss: 398.7948, val_MinusLogProbMetric: 398.7948

Epoch 144: val_loss did not improve from 397.68936
196/196 - 9s - loss: 392.7074 - MinusLogProbMetric: 392.7074 - val_loss: 398.7948 - val_MinusLogProbMetric: 398.7948 - lr: 1.1111e-04 - 9s/epoch - 48ms/step
Epoch 145/1000
2023-10-02 17:28:01.316 
Epoch 145/1000 
	 loss: 392.9154, MinusLogProbMetric: 392.9154, val_loss: 398.3160, val_MinusLogProbMetric: 398.3160

Epoch 145: val_loss did not improve from 397.68936
196/196 - 10s - loss: 392.9154 - MinusLogProbMetric: 392.9154 - val_loss: 398.3160 - val_MinusLogProbMetric: 398.3160 - lr: 1.1111e-04 - 10s/epoch - 50ms/step
Epoch 146/1000
2023-10-02 17:28:11.930 
Epoch 146/1000 
	 loss: 392.6718, MinusLogProbMetric: 392.6718, val_loss: 398.5669, val_MinusLogProbMetric: 398.5669

Epoch 146: val_loss did not improve from 397.68936
196/196 - 11s - loss: 392.6718 - MinusLogProbMetric: 392.6718 - val_loss: 398.5669 - val_MinusLogProbMetric: 398.5669 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 147/1000
2023-10-02 17:28:22.610 
Epoch 147/1000 
	 loss: 392.7670, MinusLogProbMetric: 392.7670, val_loss: 405.9073, val_MinusLogProbMetric: 405.9073

Epoch 147: val_loss did not improve from 397.68936
196/196 - 11s - loss: 392.7670 - MinusLogProbMetric: 392.7670 - val_loss: 405.9073 - val_MinusLogProbMetric: 405.9073 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 148/1000
2023-10-02 17:28:32.917 
Epoch 148/1000 
	 loss: 393.2838, MinusLogProbMetric: 393.2838, val_loss: 399.2449, val_MinusLogProbMetric: 399.2449

Epoch 148: val_loss did not improve from 397.68936
196/196 - 10s - loss: 393.2838 - MinusLogProbMetric: 393.2838 - val_loss: 399.2449 - val_MinusLogProbMetric: 399.2449 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 149/1000
2023-10-02 17:28:43.316 
Epoch 149/1000 
	 loss: 392.6386, MinusLogProbMetric: 392.6386, val_loss: 398.7459, val_MinusLogProbMetric: 398.7459

Epoch 149: val_loss did not improve from 397.68936
196/196 - 10s - loss: 392.6386 - MinusLogProbMetric: 392.6386 - val_loss: 398.7459 - val_MinusLogProbMetric: 398.7459 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 150/1000
2023-10-02 17:28:53.630 
Epoch 150/1000 
	 loss: 392.5348, MinusLogProbMetric: 392.5348, val_loss: 398.0860, val_MinusLogProbMetric: 398.0860

Epoch 150: val_loss did not improve from 397.68936
196/196 - 10s - loss: 392.5348 - MinusLogProbMetric: 392.5348 - val_loss: 398.0860 - val_MinusLogProbMetric: 398.0860 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 151/1000
2023-10-02 17:29:04.261 
Epoch 151/1000 
	 loss: 392.9961, MinusLogProbMetric: 392.9961, val_loss: 397.9949, val_MinusLogProbMetric: 397.9949

Epoch 151: val_loss did not improve from 397.68936
196/196 - 11s - loss: 392.9961 - MinusLogProbMetric: 392.9961 - val_loss: 397.9949 - val_MinusLogProbMetric: 397.9949 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 152/1000
2023-10-02 17:29:14.753 
Epoch 152/1000 
	 loss: 392.6616, MinusLogProbMetric: 392.6616, val_loss: 397.6176, val_MinusLogProbMetric: 397.6176

Epoch 152: val_loss improved from 397.68936 to 397.61761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 392.6616 - MinusLogProbMetric: 392.6616 - val_loss: 397.6176 - val_MinusLogProbMetric: 397.6176 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 153/1000
2023-10-02 17:29:25.549 
Epoch 153/1000 
	 loss: 392.6882, MinusLogProbMetric: 392.6882, val_loss: 398.5887, val_MinusLogProbMetric: 398.5887

Epoch 153: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.6882 - MinusLogProbMetric: 392.6882 - val_loss: 398.5887 - val_MinusLogProbMetric: 398.5887 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 154/1000
2023-10-02 17:29:36.201 
Epoch 154/1000 
	 loss: 392.9657, MinusLogProbMetric: 392.9657, val_loss: 398.6978, val_MinusLogProbMetric: 398.6978

Epoch 154: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.9657 - MinusLogProbMetric: 392.9657 - val_loss: 398.6978 - val_MinusLogProbMetric: 398.6978 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 155/1000
2023-10-02 17:29:46.688 
Epoch 155/1000 
	 loss: 392.5423, MinusLogProbMetric: 392.5423, val_loss: 398.3863, val_MinusLogProbMetric: 398.3863

Epoch 155: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.5423 - MinusLogProbMetric: 392.5423 - val_loss: 398.3863 - val_MinusLogProbMetric: 398.3863 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 156/1000
2023-10-02 17:29:56.980 
Epoch 156/1000 
	 loss: 392.6822, MinusLogProbMetric: 392.6822, val_loss: 402.0313, val_MinusLogProbMetric: 402.0313

Epoch 156: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.6822 - MinusLogProbMetric: 392.6822 - val_loss: 402.0313 - val_MinusLogProbMetric: 402.0313 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 157/1000
2023-10-02 17:30:07.530 
Epoch 157/1000 
	 loss: 392.4447, MinusLogProbMetric: 392.4447, val_loss: 398.8775, val_MinusLogProbMetric: 398.8775

Epoch 157: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.4447 - MinusLogProbMetric: 392.4447 - val_loss: 398.8775 - val_MinusLogProbMetric: 398.8775 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 158/1000
2023-10-02 17:30:17.953 
Epoch 158/1000 
	 loss: 392.5582, MinusLogProbMetric: 392.5582, val_loss: 399.4647, val_MinusLogProbMetric: 399.4647

Epoch 158: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.5582 - MinusLogProbMetric: 392.5582 - val_loss: 399.4647 - val_MinusLogProbMetric: 399.4647 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 159/1000
2023-10-02 17:30:28.174 
Epoch 159/1000 
	 loss: 392.3687, MinusLogProbMetric: 392.3687, val_loss: 405.4706, val_MinusLogProbMetric: 405.4706

Epoch 159: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.3687 - MinusLogProbMetric: 392.3687 - val_loss: 405.4706 - val_MinusLogProbMetric: 405.4706 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 160/1000
2023-10-02 17:30:38.698 
Epoch 160/1000 
	 loss: 392.8571, MinusLogProbMetric: 392.8571, val_loss: 399.8609, val_MinusLogProbMetric: 399.8609

Epoch 160: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.8571 - MinusLogProbMetric: 392.8571 - val_loss: 399.8609 - val_MinusLogProbMetric: 399.8609 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 161/1000
2023-10-02 17:30:49.138 
Epoch 161/1000 
	 loss: 392.4712, MinusLogProbMetric: 392.4712, val_loss: 403.6804, val_MinusLogProbMetric: 403.6804

Epoch 161: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.4712 - MinusLogProbMetric: 392.4712 - val_loss: 403.6804 - val_MinusLogProbMetric: 403.6804 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 162/1000
2023-10-02 17:30:59.626 
Epoch 162/1000 
	 loss: 392.4307, MinusLogProbMetric: 392.4307, val_loss: 400.8841, val_MinusLogProbMetric: 400.8841

Epoch 162: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.4307 - MinusLogProbMetric: 392.4307 - val_loss: 400.8841 - val_MinusLogProbMetric: 400.8841 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 163/1000
2023-10-02 17:31:10.156 
Epoch 163/1000 
	 loss: 392.6979, MinusLogProbMetric: 392.6979, val_loss: 400.4997, val_MinusLogProbMetric: 400.4997

Epoch 163: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.6979 - MinusLogProbMetric: 392.6979 - val_loss: 400.4997 - val_MinusLogProbMetric: 400.4997 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 164/1000
2023-10-02 17:31:20.575 
Epoch 164/1000 
	 loss: 392.2945, MinusLogProbMetric: 392.2945, val_loss: 402.1708, val_MinusLogProbMetric: 402.1708

Epoch 164: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.2945 - MinusLogProbMetric: 392.2945 - val_loss: 402.1708 - val_MinusLogProbMetric: 402.1708 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 165/1000
2023-10-02 17:31:31.227 
Epoch 165/1000 
	 loss: 392.2354, MinusLogProbMetric: 392.2354, val_loss: 399.1222, val_MinusLogProbMetric: 399.1222

Epoch 165: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.2354 - MinusLogProbMetric: 392.2354 - val_loss: 399.1222 - val_MinusLogProbMetric: 399.1222 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 166/1000
2023-10-02 17:31:41.618 
Epoch 166/1000 
	 loss: 392.5608, MinusLogProbMetric: 392.5608, val_loss: 399.5928, val_MinusLogProbMetric: 399.5928

Epoch 166: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.5608 - MinusLogProbMetric: 392.5608 - val_loss: 399.5928 - val_MinusLogProbMetric: 399.5928 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 167/1000
2023-10-02 17:31:51.972 
Epoch 167/1000 
	 loss: 392.1793, MinusLogProbMetric: 392.1793, val_loss: 400.8165, val_MinusLogProbMetric: 400.8165

Epoch 167: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.1793 - MinusLogProbMetric: 392.1793 - val_loss: 400.8165 - val_MinusLogProbMetric: 400.8165 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 168/1000
2023-10-02 17:32:02.585 
Epoch 168/1000 
	 loss: 392.2343, MinusLogProbMetric: 392.2343, val_loss: 400.3068, val_MinusLogProbMetric: 400.3068

Epoch 168: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.2343 - MinusLogProbMetric: 392.2343 - val_loss: 400.3068 - val_MinusLogProbMetric: 400.3068 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 169/1000
2023-10-02 17:32:13.186 
Epoch 169/1000 
	 loss: 392.5072, MinusLogProbMetric: 392.5072, val_loss: 399.9170, val_MinusLogProbMetric: 399.9170

Epoch 169: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.5072 - MinusLogProbMetric: 392.5072 - val_loss: 399.9170 - val_MinusLogProbMetric: 399.9170 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 170/1000
2023-10-02 17:32:23.527 
Epoch 170/1000 
	 loss: 392.2846, MinusLogProbMetric: 392.2846, val_loss: 399.6065, val_MinusLogProbMetric: 399.6065

Epoch 170: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.2846 - MinusLogProbMetric: 392.2846 - val_loss: 399.6065 - val_MinusLogProbMetric: 399.6065 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 171/1000
2023-10-02 17:32:34.072 
Epoch 171/1000 
	 loss: 392.3598, MinusLogProbMetric: 392.3598, val_loss: 399.0407, val_MinusLogProbMetric: 399.0407

Epoch 171: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.3598 - MinusLogProbMetric: 392.3598 - val_loss: 399.0407 - val_MinusLogProbMetric: 399.0407 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 172/1000
2023-10-02 17:32:44.659 
Epoch 172/1000 
	 loss: 392.2232, MinusLogProbMetric: 392.2232, val_loss: 399.1955, val_MinusLogProbMetric: 399.1955

Epoch 172: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.2232 - MinusLogProbMetric: 392.2232 - val_loss: 399.1955 - val_MinusLogProbMetric: 399.1955 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 173/1000
2023-10-02 17:32:55.259 
Epoch 173/1000 
	 loss: 392.2224, MinusLogProbMetric: 392.2224, val_loss: 398.0075, val_MinusLogProbMetric: 398.0075

Epoch 173: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.2224 - MinusLogProbMetric: 392.2224 - val_loss: 398.0075 - val_MinusLogProbMetric: 398.0075 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 174/1000
2023-10-02 17:33:05.458 
Epoch 174/1000 
	 loss: 393.6946, MinusLogProbMetric: 393.6946, val_loss: 404.5456, val_MinusLogProbMetric: 404.5456

Epoch 174: val_loss did not improve from 397.61761
196/196 - 10s - loss: 393.6946 - MinusLogProbMetric: 393.6946 - val_loss: 404.5456 - val_MinusLogProbMetric: 404.5456 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 175/1000
2023-10-02 17:33:16.028 
Epoch 175/1000 
	 loss: 392.0100, MinusLogProbMetric: 392.0100, val_loss: 398.6615, val_MinusLogProbMetric: 398.6615

Epoch 175: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.0100 - MinusLogProbMetric: 392.0100 - val_loss: 398.6615 - val_MinusLogProbMetric: 398.6615 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 176/1000
2023-10-02 17:33:26.760 
Epoch 176/1000 
	 loss: 391.9346, MinusLogProbMetric: 391.9346, val_loss: 403.5215, val_MinusLogProbMetric: 403.5215

Epoch 176: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.9346 - MinusLogProbMetric: 391.9346 - val_loss: 403.5215 - val_MinusLogProbMetric: 403.5215 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 177/1000
2023-10-02 17:33:37.281 
Epoch 177/1000 
	 loss: 392.1124, MinusLogProbMetric: 392.1124, val_loss: 399.3480, val_MinusLogProbMetric: 399.3480

Epoch 177: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.1124 - MinusLogProbMetric: 392.1124 - val_loss: 399.3480 - val_MinusLogProbMetric: 399.3480 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 178/1000
2023-10-02 17:33:47.863 
Epoch 178/1000 
	 loss: 392.0704, MinusLogProbMetric: 392.0704, val_loss: 398.6843, val_MinusLogProbMetric: 398.6843

Epoch 178: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.0704 - MinusLogProbMetric: 392.0704 - val_loss: 398.6843 - val_MinusLogProbMetric: 398.6843 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 179/1000
2023-10-02 17:33:58.441 
Epoch 179/1000 
	 loss: 391.9892, MinusLogProbMetric: 391.9892, val_loss: 398.7848, val_MinusLogProbMetric: 398.7848

Epoch 179: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.9892 - MinusLogProbMetric: 391.9892 - val_loss: 398.7848 - val_MinusLogProbMetric: 398.7848 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 180/1000
2023-10-02 17:34:09.246 
Epoch 180/1000 
	 loss: 391.9052, MinusLogProbMetric: 391.9052, val_loss: 405.0411, val_MinusLogProbMetric: 405.0411

Epoch 180: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.9052 - MinusLogProbMetric: 391.9052 - val_loss: 405.0411 - val_MinusLogProbMetric: 405.0411 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 181/1000
2023-10-02 17:34:19.710 
Epoch 181/1000 
	 loss: 391.8888, MinusLogProbMetric: 391.8888, val_loss: 398.6206, val_MinusLogProbMetric: 398.6206

Epoch 181: val_loss did not improve from 397.61761
196/196 - 10s - loss: 391.8888 - MinusLogProbMetric: 391.8888 - val_loss: 398.6206 - val_MinusLogProbMetric: 398.6206 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 182/1000
2023-10-02 17:34:30.234 
Epoch 182/1000 
	 loss: 391.9698, MinusLogProbMetric: 391.9698, val_loss: 409.4259, val_MinusLogProbMetric: 409.4259

Epoch 182: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.9698 - MinusLogProbMetric: 391.9698 - val_loss: 409.4259 - val_MinusLogProbMetric: 409.4259 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 183/1000
2023-10-02 17:34:40.873 
Epoch 183/1000 
	 loss: 392.4995, MinusLogProbMetric: 392.4995, val_loss: 402.0165, val_MinusLogProbMetric: 402.0165

Epoch 183: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.4995 - MinusLogProbMetric: 392.4995 - val_loss: 402.0165 - val_MinusLogProbMetric: 402.0165 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 184/1000
2023-10-02 17:34:51.179 
Epoch 184/1000 
	 loss: 392.0520, MinusLogProbMetric: 392.0520, val_loss: 398.3943, val_MinusLogProbMetric: 398.3943

Epoch 184: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.0520 - MinusLogProbMetric: 392.0520 - val_loss: 398.3943 - val_MinusLogProbMetric: 398.3943 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 185/1000
2023-10-02 17:35:01.561 
Epoch 185/1000 
	 loss: 391.9083, MinusLogProbMetric: 391.9083, val_loss: 400.1807, val_MinusLogProbMetric: 400.1807

Epoch 185: val_loss did not improve from 397.61761
196/196 - 10s - loss: 391.9083 - MinusLogProbMetric: 391.9083 - val_loss: 400.1807 - val_MinusLogProbMetric: 400.1807 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 186/1000
2023-10-02 17:35:12.113 
Epoch 186/1000 
	 loss: 391.7216, MinusLogProbMetric: 391.7216, val_loss: 398.0777, val_MinusLogProbMetric: 398.0777

Epoch 186: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.7216 - MinusLogProbMetric: 391.7216 - val_loss: 398.0777 - val_MinusLogProbMetric: 398.0777 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 187/1000
2023-10-02 17:35:22.893 
Epoch 187/1000 
	 loss: 391.6764, MinusLogProbMetric: 391.6764, val_loss: 398.2429, val_MinusLogProbMetric: 398.2429

Epoch 187: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.6764 - MinusLogProbMetric: 391.6764 - val_loss: 398.2429 - val_MinusLogProbMetric: 398.2429 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 188/1000
2023-10-02 17:35:33.338 
Epoch 188/1000 
	 loss: 393.1008, MinusLogProbMetric: 393.1008, val_loss: 398.1644, val_MinusLogProbMetric: 398.1644

Epoch 188: val_loss did not improve from 397.61761
196/196 - 10s - loss: 393.1008 - MinusLogProbMetric: 393.1008 - val_loss: 398.1644 - val_MinusLogProbMetric: 398.1644 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 189/1000
2023-10-02 17:35:43.621 
Epoch 189/1000 
	 loss: 392.0070, MinusLogProbMetric: 392.0070, val_loss: 398.9250, val_MinusLogProbMetric: 398.9250

Epoch 189: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.0070 - MinusLogProbMetric: 392.0070 - val_loss: 398.9250 - val_MinusLogProbMetric: 398.9250 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 190/1000
2023-10-02 17:35:54.107 
Epoch 190/1000 
	 loss: 391.6771, MinusLogProbMetric: 391.6771, val_loss: 398.7937, val_MinusLogProbMetric: 398.7937

Epoch 190: val_loss did not improve from 397.61761
196/196 - 10s - loss: 391.6771 - MinusLogProbMetric: 391.6771 - val_loss: 398.7937 - val_MinusLogProbMetric: 398.7937 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 191/1000
2023-10-02 17:36:05.103 
Epoch 191/1000 
	 loss: 391.9286, MinusLogProbMetric: 391.9286, val_loss: 399.8577, val_MinusLogProbMetric: 399.8577

Epoch 191: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.9286 - MinusLogProbMetric: 391.9286 - val_loss: 399.8577 - val_MinusLogProbMetric: 399.8577 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 192/1000
2023-10-02 17:36:15.529 
Epoch 192/1000 
	 loss: 391.8607, MinusLogProbMetric: 391.8607, val_loss: 399.6443, val_MinusLogProbMetric: 399.6443

Epoch 192: val_loss did not improve from 397.61761
196/196 - 10s - loss: 391.8607 - MinusLogProbMetric: 391.8607 - val_loss: 399.6443 - val_MinusLogProbMetric: 399.6443 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 193/1000
2023-10-02 17:36:26.046 
Epoch 193/1000 
	 loss: 391.7428, MinusLogProbMetric: 391.7428, val_loss: 404.0729, val_MinusLogProbMetric: 404.0729

Epoch 193: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.7428 - MinusLogProbMetric: 391.7428 - val_loss: 404.0729 - val_MinusLogProbMetric: 404.0729 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 194/1000
2023-10-02 17:36:36.644 
Epoch 194/1000 
	 loss: 391.7683, MinusLogProbMetric: 391.7683, val_loss: 401.0195, val_MinusLogProbMetric: 401.0195

Epoch 194: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.7683 - MinusLogProbMetric: 391.7683 - val_loss: 401.0195 - val_MinusLogProbMetric: 401.0195 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 195/1000
2023-10-02 17:36:47.038 
Epoch 195/1000 
	 loss: 392.3929, MinusLogProbMetric: 392.3929, val_loss: 399.1813, val_MinusLogProbMetric: 399.1813

Epoch 195: val_loss did not improve from 397.61761
196/196 - 10s - loss: 392.3929 - MinusLogProbMetric: 392.3929 - val_loss: 399.1813 - val_MinusLogProbMetric: 399.1813 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 196/1000
2023-10-02 17:36:57.584 
Epoch 196/1000 
	 loss: 391.3191, MinusLogProbMetric: 391.3191, val_loss: 398.7883, val_MinusLogProbMetric: 398.7883

Epoch 196: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.3191 - MinusLogProbMetric: 391.3191 - val_loss: 398.7883 - val_MinusLogProbMetric: 398.7883 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 197/1000
2023-10-02 17:37:08.237 
Epoch 197/1000 
	 loss: 392.4550, MinusLogProbMetric: 392.4550, val_loss: 399.3532, val_MinusLogProbMetric: 399.3532

Epoch 197: val_loss did not improve from 397.61761
196/196 - 11s - loss: 392.4550 - MinusLogProbMetric: 392.4550 - val_loss: 399.3532 - val_MinusLogProbMetric: 399.3532 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 198/1000
2023-10-02 17:37:19.008 
Epoch 198/1000 
	 loss: 391.4270, MinusLogProbMetric: 391.4270, val_loss: 398.4509, val_MinusLogProbMetric: 398.4509

Epoch 198: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.4270 - MinusLogProbMetric: 391.4270 - val_loss: 398.4509 - val_MinusLogProbMetric: 398.4509 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 199/1000
2023-10-02 17:37:29.542 
Epoch 199/1000 
	 loss: 391.7234, MinusLogProbMetric: 391.7234, val_loss: 399.2490, val_MinusLogProbMetric: 399.2490

Epoch 199: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.7234 - MinusLogProbMetric: 391.7234 - val_loss: 399.2490 - val_MinusLogProbMetric: 399.2490 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 200/1000
2023-10-02 17:37:40.113 
Epoch 200/1000 
	 loss: 391.4233, MinusLogProbMetric: 391.4233, val_loss: 399.1191, val_MinusLogProbMetric: 399.1191

Epoch 200: val_loss did not improve from 397.61761
196/196 - 11s - loss: 391.4233 - MinusLogProbMetric: 391.4233 - val_loss: 399.1191 - val_MinusLogProbMetric: 399.1191 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 201/1000
2023-10-02 17:37:50.558 
Epoch 201/1000 
	 loss: 391.5936, MinusLogProbMetric: 391.5936, val_loss: 398.9138, val_MinusLogProbMetric: 398.9138

Epoch 201: val_loss did not improve from 397.61761
196/196 - 10s - loss: 391.5936 - MinusLogProbMetric: 391.5936 - val_loss: 398.9138 - val_MinusLogProbMetric: 398.9138 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 202/1000
2023-10-02 17:38:01.005 
Epoch 202/1000 
	 loss: 391.5607, MinusLogProbMetric: 391.5607, val_loss: 400.7442, val_MinusLogProbMetric: 400.7442

Epoch 202: val_loss did not improve from 397.61761
196/196 - 10s - loss: 391.5607 - MinusLogProbMetric: 391.5607 - val_loss: 400.7442 - val_MinusLogProbMetric: 400.7442 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 203/1000
2023-10-02 17:38:11.639 
Epoch 203/1000 
	 loss: 388.3286, MinusLogProbMetric: 388.3286, val_loss: 396.0339, val_MinusLogProbMetric: 396.0339

Epoch 203: val_loss improved from 397.61761 to 396.03391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 388.3286 - MinusLogProbMetric: 388.3286 - val_loss: 396.0339 - val_MinusLogProbMetric: 396.0339 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 204/1000
2023-10-02 17:38:22.707 
Epoch 204/1000 
	 loss: 388.0259, MinusLogProbMetric: 388.0259, val_loss: 396.0315, val_MinusLogProbMetric: 396.0315

Epoch 204: val_loss improved from 396.03391 to 396.03146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 388.0259 - MinusLogProbMetric: 388.0259 - val_loss: 396.0315 - val_MinusLogProbMetric: 396.0315 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 205/1000
2023-10-02 17:38:33.441 
Epoch 205/1000 
	 loss: 387.9591, MinusLogProbMetric: 387.9591, val_loss: 396.1534, val_MinusLogProbMetric: 396.1534

Epoch 205: val_loss did not improve from 396.03146
196/196 - 10s - loss: 387.9591 - MinusLogProbMetric: 387.9591 - val_loss: 396.1534 - val_MinusLogProbMetric: 396.1534 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 206/1000
2023-10-02 17:38:43.981 
Epoch 206/1000 
	 loss: 388.3846, MinusLogProbMetric: 388.3846, val_loss: 396.4262, val_MinusLogProbMetric: 396.4262

Epoch 206: val_loss did not improve from 396.03146
196/196 - 11s - loss: 388.3846 - MinusLogProbMetric: 388.3846 - val_loss: 396.4262 - val_MinusLogProbMetric: 396.4262 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 207/1000
2023-10-02 17:38:54.442 
Epoch 207/1000 
	 loss: 388.1636, MinusLogProbMetric: 388.1636, val_loss: 395.8145, val_MinusLogProbMetric: 395.8145

Epoch 207: val_loss improved from 396.03146 to 395.81454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 388.1636 - MinusLogProbMetric: 388.1636 - val_loss: 395.8145 - val_MinusLogProbMetric: 395.8145 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 208/1000
2023-10-02 17:39:05.376 
Epoch 208/1000 
	 loss: 388.1306, MinusLogProbMetric: 388.1306, val_loss: 396.0258, val_MinusLogProbMetric: 396.0258

Epoch 208: val_loss did not improve from 395.81454
196/196 - 10s - loss: 388.1306 - MinusLogProbMetric: 388.1306 - val_loss: 396.0258 - val_MinusLogProbMetric: 396.0258 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 209/1000
2023-10-02 17:39:15.746 
Epoch 209/1000 
	 loss: 388.2748, MinusLogProbMetric: 388.2748, val_loss: 396.3375, val_MinusLogProbMetric: 396.3375

Epoch 209: val_loss did not improve from 395.81454
196/196 - 10s - loss: 388.2748 - MinusLogProbMetric: 388.2748 - val_loss: 396.3375 - val_MinusLogProbMetric: 396.3375 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 210/1000
2023-10-02 17:39:26.110 
Epoch 210/1000 
	 loss: 388.1924, MinusLogProbMetric: 388.1924, val_loss: 396.3669, val_MinusLogProbMetric: 396.3669

Epoch 210: val_loss did not improve from 395.81454
196/196 - 10s - loss: 388.1924 - MinusLogProbMetric: 388.1924 - val_loss: 396.3669 - val_MinusLogProbMetric: 396.3669 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 211/1000
2023-10-02 17:39:36.665 
Epoch 211/1000 
	 loss: 388.3426, MinusLogProbMetric: 388.3426, val_loss: 396.6047, val_MinusLogProbMetric: 396.6047

Epoch 211: val_loss did not improve from 395.81454
196/196 - 11s - loss: 388.3426 - MinusLogProbMetric: 388.3426 - val_loss: 396.6047 - val_MinusLogProbMetric: 396.6047 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 212/1000
2023-10-02 17:39:47.144 
Epoch 212/1000 
	 loss: 388.0907, MinusLogProbMetric: 388.0907, val_loss: 396.4620, val_MinusLogProbMetric: 396.4620

Epoch 212: val_loss did not improve from 395.81454
196/196 - 10s - loss: 388.0907 - MinusLogProbMetric: 388.0907 - val_loss: 396.4620 - val_MinusLogProbMetric: 396.4620 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 213/1000
2023-10-02 17:39:57.677 
Epoch 213/1000 
	 loss: 388.5091, MinusLogProbMetric: 388.5091, val_loss: 395.8871, val_MinusLogProbMetric: 395.8871

Epoch 213: val_loss did not improve from 395.81454
196/196 - 11s - loss: 388.5091 - MinusLogProbMetric: 388.5091 - val_loss: 395.8871 - val_MinusLogProbMetric: 395.8871 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 214/1000
2023-10-02 17:40:08.439 
Epoch 214/1000 
	 loss: 388.3198, MinusLogProbMetric: 388.3198, val_loss: 396.7529, val_MinusLogProbMetric: 396.7529

Epoch 214: val_loss did not improve from 395.81454
196/196 - 11s - loss: 388.3198 - MinusLogProbMetric: 388.3198 - val_loss: 396.7529 - val_MinusLogProbMetric: 396.7529 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 215/1000
2023-10-02 17:40:18.975 
Epoch 215/1000 
	 loss: 388.2092, MinusLogProbMetric: 388.2092, val_loss: 396.1670, val_MinusLogProbMetric: 396.1670

Epoch 215: val_loss did not improve from 395.81454
196/196 - 11s - loss: 388.2092 - MinusLogProbMetric: 388.2092 - val_loss: 396.1670 - val_MinusLogProbMetric: 396.1670 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 216/1000
2023-10-02 17:40:29.304 
Epoch 216/1000 
	 loss: 388.3804, MinusLogProbMetric: 388.3804, val_loss: 397.0804, val_MinusLogProbMetric: 397.0804

Epoch 216: val_loss did not improve from 395.81454
196/196 - 10s - loss: 388.3804 - MinusLogProbMetric: 388.3804 - val_loss: 397.0804 - val_MinusLogProbMetric: 397.0804 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 217/1000
2023-10-02 17:40:39.792 
Epoch 217/1000 
	 loss: 388.0881, MinusLogProbMetric: 388.0881, val_loss: 396.7085, val_MinusLogProbMetric: 396.7085

Epoch 217: val_loss did not improve from 395.81454
196/196 - 10s - loss: 388.0881 - MinusLogProbMetric: 388.0881 - val_loss: 396.7085 - val_MinusLogProbMetric: 396.7085 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 218/1000
2023-10-02 17:40:50.779 
Epoch 218/1000 
	 loss: 387.9881, MinusLogProbMetric: 387.9881, val_loss: 395.7873, val_MinusLogProbMetric: 395.7873

Epoch 218: val_loss improved from 395.81454 to 395.78732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 387.9881 - MinusLogProbMetric: 387.9881 - val_loss: 395.7873 - val_MinusLogProbMetric: 395.7873 - lr: 5.5556e-05 - 12s/epoch - 59ms/step
Epoch 219/1000
2023-10-02 17:41:01.832 
Epoch 219/1000 
	 loss: 388.2092, MinusLogProbMetric: 388.2092, val_loss: 396.5839, val_MinusLogProbMetric: 396.5839

Epoch 219: val_loss did not improve from 395.78732
196/196 - 11s - loss: 388.2092 - MinusLogProbMetric: 388.2092 - val_loss: 396.5839 - val_MinusLogProbMetric: 396.5839 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 220/1000
2023-10-02 17:41:12.338 
Epoch 220/1000 
	 loss: 388.1206, MinusLogProbMetric: 388.1206, val_loss: 396.1429, val_MinusLogProbMetric: 396.1429

Epoch 220: val_loss did not improve from 395.78732
196/196 - 10s - loss: 388.1206 - MinusLogProbMetric: 388.1206 - val_loss: 396.1429 - val_MinusLogProbMetric: 396.1429 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 221/1000
2023-10-02 17:41:22.709 
Epoch 221/1000 
	 loss: 388.1340, MinusLogProbMetric: 388.1340, val_loss: 397.8211, val_MinusLogProbMetric: 397.8211

Epoch 221: val_loss did not improve from 395.78732
196/196 - 10s - loss: 388.1340 - MinusLogProbMetric: 388.1340 - val_loss: 397.8211 - val_MinusLogProbMetric: 397.8211 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 222/1000
2023-10-02 17:41:33.224 
Epoch 222/1000 
	 loss: 388.2176, MinusLogProbMetric: 388.2176, val_loss: 396.4418, val_MinusLogProbMetric: 396.4418

Epoch 222: val_loss did not improve from 395.78732
196/196 - 11s - loss: 388.2176 - MinusLogProbMetric: 388.2176 - val_loss: 396.4418 - val_MinusLogProbMetric: 396.4418 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 223/1000
2023-10-02 17:41:43.767 
Epoch 223/1000 
	 loss: 388.0294, MinusLogProbMetric: 388.0294, val_loss: 398.4478, val_MinusLogProbMetric: 398.4478

Epoch 223: val_loss did not improve from 395.78732
196/196 - 11s - loss: 388.0294 - MinusLogProbMetric: 388.0294 - val_loss: 398.4478 - val_MinusLogProbMetric: 398.4478 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 224/1000
2023-10-02 17:41:54.185 
Epoch 224/1000 
	 loss: 388.2597, MinusLogProbMetric: 388.2597, val_loss: 395.9601, val_MinusLogProbMetric: 395.9601

Epoch 224: val_loss did not improve from 395.78732
196/196 - 10s - loss: 388.2597 - MinusLogProbMetric: 388.2597 - val_loss: 395.9601 - val_MinusLogProbMetric: 395.9601 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 225/1000
2023-10-02 17:42:04.819 
Epoch 225/1000 
	 loss: 388.0332, MinusLogProbMetric: 388.0332, val_loss: 395.6617, val_MinusLogProbMetric: 395.6617

Epoch 225: val_loss improved from 395.78732 to 395.66171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 388.0332 - MinusLogProbMetric: 388.0332 - val_loss: 395.6617 - val_MinusLogProbMetric: 395.6617 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 226/1000
2023-10-02 17:42:15.526 
Epoch 226/1000 
	 loss: 388.3128, MinusLogProbMetric: 388.3128, val_loss: 397.6573, val_MinusLogProbMetric: 397.6573

Epoch 226: val_loss did not improve from 395.66171
196/196 - 10s - loss: 388.3128 - MinusLogProbMetric: 388.3128 - val_loss: 397.6573 - val_MinusLogProbMetric: 397.6573 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 227/1000
2023-10-02 17:42:25.972 
Epoch 227/1000 
	 loss: 388.1838, MinusLogProbMetric: 388.1838, val_loss: 396.3914, val_MinusLogProbMetric: 396.3914

Epoch 227: val_loss did not improve from 395.66171
196/196 - 10s - loss: 388.1838 - MinusLogProbMetric: 388.1838 - val_loss: 396.3914 - val_MinusLogProbMetric: 396.3914 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 228/1000
2023-10-02 17:42:36.386 
Epoch 228/1000 
	 loss: 388.1712, MinusLogProbMetric: 388.1712, val_loss: 397.5292, val_MinusLogProbMetric: 397.5292

Epoch 228: val_loss did not improve from 395.66171
196/196 - 10s - loss: 388.1712 - MinusLogProbMetric: 388.1712 - val_loss: 397.5292 - val_MinusLogProbMetric: 397.5292 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 229/1000
2023-10-02 17:42:47.091 
Epoch 229/1000 
	 loss: 388.3186, MinusLogProbMetric: 388.3186, val_loss: 395.8129, val_MinusLogProbMetric: 395.8129

Epoch 229: val_loss did not improve from 395.66171
196/196 - 11s - loss: 388.3186 - MinusLogProbMetric: 388.3186 - val_loss: 395.8129 - val_MinusLogProbMetric: 395.8129 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 230/1000
2023-10-02 17:42:57.589 
Epoch 230/1000 
	 loss: 388.0591, MinusLogProbMetric: 388.0591, val_loss: 395.9565, val_MinusLogProbMetric: 395.9565

Epoch 230: val_loss did not improve from 395.66171
196/196 - 10s - loss: 388.0591 - MinusLogProbMetric: 388.0591 - val_loss: 395.9565 - val_MinusLogProbMetric: 395.9565 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 231/1000
2023-10-02 17:43:08.065 
Epoch 231/1000 
	 loss: 388.3752, MinusLogProbMetric: 388.3752, val_loss: 395.7101, val_MinusLogProbMetric: 395.7101

Epoch 231: val_loss did not improve from 395.66171
196/196 - 10s - loss: 388.3752 - MinusLogProbMetric: 388.3752 - val_loss: 395.7101 - val_MinusLogProbMetric: 395.7101 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 232/1000
2023-10-02 17:43:18.340 
Epoch 232/1000 
	 loss: 388.1725, MinusLogProbMetric: 388.1725, val_loss: 397.2517, val_MinusLogProbMetric: 397.2517

Epoch 232: val_loss did not improve from 395.66171
196/196 - 10s - loss: 388.1725 - MinusLogProbMetric: 388.1725 - val_loss: 397.2517 - val_MinusLogProbMetric: 397.2517 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 233/1000
2023-10-02 17:43:28.771 
Epoch 233/1000 
	 loss: 387.9090, MinusLogProbMetric: 387.9090, val_loss: 396.0843, val_MinusLogProbMetric: 396.0843

Epoch 233: val_loss did not improve from 395.66171
196/196 - 10s - loss: 387.9090 - MinusLogProbMetric: 387.9090 - val_loss: 396.0843 - val_MinusLogProbMetric: 396.0843 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 234/1000
2023-10-02 17:43:39.112 
Epoch 234/1000 
	 loss: 388.1396, MinusLogProbMetric: 388.1396, val_loss: 395.9880, val_MinusLogProbMetric: 395.9880

Epoch 234: val_loss did not improve from 395.66171
196/196 - 10s - loss: 388.1396 - MinusLogProbMetric: 388.1396 - val_loss: 395.9880 - val_MinusLogProbMetric: 395.9880 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 235/1000
2023-10-02 17:43:49.591 
Epoch 235/1000 
	 loss: 387.7572, MinusLogProbMetric: 387.7572, val_loss: 396.4389, val_MinusLogProbMetric: 396.4389

Epoch 235: val_loss did not improve from 395.66171
196/196 - 10s - loss: 387.7572 - MinusLogProbMetric: 387.7572 - val_loss: 396.4389 - val_MinusLogProbMetric: 396.4389 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 236/1000
2023-10-02 17:44:00.028 
Epoch 236/1000 
	 loss: 388.2051, MinusLogProbMetric: 388.2051, val_loss: 395.5649, val_MinusLogProbMetric: 395.5649

Epoch 236: val_loss improved from 395.66171 to 395.56488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 388.2051 - MinusLogProbMetric: 388.2051 - val_loss: 395.5649 - val_MinusLogProbMetric: 395.5649 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 237/1000
2023-10-02 17:44:10.771 
Epoch 237/1000 
	 loss: 387.8812, MinusLogProbMetric: 387.8812, val_loss: 395.7255, val_MinusLogProbMetric: 395.7255

Epoch 237: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.8812 - MinusLogProbMetric: 387.8812 - val_loss: 395.7255 - val_MinusLogProbMetric: 395.7255 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 238/1000
2023-10-02 17:44:21.302 
Epoch 238/1000 
	 loss: 388.1257, MinusLogProbMetric: 388.1257, val_loss: 396.1585, val_MinusLogProbMetric: 396.1585

Epoch 238: val_loss did not improve from 395.56488
196/196 - 11s - loss: 388.1257 - MinusLogProbMetric: 388.1257 - val_loss: 396.1585 - val_MinusLogProbMetric: 396.1585 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 239/1000
2023-10-02 17:44:31.816 
Epoch 239/1000 
	 loss: 388.2140, MinusLogProbMetric: 388.2140, val_loss: 396.6956, val_MinusLogProbMetric: 396.6956

Epoch 239: val_loss did not improve from 395.56488
196/196 - 11s - loss: 388.2140 - MinusLogProbMetric: 388.2140 - val_loss: 396.6956 - val_MinusLogProbMetric: 396.6956 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 240/1000
2023-10-02 17:44:42.369 
Epoch 240/1000 
	 loss: 388.1053, MinusLogProbMetric: 388.1053, val_loss: 395.7681, val_MinusLogProbMetric: 395.7681

Epoch 240: val_loss did not improve from 395.56488
196/196 - 11s - loss: 388.1053 - MinusLogProbMetric: 388.1053 - val_loss: 395.7681 - val_MinusLogProbMetric: 395.7681 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 241/1000
2023-10-02 17:44:52.930 
Epoch 241/1000 
	 loss: 388.1138, MinusLogProbMetric: 388.1138, val_loss: 397.2523, val_MinusLogProbMetric: 397.2523

Epoch 241: val_loss did not improve from 395.56488
196/196 - 11s - loss: 388.1138 - MinusLogProbMetric: 388.1138 - val_loss: 397.2523 - val_MinusLogProbMetric: 397.2523 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 242/1000
2023-10-02 17:45:03.346 
Epoch 242/1000 
	 loss: 387.9814, MinusLogProbMetric: 387.9814, val_loss: 396.0815, val_MinusLogProbMetric: 396.0815

Epoch 242: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.9814 - MinusLogProbMetric: 387.9814 - val_loss: 396.0815 - val_MinusLogProbMetric: 396.0815 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 243/1000
2023-10-02 17:45:14.002 
Epoch 243/1000 
	 loss: 388.1410, MinusLogProbMetric: 388.1410, val_loss: 396.2507, val_MinusLogProbMetric: 396.2507

Epoch 243: val_loss did not improve from 395.56488
196/196 - 11s - loss: 388.1410 - MinusLogProbMetric: 388.1410 - val_loss: 396.2507 - val_MinusLogProbMetric: 396.2507 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 244/1000
2023-10-02 17:45:24.550 
Epoch 244/1000 
	 loss: 387.8706, MinusLogProbMetric: 387.8706, val_loss: 396.3545, val_MinusLogProbMetric: 396.3545

Epoch 244: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.8706 - MinusLogProbMetric: 387.8706 - val_loss: 396.3545 - val_MinusLogProbMetric: 396.3545 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 245/1000
2023-10-02 17:45:35.024 
Epoch 245/1000 
	 loss: 388.0976, MinusLogProbMetric: 388.0976, val_loss: 395.7289, val_MinusLogProbMetric: 395.7289

Epoch 245: val_loss did not improve from 395.56488
196/196 - 10s - loss: 388.0976 - MinusLogProbMetric: 388.0976 - val_loss: 395.7289 - val_MinusLogProbMetric: 395.7289 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 246/1000
2023-10-02 17:45:45.468 
Epoch 246/1000 
	 loss: 388.0336, MinusLogProbMetric: 388.0336, val_loss: 396.0225, val_MinusLogProbMetric: 396.0225

Epoch 246: val_loss did not improve from 395.56488
196/196 - 10s - loss: 388.0336 - MinusLogProbMetric: 388.0336 - val_loss: 396.0225 - val_MinusLogProbMetric: 396.0225 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 247/1000
2023-10-02 17:45:56.171 
Epoch 247/1000 
	 loss: 387.7433, MinusLogProbMetric: 387.7433, val_loss: 396.3672, val_MinusLogProbMetric: 396.3672

Epoch 247: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.7433 - MinusLogProbMetric: 387.7433 - val_loss: 396.3672 - val_MinusLogProbMetric: 396.3672 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 248/1000
2023-10-02 17:46:06.672 
Epoch 248/1000 
	 loss: 388.0318, MinusLogProbMetric: 388.0318, val_loss: 396.4309, val_MinusLogProbMetric: 396.4309

Epoch 248: val_loss did not improve from 395.56488
196/196 - 10s - loss: 388.0318 - MinusLogProbMetric: 388.0318 - val_loss: 396.4309 - val_MinusLogProbMetric: 396.4309 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 249/1000
2023-10-02 17:46:17.250 
Epoch 249/1000 
	 loss: 387.8546, MinusLogProbMetric: 387.8546, val_loss: 396.5981, val_MinusLogProbMetric: 396.5981

Epoch 249: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.8546 - MinusLogProbMetric: 387.8546 - val_loss: 396.5981 - val_MinusLogProbMetric: 396.5981 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 250/1000
2023-10-02 17:46:27.830 
Epoch 250/1000 
	 loss: 387.9078, MinusLogProbMetric: 387.9078, val_loss: 401.0469, val_MinusLogProbMetric: 401.0469

Epoch 250: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.9078 - MinusLogProbMetric: 387.9078 - val_loss: 401.0469 - val_MinusLogProbMetric: 401.0469 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 251/1000
2023-10-02 17:46:38.410 
Epoch 251/1000 
	 loss: 387.7863, MinusLogProbMetric: 387.7863, val_loss: 397.1176, val_MinusLogProbMetric: 397.1176

Epoch 251: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.7863 - MinusLogProbMetric: 387.7863 - val_loss: 397.1176 - val_MinusLogProbMetric: 397.1176 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 252/1000
2023-10-02 17:46:48.755 
Epoch 252/1000 
	 loss: 387.9474, MinusLogProbMetric: 387.9474, val_loss: 397.1781, val_MinusLogProbMetric: 397.1781

Epoch 252: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.9474 - MinusLogProbMetric: 387.9474 - val_loss: 397.1781 - val_MinusLogProbMetric: 397.1781 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 253/1000
2023-10-02 17:46:59.297 
Epoch 253/1000 
	 loss: 387.9861, MinusLogProbMetric: 387.9861, val_loss: 396.0389, val_MinusLogProbMetric: 396.0389

Epoch 253: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.9861 - MinusLogProbMetric: 387.9861 - val_loss: 396.0389 - val_MinusLogProbMetric: 396.0389 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 254/1000
2023-10-02 17:47:09.892 
Epoch 254/1000 
	 loss: 387.8739, MinusLogProbMetric: 387.8739, val_loss: 396.4214, val_MinusLogProbMetric: 396.4214

Epoch 254: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.8739 - MinusLogProbMetric: 387.8739 - val_loss: 396.4214 - val_MinusLogProbMetric: 396.4214 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 255/1000
2023-10-02 17:47:20.652 
Epoch 255/1000 
	 loss: 388.0022, MinusLogProbMetric: 388.0022, val_loss: 398.8208, val_MinusLogProbMetric: 398.8208

Epoch 255: val_loss did not improve from 395.56488
196/196 - 11s - loss: 388.0022 - MinusLogProbMetric: 388.0022 - val_loss: 398.8208 - val_MinusLogProbMetric: 398.8208 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 256/1000
2023-10-02 17:47:31.281 
Epoch 256/1000 
	 loss: 387.9960, MinusLogProbMetric: 387.9960, val_loss: 397.8160, val_MinusLogProbMetric: 397.8160

Epoch 256: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.9960 - MinusLogProbMetric: 387.9960 - val_loss: 397.8160 - val_MinusLogProbMetric: 397.8160 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 257/1000
2023-10-02 17:47:41.481 
Epoch 257/1000 
	 loss: 387.9033, MinusLogProbMetric: 387.9033, val_loss: 395.5918, val_MinusLogProbMetric: 395.5918

Epoch 257: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.9033 - MinusLogProbMetric: 387.9033 - val_loss: 395.5918 - val_MinusLogProbMetric: 395.5918 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 258/1000
2023-10-02 17:47:51.929 
Epoch 258/1000 
	 loss: 387.7726, MinusLogProbMetric: 387.7726, val_loss: 396.7994, val_MinusLogProbMetric: 396.7994

Epoch 258: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.7726 - MinusLogProbMetric: 387.7726 - val_loss: 396.7994 - val_MinusLogProbMetric: 396.7994 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 259/1000
2023-10-02 17:48:02.556 
Epoch 259/1000 
	 loss: 387.7929, MinusLogProbMetric: 387.7929, val_loss: 395.7136, val_MinusLogProbMetric: 395.7136

Epoch 259: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.7929 - MinusLogProbMetric: 387.7929 - val_loss: 395.7136 - val_MinusLogProbMetric: 395.7136 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 260/1000
2023-10-02 17:48:13.128 
Epoch 260/1000 
	 loss: 387.9692, MinusLogProbMetric: 387.9692, val_loss: 395.6809, val_MinusLogProbMetric: 395.6809

Epoch 260: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.9692 - MinusLogProbMetric: 387.9692 - val_loss: 395.6809 - val_MinusLogProbMetric: 395.6809 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 261/1000
2023-10-02 17:48:23.579 
Epoch 261/1000 
	 loss: 387.7850, MinusLogProbMetric: 387.7850, val_loss: 395.7835, val_MinusLogProbMetric: 395.7835

Epoch 261: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.7850 - MinusLogProbMetric: 387.7850 - val_loss: 395.7835 - val_MinusLogProbMetric: 395.7835 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 262/1000
2023-10-02 17:48:34.390 
Epoch 262/1000 
	 loss: 387.8461, MinusLogProbMetric: 387.8461, val_loss: 395.7153, val_MinusLogProbMetric: 395.7153

Epoch 262: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.8461 - MinusLogProbMetric: 387.8461 - val_loss: 395.7153 - val_MinusLogProbMetric: 395.7153 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 263/1000
2023-10-02 17:48:44.885 
Epoch 263/1000 
	 loss: 387.8654, MinusLogProbMetric: 387.8654, val_loss: 396.2762, val_MinusLogProbMetric: 396.2762

Epoch 263: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.8654 - MinusLogProbMetric: 387.8654 - val_loss: 396.2762 - val_MinusLogProbMetric: 396.2762 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 264/1000
2023-10-02 17:48:55.200 
Epoch 264/1000 
	 loss: 387.6187, MinusLogProbMetric: 387.6187, val_loss: 395.7845, val_MinusLogProbMetric: 395.7845

Epoch 264: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.6187 - MinusLogProbMetric: 387.6187 - val_loss: 395.7845 - val_MinusLogProbMetric: 395.7845 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 265/1000
2023-10-02 17:49:05.888 
Epoch 265/1000 
	 loss: 387.6927, MinusLogProbMetric: 387.6927, val_loss: 396.1476, val_MinusLogProbMetric: 396.1476

Epoch 265: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.6927 - MinusLogProbMetric: 387.6927 - val_loss: 396.1476 - val_MinusLogProbMetric: 396.1476 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 266/1000
2023-10-02 17:49:16.397 
Epoch 266/1000 
	 loss: 387.9426, MinusLogProbMetric: 387.9426, val_loss: 396.0302, val_MinusLogProbMetric: 396.0302

Epoch 266: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.9426 - MinusLogProbMetric: 387.9426 - val_loss: 396.0302 - val_MinusLogProbMetric: 396.0302 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 267/1000
2023-10-02 17:49:26.924 
Epoch 267/1000 
	 loss: 387.7291, MinusLogProbMetric: 387.7291, val_loss: 396.8577, val_MinusLogProbMetric: 396.8577

Epoch 267: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.7291 - MinusLogProbMetric: 387.7291 - val_loss: 396.8577 - val_MinusLogProbMetric: 396.8577 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 268/1000
2023-10-02 17:49:37.310 
Epoch 268/1000 
	 loss: 387.7847, MinusLogProbMetric: 387.7847, val_loss: 396.0476, val_MinusLogProbMetric: 396.0476

Epoch 268: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.7847 - MinusLogProbMetric: 387.7847 - val_loss: 396.0476 - val_MinusLogProbMetric: 396.0476 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 269/1000
2023-10-02 17:49:47.981 
Epoch 269/1000 
	 loss: 387.5789, MinusLogProbMetric: 387.5789, val_loss: 397.5231, val_MinusLogProbMetric: 397.5231

Epoch 269: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.5789 - MinusLogProbMetric: 387.5789 - val_loss: 397.5231 - val_MinusLogProbMetric: 397.5231 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 270/1000
2023-10-02 17:49:58.534 
Epoch 270/1000 
	 loss: 387.8691, MinusLogProbMetric: 387.8691, val_loss: 395.8778, val_MinusLogProbMetric: 395.8778

Epoch 270: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.8691 - MinusLogProbMetric: 387.8691 - val_loss: 395.8778 - val_MinusLogProbMetric: 395.8778 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 271/1000
2023-10-02 17:50:08.989 
Epoch 271/1000 
	 loss: 387.6953, MinusLogProbMetric: 387.6953, val_loss: 396.0710, val_MinusLogProbMetric: 396.0710

Epoch 271: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.6953 - MinusLogProbMetric: 387.6953 - val_loss: 396.0710 - val_MinusLogProbMetric: 396.0710 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 272/1000
2023-10-02 17:50:19.322 
Epoch 272/1000 
	 loss: 387.6516, MinusLogProbMetric: 387.6516, val_loss: 399.7967, val_MinusLogProbMetric: 399.7967

Epoch 272: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.6516 - MinusLogProbMetric: 387.6516 - val_loss: 399.7967 - val_MinusLogProbMetric: 399.7967 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 273/1000
2023-10-02 17:50:29.880 
Epoch 273/1000 
	 loss: 387.8741, MinusLogProbMetric: 387.8741, val_loss: 396.2586, val_MinusLogProbMetric: 396.2586

Epoch 273: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.8741 - MinusLogProbMetric: 387.8741 - val_loss: 396.2586 - val_MinusLogProbMetric: 396.2586 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 274/1000
2023-10-02 17:50:40.240 
Epoch 274/1000 
	 loss: 387.9520, MinusLogProbMetric: 387.9520, val_loss: 396.1240, val_MinusLogProbMetric: 396.1240

Epoch 274: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.9520 - MinusLogProbMetric: 387.9520 - val_loss: 396.1240 - val_MinusLogProbMetric: 396.1240 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 275/1000
2023-10-02 17:50:50.805 
Epoch 275/1000 
	 loss: 387.7378, MinusLogProbMetric: 387.7378, val_loss: 396.1604, val_MinusLogProbMetric: 396.1604

Epoch 275: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.7378 - MinusLogProbMetric: 387.7378 - val_loss: 396.1604 - val_MinusLogProbMetric: 396.1604 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 276/1000
2023-10-02 17:51:01.550 
Epoch 276/1000 
	 loss: 387.6350, MinusLogProbMetric: 387.6350, val_loss: 396.2777, val_MinusLogProbMetric: 396.2777

Epoch 276: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.6350 - MinusLogProbMetric: 387.6350 - val_loss: 396.2777 - val_MinusLogProbMetric: 396.2777 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 277/1000
2023-10-02 17:51:12.006 
Epoch 277/1000 
	 loss: 387.5430, MinusLogProbMetric: 387.5430, val_loss: 396.1771, val_MinusLogProbMetric: 396.1771

Epoch 277: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.5430 - MinusLogProbMetric: 387.5430 - val_loss: 396.1771 - val_MinusLogProbMetric: 396.1771 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 278/1000
2023-10-02 17:51:22.472 
Epoch 278/1000 
	 loss: 387.5923, MinusLogProbMetric: 387.5923, val_loss: 395.7756, val_MinusLogProbMetric: 395.7756

Epoch 278: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.5923 - MinusLogProbMetric: 387.5923 - val_loss: 395.7756 - val_MinusLogProbMetric: 395.7756 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 279/1000
2023-10-02 17:51:32.992 
Epoch 279/1000 
	 loss: 387.7411, MinusLogProbMetric: 387.7411, val_loss: 396.3034, val_MinusLogProbMetric: 396.3034

Epoch 279: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.7411 - MinusLogProbMetric: 387.7411 - val_loss: 396.3034 - val_MinusLogProbMetric: 396.3034 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 280/1000
2023-10-02 17:51:43.650 
Epoch 280/1000 
	 loss: 387.5382, MinusLogProbMetric: 387.5382, val_loss: 396.7662, val_MinusLogProbMetric: 396.7662

Epoch 280: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.5382 - MinusLogProbMetric: 387.5382 - val_loss: 396.7662 - val_MinusLogProbMetric: 396.7662 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 281/1000
2023-10-02 17:51:54.078 
Epoch 281/1000 
	 loss: 387.8035, MinusLogProbMetric: 387.8035, val_loss: 396.0272, val_MinusLogProbMetric: 396.0272

Epoch 281: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.8035 - MinusLogProbMetric: 387.8035 - val_loss: 396.0272 - val_MinusLogProbMetric: 396.0272 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 282/1000
2023-10-02 17:52:04.454 
Epoch 282/1000 
	 loss: 387.6671, MinusLogProbMetric: 387.6671, val_loss: 396.4838, val_MinusLogProbMetric: 396.4838

Epoch 282: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.6671 - MinusLogProbMetric: 387.6671 - val_loss: 396.4838 - val_MinusLogProbMetric: 396.4838 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 283/1000
2023-10-02 17:52:14.868 
Epoch 283/1000 
	 loss: 387.6066, MinusLogProbMetric: 387.6066, val_loss: 396.5100, val_MinusLogProbMetric: 396.5100

Epoch 283: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.6066 - MinusLogProbMetric: 387.6066 - val_loss: 396.5100 - val_MinusLogProbMetric: 396.5100 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 284/1000
2023-10-02 17:52:25.705 
Epoch 284/1000 
	 loss: 387.7585, MinusLogProbMetric: 387.7585, val_loss: 396.4737, val_MinusLogProbMetric: 396.4737

Epoch 284: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.7585 - MinusLogProbMetric: 387.7585 - val_loss: 396.4737 - val_MinusLogProbMetric: 396.4737 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 285/1000
2023-10-02 17:52:36.210 
Epoch 285/1000 
	 loss: 387.6036, MinusLogProbMetric: 387.6036, val_loss: 397.0628, val_MinusLogProbMetric: 397.0628

Epoch 285: val_loss did not improve from 395.56488
196/196 - 10s - loss: 387.6036 - MinusLogProbMetric: 387.6036 - val_loss: 397.0628 - val_MinusLogProbMetric: 397.0628 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 286/1000
2023-10-02 17:52:46.784 
Epoch 286/1000 
	 loss: 387.8240, MinusLogProbMetric: 387.8240, val_loss: 397.3710, val_MinusLogProbMetric: 397.3710

Epoch 286: val_loss did not improve from 395.56488
196/196 - 11s - loss: 387.8240 - MinusLogProbMetric: 387.8240 - val_loss: 397.3710 - val_MinusLogProbMetric: 397.3710 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 287/1000
2023-10-02 17:52:57.630 
Epoch 287/1000 
	 loss: 386.0627, MinusLogProbMetric: 386.0627, val_loss: 394.8599, val_MinusLogProbMetric: 394.8599

Epoch 287: val_loss improved from 395.56488 to 394.85989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 386.0627 - MinusLogProbMetric: 386.0627 - val_loss: 394.8599 - val_MinusLogProbMetric: 394.8599 - lr: 2.7778e-05 - 11s/epoch - 58ms/step
Epoch 288/1000
2023-10-02 17:53:08.421 
Epoch 288/1000 
	 loss: 385.9141, MinusLogProbMetric: 385.9141, val_loss: 394.9078, val_MinusLogProbMetric: 394.9078

Epoch 288: val_loss did not improve from 394.85989
196/196 - 10s - loss: 385.9141 - MinusLogProbMetric: 385.9141 - val_loss: 394.9078 - val_MinusLogProbMetric: 394.9078 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 289/1000
2023-10-02 17:53:18.871 
Epoch 289/1000 
	 loss: 385.8723, MinusLogProbMetric: 385.8723, val_loss: 394.8362, val_MinusLogProbMetric: 394.8362

Epoch 289: val_loss improved from 394.85989 to 394.83624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 385.8723 - MinusLogProbMetric: 385.8723 - val_loss: 394.8362 - val_MinusLogProbMetric: 394.8362 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 290/1000
2023-10-02 17:53:29.555 
Epoch 290/1000 
	 loss: 385.9214, MinusLogProbMetric: 385.9214, val_loss: 395.2417, val_MinusLogProbMetric: 395.2417

Epoch 290: val_loss did not improve from 394.83624
196/196 - 10s - loss: 385.9214 - MinusLogProbMetric: 385.9214 - val_loss: 395.2417 - val_MinusLogProbMetric: 395.2417 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 291/1000
2023-10-02 17:53:40.300 
Epoch 291/1000 
	 loss: 385.9535, MinusLogProbMetric: 385.9535, val_loss: 394.8468, val_MinusLogProbMetric: 394.8468

Epoch 291: val_loss did not improve from 394.83624
196/196 - 11s - loss: 385.9535 - MinusLogProbMetric: 385.9535 - val_loss: 394.8468 - val_MinusLogProbMetric: 394.8468 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 292/1000
2023-10-02 17:53:50.775 
Epoch 292/1000 
	 loss: 385.8487, MinusLogProbMetric: 385.8487, val_loss: 395.2071, val_MinusLogProbMetric: 395.2071

Epoch 292: val_loss did not improve from 394.83624
196/196 - 10s - loss: 385.8487 - MinusLogProbMetric: 385.8487 - val_loss: 395.2071 - val_MinusLogProbMetric: 395.2071 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 293/1000
2023-10-02 17:54:01.319 
Epoch 293/1000 
	 loss: 385.8771, MinusLogProbMetric: 385.8771, val_loss: 395.5672, val_MinusLogProbMetric: 395.5672

Epoch 293: val_loss did not improve from 394.83624
196/196 - 11s - loss: 385.8771 - MinusLogProbMetric: 385.8771 - val_loss: 395.5672 - val_MinusLogProbMetric: 395.5672 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 294/1000
2023-10-02 17:54:11.976 
Epoch 294/1000 
	 loss: 385.9286, MinusLogProbMetric: 385.9286, val_loss: 394.7295, val_MinusLogProbMetric: 394.7295

Epoch 294: val_loss improved from 394.83624 to 394.72952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 385.9286 - MinusLogProbMetric: 385.9286 - val_loss: 394.7295 - val_MinusLogProbMetric: 394.7295 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 295/1000
2023-10-02 17:54:22.943 
Epoch 295/1000 
	 loss: 385.9149, MinusLogProbMetric: 385.9149, val_loss: 394.8618, val_MinusLogProbMetric: 394.8618

Epoch 295: val_loss did not improve from 394.72952
196/196 - 11s - loss: 385.9149 - MinusLogProbMetric: 385.9149 - val_loss: 394.8618 - val_MinusLogProbMetric: 394.8618 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 296/1000
2023-10-02 17:54:33.498 
Epoch 296/1000 
	 loss: 385.8437, MinusLogProbMetric: 385.8437, val_loss: 394.9060, val_MinusLogProbMetric: 394.9060

Epoch 296: val_loss did not improve from 394.72952
196/196 - 11s - loss: 385.8437 - MinusLogProbMetric: 385.8437 - val_loss: 394.9060 - val_MinusLogProbMetric: 394.9060 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 297/1000
2023-10-02 17:54:44.102 
Epoch 297/1000 
	 loss: 385.8523, MinusLogProbMetric: 385.8523, val_loss: 395.2479, val_MinusLogProbMetric: 395.2479

Epoch 297: val_loss did not improve from 394.72952
196/196 - 11s - loss: 385.8523 - MinusLogProbMetric: 385.8523 - val_loss: 395.2479 - val_MinusLogProbMetric: 395.2479 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 298/1000
2023-10-02 17:54:54.622 
Epoch 298/1000 
	 loss: 386.0497, MinusLogProbMetric: 386.0497, val_loss: 394.8566, val_MinusLogProbMetric: 394.8566

Epoch 298: val_loss did not improve from 394.72952
196/196 - 11s - loss: 386.0497 - MinusLogProbMetric: 386.0497 - val_loss: 394.8566 - val_MinusLogProbMetric: 394.8566 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 299/1000
2023-10-02 17:55:05.166 
Epoch 299/1000 
	 loss: 385.8928, MinusLogProbMetric: 385.8928, val_loss: 394.8694, val_MinusLogProbMetric: 394.8694

Epoch 299: val_loss did not improve from 394.72952
196/196 - 11s - loss: 385.8928 - MinusLogProbMetric: 385.8928 - val_loss: 394.8694 - val_MinusLogProbMetric: 394.8694 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 300/1000
2023-10-02 17:55:15.760 
Epoch 300/1000 
	 loss: 385.8910, MinusLogProbMetric: 385.8910, val_loss: 394.8640, val_MinusLogProbMetric: 394.8640

Epoch 300: val_loss did not improve from 394.72952
196/196 - 11s - loss: 385.8910 - MinusLogProbMetric: 385.8910 - val_loss: 394.8640 - val_MinusLogProbMetric: 394.8640 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 301/1000
2023-10-02 17:55:26.362 
Epoch 301/1000 
	 loss: 385.8914, MinusLogProbMetric: 385.8914, val_loss: 395.4051, val_MinusLogProbMetric: 395.4051

Epoch 301: val_loss did not improve from 394.72952
196/196 - 11s - loss: 385.8914 - MinusLogProbMetric: 385.8914 - val_loss: 395.4051 - val_MinusLogProbMetric: 395.4051 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 302/1000
2023-10-02 17:55:37.064 
Epoch 302/1000 
	 loss: 385.8865, MinusLogProbMetric: 385.8865, val_loss: 395.0624, val_MinusLogProbMetric: 395.0624

Epoch 302: val_loss did not improve from 394.72952
196/196 - 11s - loss: 385.8865 - MinusLogProbMetric: 385.8865 - val_loss: 395.0624 - val_MinusLogProbMetric: 395.0624 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 303/1000
2023-10-02 17:55:47.639 
Epoch 303/1000 
	 loss: 385.7594, MinusLogProbMetric: 385.7594, val_loss: 394.6600, val_MinusLogProbMetric: 394.6600

Epoch 303: val_loss improved from 394.72952 to 394.65997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 385.7594 - MinusLogProbMetric: 385.7594 - val_loss: 394.6600 - val_MinusLogProbMetric: 394.6600 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 304/1000
2023-10-02 17:55:58.431 
Epoch 304/1000 
	 loss: 385.8432, MinusLogProbMetric: 385.8432, val_loss: 395.6996, val_MinusLogProbMetric: 395.6996

Epoch 304: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.8432 - MinusLogProbMetric: 385.8432 - val_loss: 395.6996 - val_MinusLogProbMetric: 395.6996 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 305/1000
2023-10-02 17:56:09.170 
Epoch 305/1000 
	 loss: 385.8034, MinusLogProbMetric: 385.8034, val_loss: 395.3597, val_MinusLogProbMetric: 395.3597

Epoch 305: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.8034 - MinusLogProbMetric: 385.8034 - val_loss: 395.3597 - val_MinusLogProbMetric: 395.3597 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 306/1000
2023-10-02 17:56:19.659 
Epoch 306/1000 
	 loss: 385.9401, MinusLogProbMetric: 385.9401, val_loss: 395.6085, val_MinusLogProbMetric: 395.6085

Epoch 306: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.9401 - MinusLogProbMetric: 385.9401 - val_loss: 395.6085 - val_MinusLogProbMetric: 395.6085 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 307/1000
2023-10-02 17:56:30.146 
Epoch 307/1000 
	 loss: 385.9177, MinusLogProbMetric: 385.9177, val_loss: 395.1654, val_MinusLogProbMetric: 395.1654

Epoch 307: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.9177 - MinusLogProbMetric: 385.9177 - val_loss: 395.1654 - val_MinusLogProbMetric: 395.1654 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 308/1000
2023-10-02 17:56:40.570 
Epoch 308/1000 
	 loss: 385.8835, MinusLogProbMetric: 385.8835, val_loss: 395.3256, val_MinusLogProbMetric: 395.3256

Epoch 308: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.8835 - MinusLogProbMetric: 385.8835 - val_loss: 395.3256 - val_MinusLogProbMetric: 395.3256 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 309/1000
2023-10-02 17:56:51.221 
Epoch 309/1000 
	 loss: 385.7876, MinusLogProbMetric: 385.7876, val_loss: 394.8516, val_MinusLogProbMetric: 394.8516

Epoch 309: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7876 - MinusLogProbMetric: 385.7876 - val_loss: 394.8516 - val_MinusLogProbMetric: 394.8516 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 310/1000
2023-10-02 17:57:01.704 
Epoch 310/1000 
	 loss: 385.8158, MinusLogProbMetric: 385.8158, val_loss: 395.0131, val_MinusLogProbMetric: 395.0131

Epoch 310: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.8158 - MinusLogProbMetric: 385.8158 - val_loss: 395.0131 - val_MinusLogProbMetric: 395.0131 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 311/1000
2023-10-02 17:57:12.239 
Epoch 311/1000 
	 loss: 385.8787, MinusLogProbMetric: 385.8787, val_loss: 395.2424, val_MinusLogProbMetric: 395.2424

Epoch 311: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.8787 - MinusLogProbMetric: 385.8787 - val_loss: 395.2424 - val_MinusLogProbMetric: 395.2424 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 312/1000
2023-10-02 17:57:22.879 
Epoch 312/1000 
	 loss: 385.9710, MinusLogProbMetric: 385.9710, val_loss: 395.3290, val_MinusLogProbMetric: 395.3290

Epoch 312: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.9710 - MinusLogProbMetric: 385.9710 - val_loss: 395.3290 - val_MinusLogProbMetric: 395.3290 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 313/1000
2023-10-02 17:57:33.461 
Epoch 313/1000 
	 loss: 385.9253, MinusLogProbMetric: 385.9253, val_loss: 396.2369, val_MinusLogProbMetric: 396.2369

Epoch 313: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.9253 - MinusLogProbMetric: 385.9253 - val_loss: 396.2369 - val_MinusLogProbMetric: 396.2369 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 314/1000
2023-10-02 17:57:43.913 
Epoch 314/1000 
	 loss: 385.8852, MinusLogProbMetric: 385.8852, val_loss: 394.8358, val_MinusLogProbMetric: 394.8358

Epoch 314: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.8852 - MinusLogProbMetric: 385.8852 - val_loss: 394.8358 - val_MinusLogProbMetric: 394.8358 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 315/1000
2023-10-02 17:57:54.499 
Epoch 315/1000 
	 loss: 385.8673, MinusLogProbMetric: 385.8673, val_loss: 395.0450, val_MinusLogProbMetric: 395.0450

Epoch 315: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.8673 - MinusLogProbMetric: 385.8673 - val_loss: 395.0450 - val_MinusLogProbMetric: 395.0450 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 316/1000
2023-10-02 17:58:05.058 
Epoch 316/1000 
	 loss: 385.8462, MinusLogProbMetric: 385.8462, val_loss: 395.7068, val_MinusLogProbMetric: 395.7068

Epoch 316: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.8462 - MinusLogProbMetric: 385.8462 - val_loss: 395.7068 - val_MinusLogProbMetric: 395.7068 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 317/1000
2023-10-02 17:58:15.462 
Epoch 317/1000 
	 loss: 386.0464, MinusLogProbMetric: 386.0464, val_loss: 394.9793, val_MinusLogProbMetric: 394.9793

Epoch 317: val_loss did not improve from 394.65997
196/196 - 10s - loss: 386.0464 - MinusLogProbMetric: 386.0464 - val_loss: 394.9793 - val_MinusLogProbMetric: 394.9793 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 318/1000
2023-10-02 17:58:26.185 
Epoch 318/1000 
	 loss: 385.7409, MinusLogProbMetric: 385.7409, val_loss: 394.8282, val_MinusLogProbMetric: 394.8282

Epoch 318: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7409 - MinusLogProbMetric: 385.7409 - val_loss: 394.8282 - val_MinusLogProbMetric: 394.8282 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 319/1000
2023-10-02 17:58:36.702 
Epoch 319/1000 
	 loss: 385.7310, MinusLogProbMetric: 385.7310, val_loss: 394.6708, val_MinusLogProbMetric: 394.6708

Epoch 319: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7310 - MinusLogProbMetric: 385.7310 - val_loss: 394.6708 - val_MinusLogProbMetric: 394.6708 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 320/1000
2023-10-02 17:58:47.362 
Epoch 320/1000 
	 loss: 385.6671, MinusLogProbMetric: 385.6671, val_loss: 395.5389, val_MinusLogProbMetric: 395.5389

Epoch 320: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.6671 - MinusLogProbMetric: 385.6671 - val_loss: 395.5389 - val_MinusLogProbMetric: 395.5389 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 321/1000
2023-10-02 17:58:57.921 
Epoch 321/1000 
	 loss: 385.9147, MinusLogProbMetric: 385.9147, val_loss: 395.0145, val_MinusLogProbMetric: 395.0145

Epoch 321: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.9147 - MinusLogProbMetric: 385.9147 - val_loss: 395.0145 - val_MinusLogProbMetric: 395.0145 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 322/1000
2023-10-02 17:59:08.498 
Epoch 322/1000 
	 loss: 385.8621, MinusLogProbMetric: 385.8621, val_loss: 394.8999, val_MinusLogProbMetric: 394.8999

Epoch 322: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.8621 - MinusLogProbMetric: 385.8621 - val_loss: 394.8999 - val_MinusLogProbMetric: 394.8999 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 323/1000
2023-10-02 17:59:19.234 
Epoch 323/1000 
	 loss: 385.7734, MinusLogProbMetric: 385.7734, val_loss: 394.7630, val_MinusLogProbMetric: 394.7630

Epoch 323: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7734 - MinusLogProbMetric: 385.7734 - val_loss: 394.7630 - val_MinusLogProbMetric: 394.7630 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 324/1000
2023-10-02 17:59:30.133 
Epoch 324/1000 
	 loss: 385.8414, MinusLogProbMetric: 385.8414, val_loss: 396.3868, val_MinusLogProbMetric: 396.3868

Epoch 324: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.8414 - MinusLogProbMetric: 385.8414 - val_loss: 396.3868 - val_MinusLogProbMetric: 396.3868 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 325/1000
2023-10-02 17:59:40.529 
Epoch 325/1000 
	 loss: 385.9252, MinusLogProbMetric: 385.9252, val_loss: 394.9806, val_MinusLogProbMetric: 394.9806

Epoch 325: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.9252 - MinusLogProbMetric: 385.9252 - val_loss: 394.9806 - val_MinusLogProbMetric: 394.9806 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 326/1000
2023-10-02 17:59:51.102 
Epoch 326/1000 
	 loss: 385.6813, MinusLogProbMetric: 385.6813, val_loss: 394.9702, val_MinusLogProbMetric: 394.9702

Epoch 326: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.6813 - MinusLogProbMetric: 385.6813 - val_loss: 394.9702 - val_MinusLogProbMetric: 394.9702 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 327/1000
2023-10-02 18:00:01.773 
Epoch 327/1000 
	 loss: 385.7001, MinusLogProbMetric: 385.7001, val_loss: 394.9301, val_MinusLogProbMetric: 394.9301

Epoch 327: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7001 - MinusLogProbMetric: 385.7001 - val_loss: 394.9301 - val_MinusLogProbMetric: 394.9301 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 328/1000
2023-10-02 18:00:12.341 
Epoch 328/1000 
	 loss: 385.7467, MinusLogProbMetric: 385.7467, val_loss: 394.9486, val_MinusLogProbMetric: 394.9486

Epoch 328: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7467 - MinusLogProbMetric: 385.7467 - val_loss: 394.9486 - val_MinusLogProbMetric: 394.9486 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 329/1000
2023-10-02 18:00:22.775 
Epoch 329/1000 
	 loss: 385.7401, MinusLogProbMetric: 385.7401, val_loss: 395.2577, val_MinusLogProbMetric: 395.2577

Epoch 329: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.7401 - MinusLogProbMetric: 385.7401 - val_loss: 395.2577 - val_MinusLogProbMetric: 395.2577 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 330/1000
2023-10-02 18:00:33.509 
Epoch 330/1000 
	 loss: 385.6937, MinusLogProbMetric: 385.6937, val_loss: 395.0922, val_MinusLogProbMetric: 395.0922

Epoch 330: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.6937 - MinusLogProbMetric: 385.6937 - val_loss: 395.0922 - val_MinusLogProbMetric: 395.0922 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 331/1000
2023-10-02 18:00:44.079 
Epoch 331/1000 
	 loss: 385.7606, MinusLogProbMetric: 385.7606, val_loss: 395.0150, val_MinusLogProbMetric: 395.0150

Epoch 331: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7606 - MinusLogProbMetric: 385.7606 - val_loss: 395.0150 - val_MinusLogProbMetric: 395.0150 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 332/1000
2023-10-02 18:00:54.440 
Epoch 332/1000 
	 loss: 385.8196, MinusLogProbMetric: 385.8196, val_loss: 395.0592, val_MinusLogProbMetric: 395.0592

Epoch 332: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.8196 - MinusLogProbMetric: 385.8196 - val_loss: 395.0592 - val_MinusLogProbMetric: 395.0592 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 333/1000
2023-10-02 18:01:04.897 
Epoch 333/1000 
	 loss: 385.6207, MinusLogProbMetric: 385.6207, val_loss: 394.8462, val_MinusLogProbMetric: 394.8462

Epoch 333: val_loss did not improve from 394.65997
196/196 - 10s - loss: 385.6207 - MinusLogProbMetric: 385.6207 - val_loss: 394.8462 - val_MinusLogProbMetric: 394.8462 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 334/1000
2023-10-02 18:01:15.520 
Epoch 334/1000 
	 loss: 385.6953, MinusLogProbMetric: 385.6953, val_loss: 395.2669, val_MinusLogProbMetric: 395.2669

Epoch 334: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.6953 - MinusLogProbMetric: 385.6953 - val_loss: 395.2669 - val_MinusLogProbMetric: 395.2669 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 335/1000
2023-10-02 18:01:26.114 
Epoch 335/1000 
	 loss: 385.6897, MinusLogProbMetric: 385.6897, val_loss: 395.0174, val_MinusLogProbMetric: 395.0174

Epoch 335: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.6897 - MinusLogProbMetric: 385.6897 - val_loss: 395.0174 - val_MinusLogProbMetric: 395.0174 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 336/1000
2023-10-02 18:01:36.752 
Epoch 336/1000 
	 loss: 385.7578, MinusLogProbMetric: 385.7578, val_loss: 394.8589, val_MinusLogProbMetric: 394.8589

Epoch 336: val_loss did not improve from 394.65997
196/196 - 11s - loss: 385.7578 - MinusLogProbMetric: 385.7578 - val_loss: 394.8589 - val_MinusLogProbMetric: 394.8589 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 337/1000
2023-10-02 18:01:47.393 
Epoch 337/1000 
	 loss: 385.6370, MinusLogProbMetric: 385.6370, val_loss: 394.5532, val_MinusLogProbMetric: 394.5532

Epoch 337: val_loss improved from 394.65997 to 394.55316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 385.6370 - MinusLogProbMetric: 385.6370 - val_loss: 394.5532 - val_MinusLogProbMetric: 394.5532 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 338/1000
2023-10-02 18:01:58.261 
Epoch 338/1000 
	 loss: 385.7616, MinusLogProbMetric: 385.7616, val_loss: 395.4062, val_MinusLogProbMetric: 395.4062

Epoch 338: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.7616 - MinusLogProbMetric: 385.7616 - val_loss: 395.4062 - val_MinusLogProbMetric: 395.4062 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 339/1000
2023-10-02 18:02:08.785 
Epoch 339/1000 
	 loss: 385.8435, MinusLogProbMetric: 385.8435, val_loss: 395.0492, val_MinusLogProbMetric: 395.0492

Epoch 339: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.8435 - MinusLogProbMetric: 385.8435 - val_loss: 395.0492 - val_MinusLogProbMetric: 395.0492 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 340/1000
2023-10-02 18:02:19.250 
Epoch 340/1000 
	 loss: 385.6248, MinusLogProbMetric: 385.6248, val_loss: 394.9569, val_MinusLogProbMetric: 394.9569

Epoch 340: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.6248 - MinusLogProbMetric: 385.6248 - val_loss: 394.9569 - val_MinusLogProbMetric: 394.9569 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 341/1000
2023-10-02 18:02:29.797 
Epoch 341/1000 
	 loss: 385.7350, MinusLogProbMetric: 385.7350, val_loss: 395.2687, val_MinusLogProbMetric: 395.2687

Epoch 341: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.7350 - MinusLogProbMetric: 385.7350 - val_loss: 395.2687 - val_MinusLogProbMetric: 395.2687 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 342/1000
2023-10-02 18:02:40.325 
Epoch 342/1000 
	 loss: 385.7451, MinusLogProbMetric: 385.7451, val_loss: 395.0754, val_MinusLogProbMetric: 395.0754

Epoch 342: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.7451 - MinusLogProbMetric: 385.7451 - val_loss: 395.0754 - val_MinusLogProbMetric: 395.0754 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 343/1000
2023-10-02 18:02:50.835 
Epoch 343/1000 
	 loss: 385.6661, MinusLogProbMetric: 385.6661, val_loss: 395.4508, val_MinusLogProbMetric: 395.4508

Epoch 343: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6661 - MinusLogProbMetric: 385.6661 - val_loss: 395.4508 - val_MinusLogProbMetric: 395.4508 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 344/1000
2023-10-02 18:03:01.280 
Epoch 344/1000 
	 loss: 385.6469, MinusLogProbMetric: 385.6469, val_loss: 396.5379, val_MinusLogProbMetric: 396.5379

Epoch 344: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.6469 - MinusLogProbMetric: 385.6469 - val_loss: 396.5379 - val_MinusLogProbMetric: 396.5379 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 345/1000
2023-10-02 18:03:12.035 
Epoch 345/1000 
	 loss: 385.6641, MinusLogProbMetric: 385.6641, val_loss: 395.1372, val_MinusLogProbMetric: 395.1372

Epoch 345: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6641 - MinusLogProbMetric: 385.6641 - val_loss: 395.1372 - val_MinusLogProbMetric: 395.1372 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 346/1000
2023-10-02 18:03:22.453 
Epoch 346/1000 
	 loss: 385.5421, MinusLogProbMetric: 385.5421, val_loss: 394.7766, val_MinusLogProbMetric: 394.7766

Epoch 346: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.5421 - MinusLogProbMetric: 385.5421 - val_loss: 394.7766 - val_MinusLogProbMetric: 394.7766 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 347/1000
2023-10-02 18:03:32.876 
Epoch 347/1000 
	 loss: 385.7209, MinusLogProbMetric: 385.7209, val_loss: 395.4371, val_MinusLogProbMetric: 395.4371

Epoch 347: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.7209 - MinusLogProbMetric: 385.7209 - val_loss: 395.4371 - val_MinusLogProbMetric: 395.4371 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 348/1000
2023-10-02 18:03:43.500 
Epoch 348/1000 
	 loss: 386.0074, MinusLogProbMetric: 386.0074, val_loss: 395.5691, val_MinusLogProbMetric: 395.5691

Epoch 348: val_loss did not improve from 394.55316
196/196 - 11s - loss: 386.0074 - MinusLogProbMetric: 386.0074 - val_loss: 395.5691 - val_MinusLogProbMetric: 395.5691 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 349/1000
2023-10-02 18:03:54.079 
Epoch 349/1000 
	 loss: 385.7145, MinusLogProbMetric: 385.7145, val_loss: 394.9610, val_MinusLogProbMetric: 394.9610

Epoch 349: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.7145 - MinusLogProbMetric: 385.7145 - val_loss: 394.9610 - val_MinusLogProbMetric: 394.9610 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 350/1000
2023-10-02 18:04:04.729 
Epoch 350/1000 
	 loss: 385.6871, MinusLogProbMetric: 385.6871, val_loss: 394.7132, val_MinusLogProbMetric: 394.7132

Epoch 350: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6871 - MinusLogProbMetric: 385.6871 - val_loss: 394.7132 - val_MinusLogProbMetric: 394.7132 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 351/1000
2023-10-02 18:04:15.237 
Epoch 351/1000 
	 loss: 385.5617, MinusLogProbMetric: 385.5617, val_loss: 394.9232, val_MinusLogProbMetric: 394.9232

Epoch 351: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.5617 - MinusLogProbMetric: 385.5617 - val_loss: 394.9232 - val_MinusLogProbMetric: 394.9232 - lr: 2.7778e-05 - 10s/epoch - 54ms/step
Epoch 352/1000
2023-10-02 18:04:25.808 
Epoch 352/1000 
	 loss: 385.7275, MinusLogProbMetric: 385.7275, val_loss: 395.9628, val_MinusLogProbMetric: 395.9628

Epoch 352: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.7275 - MinusLogProbMetric: 385.7275 - val_loss: 395.9628 - val_MinusLogProbMetric: 395.9628 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 353/1000
2023-10-02 18:04:36.405 
Epoch 353/1000 
	 loss: 385.6945, MinusLogProbMetric: 385.6945, val_loss: 394.9050, val_MinusLogProbMetric: 394.9050

Epoch 353: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6945 - MinusLogProbMetric: 385.6945 - val_loss: 394.9050 - val_MinusLogProbMetric: 394.9050 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 354/1000
2023-10-02 18:04:46.860 
Epoch 354/1000 
	 loss: 385.8420, MinusLogProbMetric: 385.8420, val_loss: 395.5807, val_MinusLogProbMetric: 395.5807

Epoch 354: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.8420 - MinusLogProbMetric: 385.8420 - val_loss: 395.5807 - val_MinusLogProbMetric: 395.5807 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 355/1000
2023-10-02 18:04:57.492 
Epoch 355/1000 
	 loss: 385.6454, MinusLogProbMetric: 385.6454, val_loss: 394.7262, val_MinusLogProbMetric: 394.7262

Epoch 355: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6454 - MinusLogProbMetric: 385.6454 - val_loss: 394.7262 - val_MinusLogProbMetric: 394.7262 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 356/1000
2023-10-02 18:05:08.261 
Epoch 356/1000 
	 loss: 385.5651, MinusLogProbMetric: 385.5651, val_loss: 394.9450, val_MinusLogProbMetric: 394.9450

Epoch 356: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.5651 - MinusLogProbMetric: 385.5651 - val_loss: 394.9450 - val_MinusLogProbMetric: 394.9450 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 357/1000
2023-10-02 18:05:18.736 
Epoch 357/1000 
	 loss: 385.5322, MinusLogProbMetric: 385.5322, val_loss: 394.8175, val_MinusLogProbMetric: 394.8175

Epoch 357: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.5322 - MinusLogProbMetric: 385.5322 - val_loss: 394.8175 - val_MinusLogProbMetric: 394.8175 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 358/1000
2023-10-02 18:05:29.168 
Epoch 358/1000 
	 loss: 385.7353, MinusLogProbMetric: 385.7353, val_loss: 395.1606, val_MinusLogProbMetric: 395.1606

Epoch 358: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.7353 - MinusLogProbMetric: 385.7353 - val_loss: 395.1606 - val_MinusLogProbMetric: 395.1606 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 359/1000
2023-10-02 18:05:39.832 
Epoch 359/1000 
	 loss: 385.6940, MinusLogProbMetric: 385.6940, val_loss: 394.8921, val_MinusLogProbMetric: 394.8921

Epoch 359: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6940 - MinusLogProbMetric: 385.6940 - val_loss: 394.8921 - val_MinusLogProbMetric: 394.8921 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 360/1000
2023-10-02 18:05:50.332 
Epoch 360/1000 
	 loss: 385.8924, MinusLogProbMetric: 385.8924, val_loss: 395.7577, val_MinusLogProbMetric: 395.7577

Epoch 360: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.8924 - MinusLogProbMetric: 385.8924 - val_loss: 395.7577 - val_MinusLogProbMetric: 395.7577 - lr: 2.7778e-05 - 10s/epoch - 54ms/step
Epoch 361/1000
2023-10-02 18:06:00.799 
Epoch 361/1000 
	 loss: 386.1177, MinusLogProbMetric: 386.1177, val_loss: 395.4028, val_MinusLogProbMetric: 395.4028

Epoch 361: val_loss did not improve from 394.55316
196/196 - 10s - loss: 386.1177 - MinusLogProbMetric: 386.1177 - val_loss: 395.4028 - val_MinusLogProbMetric: 395.4028 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 362/1000
2023-10-02 18:06:11.208 
Epoch 362/1000 
	 loss: 385.8152, MinusLogProbMetric: 385.8152, val_loss: 394.8379, val_MinusLogProbMetric: 394.8379

Epoch 362: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.8152 - MinusLogProbMetric: 385.8152 - val_loss: 394.8379 - val_MinusLogProbMetric: 394.8379 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 363/1000
2023-10-02 18:06:21.995 
Epoch 363/1000 
	 loss: 385.8465, MinusLogProbMetric: 385.8465, val_loss: 395.4842, val_MinusLogProbMetric: 395.4842

Epoch 363: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.8465 - MinusLogProbMetric: 385.8465 - val_loss: 395.4842 - val_MinusLogProbMetric: 395.4842 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 364/1000
2023-10-02 18:06:32.459 
Epoch 364/1000 
	 loss: 385.6406, MinusLogProbMetric: 385.6406, val_loss: 395.2498, val_MinusLogProbMetric: 395.2498

Epoch 364: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.6406 - MinusLogProbMetric: 385.6406 - val_loss: 395.2498 - val_MinusLogProbMetric: 395.2498 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 365/1000
2023-10-02 18:06:42.985 
Epoch 365/1000 
	 loss: 385.4973, MinusLogProbMetric: 385.4973, val_loss: 395.7716, val_MinusLogProbMetric: 395.7716

Epoch 365: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4973 - MinusLogProbMetric: 385.4973 - val_loss: 395.7716 - val_MinusLogProbMetric: 395.7716 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 366/1000
2023-10-02 18:06:53.556 
Epoch 366/1000 
	 loss: 385.4772, MinusLogProbMetric: 385.4772, val_loss: 395.3396, val_MinusLogProbMetric: 395.3396

Epoch 366: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4772 - MinusLogProbMetric: 385.4772 - val_loss: 395.3396 - val_MinusLogProbMetric: 395.3396 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 367/1000
2023-10-02 18:07:04.175 
Epoch 367/1000 
	 loss: 385.4748, MinusLogProbMetric: 385.4748, val_loss: 395.5932, val_MinusLogProbMetric: 395.5932

Epoch 367: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4748 - MinusLogProbMetric: 385.4748 - val_loss: 395.5932 - val_MinusLogProbMetric: 395.5932 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 368/1000
2023-10-02 18:07:14.675 
Epoch 368/1000 
	 loss: 385.4420, MinusLogProbMetric: 385.4420, val_loss: 394.8180, val_MinusLogProbMetric: 394.8180

Epoch 368: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.4420 - MinusLogProbMetric: 385.4420 - val_loss: 394.8180 - val_MinusLogProbMetric: 394.8180 - lr: 2.7778e-05 - 10s/epoch - 54ms/step
Epoch 369/1000
2023-10-02 18:07:25.231 
Epoch 369/1000 
	 loss: 385.6085, MinusLogProbMetric: 385.6085, val_loss: 395.0307, val_MinusLogProbMetric: 395.0307

Epoch 369: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6085 - MinusLogProbMetric: 385.6085 - val_loss: 395.0307 - val_MinusLogProbMetric: 395.0307 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 370/1000
2023-10-02 18:07:35.987 
Epoch 370/1000 
	 loss: 385.4842, MinusLogProbMetric: 385.4842, val_loss: 395.0084, val_MinusLogProbMetric: 395.0084

Epoch 370: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4842 - MinusLogProbMetric: 385.4842 - val_loss: 395.0084 - val_MinusLogProbMetric: 395.0084 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 371/1000
2023-10-02 18:07:46.626 
Epoch 371/1000 
	 loss: 385.4045, MinusLogProbMetric: 385.4045, val_loss: 395.0047, val_MinusLogProbMetric: 395.0047

Epoch 371: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4045 - MinusLogProbMetric: 385.4045 - val_loss: 395.0047 - val_MinusLogProbMetric: 395.0047 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 372/1000
2023-10-02 18:07:57.232 
Epoch 372/1000 
	 loss: 385.4455, MinusLogProbMetric: 385.4455, val_loss: 395.1373, val_MinusLogProbMetric: 395.1373

Epoch 372: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4455 - MinusLogProbMetric: 385.4455 - val_loss: 395.1373 - val_MinusLogProbMetric: 395.1373 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 373/1000
2023-10-02 18:08:07.643 
Epoch 373/1000 
	 loss: 385.5854, MinusLogProbMetric: 385.5854, val_loss: 395.7911, val_MinusLogProbMetric: 395.7911

Epoch 373: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.5854 - MinusLogProbMetric: 385.5854 - val_loss: 395.7911 - val_MinusLogProbMetric: 395.7911 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 374/1000
2023-10-02 18:08:18.584 
Epoch 374/1000 
	 loss: 385.4638, MinusLogProbMetric: 385.4638, val_loss: 395.6930, val_MinusLogProbMetric: 395.6930

Epoch 374: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4638 - MinusLogProbMetric: 385.4638 - val_loss: 395.6930 - val_MinusLogProbMetric: 395.6930 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 375/1000
2023-10-02 18:08:28.985 
Epoch 375/1000 
	 loss: 385.5406, MinusLogProbMetric: 385.5406, val_loss: 394.8801, val_MinusLogProbMetric: 394.8801

Epoch 375: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.5406 - MinusLogProbMetric: 385.5406 - val_loss: 394.8801 - val_MinusLogProbMetric: 394.8801 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 376/1000
2023-10-02 18:08:39.526 
Epoch 376/1000 
	 loss: 385.6530, MinusLogProbMetric: 385.6530, val_loss: 395.1923, val_MinusLogProbMetric: 395.1923

Epoch 376: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6530 - MinusLogProbMetric: 385.6530 - val_loss: 395.1923 - val_MinusLogProbMetric: 395.1923 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 377/1000
2023-10-02 18:08:50.096 
Epoch 377/1000 
	 loss: 385.4941, MinusLogProbMetric: 385.4941, val_loss: 394.7476, val_MinusLogProbMetric: 394.7476

Epoch 377: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4941 - MinusLogProbMetric: 385.4941 - val_loss: 394.7476 - val_MinusLogProbMetric: 394.7476 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 378/1000
2023-10-02 18:09:00.720 
Epoch 378/1000 
	 loss: 385.4188, MinusLogProbMetric: 385.4188, val_loss: 395.9630, val_MinusLogProbMetric: 395.9630

Epoch 378: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4188 - MinusLogProbMetric: 385.4188 - val_loss: 395.9630 - val_MinusLogProbMetric: 395.9630 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 379/1000
2023-10-02 18:09:11.233 
Epoch 379/1000 
	 loss: 385.4456, MinusLogProbMetric: 385.4456, val_loss: 394.7474, val_MinusLogProbMetric: 394.7474

Epoch 379: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4456 - MinusLogProbMetric: 385.4456 - val_loss: 394.7474 - val_MinusLogProbMetric: 394.7474 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 380/1000
2023-10-02 18:09:21.742 
Epoch 380/1000 
	 loss: 385.3866, MinusLogProbMetric: 385.3866, val_loss: 395.5380, val_MinusLogProbMetric: 395.5380

Epoch 380: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.3866 - MinusLogProbMetric: 385.3866 - val_loss: 395.5380 - val_MinusLogProbMetric: 395.5380 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 381/1000
2023-10-02 18:09:32.151 
Epoch 381/1000 
	 loss: 385.5524, MinusLogProbMetric: 385.5524, val_loss: 395.5412, val_MinusLogProbMetric: 395.5412

Epoch 381: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.5524 - MinusLogProbMetric: 385.5524 - val_loss: 395.5412 - val_MinusLogProbMetric: 395.5412 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 382/1000
2023-10-02 18:09:42.548 
Epoch 382/1000 
	 loss: 385.7558, MinusLogProbMetric: 385.7558, val_loss: 394.9752, val_MinusLogProbMetric: 394.9752

Epoch 382: val_loss did not improve from 394.55316
196/196 - 10s - loss: 385.7558 - MinusLogProbMetric: 385.7558 - val_loss: 394.9752 - val_MinusLogProbMetric: 394.9752 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 383/1000
2023-10-02 18:09:53.097 
Epoch 383/1000 
	 loss: 385.6090, MinusLogProbMetric: 385.6090, val_loss: 394.8099, val_MinusLogProbMetric: 394.8099

Epoch 383: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.6090 - MinusLogProbMetric: 385.6090 - val_loss: 394.8099 - val_MinusLogProbMetric: 394.8099 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 384/1000
2023-10-02 18:10:03.846 
Epoch 384/1000 
	 loss: 385.4823, MinusLogProbMetric: 385.4823, val_loss: 395.3571, val_MinusLogProbMetric: 395.3571

Epoch 384: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4823 - MinusLogProbMetric: 385.4823 - val_loss: 395.3571 - val_MinusLogProbMetric: 395.3571 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 385/1000
2023-10-02 18:10:14.383 
Epoch 385/1000 
	 loss: 385.4203, MinusLogProbMetric: 385.4203, val_loss: 395.0996, val_MinusLogProbMetric: 395.0996

Epoch 385: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.4203 - MinusLogProbMetric: 385.4203 - val_loss: 395.0996 - val_MinusLogProbMetric: 395.0996 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 386/1000
2023-10-02 18:10:24.975 
Epoch 386/1000 
	 loss: 385.3639, MinusLogProbMetric: 385.3639, val_loss: 395.8901, val_MinusLogProbMetric: 395.8901

Epoch 386: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.3639 - MinusLogProbMetric: 385.3639 - val_loss: 395.8901 - val_MinusLogProbMetric: 395.8901 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 387/1000
2023-10-02 18:10:35.515 
Epoch 387/1000 
	 loss: 385.5941, MinusLogProbMetric: 385.5941, val_loss: 395.2604, val_MinusLogProbMetric: 395.2604

Epoch 387: val_loss did not improve from 394.55316
196/196 - 11s - loss: 385.5941 - MinusLogProbMetric: 385.5941 - val_loss: 395.2604 - val_MinusLogProbMetric: 395.2604 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 388/1000
2023-10-02 18:10:46.208 
Epoch 388/1000 
	 loss: 384.7272, MinusLogProbMetric: 384.7272, val_loss: 394.4727, val_MinusLogProbMetric: 394.4727

Epoch 388: val_loss improved from 394.55316 to 394.47266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 384.7272 - MinusLogProbMetric: 384.7272 - val_loss: 394.4727 - val_MinusLogProbMetric: 394.4727 - lr: 1.3889e-05 - 11s/epoch - 57ms/step
Epoch 389/1000
2023-10-02 18:10:57.208 
Epoch 389/1000 
	 loss: 384.6687, MinusLogProbMetric: 384.6687, val_loss: 394.5021, val_MinusLogProbMetric: 394.5021

Epoch 389: val_loss did not improve from 394.47266
196/196 - 10s - loss: 384.6687 - MinusLogProbMetric: 384.6687 - val_loss: 394.5021 - val_MinusLogProbMetric: 394.5021 - lr: 1.3889e-05 - 10s/epoch - 53ms/step
Epoch 390/1000
2023-10-02 18:11:07.700 
Epoch 390/1000 
	 loss: 384.6998, MinusLogProbMetric: 384.6998, val_loss: 394.5929, val_MinusLogProbMetric: 394.5929

Epoch 390: val_loss did not improve from 394.47266
196/196 - 10s - loss: 384.6998 - MinusLogProbMetric: 384.6998 - val_loss: 394.5929 - val_MinusLogProbMetric: 394.5929 - lr: 1.3889e-05 - 10s/epoch - 53ms/step
Epoch 391/1000
2023-10-02 18:11:18.300 
Epoch 391/1000 
	 loss: 384.6860, MinusLogProbMetric: 384.6860, val_loss: 394.4519, val_MinusLogProbMetric: 394.4519

Epoch 391: val_loss improved from 394.47266 to 394.45193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 384.6860 - MinusLogProbMetric: 384.6860 - val_loss: 394.4519 - val_MinusLogProbMetric: 394.4519 - lr: 1.3889e-05 - 11s/epoch - 56ms/step
Epoch 392/1000
2023-10-02 18:11:29.528 
Epoch 392/1000 
	 loss: 384.6895, MinusLogProbMetric: 384.6895, val_loss: 394.5758, val_MinusLogProbMetric: 394.5758

Epoch 392: val_loss did not improve from 394.45193
196/196 - 11s - loss: 384.6895 - MinusLogProbMetric: 384.6895 - val_loss: 394.5758 - val_MinusLogProbMetric: 394.5758 - lr: 1.3889e-05 - 11s/epoch - 55ms/step
Epoch 393/1000
2023-10-02 18:11:40.183 
Epoch 393/1000 
	 loss: 384.6940, MinusLogProbMetric: 384.6940, val_loss: 394.6011, val_MinusLogProbMetric: 394.6011

Epoch 393: val_loss did not improve from 394.45193
196/196 - 11s - loss: 384.6940 - MinusLogProbMetric: 384.6940 - val_loss: 394.6011 - val_MinusLogProbMetric: 394.6011 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 394/1000
2023-10-02 18:11:50.713 
Epoch 394/1000 
	 loss: 384.6781, MinusLogProbMetric: 384.6781, val_loss: 394.6472, val_MinusLogProbMetric: 394.6472

Epoch 394: val_loss did not improve from 394.45193
196/196 - 11s - loss: 384.6781 - MinusLogProbMetric: 384.6781 - val_loss: 394.6472 - val_MinusLogProbMetric: 394.6472 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 395/1000
2023-10-02 18:12:01.362 
Epoch 395/1000 
	 loss: 384.6960, MinusLogProbMetric: 384.6960, val_loss: 394.5573, val_MinusLogProbMetric: 394.5573

Epoch 395: val_loss did not improve from 394.45193
196/196 - 11s - loss: 384.6960 - MinusLogProbMetric: 384.6960 - val_loss: 394.5573 - val_MinusLogProbMetric: 394.5573 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 396/1000
2023-10-02 18:12:12.119 
Epoch 396/1000 
	 loss: 384.6562, MinusLogProbMetric: 384.6562, val_loss: 394.4323, val_MinusLogProbMetric: 394.4323

Epoch 396: val_loss improved from 394.45193 to 394.43228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 11s - loss: 384.6562 - MinusLogProbMetric: 384.6562 - val_loss: 394.4323 - val_MinusLogProbMetric: 394.4323 - lr: 1.3889e-05 - 11s/epoch - 57ms/step
Epoch 397/1000
2023-10-02 18:12:23.190 
Epoch 397/1000 
	 loss: 384.6479, MinusLogProbMetric: 384.6479, val_loss: 394.6427, val_MinusLogProbMetric: 394.6427

Epoch 397: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6479 - MinusLogProbMetric: 384.6479 - val_loss: 394.6427 - val_MinusLogProbMetric: 394.6427 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 398/1000
2023-10-02 18:12:33.749 
Epoch 398/1000 
	 loss: 384.6894, MinusLogProbMetric: 384.6894, val_loss: 394.5064, val_MinusLogProbMetric: 394.5064

Epoch 398: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6894 - MinusLogProbMetric: 384.6894 - val_loss: 394.5064 - val_MinusLogProbMetric: 394.5064 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 399/1000
2023-10-02 18:12:44.317 
Epoch 399/1000 
	 loss: 384.6786, MinusLogProbMetric: 384.6786, val_loss: 394.6175, val_MinusLogProbMetric: 394.6175

Epoch 399: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6786 - MinusLogProbMetric: 384.6786 - val_loss: 394.6175 - val_MinusLogProbMetric: 394.6175 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 400/1000
2023-10-02 18:12:54.846 
Epoch 400/1000 
	 loss: 384.6638, MinusLogProbMetric: 384.6638, val_loss: 394.4619, val_MinusLogProbMetric: 394.4619

Epoch 400: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6638 - MinusLogProbMetric: 384.6638 - val_loss: 394.4619 - val_MinusLogProbMetric: 394.4619 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 401/1000
2023-10-02 18:13:05.221 
Epoch 401/1000 
	 loss: 384.6253, MinusLogProbMetric: 384.6253, val_loss: 394.5505, val_MinusLogProbMetric: 394.5505

Epoch 401: val_loss did not improve from 394.43228
196/196 - 10s - loss: 384.6253 - MinusLogProbMetric: 384.6253 - val_loss: 394.5505 - val_MinusLogProbMetric: 394.5505 - lr: 1.3889e-05 - 10s/epoch - 53ms/step
Epoch 402/1000
2023-10-02 18:13:15.766 
Epoch 402/1000 
	 loss: 384.6528, MinusLogProbMetric: 384.6528, val_loss: 394.4752, val_MinusLogProbMetric: 394.4752

Epoch 402: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6528 - MinusLogProbMetric: 384.6528 - val_loss: 394.4752 - val_MinusLogProbMetric: 394.4752 - lr: 1.3889e-05 - 11s/epoch - 54ms/step
Epoch 403/1000
2023-10-02 18:13:26.973 
Epoch 403/1000 
	 loss: 384.6670, MinusLogProbMetric: 384.6670, val_loss: 394.5137, val_MinusLogProbMetric: 394.5137

Epoch 403: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6670 - MinusLogProbMetric: 384.6670 - val_loss: 394.5137 - val_MinusLogProbMetric: 394.5137 - lr: 1.3889e-05 - 11s/epoch - 57ms/step
Epoch 404/1000
2023-10-02 18:13:37.665 
Epoch 404/1000 
	 loss: 384.6638, MinusLogProbMetric: 384.6638, val_loss: 394.4743, val_MinusLogProbMetric: 394.4743

Epoch 404: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6638 - MinusLogProbMetric: 384.6638 - val_loss: 394.4743 - val_MinusLogProbMetric: 394.4743 - lr: 1.3889e-05 - 11s/epoch - 55ms/step
Epoch 405/1000
2023-10-02 18:13:48.985 
Epoch 405/1000 
	 loss: 384.6902, MinusLogProbMetric: 384.6902, val_loss: 394.6673, val_MinusLogProbMetric: 394.6673

Epoch 405: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6902 - MinusLogProbMetric: 384.6902 - val_loss: 394.6673 - val_MinusLogProbMetric: 394.6673 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 406/1000
2023-10-02 18:14:00.425 
Epoch 406/1000 
	 loss: 384.6518, MinusLogProbMetric: 384.6518, val_loss: 394.4935, val_MinusLogProbMetric: 394.4935

Epoch 406: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6518 - MinusLogProbMetric: 384.6518 - val_loss: 394.4935 - val_MinusLogProbMetric: 394.4935 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 407/1000
2023-10-02 18:14:11.640 
Epoch 407/1000 
	 loss: 384.6603, MinusLogProbMetric: 384.6603, val_loss: 394.4486, val_MinusLogProbMetric: 394.4486

Epoch 407: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6603 - MinusLogProbMetric: 384.6603 - val_loss: 394.4486 - val_MinusLogProbMetric: 394.4486 - lr: 1.3889e-05 - 11s/epoch - 57ms/step
Epoch 408/1000
2023-10-02 18:14:23.045 
Epoch 408/1000 
	 loss: 384.6352, MinusLogProbMetric: 384.6352, val_loss: 394.4988, val_MinusLogProbMetric: 394.4988

Epoch 408: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6352 - MinusLogProbMetric: 384.6352 - val_loss: 394.4988 - val_MinusLogProbMetric: 394.4988 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 409/1000
2023-10-02 18:14:34.509 
Epoch 409/1000 
	 loss: 384.6169, MinusLogProbMetric: 384.6169, val_loss: 394.5751, val_MinusLogProbMetric: 394.5751

Epoch 409: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6169 - MinusLogProbMetric: 384.6169 - val_loss: 394.5751 - val_MinusLogProbMetric: 394.5751 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 410/1000
2023-10-02 18:14:45.969 
Epoch 410/1000 
	 loss: 384.6425, MinusLogProbMetric: 384.6425, val_loss: 394.5356, val_MinusLogProbMetric: 394.5356

Epoch 410: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6425 - MinusLogProbMetric: 384.6425 - val_loss: 394.5356 - val_MinusLogProbMetric: 394.5356 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 411/1000
2023-10-02 18:14:57.386 
Epoch 411/1000 
	 loss: 384.6453, MinusLogProbMetric: 384.6453, val_loss: 394.9362, val_MinusLogProbMetric: 394.9362

Epoch 411: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6453 - MinusLogProbMetric: 384.6453 - val_loss: 394.9362 - val_MinusLogProbMetric: 394.9362 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 412/1000
2023-10-02 18:15:08.978 
Epoch 412/1000 
	 loss: 384.6447, MinusLogProbMetric: 384.6447, val_loss: 394.5073, val_MinusLogProbMetric: 394.5073

Epoch 412: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.6447 - MinusLogProbMetric: 384.6447 - val_loss: 394.5073 - val_MinusLogProbMetric: 394.5073 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 413/1000
2023-10-02 18:15:20.488 
Epoch 413/1000 
	 loss: 384.5958, MinusLogProbMetric: 384.5958, val_loss: 394.6228, val_MinusLogProbMetric: 394.6228

Epoch 413: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5958 - MinusLogProbMetric: 384.5958 - val_loss: 394.6228 - val_MinusLogProbMetric: 394.6228 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 414/1000
2023-10-02 18:15:31.904 
Epoch 414/1000 
	 loss: 384.6194, MinusLogProbMetric: 384.6194, val_loss: 394.7817, val_MinusLogProbMetric: 394.7817

Epoch 414: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6194 - MinusLogProbMetric: 384.6194 - val_loss: 394.7817 - val_MinusLogProbMetric: 394.7817 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 415/1000
2023-10-02 18:15:43.212 
Epoch 415/1000 
	 loss: 384.6028, MinusLogProbMetric: 384.6028, val_loss: 394.5904, val_MinusLogProbMetric: 394.5904

Epoch 415: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6028 - MinusLogProbMetric: 384.6028 - val_loss: 394.5904 - val_MinusLogProbMetric: 394.5904 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 416/1000
2023-10-02 18:15:54.742 
Epoch 416/1000 
	 loss: 384.6004, MinusLogProbMetric: 384.6004, val_loss: 394.5106, val_MinusLogProbMetric: 394.5106

Epoch 416: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.6004 - MinusLogProbMetric: 384.6004 - val_loss: 394.5106 - val_MinusLogProbMetric: 394.5106 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 417/1000
2023-10-02 18:16:06.299 
Epoch 417/1000 
	 loss: 384.5998, MinusLogProbMetric: 384.5998, val_loss: 394.6986, val_MinusLogProbMetric: 394.6986

Epoch 417: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5998 - MinusLogProbMetric: 384.5998 - val_loss: 394.6986 - val_MinusLogProbMetric: 394.6986 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 418/1000
2023-10-02 18:16:17.856 
Epoch 418/1000 
	 loss: 384.7262, MinusLogProbMetric: 384.7262, val_loss: 394.6196, val_MinusLogProbMetric: 394.6196

Epoch 418: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.7262 - MinusLogProbMetric: 384.7262 - val_loss: 394.6196 - val_MinusLogProbMetric: 394.6196 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 419/1000
2023-10-02 18:16:29.163 
Epoch 419/1000 
	 loss: 384.6223, MinusLogProbMetric: 384.6223, val_loss: 394.7951, val_MinusLogProbMetric: 394.7951

Epoch 419: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6223 - MinusLogProbMetric: 384.6223 - val_loss: 394.7951 - val_MinusLogProbMetric: 394.7951 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 420/1000
2023-10-02 18:16:40.586 
Epoch 420/1000 
	 loss: 384.6266, MinusLogProbMetric: 384.6266, val_loss: 394.5781, val_MinusLogProbMetric: 394.5781

Epoch 420: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6266 - MinusLogProbMetric: 384.6266 - val_loss: 394.5781 - val_MinusLogProbMetric: 394.5781 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 421/1000
2023-10-02 18:16:52.116 
Epoch 421/1000 
	 loss: 384.5723, MinusLogProbMetric: 384.5723, val_loss: 394.6114, val_MinusLogProbMetric: 394.6114

Epoch 421: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5723 - MinusLogProbMetric: 384.5723 - val_loss: 394.6114 - val_MinusLogProbMetric: 394.6114 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 422/1000
2023-10-02 18:17:03.555 
Epoch 422/1000 
	 loss: 384.5808, MinusLogProbMetric: 384.5808, val_loss: 394.5150, val_MinusLogProbMetric: 394.5150

Epoch 422: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5808 - MinusLogProbMetric: 384.5808 - val_loss: 394.5150 - val_MinusLogProbMetric: 394.5150 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 423/1000
2023-10-02 18:17:15.053 
Epoch 423/1000 
	 loss: 384.6472, MinusLogProbMetric: 384.6472, val_loss: 394.5156, val_MinusLogProbMetric: 394.5156

Epoch 423: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6472 - MinusLogProbMetric: 384.6472 - val_loss: 394.5156 - val_MinusLogProbMetric: 394.5156 - lr: 1.3889e-05 - 11s/epoch - 59ms/step
Epoch 424/1000
2023-10-02 18:17:26.384 
Epoch 424/1000 
	 loss: 384.6049, MinusLogProbMetric: 384.6049, val_loss: 394.5598, val_MinusLogProbMetric: 394.5598

Epoch 424: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6049 - MinusLogProbMetric: 384.6049 - val_loss: 394.5598 - val_MinusLogProbMetric: 394.5598 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 425/1000
2023-10-02 18:17:37.881 
Epoch 425/1000 
	 loss: 384.5682, MinusLogProbMetric: 384.5682, val_loss: 394.4664, val_MinusLogProbMetric: 394.4664

Epoch 425: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5682 - MinusLogProbMetric: 384.5682 - val_loss: 394.4664 - val_MinusLogProbMetric: 394.4664 - lr: 1.3889e-05 - 11s/epoch - 59ms/step
Epoch 426/1000
2023-10-02 18:17:49.315 
Epoch 426/1000 
	 loss: 384.5615, MinusLogProbMetric: 384.5615, val_loss: 394.5081, val_MinusLogProbMetric: 394.5081

Epoch 426: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5615 - MinusLogProbMetric: 384.5615 - val_loss: 394.5081 - val_MinusLogProbMetric: 394.5081 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 427/1000
2023-10-02 18:18:00.816 
Epoch 427/1000 
	 loss: 384.5721, MinusLogProbMetric: 384.5721, val_loss: 395.0864, val_MinusLogProbMetric: 395.0864

Epoch 427: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5721 - MinusLogProbMetric: 384.5721 - val_loss: 395.0864 - val_MinusLogProbMetric: 395.0864 - lr: 1.3889e-05 - 11s/epoch - 59ms/step
Epoch 428/1000
2023-10-02 18:18:12.324 
Epoch 428/1000 
	 loss: 384.6280, MinusLogProbMetric: 384.6280, val_loss: 394.4459, val_MinusLogProbMetric: 394.4459

Epoch 428: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.6280 - MinusLogProbMetric: 384.6280 - val_loss: 394.4459 - val_MinusLogProbMetric: 394.4459 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 429/1000
2023-10-02 18:18:23.808 
Epoch 429/1000 
	 loss: 384.5732, MinusLogProbMetric: 384.5732, val_loss: 394.5813, val_MinusLogProbMetric: 394.5813

Epoch 429: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5732 - MinusLogProbMetric: 384.5732 - val_loss: 394.5813 - val_MinusLogProbMetric: 394.5813 - lr: 1.3889e-05 - 11s/epoch - 59ms/step
Epoch 430/1000
2023-10-02 18:18:35.174 
Epoch 430/1000 
	 loss: 384.6034, MinusLogProbMetric: 384.6034, val_loss: 394.5374, val_MinusLogProbMetric: 394.5374

Epoch 430: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.6034 - MinusLogProbMetric: 384.6034 - val_loss: 394.5374 - val_MinusLogProbMetric: 394.5374 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 431/1000
2023-10-02 18:18:46.575 
Epoch 431/1000 
	 loss: 384.5371, MinusLogProbMetric: 384.5371, val_loss: 394.6702, val_MinusLogProbMetric: 394.6702

Epoch 431: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5371 - MinusLogProbMetric: 384.5371 - val_loss: 394.6702 - val_MinusLogProbMetric: 394.6702 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 432/1000
2023-10-02 18:18:57.897 
Epoch 432/1000 
	 loss: 384.5769, MinusLogProbMetric: 384.5769, val_loss: 394.4525, val_MinusLogProbMetric: 394.4525

Epoch 432: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5769 - MinusLogProbMetric: 384.5769 - val_loss: 394.4525 - val_MinusLogProbMetric: 394.4525 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 433/1000
2023-10-02 18:19:09.445 
Epoch 433/1000 
	 loss: 384.5561, MinusLogProbMetric: 384.5561, val_loss: 394.5132, val_MinusLogProbMetric: 394.5132

Epoch 433: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5561 - MinusLogProbMetric: 384.5561 - val_loss: 394.5132 - val_MinusLogProbMetric: 394.5132 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 434/1000
2023-10-02 18:19:21.191 
Epoch 434/1000 
	 loss: 384.5735, MinusLogProbMetric: 384.5735, val_loss: 394.7130, val_MinusLogProbMetric: 394.7130

Epoch 434: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5735 - MinusLogProbMetric: 384.5735 - val_loss: 394.7130 - val_MinusLogProbMetric: 394.7130 - lr: 1.3889e-05 - 12s/epoch - 60ms/step
Epoch 435/1000
2023-10-02 18:19:32.673 
Epoch 435/1000 
	 loss: 384.5578, MinusLogProbMetric: 384.5578, val_loss: 394.5928, val_MinusLogProbMetric: 394.5928

Epoch 435: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5578 - MinusLogProbMetric: 384.5578 - val_loss: 394.5928 - val_MinusLogProbMetric: 394.5928 - lr: 1.3889e-05 - 11s/epoch - 59ms/step
Epoch 436/1000
2023-10-02 18:19:44.340 
Epoch 436/1000 
	 loss: 384.5718, MinusLogProbMetric: 384.5718, val_loss: 394.4812, val_MinusLogProbMetric: 394.4812

Epoch 436: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5718 - MinusLogProbMetric: 384.5718 - val_loss: 394.4812 - val_MinusLogProbMetric: 394.4812 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 437/1000
2023-10-02 18:19:55.765 
Epoch 437/1000 
	 loss: 384.5691, MinusLogProbMetric: 384.5691, val_loss: 394.6384, val_MinusLogProbMetric: 394.6384

Epoch 437: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5691 - MinusLogProbMetric: 384.5691 - val_loss: 394.6384 - val_MinusLogProbMetric: 394.6384 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 438/1000
2023-10-02 18:20:07.295 
Epoch 438/1000 
	 loss: 384.5853, MinusLogProbMetric: 384.5853, val_loss: 394.6749, val_MinusLogProbMetric: 394.6749

Epoch 438: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5853 - MinusLogProbMetric: 384.5853 - val_loss: 394.6749 - val_MinusLogProbMetric: 394.6749 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 439/1000
2023-10-02 18:20:18.600 
Epoch 439/1000 
	 loss: 384.5177, MinusLogProbMetric: 384.5177, val_loss: 394.5786, val_MinusLogProbMetric: 394.5786

Epoch 439: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5177 - MinusLogProbMetric: 384.5177 - val_loss: 394.5786 - val_MinusLogProbMetric: 394.5786 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 440/1000
2023-10-02 18:20:30.098 
Epoch 440/1000 
	 loss: 384.5819, MinusLogProbMetric: 384.5819, val_loss: 394.8171, val_MinusLogProbMetric: 394.8171

Epoch 440: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5819 - MinusLogProbMetric: 384.5819 - val_loss: 394.8171 - val_MinusLogProbMetric: 394.8171 - lr: 1.3889e-05 - 11s/epoch - 59ms/step
Epoch 441/1000
2023-10-02 18:20:41.759 
Epoch 441/1000 
	 loss: 384.6113, MinusLogProbMetric: 384.6113, val_loss: 394.6457, val_MinusLogProbMetric: 394.6457

Epoch 441: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.6113 - MinusLogProbMetric: 384.6113 - val_loss: 394.6457 - val_MinusLogProbMetric: 394.6457 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 442/1000
2023-10-02 18:20:53.156 
Epoch 442/1000 
	 loss: 384.5108, MinusLogProbMetric: 384.5108, val_loss: 394.4933, val_MinusLogProbMetric: 394.4933

Epoch 442: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5108 - MinusLogProbMetric: 384.5108 - val_loss: 394.4933 - val_MinusLogProbMetric: 394.4933 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 443/1000
2023-10-02 18:21:04.613 
Epoch 443/1000 
	 loss: 384.5144, MinusLogProbMetric: 384.5144, val_loss: 394.7289, val_MinusLogProbMetric: 394.7289

Epoch 443: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5144 - MinusLogProbMetric: 384.5144 - val_loss: 394.7289 - val_MinusLogProbMetric: 394.7289 - lr: 1.3889e-05 - 11s/epoch - 58ms/step
Epoch 444/1000
2023-10-02 18:21:16.312 
Epoch 444/1000 
	 loss: 384.5507, MinusLogProbMetric: 384.5507, val_loss: 394.6081, val_MinusLogProbMetric: 394.6081

Epoch 444: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5507 - MinusLogProbMetric: 384.5507 - val_loss: 394.6081 - val_MinusLogProbMetric: 394.6081 - lr: 1.3889e-05 - 12s/epoch - 60ms/step
Epoch 445/1000
2023-10-02 18:21:27.397 
Epoch 445/1000 
	 loss: 384.5121, MinusLogProbMetric: 384.5121, val_loss: 394.5051, val_MinusLogProbMetric: 394.5051

Epoch 445: val_loss did not improve from 394.43228
196/196 - 11s - loss: 384.5121 - MinusLogProbMetric: 384.5121 - val_loss: 394.5051 - val_MinusLogProbMetric: 394.5051 - lr: 1.3889e-05 - 11s/epoch - 57ms/step
Epoch 446/1000
2023-10-02 18:21:38.936 
Epoch 446/1000 
	 loss: 384.5075, MinusLogProbMetric: 384.5075, val_loss: 394.4848, val_MinusLogProbMetric: 394.4848

Epoch 446: val_loss did not improve from 394.43228
196/196 - 12s - loss: 384.5075 - MinusLogProbMetric: 384.5075 - val_loss: 394.4848 - val_MinusLogProbMetric: 394.4848 - lr: 1.3889e-05 - 12s/epoch - 59ms/step
Epoch 447/1000
2023-10-02 18:21:50.459 
Epoch 447/1000 
	 loss: 384.2674, MinusLogProbMetric: 384.2674, val_loss: 394.3267, val_MinusLogProbMetric: 394.3267

Epoch 447: val_loss improved from 394.43228 to 394.32669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 384.2674 - MinusLogProbMetric: 384.2674 - val_loss: 394.3267 - val_MinusLogProbMetric: 394.3267 - lr: 6.9444e-06 - 12s/epoch - 61ms/step
Epoch 448/1000
2023-10-02 18:22:02.187 
Epoch 448/1000 
	 loss: 384.2606, MinusLogProbMetric: 384.2606, val_loss: 394.3418, val_MinusLogProbMetric: 394.3418

Epoch 448: val_loss did not improve from 394.32669
196/196 - 11s - loss: 384.2606 - MinusLogProbMetric: 384.2606 - val_loss: 394.3418 - val_MinusLogProbMetric: 394.3418 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 449/1000
2023-10-02 18:22:13.639 
Epoch 449/1000 
	 loss: 384.2434, MinusLogProbMetric: 384.2434, val_loss: 394.3710, val_MinusLogProbMetric: 394.3710

Epoch 449: val_loss did not improve from 394.32669
196/196 - 11s - loss: 384.2434 - MinusLogProbMetric: 384.2434 - val_loss: 394.3710 - val_MinusLogProbMetric: 394.3710 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 450/1000
2023-10-02 18:22:25.059 
Epoch 450/1000 
	 loss: 384.2493, MinusLogProbMetric: 384.2493, val_loss: 394.3163, val_MinusLogProbMetric: 394.3163

Epoch 450: val_loss improved from 394.32669 to 394.31631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 384.2493 - MinusLogProbMetric: 384.2493 - val_loss: 394.3163 - val_MinusLogProbMetric: 394.3163 - lr: 6.9444e-06 - 12s/epoch - 60ms/step
Epoch 451/1000
2023-10-02 18:22:36.754 
Epoch 451/1000 
	 loss: 384.2568, MinusLogProbMetric: 384.2568, val_loss: 394.3247, val_MinusLogProbMetric: 394.3247

Epoch 451: val_loss did not improve from 394.31631
196/196 - 11s - loss: 384.2568 - MinusLogProbMetric: 384.2568 - val_loss: 394.3247 - val_MinusLogProbMetric: 394.3247 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 452/1000
2023-10-02 18:22:48.095 
Epoch 452/1000 
	 loss: 384.2414, MinusLogProbMetric: 384.2414, val_loss: 394.3172, val_MinusLogProbMetric: 394.3172

Epoch 452: val_loss did not improve from 394.31631
196/196 - 11s - loss: 384.2414 - MinusLogProbMetric: 384.2414 - val_loss: 394.3172 - val_MinusLogProbMetric: 394.3172 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 453/1000
2023-10-02 18:22:59.709 
Epoch 453/1000 
	 loss: 384.2380, MinusLogProbMetric: 384.2380, val_loss: 394.3351, val_MinusLogProbMetric: 394.3351

Epoch 453: val_loss did not improve from 394.31631
196/196 - 12s - loss: 384.2380 - MinusLogProbMetric: 384.2380 - val_loss: 394.3351 - val_MinusLogProbMetric: 394.3351 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 454/1000
2023-10-02 18:23:11.162 
Epoch 454/1000 
	 loss: 384.2409, MinusLogProbMetric: 384.2409, val_loss: 394.3656, val_MinusLogProbMetric: 394.3656

Epoch 454: val_loss did not improve from 394.31631
196/196 - 11s - loss: 384.2409 - MinusLogProbMetric: 384.2409 - val_loss: 394.3656 - val_MinusLogProbMetric: 394.3656 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 455/1000
2023-10-02 18:23:22.558 
Epoch 455/1000 
	 loss: 384.2348, MinusLogProbMetric: 384.2348, val_loss: 394.3900, val_MinusLogProbMetric: 394.3900

Epoch 455: val_loss did not improve from 394.31631
196/196 - 11s - loss: 384.2348 - MinusLogProbMetric: 384.2348 - val_loss: 394.3900 - val_MinusLogProbMetric: 394.3900 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 456/1000
2023-10-02 18:23:33.960 
Epoch 456/1000 
	 loss: 384.2305, MinusLogProbMetric: 384.2305, val_loss: 394.4118, val_MinusLogProbMetric: 394.4118

Epoch 456: val_loss did not improve from 394.31631
196/196 - 11s - loss: 384.2305 - MinusLogProbMetric: 384.2305 - val_loss: 394.4118 - val_MinusLogProbMetric: 394.4118 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 457/1000
2023-10-02 18:23:45.483 
Epoch 457/1000 
	 loss: 384.2310, MinusLogProbMetric: 384.2310, val_loss: 394.3495, val_MinusLogProbMetric: 394.3495

Epoch 457: val_loss did not improve from 394.31631
196/196 - 12s - loss: 384.2310 - MinusLogProbMetric: 384.2310 - val_loss: 394.3495 - val_MinusLogProbMetric: 394.3495 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 458/1000
2023-10-02 18:23:57.039 
Epoch 458/1000 
	 loss: 384.2347, MinusLogProbMetric: 384.2347, val_loss: 394.3639, val_MinusLogProbMetric: 394.3639

Epoch 458: val_loss did not improve from 394.31631
196/196 - 12s - loss: 384.2347 - MinusLogProbMetric: 384.2347 - val_loss: 394.3639 - val_MinusLogProbMetric: 394.3639 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 459/1000
2023-10-02 18:24:08.517 
Epoch 459/1000 
	 loss: 384.2372, MinusLogProbMetric: 384.2372, val_loss: 394.3056, val_MinusLogProbMetric: 394.3056

Epoch 459: val_loss improved from 394.31631 to 394.30560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 384.2372 - MinusLogProbMetric: 384.2372 - val_loss: 394.3056 - val_MinusLogProbMetric: 394.3056 - lr: 6.9444e-06 - 12s/epoch - 61ms/step
Epoch 460/1000
2023-10-02 18:24:20.345 
Epoch 460/1000 
	 loss: 384.2361, MinusLogProbMetric: 384.2361, val_loss: 394.4359, val_MinusLogProbMetric: 394.4359

Epoch 460: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2361 - MinusLogProbMetric: 384.2361 - val_loss: 394.4359 - val_MinusLogProbMetric: 394.4359 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 461/1000
2023-10-02 18:24:31.866 
Epoch 461/1000 
	 loss: 384.2285, MinusLogProbMetric: 384.2285, val_loss: 394.3778, val_MinusLogProbMetric: 394.3778

Epoch 461: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.2285 - MinusLogProbMetric: 384.2285 - val_loss: 394.3778 - val_MinusLogProbMetric: 394.3778 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 462/1000
2023-10-02 18:24:43.435 
Epoch 462/1000 
	 loss: 384.2101, MinusLogProbMetric: 384.2101, val_loss: 394.3876, val_MinusLogProbMetric: 394.3876

Epoch 462: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.2101 - MinusLogProbMetric: 384.2101 - val_loss: 394.3876 - val_MinusLogProbMetric: 394.3876 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 463/1000
2023-10-02 18:24:54.913 
Epoch 463/1000 
	 loss: 384.2135, MinusLogProbMetric: 384.2135, val_loss: 394.3837, val_MinusLogProbMetric: 394.3837

Epoch 463: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2135 - MinusLogProbMetric: 384.2135 - val_loss: 394.3837 - val_MinusLogProbMetric: 394.3837 - lr: 6.9444e-06 - 11s/epoch - 59ms/step
Epoch 464/1000
2023-10-02 18:25:06.319 
Epoch 464/1000 
	 loss: 384.2090, MinusLogProbMetric: 384.2090, val_loss: 394.3562, val_MinusLogProbMetric: 394.3562

Epoch 464: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2090 - MinusLogProbMetric: 384.2090 - val_loss: 394.3562 - val_MinusLogProbMetric: 394.3562 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 465/1000
2023-10-02 18:25:17.767 
Epoch 465/1000 
	 loss: 384.2122, MinusLogProbMetric: 384.2122, val_loss: 394.3680, val_MinusLogProbMetric: 394.3680

Epoch 465: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2122 - MinusLogProbMetric: 384.2122 - val_loss: 394.3680 - val_MinusLogProbMetric: 394.3680 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 466/1000
2023-10-02 18:25:29.084 
Epoch 466/1000 
	 loss: 384.2316, MinusLogProbMetric: 384.2316, val_loss: 394.3605, val_MinusLogProbMetric: 394.3605

Epoch 466: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2316 - MinusLogProbMetric: 384.2316 - val_loss: 394.3605 - val_MinusLogProbMetric: 394.3605 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 467/1000
2023-10-02 18:25:40.602 
Epoch 467/1000 
	 loss: 384.2228, MinusLogProbMetric: 384.2228, val_loss: 394.4348, val_MinusLogProbMetric: 394.4348

Epoch 467: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.2228 - MinusLogProbMetric: 384.2228 - val_loss: 394.4348 - val_MinusLogProbMetric: 394.4348 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 468/1000
2023-10-02 18:25:51.937 
Epoch 468/1000 
	 loss: 384.2082, MinusLogProbMetric: 384.2082, val_loss: 394.4254, val_MinusLogProbMetric: 394.4254

Epoch 468: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2082 - MinusLogProbMetric: 384.2082 - val_loss: 394.4254 - val_MinusLogProbMetric: 394.4254 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 469/1000
2023-10-02 18:26:03.357 
Epoch 469/1000 
	 loss: 384.2033, MinusLogProbMetric: 384.2033, val_loss: 394.3804, val_MinusLogProbMetric: 394.3804

Epoch 469: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2033 - MinusLogProbMetric: 384.2033 - val_loss: 394.3804 - val_MinusLogProbMetric: 394.3804 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 470/1000
2023-10-02 18:26:14.857 
Epoch 470/1000 
	 loss: 384.2058, MinusLogProbMetric: 384.2058, val_loss: 394.3862, val_MinusLogProbMetric: 394.3862

Epoch 470: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2058 - MinusLogProbMetric: 384.2058 - val_loss: 394.3862 - val_MinusLogProbMetric: 394.3862 - lr: 6.9444e-06 - 11s/epoch - 59ms/step
Epoch 471/1000
2023-10-02 18:26:26.264 
Epoch 471/1000 
	 loss: 384.1961, MinusLogProbMetric: 384.1961, val_loss: 394.3428, val_MinusLogProbMetric: 394.3428

Epoch 471: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1961 - MinusLogProbMetric: 384.1961 - val_loss: 394.3428 - val_MinusLogProbMetric: 394.3428 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 472/1000
2023-10-02 18:26:37.865 
Epoch 472/1000 
	 loss: 384.1931, MinusLogProbMetric: 384.1931, val_loss: 394.3846, val_MinusLogProbMetric: 394.3846

Epoch 472: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1931 - MinusLogProbMetric: 384.1931 - val_loss: 394.3846 - val_MinusLogProbMetric: 394.3846 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 473/1000
2023-10-02 18:26:49.199 
Epoch 473/1000 
	 loss: 384.2052, MinusLogProbMetric: 384.2052, val_loss: 394.3559, val_MinusLogProbMetric: 394.3559

Epoch 473: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2052 - MinusLogProbMetric: 384.2052 - val_loss: 394.3559 - val_MinusLogProbMetric: 394.3559 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 474/1000
2023-10-02 18:27:00.775 
Epoch 474/1000 
	 loss: 384.2093, MinusLogProbMetric: 384.2093, val_loss: 394.3843, val_MinusLogProbMetric: 394.3843

Epoch 474: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.2093 - MinusLogProbMetric: 384.2093 - val_loss: 394.3843 - val_MinusLogProbMetric: 394.3843 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 475/1000
2023-10-02 18:27:12.083 
Epoch 475/1000 
	 loss: 384.2067, MinusLogProbMetric: 384.2067, val_loss: 394.3748, val_MinusLogProbMetric: 394.3748

Epoch 475: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2067 - MinusLogProbMetric: 384.2067 - val_loss: 394.3748 - val_MinusLogProbMetric: 394.3748 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 476/1000
2023-10-02 18:27:23.713 
Epoch 476/1000 
	 loss: 384.2119, MinusLogProbMetric: 384.2119, val_loss: 394.3887, val_MinusLogProbMetric: 394.3887

Epoch 476: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.2119 - MinusLogProbMetric: 384.2119 - val_loss: 394.3887 - val_MinusLogProbMetric: 394.3887 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 477/1000
2023-10-02 18:27:35.095 
Epoch 477/1000 
	 loss: 384.2105, MinusLogProbMetric: 384.2105, val_loss: 394.3078, val_MinusLogProbMetric: 394.3078

Epoch 477: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.2105 - MinusLogProbMetric: 384.2105 - val_loss: 394.3078 - val_MinusLogProbMetric: 394.3078 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 478/1000
2023-10-02 18:27:46.651 
Epoch 478/1000 
	 loss: 384.1922, MinusLogProbMetric: 384.1922, val_loss: 394.3921, val_MinusLogProbMetric: 394.3921

Epoch 478: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1922 - MinusLogProbMetric: 384.1922 - val_loss: 394.3921 - val_MinusLogProbMetric: 394.3921 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 479/1000
2023-10-02 18:27:58.089 
Epoch 479/1000 
	 loss: 384.1974, MinusLogProbMetric: 384.1974, val_loss: 394.3407, val_MinusLogProbMetric: 394.3407

Epoch 479: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1974 - MinusLogProbMetric: 384.1974 - val_loss: 394.3407 - val_MinusLogProbMetric: 394.3407 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 480/1000
2023-10-02 18:28:09.472 
Epoch 480/1000 
	 loss: 384.1925, MinusLogProbMetric: 384.1925, val_loss: 394.4019, val_MinusLogProbMetric: 394.4019

Epoch 480: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1925 - MinusLogProbMetric: 384.1925 - val_loss: 394.4019 - val_MinusLogProbMetric: 394.4019 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 481/1000
2023-10-02 18:28:20.791 
Epoch 481/1000 
	 loss: 384.1947, MinusLogProbMetric: 384.1947, val_loss: 394.3819, val_MinusLogProbMetric: 394.3819

Epoch 481: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1947 - MinusLogProbMetric: 384.1947 - val_loss: 394.3819 - val_MinusLogProbMetric: 394.3819 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 482/1000
2023-10-02 18:28:32.254 
Epoch 482/1000 
	 loss: 384.1988, MinusLogProbMetric: 384.1988, val_loss: 394.3992, val_MinusLogProbMetric: 394.3992

Epoch 482: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1988 - MinusLogProbMetric: 384.1988 - val_loss: 394.3992 - val_MinusLogProbMetric: 394.3992 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 483/1000
2023-10-02 18:28:43.771 
Epoch 483/1000 
	 loss: 384.1833, MinusLogProbMetric: 384.1833, val_loss: 394.3313, val_MinusLogProbMetric: 394.3313

Epoch 483: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1833 - MinusLogProbMetric: 384.1833 - val_loss: 394.3313 - val_MinusLogProbMetric: 394.3313 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 484/1000
2023-10-02 18:28:55.230 
Epoch 484/1000 
	 loss: 384.1744, MinusLogProbMetric: 384.1744, val_loss: 394.3235, val_MinusLogProbMetric: 394.3235

Epoch 484: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1744 - MinusLogProbMetric: 384.1744 - val_loss: 394.3235 - val_MinusLogProbMetric: 394.3235 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 485/1000
2023-10-02 18:29:06.556 
Epoch 485/1000 
	 loss: 384.1811, MinusLogProbMetric: 384.1811, val_loss: 394.3595, val_MinusLogProbMetric: 394.3595

Epoch 485: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1811 - MinusLogProbMetric: 384.1811 - val_loss: 394.3595 - val_MinusLogProbMetric: 394.3595 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 486/1000
2023-10-02 18:29:18.143 
Epoch 486/1000 
	 loss: 384.1792, MinusLogProbMetric: 384.1792, val_loss: 394.3123, val_MinusLogProbMetric: 394.3123

Epoch 486: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1792 - MinusLogProbMetric: 384.1792 - val_loss: 394.3123 - val_MinusLogProbMetric: 394.3123 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 487/1000
2023-10-02 18:29:29.654 
Epoch 487/1000 
	 loss: 384.1676, MinusLogProbMetric: 384.1676, val_loss: 394.3794, val_MinusLogProbMetric: 394.3794

Epoch 487: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1676 - MinusLogProbMetric: 384.1676 - val_loss: 394.3794 - val_MinusLogProbMetric: 394.3794 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 488/1000
2023-10-02 18:29:40.599 
Epoch 488/1000 
	 loss: 384.1851, MinusLogProbMetric: 384.1851, val_loss: 394.3393, val_MinusLogProbMetric: 394.3393

Epoch 488: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1851 - MinusLogProbMetric: 384.1851 - val_loss: 394.3393 - val_MinusLogProbMetric: 394.3393 - lr: 6.9444e-06 - 11s/epoch - 56ms/step
Epoch 489/1000
2023-10-02 18:29:52.100 
Epoch 489/1000 
	 loss: 384.1559, MinusLogProbMetric: 384.1559, val_loss: 394.4001, val_MinusLogProbMetric: 394.4001

Epoch 489: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1559 - MinusLogProbMetric: 384.1559 - val_loss: 394.4001 - val_MinusLogProbMetric: 394.4001 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 490/1000
2023-10-02 18:30:03.591 
Epoch 490/1000 
	 loss: 384.1745, MinusLogProbMetric: 384.1745, val_loss: 394.3472, val_MinusLogProbMetric: 394.3472

Epoch 490: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1745 - MinusLogProbMetric: 384.1745 - val_loss: 394.3472 - val_MinusLogProbMetric: 394.3472 - lr: 6.9444e-06 - 11s/epoch - 59ms/step
Epoch 491/1000
2023-10-02 18:30:14.881 
Epoch 491/1000 
	 loss: 384.1765, MinusLogProbMetric: 384.1765, val_loss: 394.4115, val_MinusLogProbMetric: 394.4115

Epoch 491: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1765 - MinusLogProbMetric: 384.1765 - val_loss: 394.4115 - val_MinusLogProbMetric: 394.4115 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 492/1000
2023-10-02 18:30:25.959 
Epoch 492/1000 
	 loss: 384.1794, MinusLogProbMetric: 384.1794, val_loss: 394.3509, val_MinusLogProbMetric: 394.3509

Epoch 492: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1794 - MinusLogProbMetric: 384.1794 - val_loss: 394.3509 - val_MinusLogProbMetric: 394.3509 - lr: 6.9444e-06 - 11s/epoch - 56ms/step
Epoch 493/1000
2023-10-02 18:30:37.389 
Epoch 493/1000 
	 loss: 384.1834, MinusLogProbMetric: 384.1834, val_loss: 394.3060, val_MinusLogProbMetric: 394.3060

Epoch 493: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1834 - MinusLogProbMetric: 384.1834 - val_loss: 394.3060 - val_MinusLogProbMetric: 394.3060 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 494/1000
2023-10-02 18:30:48.810 
Epoch 494/1000 
	 loss: 384.1626, MinusLogProbMetric: 384.1626, val_loss: 394.4310, val_MinusLogProbMetric: 394.4310

Epoch 494: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1626 - MinusLogProbMetric: 384.1626 - val_loss: 394.4310 - val_MinusLogProbMetric: 394.4310 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 495/1000
2023-10-02 18:31:00.195 
Epoch 495/1000 
	 loss: 384.1510, MinusLogProbMetric: 384.1510, val_loss: 394.3249, val_MinusLogProbMetric: 394.3249

Epoch 495: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1510 - MinusLogProbMetric: 384.1510 - val_loss: 394.3249 - val_MinusLogProbMetric: 394.3249 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 496/1000
2023-10-02 18:31:11.746 
Epoch 496/1000 
	 loss: 384.1560, MinusLogProbMetric: 384.1560, val_loss: 394.3833, val_MinusLogProbMetric: 394.3833

Epoch 496: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1560 - MinusLogProbMetric: 384.1560 - val_loss: 394.3833 - val_MinusLogProbMetric: 394.3833 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 497/1000
2023-10-02 18:31:23.034 
Epoch 497/1000 
	 loss: 384.1606, MinusLogProbMetric: 384.1606, val_loss: 394.3579, val_MinusLogProbMetric: 394.3579

Epoch 497: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1606 - MinusLogProbMetric: 384.1606 - val_loss: 394.3579 - val_MinusLogProbMetric: 394.3579 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 498/1000
2023-10-02 18:31:34.481 
Epoch 498/1000 
	 loss: 384.1678, MinusLogProbMetric: 384.1678, val_loss: 394.3823, val_MinusLogProbMetric: 394.3823

Epoch 498: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1678 - MinusLogProbMetric: 384.1678 - val_loss: 394.3823 - val_MinusLogProbMetric: 394.3823 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 499/1000
2023-10-02 18:31:45.834 
Epoch 499/1000 
	 loss: 384.1666, MinusLogProbMetric: 384.1666, val_loss: 394.3601, val_MinusLogProbMetric: 394.3601

Epoch 499: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1666 - MinusLogProbMetric: 384.1666 - val_loss: 394.3601 - val_MinusLogProbMetric: 394.3601 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 500/1000
2023-10-02 18:31:56.912 
Epoch 500/1000 
	 loss: 384.1549, MinusLogProbMetric: 384.1549, val_loss: 394.3614, val_MinusLogProbMetric: 394.3614

Epoch 500: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1549 - MinusLogProbMetric: 384.1549 - val_loss: 394.3614 - val_MinusLogProbMetric: 394.3614 - lr: 6.9444e-06 - 11s/epoch - 57ms/step
Epoch 501/1000
2023-10-02 18:32:08.311 
Epoch 501/1000 
	 loss: 384.1682, MinusLogProbMetric: 384.1682, val_loss: 394.3250, val_MinusLogProbMetric: 394.3250

Epoch 501: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1682 - MinusLogProbMetric: 384.1682 - val_loss: 394.3250 - val_MinusLogProbMetric: 394.3250 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 502/1000
2023-10-02 18:32:19.719 
Epoch 502/1000 
	 loss: 384.1698, MinusLogProbMetric: 384.1698, val_loss: 394.3219, val_MinusLogProbMetric: 394.3219

Epoch 502: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1698 - MinusLogProbMetric: 384.1698 - val_loss: 394.3219 - val_MinusLogProbMetric: 394.3219 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 503/1000
2023-10-02 18:32:31.119 
Epoch 503/1000 
	 loss: 384.1368, MinusLogProbMetric: 384.1368, val_loss: 394.4110, val_MinusLogProbMetric: 394.4110

Epoch 503: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1368 - MinusLogProbMetric: 384.1368 - val_loss: 394.4110 - val_MinusLogProbMetric: 394.4110 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 504/1000
2023-10-02 18:32:42.555 
Epoch 504/1000 
	 loss: 384.1656, MinusLogProbMetric: 384.1656, val_loss: 394.4965, val_MinusLogProbMetric: 394.4965

Epoch 504: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1656 - MinusLogProbMetric: 384.1656 - val_loss: 394.4965 - val_MinusLogProbMetric: 394.4965 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 505/1000
2023-10-02 18:32:54.053 
Epoch 505/1000 
	 loss: 384.1595, MinusLogProbMetric: 384.1595, val_loss: 394.3563, val_MinusLogProbMetric: 394.3563

Epoch 505: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1595 - MinusLogProbMetric: 384.1595 - val_loss: 394.3563 - val_MinusLogProbMetric: 394.3563 - lr: 6.9444e-06 - 11s/epoch - 59ms/step
Epoch 506/1000
2023-10-02 18:33:05.356 
Epoch 506/1000 
	 loss: 384.1571, MinusLogProbMetric: 384.1571, val_loss: 394.3625, val_MinusLogProbMetric: 394.3625

Epoch 506: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1571 - MinusLogProbMetric: 384.1571 - val_loss: 394.3625 - val_MinusLogProbMetric: 394.3625 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 507/1000
2023-10-02 18:33:16.712 
Epoch 507/1000 
	 loss: 384.1555, MinusLogProbMetric: 384.1555, val_loss: 394.4442, val_MinusLogProbMetric: 394.4442

Epoch 507: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1555 - MinusLogProbMetric: 384.1555 - val_loss: 394.4442 - val_MinusLogProbMetric: 394.4442 - lr: 6.9444e-06 - 11s/epoch - 58ms/step
Epoch 508/1000
2023-10-02 18:33:28.270 
Epoch 508/1000 
	 loss: 384.1425, MinusLogProbMetric: 384.1425, val_loss: 394.3514, val_MinusLogProbMetric: 394.3514

Epoch 508: val_loss did not improve from 394.30560
196/196 - 12s - loss: 384.1425 - MinusLogProbMetric: 384.1425 - val_loss: 394.3514 - val_MinusLogProbMetric: 394.3514 - lr: 6.9444e-06 - 12s/epoch - 59ms/step
Epoch 509/1000
2023-10-02 18:33:39.762 
Epoch 509/1000 
	 loss: 384.1412, MinusLogProbMetric: 384.1412, val_loss: 394.4498, val_MinusLogProbMetric: 394.4498

Epoch 509: val_loss did not improve from 394.30560
196/196 - 11s - loss: 384.1412 - MinusLogProbMetric: 384.1412 - val_loss: 394.4498 - val_MinusLogProbMetric: 394.4498 - lr: 6.9444e-06 - 11s/epoch - 59ms/step
Epoch 510/1000
2023-10-02 18:33:51.231 
Epoch 510/1000 
	 loss: 384.0348, MinusLogProbMetric: 384.0348, val_loss: 394.2587, val_MinusLogProbMetric: 394.2587

Epoch 510: val_loss improved from 394.30560 to 394.25867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 384.0348 - MinusLogProbMetric: 384.0348 - val_loss: 394.2587 - val_MinusLogProbMetric: 394.2587 - lr: 3.4722e-06 - 12s/epoch - 61ms/step
Epoch 511/1000
2023-10-02 18:34:03.038 
Epoch 511/1000 
	 loss: 384.0310, MinusLogProbMetric: 384.0310, val_loss: 394.2702, val_MinusLogProbMetric: 394.2702

Epoch 511: val_loss did not improve from 394.25867
196/196 - 11s - loss: 384.0310 - MinusLogProbMetric: 384.0310 - val_loss: 394.2702 - val_MinusLogProbMetric: 394.2702 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 512/1000
2023-10-02 18:34:14.567 
Epoch 512/1000 
	 loss: 384.0202, MinusLogProbMetric: 384.0202, val_loss: 394.2922, val_MinusLogProbMetric: 394.2922

Epoch 512: val_loss did not improve from 394.25867
196/196 - 12s - loss: 384.0202 - MinusLogProbMetric: 384.0202 - val_loss: 394.2922 - val_MinusLogProbMetric: 394.2922 - lr: 3.4722e-06 - 12s/epoch - 59ms/step
Epoch 513/1000
2023-10-02 18:34:26.087 
Epoch 513/1000 
	 loss: 384.0280, MinusLogProbMetric: 384.0280, val_loss: 394.2772, val_MinusLogProbMetric: 394.2772

Epoch 513: val_loss did not improve from 394.25867
196/196 - 12s - loss: 384.0280 - MinusLogProbMetric: 384.0280 - val_loss: 394.2772 - val_MinusLogProbMetric: 394.2772 - lr: 3.4722e-06 - 12s/epoch - 59ms/step
Epoch 514/1000
2023-10-02 18:34:37.414 
Epoch 514/1000 
	 loss: 384.0281, MinusLogProbMetric: 384.0281, val_loss: 394.2695, val_MinusLogProbMetric: 394.2695

Epoch 514: val_loss did not improve from 394.25867
196/196 - 11s - loss: 384.0281 - MinusLogProbMetric: 384.0281 - val_loss: 394.2695 - val_MinusLogProbMetric: 394.2695 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 515/1000
2023-10-02 18:34:48.779 
Epoch 515/1000 
	 loss: 384.0307, MinusLogProbMetric: 384.0307, val_loss: 394.3034, val_MinusLogProbMetric: 394.3034

Epoch 515: val_loss did not improve from 394.25867
196/196 - 11s - loss: 384.0307 - MinusLogProbMetric: 384.0307 - val_loss: 394.3034 - val_MinusLogProbMetric: 394.3034 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 516/1000
2023-10-02 18:35:00.252 
Epoch 516/1000 
	 loss: 384.0252, MinusLogProbMetric: 384.0252, val_loss: 394.2584, val_MinusLogProbMetric: 394.2584

Epoch 516: val_loss improved from 394.25867 to 394.25842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 384.0252 - MinusLogProbMetric: 384.0252 - val_loss: 394.2584 - val_MinusLogProbMetric: 394.2584 - lr: 3.4722e-06 - 12s/epoch - 61ms/step
Epoch 517/1000
2023-10-02 18:35:12.123 
Epoch 517/1000 
	 loss: 384.0211, MinusLogProbMetric: 384.0211, val_loss: 394.2811, val_MinusLogProbMetric: 394.2811

Epoch 517: val_loss did not improve from 394.25842
196/196 - 11s - loss: 384.0211 - MinusLogProbMetric: 384.0211 - val_loss: 394.2811 - val_MinusLogProbMetric: 394.2811 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 518/1000
2023-10-02 18:35:23.578 
Epoch 518/1000 
	 loss: 384.0242, MinusLogProbMetric: 384.0242, val_loss: 394.2917, val_MinusLogProbMetric: 394.2917

Epoch 518: val_loss did not improve from 394.25842
196/196 - 11s - loss: 384.0242 - MinusLogProbMetric: 384.0242 - val_loss: 394.2917 - val_MinusLogProbMetric: 394.2917 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 519/1000
2023-10-02 18:35:35.042 
Epoch 519/1000 
	 loss: 384.0222, MinusLogProbMetric: 384.0222, val_loss: 394.3083, val_MinusLogProbMetric: 394.3083

Epoch 519: val_loss did not improve from 394.25842
196/196 - 11s - loss: 384.0222 - MinusLogProbMetric: 384.0222 - val_loss: 394.3083 - val_MinusLogProbMetric: 394.3083 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 520/1000
2023-10-02 18:35:46.426 
Epoch 520/1000 
	 loss: 384.0155, MinusLogProbMetric: 384.0155, val_loss: 394.2572, val_MinusLogProbMetric: 394.2572

Epoch 520: val_loss improved from 394.25842 to 394.25717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 384.0155 - MinusLogProbMetric: 384.0155 - val_loss: 394.2572 - val_MinusLogProbMetric: 394.2572 - lr: 3.4722e-06 - 12s/epoch - 60ms/step
Epoch 521/1000
2023-10-02 18:35:58.390 
Epoch 521/1000 
	 loss: 384.0189, MinusLogProbMetric: 384.0189, val_loss: 394.2837, val_MinusLogProbMetric: 394.2837

Epoch 521: val_loss did not improve from 394.25717
196/196 - 12s - loss: 384.0189 - MinusLogProbMetric: 384.0189 - val_loss: 394.2837 - val_MinusLogProbMetric: 394.2837 - lr: 3.4722e-06 - 12s/epoch - 59ms/step
Epoch 522/1000
2023-10-02 18:36:09.692 
Epoch 522/1000 
	 loss: 384.0225, MinusLogProbMetric: 384.0225, val_loss: 394.2555, val_MinusLogProbMetric: 394.2555

Epoch 522: val_loss improved from 394.25717 to 394.25552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 384.0225 - MinusLogProbMetric: 384.0225 - val_loss: 394.2555 - val_MinusLogProbMetric: 394.2555 - lr: 3.4722e-06 - 12s/epoch - 60ms/step
Epoch 523/1000
2023-10-02 18:36:21.406 
Epoch 523/1000 
	 loss: 384.0204, MinusLogProbMetric: 384.0204, val_loss: 394.2610, val_MinusLogProbMetric: 394.2610

Epoch 523: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0204 - MinusLogProbMetric: 384.0204 - val_loss: 394.2610 - val_MinusLogProbMetric: 394.2610 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 524/1000
2023-10-02 18:36:32.867 
Epoch 524/1000 
	 loss: 384.0135, MinusLogProbMetric: 384.0135, val_loss: 394.2954, val_MinusLogProbMetric: 394.2954

Epoch 524: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0135 - MinusLogProbMetric: 384.0135 - val_loss: 394.2954 - val_MinusLogProbMetric: 394.2954 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 525/1000
2023-10-02 18:36:44.337 
Epoch 525/1000 
	 loss: 384.0092, MinusLogProbMetric: 384.0092, val_loss: 394.2828, val_MinusLogProbMetric: 394.2828

Epoch 525: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0092 - MinusLogProbMetric: 384.0092 - val_loss: 394.2828 - val_MinusLogProbMetric: 394.2828 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 526/1000
2023-10-02 18:36:55.702 
Epoch 526/1000 
	 loss: 384.0083, MinusLogProbMetric: 384.0083, val_loss: 394.2791, val_MinusLogProbMetric: 394.2791

Epoch 526: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0083 - MinusLogProbMetric: 384.0083 - val_loss: 394.2791 - val_MinusLogProbMetric: 394.2791 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 527/1000
2023-10-02 18:37:07.155 
Epoch 527/1000 
	 loss: 384.0090, MinusLogProbMetric: 384.0090, val_loss: 394.2838, val_MinusLogProbMetric: 394.2838

Epoch 527: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0090 - MinusLogProbMetric: 384.0090 - val_loss: 394.2838 - val_MinusLogProbMetric: 394.2838 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 528/1000
2023-10-02 18:37:18.490 
Epoch 528/1000 
	 loss: 384.0097, MinusLogProbMetric: 384.0097, val_loss: 394.2833, val_MinusLogProbMetric: 394.2833

Epoch 528: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0097 - MinusLogProbMetric: 384.0097 - val_loss: 394.2833 - val_MinusLogProbMetric: 394.2833 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 529/1000
2023-10-02 18:37:29.690 
Epoch 529/1000 
	 loss: 384.0115, MinusLogProbMetric: 384.0115, val_loss: 394.2906, val_MinusLogProbMetric: 394.2906

Epoch 529: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0115 - MinusLogProbMetric: 384.0115 - val_loss: 394.2906 - val_MinusLogProbMetric: 394.2906 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 530/1000
2023-10-02 18:37:40.688 
Epoch 530/1000 
	 loss: 384.0073, MinusLogProbMetric: 384.0073, val_loss: 394.2928, val_MinusLogProbMetric: 394.2928

Epoch 530: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0073 - MinusLogProbMetric: 384.0073 - val_loss: 394.2928 - val_MinusLogProbMetric: 394.2928 - lr: 3.4722e-06 - 11s/epoch - 56ms/step
Epoch 531/1000
2023-10-02 18:37:51.655 
Epoch 531/1000 
	 loss: 384.0072, MinusLogProbMetric: 384.0072, val_loss: 394.2953, val_MinusLogProbMetric: 394.2953

Epoch 531: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0072 - MinusLogProbMetric: 384.0072 - val_loss: 394.2953 - val_MinusLogProbMetric: 394.2953 - lr: 3.4722e-06 - 11s/epoch - 56ms/step
Epoch 532/1000
2023-10-02 18:38:02.675 
Epoch 532/1000 
	 loss: 384.0065, MinusLogProbMetric: 384.0065, val_loss: 394.3265, val_MinusLogProbMetric: 394.3265

Epoch 532: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0065 - MinusLogProbMetric: 384.0065 - val_loss: 394.3265 - val_MinusLogProbMetric: 394.3265 - lr: 3.4722e-06 - 11s/epoch - 56ms/step
Epoch 533/1000
2023-10-02 18:38:13.698 
Epoch 533/1000 
	 loss: 384.0098, MinusLogProbMetric: 384.0098, val_loss: 394.2900, val_MinusLogProbMetric: 394.2900

Epoch 533: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0098 - MinusLogProbMetric: 384.0098 - val_loss: 394.2900 - val_MinusLogProbMetric: 394.2900 - lr: 3.4722e-06 - 11s/epoch - 56ms/step
Epoch 534/1000
2023-10-02 18:38:24.920 
Epoch 534/1000 
	 loss: 384.0042, MinusLogProbMetric: 384.0042, val_loss: 394.2722, val_MinusLogProbMetric: 394.2722

Epoch 534: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0042 - MinusLogProbMetric: 384.0042 - val_loss: 394.2722 - val_MinusLogProbMetric: 394.2722 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 535/1000
2023-10-02 18:38:36.126 
Epoch 535/1000 
	 loss: 384.0022, MinusLogProbMetric: 384.0022, val_loss: 394.2916, val_MinusLogProbMetric: 394.2916

Epoch 535: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0022 - MinusLogProbMetric: 384.0022 - val_loss: 394.2916 - val_MinusLogProbMetric: 394.2916 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 536/1000
2023-10-02 18:38:47.295 
Epoch 536/1000 
	 loss: 384.0026, MinusLogProbMetric: 384.0026, val_loss: 394.3298, val_MinusLogProbMetric: 394.3298

Epoch 536: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0026 - MinusLogProbMetric: 384.0026 - val_loss: 394.3298 - val_MinusLogProbMetric: 394.3298 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 537/1000
2023-10-02 18:38:58.519 
Epoch 537/1000 
	 loss: 384.0023, MinusLogProbMetric: 384.0023, val_loss: 394.3069, val_MinusLogProbMetric: 394.3069

Epoch 537: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0023 - MinusLogProbMetric: 384.0023 - val_loss: 394.3069 - val_MinusLogProbMetric: 394.3069 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 538/1000
2023-10-02 18:39:09.767 
Epoch 538/1000 
	 loss: 383.9978, MinusLogProbMetric: 383.9978, val_loss: 394.3141, val_MinusLogProbMetric: 394.3141

Epoch 538: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9978 - MinusLogProbMetric: 383.9978 - val_loss: 394.3141 - val_MinusLogProbMetric: 394.3141 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 539/1000
2023-10-02 18:39:21.092 
Epoch 539/1000 
	 loss: 384.0031, MinusLogProbMetric: 384.0031, val_loss: 394.3052, val_MinusLogProbMetric: 394.3052

Epoch 539: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0031 - MinusLogProbMetric: 384.0031 - val_loss: 394.3052 - val_MinusLogProbMetric: 394.3052 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 540/1000
2023-10-02 18:39:32.629 
Epoch 540/1000 
	 loss: 383.9971, MinusLogProbMetric: 383.9971, val_loss: 394.2993, val_MinusLogProbMetric: 394.2993

Epoch 540: val_loss did not improve from 394.25552
196/196 - 12s - loss: 383.9971 - MinusLogProbMetric: 383.9971 - val_loss: 394.2993 - val_MinusLogProbMetric: 394.2993 - lr: 3.4722e-06 - 12s/epoch - 59ms/step
Epoch 541/1000
2023-10-02 18:39:44.166 
Epoch 541/1000 
	 loss: 383.9980, MinusLogProbMetric: 383.9980, val_loss: 394.2869, val_MinusLogProbMetric: 394.2869

Epoch 541: val_loss did not improve from 394.25552
196/196 - 12s - loss: 383.9980 - MinusLogProbMetric: 383.9980 - val_loss: 394.2869 - val_MinusLogProbMetric: 394.2869 - lr: 3.4722e-06 - 12s/epoch - 59ms/step
Epoch 542/1000
2023-10-02 18:39:55.764 
Epoch 542/1000 
	 loss: 383.9954, MinusLogProbMetric: 383.9954, val_loss: 394.3486, val_MinusLogProbMetric: 394.3486

Epoch 542: val_loss did not improve from 394.25552
196/196 - 12s - loss: 383.9954 - MinusLogProbMetric: 383.9954 - val_loss: 394.3486 - val_MinusLogProbMetric: 394.3486 - lr: 3.4722e-06 - 12s/epoch - 59ms/step
Epoch 543/1000
2023-10-02 18:40:07.162 
Epoch 543/1000 
	 loss: 383.9997, MinusLogProbMetric: 383.9997, val_loss: 394.2894, val_MinusLogProbMetric: 394.2894

Epoch 543: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9997 - MinusLogProbMetric: 383.9997 - val_loss: 394.2894 - val_MinusLogProbMetric: 394.2894 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 544/1000
2023-10-02 18:40:18.404 
Epoch 544/1000 
	 loss: 383.9996, MinusLogProbMetric: 383.9996, val_loss: 394.2913, val_MinusLogProbMetric: 394.2913

Epoch 544: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9996 - MinusLogProbMetric: 383.9996 - val_loss: 394.2913 - val_MinusLogProbMetric: 394.2913 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 545/1000
2023-10-02 18:40:29.617 
Epoch 545/1000 
	 loss: 383.9958, MinusLogProbMetric: 383.9958, val_loss: 394.2880, val_MinusLogProbMetric: 394.2880

Epoch 545: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9958 - MinusLogProbMetric: 383.9958 - val_loss: 394.2880 - val_MinusLogProbMetric: 394.2880 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 546/1000
2023-10-02 18:40:40.808 
Epoch 546/1000 
	 loss: 383.9983, MinusLogProbMetric: 383.9983, val_loss: 394.3055, val_MinusLogProbMetric: 394.3055

Epoch 546: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9983 - MinusLogProbMetric: 383.9983 - val_loss: 394.3055 - val_MinusLogProbMetric: 394.3055 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 547/1000
2023-10-02 18:40:51.996 
Epoch 547/1000 
	 loss: 383.9970, MinusLogProbMetric: 383.9970, val_loss: 394.2891, val_MinusLogProbMetric: 394.2891

Epoch 547: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9970 - MinusLogProbMetric: 383.9970 - val_loss: 394.2891 - val_MinusLogProbMetric: 394.2891 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 548/1000
2023-10-02 18:41:03.140 
Epoch 548/1000 
	 loss: 383.9938, MinusLogProbMetric: 383.9938, val_loss: 394.4011, val_MinusLogProbMetric: 394.4011

Epoch 548: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9938 - MinusLogProbMetric: 383.9938 - val_loss: 394.4011 - val_MinusLogProbMetric: 394.4011 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 549/1000
2023-10-02 18:41:14.199 
Epoch 549/1000 
	 loss: 384.0040, MinusLogProbMetric: 384.0040, val_loss: 394.2867, val_MinusLogProbMetric: 394.2867

Epoch 549: val_loss did not improve from 394.25552
196/196 - 11s - loss: 384.0040 - MinusLogProbMetric: 384.0040 - val_loss: 394.2867 - val_MinusLogProbMetric: 394.2867 - lr: 3.4722e-06 - 11s/epoch - 56ms/step
Epoch 550/1000
2023-10-02 18:41:25.264 
Epoch 550/1000 
	 loss: 383.9929, MinusLogProbMetric: 383.9929, val_loss: 394.3438, val_MinusLogProbMetric: 394.3438

Epoch 550: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9929 - MinusLogProbMetric: 383.9929 - val_loss: 394.3438 - val_MinusLogProbMetric: 394.3438 - lr: 3.4722e-06 - 11s/epoch - 56ms/step
Epoch 551/1000
2023-10-02 18:41:36.452 
Epoch 551/1000 
	 loss: 383.9968, MinusLogProbMetric: 383.9968, val_loss: 394.3038, val_MinusLogProbMetric: 394.3038

Epoch 551: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9968 - MinusLogProbMetric: 383.9968 - val_loss: 394.3038 - val_MinusLogProbMetric: 394.3038 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 552/1000
2023-10-02 18:41:47.684 
Epoch 552/1000 
	 loss: 383.9955, MinusLogProbMetric: 383.9955, val_loss: 394.2764, val_MinusLogProbMetric: 394.2764

Epoch 552: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9955 - MinusLogProbMetric: 383.9955 - val_loss: 394.2764 - val_MinusLogProbMetric: 394.2764 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 553/1000
2023-10-02 18:41:59.133 
Epoch 553/1000 
	 loss: 383.9888, MinusLogProbMetric: 383.9888, val_loss: 394.3630, val_MinusLogProbMetric: 394.3630

Epoch 553: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9888 - MinusLogProbMetric: 383.9888 - val_loss: 394.3630 - val_MinusLogProbMetric: 394.3630 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 554/1000
2023-10-02 18:42:10.250 
Epoch 554/1000 
	 loss: 383.9916, MinusLogProbMetric: 383.9916, val_loss: 394.3541, val_MinusLogProbMetric: 394.3541

Epoch 554: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9916 - MinusLogProbMetric: 383.9916 - val_loss: 394.3541 - val_MinusLogProbMetric: 394.3541 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 555/1000
2023-10-02 18:42:21.537 
Epoch 555/1000 
	 loss: 383.9918, MinusLogProbMetric: 383.9918, val_loss: 394.2850, val_MinusLogProbMetric: 394.2850

Epoch 555: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9918 - MinusLogProbMetric: 383.9918 - val_loss: 394.2850 - val_MinusLogProbMetric: 394.2850 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 556/1000
2023-10-02 18:42:32.790 
Epoch 556/1000 
	 loss: 383.9862, MinusLogProbMetric: 383.9862, val_loss: 394.3064, val_MinusLogProbMetric: 394.3064

Epoch 556: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9862 - MinusLogProbMetric: 383.9862 - val_loss: 394.3064 - val_MinusLogProbMetric: 394.3064 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 557/1000
2023-10-02 18:42:44.099 
Epoch 557/1000 
	 loss: 383.9863, MinusLogProbMetric: 383.9863, val_loss: 394.2760, val_MinusLogProbMetric: 394.2760

Epoch 557: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9863 - MinusLogProbMetric: 383.9863 - val_loss: 394.2760 - val_MinusLogProbMetric: 394.2760 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 558/1000
2023-10-02 18:42:55.471 
Epoch 558/1000 
	 loss: 383.9910, MinusLogProbMetric: 383.9910, val_loss: 394.2792, val_MinusLogProbMetric: 394.2792

Epoch 558: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9910 - MinusLogProbMetric: 383.9910 - val_loss: 394.2792 - val_MinusLogProbMetric: 394.2792 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 559/1000
2023-10-02 18:43:06.670 
Epoch 559/1000 
	 loss: 383.9814, MinusLogProbMetric: 383.9814, val_loss: 394.3051, val_MinusLogProbMetric: 394.3051

Epoch 559: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9814 - MinusLogProbMetric: 383.9814 - val_loss: 394.3051 - val_MinusLogProbMetric: 394.3051 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 560/1000
2023-10-02 18:43:17.863 
Epoch 560/1000 
	 loss: 383.9896, MinusLogProbMetric: 383.9896, val_loss: 394.2858, val_MinusLogProbMetric: 394.2858

Epoch 560: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9896 - MinusLogProbMetric: 383.9896 - val_loss: 394.2858 - val_MinusLogProbMetric: 394.2858 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 561/1000
2023-10-02 18:43:29.098 
Epoch 561/1000 
	 loss: 383.9888, MinusLogProbMetric: 383.9888, val_loss: 394.2936, val_MinusLogProbMetric: 394.2936

Epoch 561: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9888 - MinusLogProbMetric: 383.9888 - val_loss: 394.2936 - val_MinusLogProbMetric: 394.2936 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 562/1000
2023-10-02 18:43:40.386 
Epoch 562/1000 
	 loss: 383.9882, MinusLogProbMetric: 383.9882, val_loss: 394.3077, val_MinusLogProbMetric: 394.3077

Epoch 562: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9882 - MinusLogProbMetric: 383.9882 - val_loss: 394.3077 - val_MinusLogProbMetric: 394.3077 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 563/1000
2023-10-02 18:43:51.823 
Epoch 563/1000 
	 loss: 383.9923, MinusLogProbMetric: 383.9923, val_loss: 394.3103, val_MinusLogProbMetric: 394.3103

Epoch 563: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9923 - MinusLogProbMetric: 383.9923 - val_loss: 394.3103 - val_MinusLogProbMetric: 394.3103 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 564/1000
2023-10-02 18:44:03.037 
Epoch 564/1000 
	 loss: 383.9850, MinusLogProbMetric: 383.9850, val_loss: 394.3197, val_MinusLogProbMetric: 394.3197

Epoch 564: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9850 - MinusLogProbMetric: 383.9850 - val_loss: 394.3197 - val_MinusLogProbMetric: 394.3197 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 565/1000
2023-10-02 18:44:14.424 
Epoch 565/1000 
	 loss: 383.9808, MinusLogProbMetric: 383.9808, val_loss: 394.2871, val_MinusLogProbMetric: 394.2871

Epoch 565: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9808 - MinusLogProbMetric: 383.9808 - val_loss: 394.2871 - val_MinusLogProbMetric: 394.2871 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 566/1000
2023-10-02 18:44:25.690 
Epoch 566/1000 
	 loss: 383.9854, MinusLogProbMetric: 383.9854, val_loss: 394.2817, val_MinusLogProbMetric: 394.2817

Epoch 566: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9854 - MinusLogProbMetric: 383.9854 - val_loss: 394.2817 - val_MinusLogProbMetric: 394.2817 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 567/1000
2023-10-02 18:44:36.982 
Epoch 567/1000 
	 loss: 383.9856, MinusLogProbMetric: 383.9856, val_loss: 394.2997, val_MinusLogProbMetric: 394.2997

Epoch 567: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9856 - MinusLogProbMetric: 383.9856 - val_loss: 394.2997 - val_MinusLogProbMetric: 394.2997 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 568/1000
2023-10-02 18:44:48.206 
Epoch 568/1000 
	 loss: 383.9880, MinusLogProbMetric: 383.9880, val_loss: 394.3170, val_MinusLogProbMetric: 394.3170

Epoch 568: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9880 - MinusLogProbMetric: 383.9880 - val_loss: 394.3170 - val_MinusLogProbMetric: 394.3170 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 569/1000
2023-10-02 18:44:59.443 
Epoch 569/1000 
	 loss: 383.9848, MinusLogProbMetric: 383.9848, val_loss: 394.3084, val_MinusLogProbMetric: 394.3084

Epoch 569: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9848 - MinusLogProbMetric: 383.9848 - val_loss: 394.3084 - val_MinusLogProbMetric: 394.3084 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 570/1000
2023-10-02 18:45:10.716 
Epoch 570/1000 
	 loss: 383.9873, MinusLogProbMetric: 383.9873, val_loss: 394.2732, val_MinusLogProbMetric: 394.2732

Epoch 570: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9873 - MinusLogProbMetric: 383.9873 - val_loss: 394.2732 - val_MinusLogProbMetric: 394.2732 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 571/1000
2023-10-02 18:45:21.852 
Epoch 571/1000 
	 loss: 383.9854, MinusLogProbMetric: 383.9854, val_loss: 394.3069, val_MinusLogProbMetric: 394.3069

Epoch 571: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9854 - MinusLogProbMetric: 383.9854 - val_loss: 394.3069 - val_MinusLogProbMetric: 394.3069 - lr: 3.4722e-06 - 11s/epoch - 57ms/step
Epoch 572/1000
2023-10-02 18:45:33.238 
Epoch 572/1000 
	 loss: 383.9819, MinusLogProbMetric: 383.9819, val_loss: 394.2810, val_MinusLogProbMetric: 394.2810

Epoch 572: val_loss did not improve from 394.25552
196/196 - 11s - loss: 383.9819 - MinusLogProbMetric: 383.9819 - val_loss: 394.2810 - val_MinusLogProbMetric: 394.2810 - lr: 3.4722e-06 - 11s/epoch - 58ms/step
Epoch 573/1000
2023-10-02 18:45:44.954 
Epoch 573/1000 
	 loss: 383.9245, MinusLogProbMetric: 383.9245, val_loss: 394.2566, val_MinusLogProbMetric: 394.2566

Epoch 573: val_loss did not improve from 394.25552
196/196 - 12s - loss: 383.9245 - MinusLogProbMetric: 383.9245 - val_loss: 394.2566 - val_MinusLogProbMetric: 394.2566 - lr: 1.7361e-06 - 12s/epoch - 60ms/step
Epoch 574/1000
2023-10-02 18:45:56.610 
Epoch 574/1000 
	 loss: 383.9251, MinusLogProbMetric: 383.9251, val_loss: 394.2539, val_MinusLogProbMetric: 394.2539

Epoch 574: val_loss improved from 394.25552 to 394.25394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 383.9251 - MinusLogProbMetric: 383.9251 - val_loss: 394.2539 - val_MinusLogProbMetric: 394.2539 - lr: 1.7361e-06 - 12s/epoch - 62ms/step
Epoch 575/1000
2023-10-02 18:46:08.599 
Epoch 575/1000 
	 loss: 383.9212, MinusLogProbMetric: 383.9212, val_loss: 394.2705, val_MinusLogProbMetric: 394.2705

Epoch 575: val_loss did not improve from 394.25394
196/196 - 11s - loss: 383.9212 - MinusLogProbMetric: 383.9212 - val_loss: 394.2705 - val_MinusLogProbMetric: 394.2705 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 576/1000
2023-10-02 18:46:20.095 
Epoch 576/1000 
	 loss: 383.9237, MinusLogProbMetric: 383.9237, val_loss: 394.2921, val_MinusLogProbMetric: 394.2921

Epoch 576: val_loss did not improve from 394.25394
196/196 - 11s - loss: 383.9237 - MinusLogProbMetric: 383.9237 - val_loss: 394.2921 - val_MinusLogProbMetric: 394.2921 - lr: 1.7361e-06 - 11s/epoch - 59ms/step
Epoch 577/1000
2023-10-02 18:46:31.310 
Epoch 577/1000 
	 loss: 383.9251, MinusLogProbMetric: 383.9251, val_loss: 394.2709, val_MinusLogProbMetric: 394.2709

Epoch 577: val_loss did not improve from 394.25394
196/196 - 11s - loss: 383.9251 - MinusLogProbMetric: 383.9251 - val_loss: 394.2709 - val_MinusLogProbMetric: 394.2709 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 578/1000
2023-10-02 18:46:42.504 
Epoch 578/1000 
	 loss: 383.9196, MinusLogProbMetric: 383.9196, val_loss: 394.2703, val_MinusLogProbMetric: 394.2703

Epoch 578: val_loss did not improve from 394.25394
196/196 - 11s - loss: 383.9196 - MinusLogProbMetric: 383.9196 - val_loss: 394.2703 - val_MinusLogProbMetric: 394.2703 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 579/1000
2023-10-02 18:46:54.051 
Epoch 579/1000 
	 loss: 383.9193, MinusLogProbMetric: 383.9193, val_loss: 394.2513, val_MinusLogProbMetric: 394.2513

Epoch 579: val_loss improved from 394.25394 to 394.25131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 383.9193 - MinusLogProbMetric: 383.9193 - val_loss: 394.2513 - val_MinusLogProbMetric: 394.2513 - lr: 1.7361e-06 - 12s/epoch - 61ms/step
Epoch 580/1000
2023-10-02 18:47:05.659 
Epoch 580/1000 
	 loss: 383.9225, MinusLogProbMetric: 383.9225, val_loss: 394.2417, val_MinusLogProbMetric: 394.2417

Epoch 580: val_loss improved from 394.25131 to 394.24170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 383.9225 - MinusLogProbMetric: 383.9225 - val_loss: 394.2417 - val_MinusLogProbMetric: 394.2417 - lr: 1.7361e-06 - 12s/epoch - 59ms/step
Epoch 581/1000
2023-10-02 18:47:17.320 
Epoch 581/1000 
	 loss: 383.9225, MinusLogProbMetric: 383.9225, val_loss: 394.2548, val_MinusLogProbMetric: 394.2548

Epoch 581: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9225 - MinusLogProbMetric: 383.9225 - val_loss: 394.2548 - val_MinusLogProbMetric: 394.2548 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 582/1000
2023-10-02 18:47:28.430 
Epoch 582/1000 
	 loss: 383.9277, MinusLogProbMetric: 383.9277, val_loss: 394.2727, val_MinusLogProbMetric: 394.2727

Epoch 582: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9277 - MinusLogProbMetric: 383.9277 - val_loss: 394.2727 - val_MinusLogProbMetric: 394.2727 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 583/1000
2023-10-02 18:47:39.615 
Epoch 583/1000 
	 loss: 383.9220, MinusLogProbMetric: 383.9220, val_loss: 394.2593, val_MinusLogProbMetric: 394.2593

Epoch 583: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9220 - MinusLogProbMetric: 383.9220 - val_loss: 394.2593 - val_MinusLogProbMetric: 394.2593 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 584/1000
2023-10-02 18:47:50.968 
Epoch 584/1000 
	 loss: 383.9178, MinusLogProbMetric: 383.9178, val_loss: 394.2830, val_MinusLogProbMetric: 394.2830

Epoch 584: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9178 - MinusLogProbMetric: 383.9178 - val_loss: 394.2830 - val_MinusLogProbMetric: 394.2830 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 585/1000
2023-10-02 18:48:02.224 
Epoch 585/1000 
	 loss: 383.9177, MinusLogProbMetric: 383.9177, val_loss: 394.3037, val_MinusLogProbMetric: 394.3037

Epoch 585: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9177 - MinusLogProbMetric: 383.9177 - val_loss: 394.3037 - val_MinusLogProbMetric: 394.3037 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 586/1000
2023-10-02 18:48:13.728 
Epoch 586/1000 
	 loss: 383.9171, MinusLogProbMetric: 383.9171, val_loss: 394.2726, val_MinusLogProbMetric: 394.2726

Epoch 586: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9171 - MinusLogProbMetric: 383.9171 - val_loss: 394.2726 - val_MinusLogProbMetric: 394.2726 - lr: 1.7361e-06 - 11s/epoch - 59ms/step
Epoch 587/1000
2023-10-02 18:48:25.294 
Epoch 587/1000 
	 loss: 383.9190, MinusLogProbMetric: 383.9190, val_loss: 394.2667, val_MinusLogProbMetric: 394.2667

Epoch 587: val_loss did not improve from 394.24170
196/196 - 12s - loss: 383.9190 - MinusLogProbMetric: 383.9190 - val_loss: 394.2667 - val_MinusLogProbMetric: 394.2667 - lr: 1.7361e-06 - 12s/epoch - 59ms/step
Epoch 588/1000
2023-10-02 18:48:36.469 
Epoch 588/1000 
	 loss: 383.9152, MinusLogProbMetric: 383.9152, val_loss: 394.2987, val_MinusLogProbMetric: 394.2987

Epoch 588: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9152 - MinusLogProbMetric: 383.9152 - val_loss: 394.2987 - val_MinusLogProbMetric: 394.2987 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 589/1000
2023-10-02 18:48:47.965 
Epoch 589/1000 
	 loss: 383.9194, MinusLogProbMetric: 383.9194, val_loss: 394.2570, val_MinusLogProbMetric: 394.2570

Epoch 589: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9194 - MinusLogProbMetric: 383.9194 - val_loss: 394.2570 - val_MinusLogProbMetric: 394.2570 - lr: 1.7361e-06 - 11s/epoch - 59ms/step
Epoch 590/1000
2023-10-02 18:48:59.470 
Epoch 590/1000 
	 loss: 383.9178, MinusLogProbMetric: 383.9178, val_loss: 394.2755, val_MinusLogProbMetric: 394.2755

Epoch 590: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9178 - MinusLogProbMetric: 383.9178 - val_loss: 394.2755 - val_MinusLogProbMetric: 394.2755 - lr: 1.7361e-06 - 11s/epoch - 59ms/step
Epoch 591/1000
2023-10-02 18:49:10.783 
Epoch 591/1000 
	 loss: 383.9174, MinusLogProbMetric: 383.9174, val_loss: 394.2504, val_MinusLogProbMetric: 394.2504

Epoch 591: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9174 - MinusLogProbMetric: 383.9174 - val_loss: 394.2504 - val_MinusLogProbMetric: 394.2504 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 592/1000
2023-10-02 18:49:22.105 
Epoch 592/1000 
	 loss: 383.9136, MinusLogProbMetric: 383.9136, val_loss: 394.2780, val_MinusLogProbMetric: 394.2780

Epoch 592: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9136 - MinusLogProbMetric: 383.9136 - val_loss: 394.2780 - val_MinusLogProbMetric: 394.2780 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 593/1000
2023-10-02 18:49:32.474 
Epoch 593/1000 
	 loss: 383.9135, MinusLogProbMetric: 383.9135, val_loss: 394.2610, val_MinusLogProbMetric: 394.2610

Epoch 593: val_loss did not improve from 394.24170
196/196 - 10s - loss: 383.9135 - MinusLogProbMetric: 383.9135 - val_loss: 394.2610 - val_MinusLogProbMetric: 394.2610 - lr: 1.7361e-06 - 10s/epoch - 53ms/step
Epoch 594/1000
2023-10-02 18:49:42.606 
Epoch 594/1000 
	 loss: 383.9099, MinusLogProbMetric: 383.9099, val_loss: 394.2514, val_MinusLogProbMetric: 394.2514

Epoch 594: val_loss did not improve from 394.24170
196/196 - 10s - loss: 383.9099 - MinusLogProbMetric: 383.9099 - val_loss: 394.2514 - val_MinusLogProbMetric: 394.2514 - lr: 1.7361e-06 - 10s/epoch - 52ms/step
Epoch 595/1000
2023-10-02 18:49:53.757 
Epoch 595/1000 
	 loss: 383.9103, MinusLogProbMetric: 383.9103, val_loss: 394.2542, val_MinusLogProbMetric: 394.2542

Epoch 595: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9103 - MinusLogProbMetric: 383.9103 - val_loss: 394.2542 - val_MinusLogProbMetric: 394.2542 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 596/1000
2023-10-02 18:50:04.777 
Epoch 596/1000 
	 loss: 383.9096, MinusLogProbMetric: 383.9096, val_loss: 394.2621, val_MinusLogProbMetric: 394.2621

Epoch 596: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9096 - MinusLogProbMetric: 383.9096 - val_loss: 394.2621 - val_MinusLogProbMetric: 394.2621 - lr: 1.7361e-06 - 11s/epoch - 56ms/step
Epoch 597/1000
2023-10-02 18:50:15.890 
Epoch 597/1000 
	 loss: 383.9132, MinusLogProbMetric: 383.9132, val_loss: 394.2583, val_MinusLogProbMetric: 394.2583

Epoch 597: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9132 - MinusLogProbMetric: 383.9132 - val_loss: 394.2583 - val_MinusLogProbMetric: 394.2583 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 598/1000
2023-10-02 18:50:27.126 
Epoch 598/1000 
	 loss: 383.9107, MinusLogProbMetric: 383.9107, val_loss: 394.2559, val_MinusLogProbMetric: 394.2559

Epoch 598: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9107 - MinusLogProbMetric: 383.9107 - val_loss: 394.2559 - val_MinusLogProbMetric: 394.2559 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 599/1000
2023-10-02 18:50:38.309 
Epoch 599/1000 
	 loss: 383.9104, MinusLogProbMetric: 383.9104, val_loss: 394.2597, val_MinusLogProbMetric: 394.2597

Epoch 599: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9104 - MinusLogProbMetric: 383.9104 - val_loss: 394.2597 - val_MinusLogProbMetric: 394.2597 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 600/1000
2023-10-02 18:50:49.397 
Epoch 600/1000 
	 loss: 383.9098, MinusLogProbMetric: 383.9098, val_loss: 394.2936, val_MinusLogProbMetric: 394.2936

Epoch 600: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9098 - MinusLogProbMetric: 383.9098 - val_loss: 394.2936 - val_MinusLogProbMetric: 394.2936 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 601/1000
2023-10-02 18:51:00.827 
Epoch 601/1000 
	 loss: 383.9081, MinusLogProbMetric: 383.9081, val_loss: 394.2663, val_MinusLogProbMetric: 394.2663

Epoch 601: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9081 - MinusLogProbMetric: 383.9081 - val_loss: 394.2663 - val_MinusLogProbMetric: 394.2663 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 602/1000
2023-10-02 18:51:12.485 
Epoch 602/1000 
	 loss: 383.9129, MinusLogProbMetric: 383.9129, val_loss: 394.3083, val_MinusLogProbMetric: 394.3083

Epoch 602: val_loss did not improve from 394.24170
196/196 - 12s - loss: 383.9129 - MinusLogProbMetric: 383.9129 - val_loss: 394.3083 - val_MinusLogProbMetric: 394.3083 - lr: 1.7361e-06 - 12s/epoch - 59ms/step
Epoch 603/1000
2023-10-02 18:51:24.204 
Epoch 603/1000 
	 loss: 383.9063, MinusLogProbMetric: 383.9063, val_loss: 394.2727, val_MinusLogProbMetric: 394.2727

Epoch 603: val_loss did not improve from 394.24170
196/196 - 12s - loss: 383.9063 - MinusLogProbMetric: 383.9063 - val_loss: 394.2727 - val_MinusLogProbMetric: 394.2727 - lr: 1.7361e-06 - 12s/epoch - 60ms/step
Epoch 604/1000
2023-10-02 18:51:35.759 
Epoch 604/1000 
	 loss: 383.9120, MinusLogProbMetric: 383.9120, val_loss: 394.2776, val_MinusLogProbMetric: 394.2776

Epoch 604: val_loss did not improve from 394.24170
196/196 - 12s - loss: 383.9120 - MinusLogProbMetric: 383.9120 - val_loss: 394.2776 - val_MinusLogProbMetric: 394.2776 - lr: 1.7361e-06 - 12s/epoch - 59ms/step
Epoch 605/1000
2023-10-02 18:51:47.036 
Epoch 605/1000 
	 loss: 383.9086, MinusLogProbMetric: 383.9086, val_loss: 394.2619, val_MinusLogProbMetric: 394.2619

Epoch 605: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9086 - MinusLogProbMetric: 383.9086 - val_loss: 394.2619 - val_MinusLogProbMetric: 394.2619 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 606/1000
2023-10-02 18:51:58.223 
Epoch 606/1000 
	 loss: 383.9091, MinusLogProbMetric: 383.9091, val_loss: 394.2685, val_MinusLogProbMetric: 394.2685

Epoch 606: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9091 - MinusLogProbMetric: 383.9091 - val_loss: 394.2685 - val_MinusLogProbMetric: 394.2685 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 607/1000
2023-10-02 18:52:09.412 
Epoch 607/1000 
	 loss: 383.9075, MinusLogProbMetric: 383.9075, val_loss: 394.2778, val_MinusLogProbMetric: 394.2778

Epoch 607: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9075 - MinusLogProbMetric: 383.9075 - val_loss: 394.2778 - val_MinusLogProbMetric: 394.2778 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 608/1000
2023-10-02 18:52:20.809 
Epoch 608/1000 
	 loss: 383.9069, MinusLogProbMetric: 383.9069, val_loss: 394.2765, val_MinusLogProbMetric: 394.2765

Epoch 608: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9069 - MinusLogProbMetric: 383.9069 - val_loss: 394.2765 - val_MinusLogProbMetric: 394.2765 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 609/1000
2023-10-02 18:52:32.198 
Epoch 609/1000 
	 loss: 383.9090, MinusLogProbMetric: 383.9090, val_loss: 394.2613, val_MinusLogProbMetric: 394.2613

Epoch 609: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9090 - MinusLogProbMetric: 383.9090 - val_loss: 394.2613 - val_MinusLogProbMetric: 394.2613 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 610/1000
2023-10-02 18:52:43.567 
Epoch 610/1000 
	 loss: 383.9041, MinusLogProbMetric: 383.9041, val_loss: 394.2567, val_MinusLogProbMetric: 394.2567

Epoch 610: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9041 - MinusLogProbMetric: 383.9041 - val_loss: 394.2567 - val_MinusLogProbMetric: 394.2567 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 611/1000
2023-10-02 18:52:54.860 
Epoch 611/1000 
	 loss: 383.9068, MinusLogProbMetric: 383.9068, val_loss: 394.2866, val_MinusLogProbMetric: 394.2866

Epoch 611: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9068 - MinusLogProbMetric: 383.9068 - val_loss: 394.2866 - val_MinusLogProbMetric: 394.2866 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 612/1000
2023-10-02 18:53:06.155 
Epoch 612/1000 
	 loss: 383.9073, MinusLogProbMetric: 383.9073, val_loss: 394.2953, val_MinusLogProbMetric: 394.2953

Epoch 612: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9073 - MinusLogProbMetric: 383.9073 - val_loss: 394.2953 - val_MinusLogProbMetric: 394.2953 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 613/1000
2023-10-02 18:53:17.338 
Epoch 613/1000 
	 loss: 383.9051, MinusLogProbMetric: 383.9051, val_loss: 394.2787, val_MinusLogProbMetric: 394.2787

Epoch 613: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9051 - MinusLogProbMetric: 383.9051 - val_loss: 394.2787 - val_MinusLogProbMetric: 394.2787 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 614/1000
2023-10-02 18:53:28.587 
Epoch 614/1000 
	 loss: 383.9072, MinusLogProbMetric: 383.9072, val_loss: 394.2689, val_MinusLogProbMetric: 394.2689

Epoch 614: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9072 - MinusLogProbMetric: 383.9072 - val_loss: 394.2689 - val_MinusLogProbMetric: 394.2689 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 615/1000
2023-10-02 18:53:39.919 
Epoch 615/1000 
	 loss: 383.9034, MinusLogProbMetric: 383.9034, val_loss: 394.2900, val_MinusLogProbMetric: 394.2900

Epoch 615: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9034 - MinusLogProbMetric: 383.9034 - val_loss: 394.2900 - val_MinusLogProbMetric: 394.2900 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 616/1000
2023-10-02 18:53:51.049 
Epoch 616/1000 
	 loss: 383.9044, MinusLogProbMetric: 383.9044, val_loss: 394.2763, val_MinusLogProbMetric: 394.2763

Epoch 616: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9044 - MinusLogProbMetric: 383.9044 - val_loss: 394.2763 - val_MinusLogProbMetric: 394.2763 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 617/1000
2023-10-02 18:54:02.335 
Epoch 617/1000 
	 loss: 383.9018, MinusLogProbMetric: 383.9018, val_loss: 394.2700, val_MinusLogProbMetric: 394.2700

Epoch 617: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9018 - MinusLogProbMetric: 383.9018 - val_loss: 394.2700 - val_MinusLogProbMetric: 394.2700 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 618/1000
2023-10-02 18:54:13.438 
Epoch 618/1000 
	 loss: 383.9004, MinusLogProbMetric: 383.9004, val_loss: 394.2790, val_MinusLogProbMetric: 394.2790

Epoch 618: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9004 - MinusLogProbMetric: 383.9004 - val_loss: 394.2790 - val_MinusLogProbMetric: 394.2790 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 619/1000
2023-10-02 18:54:24.789 
Epoch 619/1000 
	 loss: 383.9000, MinusLogProbMetric: 383.9000, val_loss: 394.2942, val_MinusLogProbMetric: 394.2942

Epoch 619: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9000 - MinusLogProbMetric: 383.9000 - val_loss: 394.2942 - val_MinusLogProbMetric: 394.2942 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 620/1000
2023-10-02 18:54:36.170 
Epoch 620/1000 
	 loss: 383.9017, MinusLogProbMetric: 383.9017, val_loss: 394.2615, val_MinusLogProbMetric: 394.2615

Epoch 620: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9017 - MinusLogProbMetric: 383.9017 - val_loss: 394.2615 - val_MinusLogProbMetric: 394.2615 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 621/1000
2023-10-02 18:54:47.438 
Epoch 621/1000 
	 loss: 383.8999, MinusLogProbMetric: 383.8999, val_loss: 394.2699, val_MinusLogProbMetric: 394.2699

Epoch 621: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8999 - MinusLogProbMetric: 383.8999 - val_loss: 394.2699 - val_MinusLogProbMetric: 394.2699 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 622/1000
2023-10-02 18:54:58.667 
Epoch 622/1000 
	 loss: 383.8979, MinusLogProbMetric: 383.8979, val_loss: 394.2792, val_MinusLogProbMetric: 394.2792

Epoch 622: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8979 - MinusLogProbMetric: 383.8979 - val_loss: 394.2792 - val_MinusLogProbMetric: 394.2792 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 623/1000
2023-10-02 18:55:10.051 
Epoch 623/1000 
	 loss: 383.9011, MinusLogProbMetric: 383.9011, val_loss: 394.2894, val_MinusLogProbMetric: 394.2894

Epoch 623: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9011 - MinusLogProbMetric: 383.9011 - val_loss: 394.2894 - val_MinusLogProbMetric: 394.2894 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 624/1000
2023-10-02 18:55:21.474 
Epoch 624/1000 
	 loss: 383.8983, MinusLogProbMetric: 383.8983, val_loss: 394.2647, val_MinusLogProbMetric: 394.2647

Epoch 624: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8983 - MinusLogProbMetric: 383.8983 - val_loss: 394.2647 - val_MinusLogProbMetric: 394.2647 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 625/1000
2023-10-02 18:55:32.519 
Epoch 625/1000 
	 loss: 383.9006, MinusLogProbMetric: 383.9006, val_loss: 394.2804, val_MinusLogProbMetric: 394.2804

Epoch 625: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9006 - MinusLogProbMetric: 383.9006 - val_loss: 394.2804 - val_MinusLogProbMetric: 394.2804 - lr: 1.7361e-06 - 11s/epoch - 56ms/step
Epoch 626/1000
2023-10-02 18:55:43.741 
Epoch 626/1000 
	 loss: 383.9001, MinusLogProbMetric: 383.9001, val_loss: 394.2737, val_MinusLogProbMetric: 394.2737

Epoch 626: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.9001 - MinusLogProbMetric: 383.9001 - val_loss: 394.2737 - val_MinusLogProbMetric: 394.2737 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 627/1000
2023-10-02 18:55:54.928 
Epoch 627/1000 
	 loss: 383.8987, MinusLogProbMetric: 383.8987, val_loss: 394.2565, val_MinusLogProbMetric: 394.2565

Epoch 627: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8987 - MinusLogProbMetric: 383.8987 - val_loss: 394.2565 - val_MinusLogProbMetric: 394.2565 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 628/1000
2023-10-02 18:56:06.229 
Epoch 628/1000 
	 loss: 383.8951, MinusLogProbMetric: 383.8951, val_loss: 394.2599, val_MinusLogProbMetric: 394.2599

Epoch 628: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8951 - MinusLogProbMetric: 383.8951 - val_loss: 394.2599 - val_MinusLogProbMetric: 394.2599 - lr: 1.7361e-06 - 11s/epoch - 58ms/step
Epoch 629/1000
2023-10-02 18:56:17.440 
Epoch 629/1000 
	 loss: 383.8943, MinusLogProbMetric: 383.8943, val_loss: 394.2593, val_MinusLogProbMetric: 394.2593

Epoch 629: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8943 - MinusLogProbMetric: 383.8943 - val_loss: 394.2593 - val_MinusLogProbMetric: 394.2593 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 630/1000
2023-10-02 18:56:28.642 
Epoch 630/1000 
	 loss: 383.8991, MinusLogProbMetric: 383.8991, val_loss: 394.2691, val_MinusLogProbMetric: 394.2691

Epoch 630: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8991 - MinusLogProbMetric: 383.8991 - val_loss: 394.2691 - val_MinusLogProbMetric: 394.2691 - lr: 1.7361e-06 - 11s/epoch - 57ms/step
Epoch 631/1000
2023-10-02 18:56:39.845 
Epoch 631/1000 
	 loss: 383.8712, MinusLogProbMetric: 383.8712, val_loss: 394.2515, val_MinusLogProbMetric: 394.2515

Epoch 631: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8712 - MinusLogProbMetric: 383.8712 - val_loss: 394.2515 - val_MinusLogProbMetric: 394.2515 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 632/1000
2023-10-02 18:56:50.863 
Epoch 632/1000 
	 loss: 383.8700, MinusLogProbMetric: 383.8700, val_loss: 394.2543, val_MinusLogProbMetric: 394.2543

Epoch 632: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8700 - MinusLogProbMetric: 383.8700 - val_loss: 394.2543 - val_MinusLogProbMetric: 394.2543 - lr: 1.0000e-06 - 11s/epoch - 56ms/step
Epoch 633/1000
2023-10-02 18:57:02.072 
Epoch 633/1000 
	 loss: 383.8708, MinusLogProbMetric: 383.8708, val_loss: 394.2521, val_MinusLogProbMetric: 394.2521

Epoch 633: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8708 - MinusLogProbMetric: 383.8708 - val_loss: 394.2521 - val_MinusLogProbMetric: 394.2521 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 634/1000
2023-10-02 18:57:13.260 
Epoch 634/1000 
	 loss: 383.8698, MinusLogProbMetric: 383.8698, val_loss: 394.2506, val_MinusLogProbMetric: 394.2506

Epoch 634: val_loss did not improve from 394.24170
196/196 - 11s - loss: 383.8698 - MinusLogProbMetric: 383.8698 - val_loss: 394.2506 - val_MinusLogProbMetric: 394.2506 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 635/1000
2023-10-02 18:57:24.427 
Epoch 635/1000 
	 loss: 383.8694, MinusLogProbMetric: 383.8694, val_loss: 394.2353, val_MinusLogProbMetric: 394.2353

Epoch 635: val_loss improved from 394.24170 to 394.23529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_340/weights/best_weights.h5
196/196 - 12s - loss: 383.8694 - MinusLogProbMetric: 383.8694 - val_loss: 394.2353 - val_MinusLogProbMetric: 394.2353 - lr: 1.0000e-06 - 12s/epoch - 59ms/step
Epoch 636/1000
2023-10-02 18:57:36.214 
Epoch 636/1000 
	 loss: 383.8690, MinusLogProbMetric: 383.8690, val_loss: 394.2501, val_MinusLogProbMetric: 394.2501

Epoch 636: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8690 - MinusLogProbMetric: 383.8690 - val_loss: 394.2501 - val_MinusLogProbMetric: 394.2501 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 637/1000
2023-10-02 18:57:47.435 
Epoch 637/1000 
	 loss: 383.8705, MinusLogProbMetric: 383.8705, val_loss: 394.2552, val_MinusLogProbMetric: 394.2552

Epoch 637: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8705 - MinusLogProbMetric: 383.8705 - val_loss: 394.2552 - val_MinusLogProbMetric: 394.2552 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 638/1000
2023-10-02 18:57:58.618 
Epoch 638/1000 
	 loss: 383.8678, MinusLogProbMetric: 383.8678, val_loss: 394.2510, val_MinusLogProbMetric: 394.2510

Epoch 638: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8678 - MinusLogProbMetric: 383.8678 - val_loss: 394.2510 - val_MinusLogProbMetric: 394.2510 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 639/1000
2023-10-02 18:58:09.859 
Epoch 639/1000 
	 loss: 383.8695, MinusLogProbMetric: 383.8695, val_loss: 394.2567, val_MinusLogProbMetric: 394.2567

Epoch 639: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8695 - MinusLogProbMetric: 383.8695 - val_loss: 394.2567 - val_MinusLogProbMetric: 394.2567 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 640/1000
2023-10-02 18:58:21.134 
Epoch 640/1000 
	 loss: 383.8682, MinusLogProbMetric: 383.8682, val_loss: 394.2642, val_MinusLogProbMetric: 394.2642

Epoch 640: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8682 - MinusLogProbMetric: 383.8682 - val_loss: 394.2642 - val_MinusLogProbMetric: 394.2642 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 641/1000
2023-10-02 18:58:32.289 
Epoch 641/1000 
	 loss: 383.8684, MinusLogProbMetric: 383.8684, val_loss: 394.2436, val_MinusLogProbMetric: 394.2436

Epoch 641: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8684 - MinusLogProbMetric: 383.8684 - val_loss: 394.2436 - val_MinusLogProbMetric: 394.2436 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 642/1000
2023-10-02 18:58:43.591 
Epoch 642/1000 
	 loss: 383.8671, MinusLogProbMetric: 383.8671, val_loss: 394.2509, val_MinusLogProbMetric: 394.2509

Epoch 642: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8671 - MinusLogProbMetric: 383.8671 - val_loss: 394.2509 - val_MinusLogProbMetric: 394.2509 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 643/1000
2023-10-02 18:58:54.779 
Epoch 643/1000 
	 loss: 383.8684, MinusLogProbMetric: 383.8684, val_loss: 394.2705, val_MinusLogProbMetric: 394.2705

Epoch 643: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8684 - MinusLogProbMetric: 383.8684 - val_loss: 394.2705 - val_MinusLogProbMetric: 394.2705 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 644/1000
2023-10-02 18:59:06.012 
Epoch 644/1000 
	 loss: 383.8656, MinusLogProbMetric: 383.8656, val_loss: 394.2470, val_MinusLogProbMetric: 394.2470

Epoch 644: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8656 - MinusLogProbMetric: 383.8656 - val_loss: 394.2470 - val_MinusLogProbMetric: 394.2470 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 645/1000
2023-10-02 18:59:17.243 
Epoch 645/1000 
	 loss: 383.8675, MinusLogProbMetric: 383.8675, val_loss: 394.2486, val_MinusLogProbMetric: 394.2486

Epoch 645: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8675 - MinusLogProbMetric: 383.8675 - val_loss: 394.2486 - val_MinusLogProbMetric: 394.2486 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 646/1000
2023-10-02 18:59:28.438 
Epoch 646/1000 
	 loss: 383.8655, MinusLogProbMetric: 383.8655, val_loss: 394.2568, val_MinusLogProbMetric: 394.2568

Epoch 646: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8655 - MinusLogProbMetric: 383.8655 - val_loss: 394.2568 - val_MinusLogProbMetric: 394.2568 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 647/1000
2023-10-02 18:59:39.729 
Epoch 647/1000 
	 loss: 383.8663, MinusLogProbMetric: 383.8663, val_loss: 394.2527, val_MinusLogProbMetric: 394.2527

Epoch 647: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8663 - MinusLogProbMetric: 383.8663 - val_loss: 394.2527 - val_MinusLogProbMetric: 394.2527 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 648/1000
2023-10-02 18:59:50.991 
Epoch 648/1000 
	 loss: 383.8680, MinusLogProbMetric: 383.8680, val_loss: 394.2558, val_MinusLogProbMetric: 394.2558

Epoch 648: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8680 - MinusLogProbMetric: 383.8680 - val_loss: 394.2558 - val_MinusLogProbMetric: 394.2558 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 649/1000
2023-10-02 19:00:02.172 
Epoch 649/1000 
	 loss: 383.8669, MinusLogProbMetric: 383.8669, val_loss: 394.2557, val_MinusLogProbMetric: 394.2557

Epoch 649: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8669 - MinusLogProbMetric: 383.8669 - val_loss: 394.2557 - val_MinusLogProbMetric: 394.2557 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 650/1000
2023-10-02 19:00:13.461 
Epoch 650/1000 
	 loss: 383.8640, MinusLogProbMetric: 383.8640, val_loss: 394.2484, val_MinusLogProbMetric: 394.2484

Epoch 650: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8640 - MinusLogProbMetric: 383.8640 - val_loss: 394.2484 - val_MinusLogProbMetric: 394.2484 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 651/1000
2023-10-02 19:00:24.631 
Epoch 651/1000 
	 loss: 383.8659, MinusLogProbMetric: 383.8659, val_loss: 394.2549, val_MinusLogProbMetric: 394.2549

Epoch 651: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8659 - MinusLogProbMetric: 383.8659 - val_loss: 394.2549 - val_MinusLogProbMetric: 394.2549 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 652/1000
2023-10-02 19:00:35.963 
Epoch 652/1000 
	 loss: 383.8646, MinusLogProbMetric: 383.8646, val_loss: 394.2553, val_MinusLogProbMetric: 394.2553

Epoch 652: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8646 - MinusLogProbMetric: 383.8646 - val_loss: 394.2553 - val_MinusLogProbMetric: 394.2553 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 653/1000
2023-10-02 19:00:47.420 
Epoch 653/1000 
	 loss: 383.8646, MinusLogProbMetric: 383.8646, val_loss: 394.2497, val_MinusLogProbMetric: 394.2497

Epoch 653: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8646 - MinusLogProbMetric: 383.8646 - val_loss: 394.2497 - val_MinusLogProbMetric: 394.2497 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 654/1000
2023-10-02 19:00:58.831 
Epoch 654/1000 
	 loss: 383.8664, MinusLogProbMetric: 383.8664, val_loss: 394.2469, val_MinusLogProbMetric: 394.2469

Epoch 654: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8664 - MinusLogProbMetric: 383.8664 - val_loss: 394.2469 - val_MinusLogProbMetric: 394.2469 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 655/1000
2023-10-02 19:01:10.380 
Epoch 655/1000 
	 loss: 383.8675, MinusLogProbMetric: 383.8675, val_loss: 394.2435, val_MinusLogProbMetric: 394.2435

Epoch 655: val_loss did not improve from 394.23529
196/196 - 12s - loss: 383.8675 - MinusLogProbMetric: 383.8675 - val_loss: 394.2435 - val_MinusLogProbMetric: 394.2435 - lr: 1.0000e-06 - 12s/epoch - 59ms/step
Epoch 656/1000
2023-10-02 19:01:21.680 
Epoch 656/1000 
	 loss: 383.8651, MinusLogProbMetric: 383.8651, val_loss: 394.2643, val_MinusLogProbMetric: 394.2643

Epoch 656: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8651 - MinusLogProbMetric: 383.8651 - val_loss: 394.2643 - val_MinusLogProbMetric: 394.2643 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 657/1000
2023-10-02 19:01:32.953 
Epoch 657/1000 
	 loss: 383.8633, MinusLogProbMetric: 383.8633, val_loss: 394.2495, val_MinusLogProbMetric: 394.2495

Epoch 657: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8633 - MinusLogProbMetric: 383.8633 - val_loss: 394.2495 - val_MinusLogProbMetric: 394.2495 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 658/1000
2023-10-02 19:01:44.239 
Epoch 658/1000 
	 loss: 383.8647, MinusLogProbMetric: 383.8647, val_loss: 394.2684, val_MinusLogProbMetric: 394.2684

Epoch 658: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8647 - MinusLogProbMetric: 383.8647 - val_loss: 394.2684 - val_MinusLogProbMetric: 394.2684 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 659/1000
2023-10-02 19:01:55.390 
Epoch 659/1000 
	 loss: 383.8648, MinusLogProbMetric: 383.8648, val_loss: 394.2559, val_MinusLogProbMetric: 394.2559

Epoch 659: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8648 - MinusLogProbMetric: 383.8648 - val_loss: 394.2559 - val_MinusLogProbMetric: 394.2559 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 660/1000
2023-10-02 19:02:06.705 
Epoch 660/1000 
	 loss: 383.8631, MinusLogProbMetric: 383.8631, val_loss: 394.2454, val_MinusLogProbMetric: 394.2454

Epoch 660: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8631 - MinusLogProbMetric: 383.8631 - val_loss: 394.2454 - val_MinusLogProbMetric: 394.2454 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 661/1000
2023-10-02 19:02:18.168 
Epoch 661/1000 
	 loss: 383.8654, MinusLogProbMetric: 383.8654, val_loss: 394.2512, val_MinusLogProbMetric: 394.2512

Epoch 661: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8654 - MinusLogProbMetric: 383.8654 - val_loss: 394.2512 - val_MinusLogProbMetric: 394.2512 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 662/1000
2023-10-02 19:02:29.343 
Epoch 662/1000 
	 loss: 383.8649, MinusLogProbMetric: 383.8649, val_loss: 394.2419, val_MinusLogProbMetric: 394.2419

Epoch 662: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8649 - MinusLogProbMetric: 383.8649 - val_loss: 394.2419 - val_MinusLogProbMetric: 394.2419 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 663/1000
2023-10-02 19:02:40.572 
Epoch 663/1000 
	 loss: 383.8634, MinusLogProbMetric: 383.8634, val_loss: 394.2693, val_MinusLogProbMetric: 394.2693

Epoch 663: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8634 - MinusLogProbMetric: 383.8634 - val_loss: 394.2693 - val_MinusLogProbMetric: 394.2693 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 664/1000
2023-10-02 19:02:52.068 
Epoch 664/1000 
	 loss: 383.8622, MinusLogProbMetric: 383.8622, val_loss: 394.2564, val_MinusLogProbMetric: 394.2564

Epoch 664: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8622 - MinusLogProbMetric: 383.8622 - val_loss: 394.2564 - val_MinusLogProbMetric: 394.2564 - lr: 1.0000e-06 - 11s/epoch - 59ms/step
Epoch 665/1000
2023-10-02 19:03:03.300 
Epoch 665/1000 
	 loss: 383.8615, MinusLogProbMetric: 383.8615, val_loss: 394.2589, val_MinusLogProbMetric: 394.2589

Epoch 665: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8615 - MinusLogProbMetric: 383.8615 - val_loss: 394.2589 - val_MinusLogProbMetric: 394.2589 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 666/1000
2023-10-02 19:03:14.493 
Epoch 666/1000 
	 loss: 383.8607, MinusLogProbMetric: 383.8607, val_loss: 394.2550, val_MinusLogProbMetric: 394.2550

Epoch 666: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8607 - MinusLogProbMetric: 383.8607 - val_loss: 394.2550 - val_MinusLogProbMetric: 394.2550 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 667/1000
2023-10-02 19:03:25.667 
Epoch 667/1000 
	 loss: 383.8628, MinusLogProbMetric: 383.8628, val_loss: 394.2639, val_MinusLogProbMetric: 394.2639

Epoch 667: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8628 - MinusLogProbMetric: 383.8628 - val_loss: 394.2639 - val_MinusLogProbMetric: 394.2639 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 668/1000
2023-10-02 19:03:36.829 
Epoch 668/1000 
	 loss: 383.8625, MinusLogProbMetric: 383.8625, val_loss: 394.2505, val_MinusLogProbMetric: 394.2505

Epoch 668: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8625 - MinusLogProbMetric: 383.8625 - val_loss: 394.2505 - val_MinusLogProbMetric: 394.2505 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 669/1000
2023-10-02 19:03:48.121 
Epoch 669/1000 
	 loss: 383.8614, MinusLogProbMetric: 383.8614, val_loss: 394.2673, val_MinusLogProbMetric: 394.2673

Epoch 669: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8614 - MinusLogProbMetric: 383.8614 - val_loss: 394.2673 - val_MinusLogProbMetric: 394.2673 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 670/1000
2023-10-02 19:03:59.479 
Epoch 670/1000 
	 loss: 383.8625, MinusLogProbMetric: 383.8625, val_loss: 394.2714, val_MinusLogProbMetric: 394.2714

Epoch 670: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8625 - MinusLogProbMetric: 383.8625 - val_loss: 394.2714 - val_MinusLogProbMetric: 394.2714 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 671/1000
2023-10-02 19:04:11.001 
Epoch 671/1000 
	 loss: 383.8608, MinusLogProbMetric: 383.8608, val_loss: 394.2490, val_MinusLogProbMetric: 394.2490

Epoch 671: val_loss did not improve from 394.23529
196/196 - 12s - loss: 383.8608 - MinusLogProbMetric: 383.8608 - val_loss: 394.2490 - val_MinusLogProbMetric: 394.2490 - lr: 1.0000e-06 - 12s/epoch - 59ms/step
Epoch 672/1000
2023-10-02 19:04:22.490 
Epoch 672/1000 
	 loss: 383.8618, MinusLogProbMetric: 383.8618, val_loss: 394.2493, val_MinusLogProbMetric: 394.2493

Epoch 672: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8618 - MinusLogProbMetric: 383.8618 - val_loss: 394.2493 - val_MinusLogProbMetric: 394.2493 - lr: 1.0000e-06 - 11s/epoch - 59ms/step
Epoch 673/1000
2023-10-02 19:04:34.056 
Epoch 673/1000 
	 loss: 383.8618, MinusLogProbMetric: 383.8618, val_loss: 394.2567, val_MinusLogProbMetric: 394.2567

Epoch 673: val_loss did not improve from 394.23529
196/196 - 12s - loss: 383.8618 - MinusLogProbMetric: 383.8618 - val_loss: 394.2567 - val_MinusLogProbMetric: 394.2567 - lr: 1.0000e-06 - 12s/epoch - 59ms/step
Epoch 674/1000
2023-10-02 19:04:45.405 
Epoch 674/1000 
	 loss: 383.8600, MinusLogProbMetric: 383.8600, val_loss: 394.2610, val_MinusLogProbMetric: 394.2610

Epoch 674: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8600 - MinusLogProbMetric: 383.8600 - val_loss: 394.2610 - val_MinusLogProbMetric: 394.2610 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 675/1000
2023-10-02 19:04:56.795 
Epoch 675/1000 
	 loss: 383.8597, MinusLogProbMetric: 383.8597, val_loss: 394.2563, val_MinusLogProbMetric: 394.2563

Epoch 675: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8597 - MinusLogProbMetric: 383.8597 - val_loss: 394.2563 - val_MinusLogProbMetric: 394.2563 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 676/1000
2023-10-02 19:05:08.128 
Epoch 676/1000 
	 loss: 383.8610, MinusLogProbMetric: 383.8610, val_loss: 394.2520, val_MinusLogProbMetric: 394.2520

Epoch 676: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8610 - MinusLogProbMetric: 383.8610 - val_loss: 394.2520 - val_MinusLogProbMetric: 394.2520 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 677/1000
2023-10-02 19:05:19.348 
Epoch 677/1000 
	 loss: 383.8600, MinusLogProbMetric: 383.8600, val_loss: 394.2660, val_MinusLogProbMetric: 394.2660

Epoch 677: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8600 - MinusLogProbMetric: 383.8600 - val_loss: 394.2660 - val_MinusLogProbMetric: 394.2660 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 678/1000
2023-10-02 19:05:30.586 
Epoch 678/1000 
	 loss: 383.8601, MinusLogProbMetric: 383.8601, val_loss: 394.2610, val_MinusLogProbMetric: 394.2610

Epoch 678: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8601 - MinusLogProbMetric: 383.8601 - val_loss: 394.2610 - val_MinusLogProbMetric: 394.2610 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 679/1000
2023-10-02 19:05:42.144 
Epoch 679/1000 
	 loss: 383.8587, MinusLogProbMetric: 383.8587, val_loss: 394.2538, val_MinusLogProbMetric: 394.2538

Epoch 679: val_loss did not improve from 394.23529
196/196 - 12s - loss: 383.8587 - MinusLogProbMetric: 383.8587 - val_loss: 394.2538 - val_MinusLogProbMetric: 394.2538 - lr: 1.0000e-06 - 12s/epoch - 59ms/step
Epoch 680/1000
2023-10-02 19:05:53.566 
Epoch 680/1000 
	 loss: 383.8600, MinusLogProbMetric: 383.8600, val_loss: 394.2655, val_MinusLogProbMetric: 394.2655

Epoch 680: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8600 - MinusLogProbMetric: 383.8600 - val_loss: 394.2655 - val_MinusLogProbMetric: 394.2655 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 681/1000
2023-10-02 19:06:04.931 
Epoch 681/1000 
	 loss: 383.8591, MinusLogProbMetric: 383.8591, val_loss: 394.2555, val_MinusLogProbMetric: 394.2555

Epoch 681: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8591 - MinusLogProbMetric: 383.8591 - val_loss: 394.2555 - val_MinusLogProbMetric: 394.2555 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 682/1000
2023-10-02 19:06:16.243 
Epoch 682/1000 
	 loss: 383.8604, MinusLogProbMetric: 383.8604, val_loss: 394.2572, val_MinusLogProbMetric: 394.2572

Epoch 682: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8604 - MinusLogProbMetric: 383.8604 - val_loss: 394.2572 - val_MinusLogProbMetric: 394.2572 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 683/1000
2023-10-02 19:06:27.359 
Epoch 683/1000 
	 loss: 383.8575, MinusLogProbMetric: 383.8575, val_loss: 394.2663, val_MinusLogProbMetric: 394.2663

Epoch 683: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8575 - MinusLogProbMetric: 383.8575 - val_loss: 394.2663 - val_MinusLogProbMetric: 394.2663 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 684/1000
2023-10-02 19:06:38.581 
Epoch 684/1000 
	 loss: 383.8598, MinusLogProbMetric: 383.8598, val_loss: 394.2515, val_MinusLogProbMetric: 394.2515

Epoch 684: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8598 - MinusLogProbMetric: 383.8598 - val_loss: 394.2515 - val_MinusLogProbMetric: 394.2515 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 685/1000
2023-10-02 19:06:49.909 
Epoch 685/1000 
	 loss: 383.8578, MinusLogProbMetric: 383.8578, val_loss: 394.2574, val_MinusLogProbMetric: 394.2574

Epoch 685: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8578 - MinusLogProbMetric: 383.8578 - val_loss: 394.2574 - val_MinusLogProbMetric: 394.2574 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 686/1000
2023-10-02 19:07:01.248 
Epoch 686/1000 
	 loss: 383.8608, MinusLogProbMetric: 383.8608, val_loss: 394.2675, val_MinusLogProbMetric: 394.2675

Epoch 686: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8608 - MinusLogProbMetric: 383.8608 - val_loss: 394.2675 - val_MinusLogProbMetric: 394.2675 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 687/1000
2023-10-02 19:07:12.327 
Epoch 687/1000 
	 loss: 383.8585, MinusLogProbMetric: 383.8585, val_loss: 394.2524, val_MinusLogProbMetric: 394.2524

Epoch 687: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8585 - MinusLogProbMetric: 383.8585 - val_loss: 394.2524 - val_MinusLogProbMetric: 394.2524 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 688/1000
2023-10-02 19:07:23.531 
Epoch 688/1000 
	 loss: 383.8566, MinusLogProbMetric: 383.8566, val_loss: 394.2696, val_MinusLogProbMetric: 394.2696

Epoch 688: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8566 - MinusLogProbMetric: 383.8566 - val_loss: 394.2696 - val_MinusLogProbMetric: 394.2696 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 689/1000
2023-10-02 19:07:34.895 
Epoch 689/1000 
	 loss: 383.8597, MinusLogProbMetric: 383.8597, val_loss: 394.2659, val_MinusLogProbMetric: 394.2659

Epoch 689: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8597 - MinusLogProbMetric: 383.8597 - val_loss: 394.2659 - val_MinusLogProbMetric: 394.2659 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 690/1000
2023-10-02 19:07:46.220 
Epoch 690/1000 
	 loss: 383.8571, MinusLogProbMetric: 383.8571, val_loss: 394.2657, val_MinusLogProbMetric: 394.2657

Epoch 690: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8571 - MinusLogProbMetric: 383.8571 - val_loss: 394.2657 - val_MinusLogProbMetric: 394.2657 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 691/1000
2023-10-02 19:07:57.310 
Epoch 691/1000 
	 loss: 383.8586, MinusLogProbMetric: 383.8586, val_loss: 394.2511, val_MinusLogProbMetric: 394.2511

Epoch 691: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8586 - MinusLogProbMetric: 383.8586 - val_loss: 394.2511 - val_MinusLogProbMetric: 394.2511 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 692/1000
2023-10-02 19:08:08.691 
Epoch 692/1000 
	 loss: 383.8563, MinusLogProbMetric: 383.8563, val_loss: 394.2598, val_MinusLogProbMetric: 394.2598

Epoch 692: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8563 - MinusLogProbMetric: 383.8563 - val_loss: 394.2598 - val_MinusLogProbMetric: 394.2598 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 693/1000
2023-10-02 19:08:19.890 
Epoch 693/1000 
	 loss: 383.8570, MinusLogProbMetric: 383.8570, val_loss: 394.2601, val_MinusLogProbMetric: 394.2601

Epoch 693: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8570 - MinusLogProbMetric: 383.8570 - val_loss: 394.2601 - val_MinusLogProbMetric: 394.2601 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 694/1000
2023-10-02 19:08:31.192 
Epoch 694/1000 
	 loss: 383.8579, MinusLogProbMetric: 383.8579, val_loss: 394.2534, val_MinusLogProbMetric: 394.2534

Epoch 694: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8579 - MinusLogProbMetric: 383.8579 - val_loss: 394.2534 - val_MinusLogProbMetric: 394.2534 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 695/1000
2023-10-02 19:08:42.539 
Epoch 695/1000 
	 loss: 383.8594, MinusLogProbMetric: 383.8594, val_loss: 394.2797, val_MinusLogProbMetric: 394.2797

Epoch 695: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8594 - MinusLogProbMetric: 383.8594 - val_loss: 394.2797 - val_MinusLogProbMetric: 394.2797 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 696/1000
2023-10-02 19:08:53.921 
Epoch 696/1000 
	 loss: 383.8580, MinusLogProbMetric: 383.8580, val_loss: 394.2539, val_MinusLogProbMetric: 394.2539

Epoch 696: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8580 - MinusLogProbMetric: 383.8580 - val_loss: 394.2539 - val_MinusLogProbMetric: 394.2539 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 697/1000
2023-10-02 19:09:05.165 
Epoch 697/1000 
	 loss: 383.8557, MinusLogProbMetric: 383.8557, val_loss: 394.2550, val_MinusLogProbMetric: 394.2550

Epoch 697: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8557 - MinusLogProbMetric: 383.8557 - val_loss: 394.2550 - val_MinusLogProbMetric: 394.2550 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 698/1000
2023-10-02 19:09:16.525 
Epoch 698/1000 
	 loss: 383.8541, MinusLogProbMetric: 383.8541, val_loss: 394.2458, val_MinusLogProbMetric: 394.2458

Epoch 698: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8541 - MinusLogProbMetric: 383.8541 - val_loss: 394.2458 - val_MinusLogProbMetric: 394.2458 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 699/1000
2023-10-02 19:09:27.738 
Epoch 699/1000 
	 loss: 383.8567, MinusLogProbMetric: 383.8567, val_loss: 394.2429, val_MinusLogProbMetric: 394.2429

Epoch 699: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8567 - MinusLogProbMetric: 383.8567 - val_loss: 394.2429 - val_MinusLogProbMetric: 394.2429 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 700/1000
2023-10-02 19:09:39.026 
Epoch 700/1000 
	 loss: 383.8556, MinusLogProbMetric: 383.8556, val_loss: 394.2498, val_MinusLogProbMetric: 394.2498

Epoch 700: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8556 - MinusLogProbMetric: 383.8556 - val_loss: 394.2498 - val_MinusLogProbMetric: 394.2498 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 701/1000
2023-10-02 19:09:50.326 
Epoch 701/1000 
	 loss: 383.8568, MinusLogProbMetric: 383.8568, val_loss: 394.2639, val_MinusLogProbMetric: 394.2639

Epoch 701: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8568 - MinusLogProbMetric: 383.8568 - val_loss: 394.2639 - val_MinusLogProbMetric: 394.2639 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 702/1000
2023-10-02 19:10:01.379 
Epoch 702/1000 
	 loss: 383.8560, MinusLogProbMetric: 383.8560, val_loss: 394.2521, val_MinusLogProbMetric: 394.2521

Epoch 702: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8560 - MinusLogProbMetric: 383.8560 - val_loss: 394.2521 - val_MinusLogProbMetric: 394.2521 - lr: 1.0000e-06 - 11s/epoch - 56ms/step
Epoch 703/1000
2023-10-02 19:10:12.609 
Epoch 703/1000 
	 loss: 383.8555, MinusLogProbMetric: 383.8555, val_loss: 394.2533, val_MinusLogProbMetric: 394.2533

Epoch 703: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8555 - MinusLogProbMetric: 383.8555 - val_loss: 394.2533 - val_MinusLogProbMetric: 394.2533 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 704/1000
2023-10-02 19:10:24.024 
Epoch 704/1000 
	 loss: 383.8559, MinusLogProbMetric: 383.8559, val_loss: 394.2649, val_MinusLogProbMetric: 394.2649

Epoch 704: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8559 - MinusLogProbMetric: 383.8559 - val_loss: 394.2649 - val_MinusLogProbMetric: 394.2649 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 705/1000
2023-10-02 19:10:35.372 
Epoch 705/1000 
	 loss: 383.8551, MinusLogProbMetric: 383.8551, val_loss: 394.2526, val_MinusLogProbMetric: 394.2526

Epoch 705: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8551 - MinusLogProbMetric: 383.8551 - val_loss: 394.2526 - val_MinusLogProbMetric: 394.2526 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 706/1000
2023-10-02 19:10:46.211 
Epoch 706/1000 
	 loss: 383.8557, MinusLogProbMetric: 383.8557, val_loss: 394.2540, val_MinusLogProbMetric: 394.2540

Epoch 706: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8557 - MinusLogProbMetric: 383.8557 - val_loss: 394.2540 - val_MinusLogProbMetric: 394.2540 - lr: 1.0000e-06 - 11s/epoch - 55ms/step
Epoch 707/1000
2023-10-02 19:10:57.491 
Epoch 707/1000 
	 loss: 383.8541, MinusLogProbMetric: 383.8541, val_loss: 394.2582, val_MinusLogProbMetric: 394.2582

Epoch 707: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8541 - MinusLogProbMetric: 383.8541 - val_loss: 394.2582 - val_MinusLogProbMetric: 394.2582 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 708/1000
2023-10-02 19:11:08.593 
Epoch 708/1000 
	 loss: 383.8531, MinusLogProbMetric: 383.8531, val_loss: 394.2507, val_MinusLogProbMetric: 394.2507

Epoch 708: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8531 - MinusLogProbMetric: 383.8531 - val_loss: 394.2507 - val_MinusLogProbMetric: 394.2507 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 709/1000
2023-10-02 19:11:19.819 
Epoch 709/1000 
	 loss: 383.8544, MinusLogProbMetric: 383.8544, val_loss: 394.2557, val_MinusLogProbMetric: 394.2557

Epoch 709: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8544 - MinusLogProbMetric: 383.8544 - val_loss: 394.2557 - val_MinusLogProbMetric: 394.2557 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 710/1000
2023-10-02 19:11:31.167 
Epoch 710/1000 
	 loss: 383.8543, MinusLogProbMetric: 383.8543, val_loss: 394.2650, val_MinusLogProbMetric: 394.2650

Epoch 710: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8543 - MinusLogProbMetric: 383.8543 - val_loss: 394.2650 - val_MinusLogProbMetric: 394.2650 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 711/1000
2023-10-02 19:11:42.335 
Epoch 711/1000 
	 loss: 383.8526, MinusLogProbMetric: 383.8526, val_loss: 394.2492, val_MinusLogProbMetric: 394.2492

Epoch 711: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8526 - MinusLogProbMetric: 383.8526 - val_loss: 394.2492 - val_MinusLogProbMetric: 394.2492 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 712/1000
2023-10-02 19:11:53.594 
Epoch 712/1000 
	 loss: 383.8512, MinusLogProbMetric: 383.8512, val_loss: 394.2586, val_MinusLogProbMetric: 394.2586

Epoch 712: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8512 - MinusLogProbMetric: 383.8512 - val_loss: 394.2586 - val_MinusLogProbMetric: 394.2586 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 713/1000
2023-10-02 19:12:04.922 
Epoch 713/1000 
	 loss: 383.8509, MinusLogProbMetric: 383.8509, val_loss: 394.2452, val_MinusLogProbMetric: 394.2452

Epoch 713: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8509 - MinusLogProbMetric: 383.8509 - val_loss: 394.2452 - val_MinusLogProbMetric: 394.2452 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 714/1000
2023-10-02 19:12:16.182 
Epoch 714/1000 
	 loss: 383.8502, MinusLogProbMetric: 383.8502, val_loss: 394.2579, val_MinusLogProbMetric: 394.2579

Epoch 714: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8502 - MinusLogProbMetric: 383.8502 - val_loss: 394.2579 - val_MinusLogProbMetric: 394.2579 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 715/1000
2023-10-02 19:12:27.222 
Epoch 715/1000 
	 loss: 383.8505, MinusLogProbMetric: 383.8505, val_loss: 394.2743, val_MinusLogProbMetric: 394.2743

Epoch 715: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8505 - MinusLogProbMetric: 383.8505 - val_loss: 394.2743 - val_MinusLogProbMetric: 394.2743 - lr: 1.0000e-06 - 11s/epoch - 56ms/step
Epoch 716/1000
2023-10-02 19:12:38.374 
Epoch 716/1000 
	 loss: 383.8504, MinusLogProbMetric: 383.8504, val_loss: 394.2540, val_MinusLogProbMetric: 394.2540

Epoch 716: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8504 - MinusLogProbMetric: 383.8504 - val_loss: 394.2540 - val_MinusLogProbMetric: 394.2540 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 717/1000
2023-10-02 19:12:49.717 
Epoch 717/1000 
	 loss: 383.8495, MinusLogProbMetric: 383.8495, val_loss: 394.2707, val_MinusLogProbMetric: 394.2707

Epoch 717: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8495 - MinusLogProbMetric: 383.8495 - val_loss: 394.2707 - val_MinusLogProbMetric: 394.2707 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 718/1000
2023-10-02 19:13:00.951 
Epoch 718/1000 
	 loss: 383.8497, MinusLogProbMetric: 383.8497, val_loss: 394.2613, val_MinusLogProbMetric: 394.2613

Epoch 718: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8497 - MinusLogProbMetric: 383.8497 - val_loss: 394.2613 - val_MinusLogProbMetric: 394.2613 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 719/1000
2023-10-02 19:13:12.203 
Epoch 719/1000 
	 loss: 383.8493, MinusLogProbMetric: 383.8493, val_loss: 394.2553, val_MinusLogProbMetric: 394.2553

Epoch 719: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8493 - MinusLogProbMetric: 383.8493 - val_loss: 394.2553 - val_MinusLogProbMetric: 394.2553 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 720/1000
2023-10-02 19:13:23.521 
Epoch 720/1000 
	 loss: 383.8476, MinusLogProbMetric: 383.8476, val_loss: 394.2482, val_MinusLogProbMetric: 394.2482

Epoch 720: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8476 - MinusLogProbMetric: 383.8476 - val_loss: 394.2482 - val_MinusLogProbMetric: 394.2482 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 721/1000
2023-10-02 19:13:34.770 
Epoch 721/1000 
	 loss: 383.8502, MinusLogProbMetric: 383.8502, val_loss: 394.2507, val_MinusLogProbMetric: 394.2507

Epoch 721: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8502 - MinusLogProbMetric: 383.8502 - val_loss: 394.2507 - val_MinusLogProbMetric: 394.2507 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 722/1000
2023-10-02 19:13:46.032 
Epoch 722/1000 
	 loss: 383.8486, MinusLogProbMetric: 383.8486, val_loss: 394.2508, val_MinusLogProbMetric: 394.2508

Epoch 722: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8486 - MinusLogProbMetric: 383.8486 - val_loss: 394.2508 - val_MinusLogProbMetric: 394.2508 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 723/1000
2023-10-02 19:13:57.251 
Epoch 723/1000 
	 loss: 383.8484, MinusLogProbMetric: 383.8484, val_loss: 394.2577, val_MinusLogProbMetric: 394.2577

Epoch 723: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8484 - MinusLogProbMetric: 383.8484 - val_loss: 394.2577 - val_MinusLogProbMetric: 394.2577 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 724/1000
2023-10-02 19:14:08.573 
Epoch 724/1000 
	 loss: 383.8481, MinusLogProbMetric: 383.8481, val_loss: 394.2613, val_MinusLogProbMetric: 394.2613

Epoch 724: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8481 - MinusLogProbMetric: 383.8481 - val_loss: 394.2613 - val_MinusLogProbMetric: 394.2613 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 725/1000
2023-10-02 19:14:19.659 
Epoch 725/1000 
	 loss: 383.8488, MinusLogProbMetric: 383.8488, val_loss: 394.2447, val_MinusLogProbMetric: 394.2447

Epoch 725: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8488 - MinusLogProbMetric: 383.8488 - val_loss: 394.2447 - val_MinusLogProbMetric: 394.2447 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 726/1000
2023-10-02 19:14:30.771 
Epoch 726/1000 
	 loss: 383.8463, MinusLogProbMetric: 383.8463, val_loss: 394.2595, val_MinusLogProbMetric: 394.2595

Epoch 726: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8463 - MinusLogProbMetric: 383.8463 - val_loss: 394.2595 - val_MinusLogProbMetric: 394.2595 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 727/1000
2023-10-02 19:14:42.103 
Epoch 727/1000 
	 loss: 383.8501, MinusLogProbMetric: 383.8501, val_loss: 394.2512, val_MinusLogProbMetric: 394.2512

Epoch 727: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8501 - MinusLogProbMetric: 383.8501 - val_loss: 394.2512 - val_MinusLogProbMetric: 394.2512 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 728/1000
2023-10-02 19:14:53.288 
Epoch 728/1000 
	 loss: 383.8471, MinusLogProbMetric: 383.8471, val_loss: 394.2494, val_MinusLogProbMetric: 394.2494

Epoch 728: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8471 - MinusLogProbMetric: 383.8471 - val_loss: 394.2494 - val_MinusLogProbMetric: 394.2494 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 729/1000
2023-10-02 19:15:04.550 
Epoch 729/1000 
	 loss: 383.8489, MinusLogProbMetric: 383.8489, val_loss: 394.2566, val_MinusLogProbMetric: 394.2566

Epoch 729: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8489 - MinusLogProbMetric: 383.8489 - val_loss: 394.2566 - val_MinusLogProbMetric: 394.2566 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 730/1000
2023-10-02 19:15:16.083 
Epoch 730/1000 
	 loss: 383.8492, MinusLogProbMetric: 383.8492, val_loss: 394.2464, val_MinusLogProbMetric: 394.2464

Epoch 730: val_loss did not improve from 394.23529
196/196 - 12s - loss: 383.8492 - MinusLogProbMetric: 383.8492 - val_loss: 394.2464 - val_MinusLogProbMetric: 394.2464 - lr: 1.0000e-06 - 12s/epoch - 59ms/step
Epoch 731/1000
2023-10-02 19:15:27.421 
Epoch 731/1000 
	 loss: 383.8483, MinusLogProbMetric: 383.8483, val_loss: 394.2485, val_MinusLogProbMetric: 394.2485

Epoch 731: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8483 - MinusLogProbMetric: 383.8483 - val_loss: 394.2485 - val_MinusLogProbMetric: 394.2485 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 732/1000
2023-10-02 19:15:38.644 
Epoch 732/1000 
	 loss: 383.8510, MinusLogProbMetric: 383.8510, val_loss: 394.2600, val_MinusLogProbMetric: 394.2600

Epoch 732: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8510 - MinusLogProbMetric: 383.8510 - val_loss: 394.2600 - val_MinusLogProbMetric: 394.2600 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 733/1000
2023-10-02 19:15:49.834 
Epoch 733/1000 
	 loss: 383.8484, MinusLogProbMetric: 383.8484, val_loss: 394.2424, val_MinusLogProbMetric: 394.2424

Epoch 733: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8484 - MinusLogProbMetric: 383.8484 - val_loss: 394.2424 - val_MinusLogProbMetric: 394.2424 - lr: 1.0000e-06 - 11s/epoch - 57ms/step
Epoch 734/1000
2023-10-02 19:16:01.180 
Epoch 734/1000 
	 loss: 383.8510, MinusLogProbMetric: 383.8510, val_loss: 394.2560, val_MinusLogProbMetric: 394.2560

Epoch 734: val_loss did not improve from 394.23529
196/196 - 11s - loss: 383.8510 - MinusLogProbMetric: 383.8510 - val_loss: 394.2560 - val_MinusLogProbMetric: 394.2560 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 735/1000
2023-10-02 19:16:12.415 
Epoch 735/1000 
	 loss: 383.8501, MinusLogProbMetric: 383.8501, val_loss: 394.2849, val_MinusLogProbMetric: 394.2849

Epoch 735: val_loss did not improve from 394.23529
Restoring model weights from the end of the best epoch: 635.
196/196 - 11s - loss: 383.8501 - MinusLogProbMetric: 383.8501 - val_loss: 394.2849 - val_MinusLogProbMetric: 394.2849 - lr: 1.0000e-06 - 11s/epoch - 58ms/step
Epoch 735: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 5383.411918793106 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 5317.995313320076 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 5262.360702644917 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 5289.083575031953 seconds.
Training succeeded with seed 520.
Model trained in 8042.98 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 21315.12 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 21315.37 s.
===========
Run 340/360 done in 30786.73 s.
===========

Directory ../../results/MAFN_new/run_341/ already exists.
Skipping it.
===========
Run 341/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_342/ already exists.
Skipping it.
===========
Run 342/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_343/ already exists.
Skipping it.
===========
Run 343/360 already exists. Skipping it.
===========

===========
Generating train data for run 344.
===========
Train data generated in 0.54 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.8975353,  7.5487056,  5.3180313, ...,  7.979155 ,  2.5296934,
         6.754205 ],
       [ 7.704416 ,  4.391162 ,  5.173328 , ...,  4.413081 ,  7.8835597,
         6.9377937],
       [ 8.109386 ,  4.318234 ,  5.3090944, ...,  3.0852916,  8.464321 ,
         6.8667717],
       ...,
       [ 5.740096 ,  5.972522 ,  6.2622604, ..., 10.652689 ,  2.5040557,
         6.768634 ],
       [ 6.2015314,  0.5028701,  4.7167926, ...,  4.818747 ,  6.309916 ,
         4.885091 ],
       [ 5.1562834,  7.5026083,  5.1422505, ...,  8.459682 ,  2.4006338,
         6.775316 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_344
self.data_kwargs: {'seed': 541}
self.x_data: [[ 8.944346    5.071281    5.301635   ...  3.2345712   8.218713
   7.315834  ]
 [ 7.6530247   4.23377     5.229719   ...  2.9577737   7.8780494
   6.7679396 ]
 [ 8.655344    4.436833    5.191878   ...  4.1375995   8.791849
   7.4483323 ]
 ...
 [ 6.3798065  -0.45195007  4.9196353  ...  4.8821535   6.055903
   6.0125017 ]
 [ 7.756657    4.538881    5.3200464  ...  2.765904    9.36033
   7.000672  ]
 [ 6.0584593   7.6354427   6.779557   ...  9.598122    2.6918387
   6.656139  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_45 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7fb7982c5d20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb22c5ef20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb22c5ef20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb23516860>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb76c62a440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb76c62a920>, <keras.callbacks.ModelCheckpoint object at 0x7fb76c62a9e0>, <keras.callbacks.EarlyStopping object at 0x7fb76c62ac50>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb76c62ac80>, <keras.callbacks.TerminateOnNaN object at 0x7fb76c62a8c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.8975353,  7.5487056,  5.3180313, ...,  7.979155 ,  2.5296934,
         6.754205 ],
       [ 7.704416 ,  4.391162 ,  5.173328 , ...,  4.413081 ,  7.8835597,
         6.9377937],
       [ 8.109386 ,  4.318234 ,  5.3090944, ...,  3.0852916,  8.464321 ,
         6.8667717],
       ...,
       [ 5.740096 ,  5.972522 ,  6.2622604, ..., 10.652689 ,  2.5040557,
         6.768634 ],
       [ 6.2015314,  0.5028701,  4.7167926, ...,  4.818747 ,  6.309916 ,
         4.885091 ],
       [ 5.1562834,  7.5026083,  5.1422505, ...,  8.459682 ,  2.4006338,
         6.775316 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/360 with hyperparameters:
timestamp = 2023-10-03 01:11:30.048227
ndims = 1000
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 8.9443464e+00  5.0712810e+00  5.3016348e+00  2.5552166e+00
  5.8691235e+00  1.8351557e+00  5.6839700e+00  1.7029351e+00
  1.6637374e+00  4.2301202e+00  5.1187129e+00  1.5429424e+00
  8.5022074e-01  5.4076612e-03  6.4514995e-02  2.8798556e+00
  4.5537715e+00  8.2742701e+00  5.2343698e+00  9.2561474e+00
 -6.3965023e-01  1.9017107e+00  3.2529631e+00  5.8927112e+00
  8.1674862e+00  3.4725866e+00  2.8051417e+00  5.7039076e-01
  8.7409286e+00  4.2301049e+00  6.3036456e+00  1.0560735e+01
 -7.2515529e-01  1.4195669e+00  7.2751231e+00  8.1002569e+00
  8.2396307e+00  4.4850531e+00  9.8329144e+00  3.6185634e+00
  5.9540081e+00  1.3118193e+00  4.2508869e+00  7.3485479e+00
  2.4983881e+00  9.7614374e+00  7.1622887e+00  2.8269064e+00
  7.6845107e+00  8.6501408e-01  9.8061056e+00 -3.0315861e-01
  8.5370369e+00  4.0063114e+00  7.6050367e+00  1.2078162e+00
  1.8064864e+00  7.4926944e+00  6.1466155e+00  1.8077860e+00
  4.0056338e+00  3.7202733e+00  5.6709218e+00  7.4053359e+00
  8.8831940e+00  4.4600101e+00  3.8643827e+00  2.1389792e+00
  7.8185973e+00  6.8649607e+00  5.1795917e+00  5.0133357e+00
  6.3993440e+00  1.7259912e+00 -5.3211361e-02  2.7235508e+00
  6.2138977e+00  4.8363299e+00  8.0572672e+00  8.1861324e+00
  5.0370812e+00  9.4596857e-01  8.6785088e+00  6.3420782e+00
  8.5438375e+00  7.7141809e+00  9.4851608e+00  3.6518431e+00
  7.0112877e+00  3.4836280e+00  5.5378804e+00  3.1639743e+00
  3.5213878e+00  9.4826164e+00  2.2823873e+00  2.0409453e+00
  4.4045577e+00  2.1437628e+00  4.6174788e+00  8.6706190e+00
  1.3972132e+00  4.1895037e+00  4.3430986e+00  5.4401283e+00
  9.7749367e+00  6.8143382e+00  6.8082824e+00  1.0939198e+01
  4.4761267e+00  2.8583031e+00  8.0687547e-01  1.6127855e+00
  3.0704210e+00  8.1066191e-01  7.7518625e+00  5.3930078e+00
  6.5707617e+00  7.7133188e+00  2.3429046e+00  2.8573308e+00
  7.0358224e+00  3.3356471e+00  1.1319630e-01  3.9435978e+00
  9.2751169e+00  4.1005511e+00  2.7870104e+00  9.8699589e+00
  4.6794405e+00  3.2451954e+00  8.7666917e-01  8.2957745e+00
  7.3025146e+00  9.4455490e+00  2.2730801e+00  1.3725048e+00
  7.2411389e+00  1.6464136e+00  2.7701192e+00  2.3040595e+00
  8.2030325e+00  6.8112426e+00  5.2378827e-01  4.6661735e+00
  8.6955223e+00  5.4876552e+00  5.7896667e+00  1.8971643e+00
  8.5775633e+00  5.5975456e+00  7.5397387e+00  1.0131255e+01
  3.9538219e+00  8.4712620e+00  1.9050634e-01  1.1027322e+00
  5.7999635e+00  3.1462212e+00  2.6874499e+00  3.1894717e+00
  3.2590761e+00  7.7746787e+00  6.2896113e+00  7.8990769e+00
  9.0200062e+00  8.5058842e+00  5.2181306e+00  4.2223444e+00
  8.4943619e+00  6.3309250e+00  2.2411201e+00  5.3274450e+00
  5.1067290e+00  9.8373957e+00  8.6932826e-01  6.0806484e+00
  6.2697802e+00  2.7620327e+00  9.9388742e+00  9.1199150e+00
  9.0355330e+00  2.3999453e-01  6.0671291e+00  4.1604033e+00
  9.0283689e+00  6.2536740e+00  5.2465343e+00  1.1988592e+00
  4.4932823e+00  6.3174534e+00  9.6449385e+00  6.4151430e+00
  9.6748543e+00  1.0681762e+01  6.5413651e+00  4.8880482e+00
  4.6189990e+00  8.0417252e+00  1.5292343e-01  1.1392257e+00
  1.2850125e+00  7.6761832e+00  3.2401102e+00  6.3397503e+00
  5.6845374e+00  3.4693668e+00  5.5460801e+00  5.9178486e+00
  9.4156847e+00  5.1198149e+00  6.1204705e+00  4.3263590e-01
  8.7820969e+00  5.6329536e+00  7.2442002e+00 -2.2960544e-02
  4.9573855e+00  2.2288504e+00  2.6121609e+00  7.4535289e+00
  5.4845848e+00  4.9085340e+00 -2.4640819e-01  9.6421938e+00
  8.9332962e+00  2.0278876e+00 -1.9537413e-01  5.4330840e+00
  9.6458216e+00  9.3649035e+00  5.6065726e+00  3.7719359e+00
  3.5931334e+00  9.0328960e+00  6.7254982e+00  1.4752012e+00
  1.9177732e+00  2.7959094e+00  1.3787949e+00  4.5886636e+00
  1.1515390e+01  7.3938780e+00  8.5164428e-01  1.2635899e-01
  3.6772094e+00  3.8027265e+00  9.1101971e+00  9.5930605e+00
  4.0264854e+00  6.9286065e+00  9.9545469e+00  7.8195210e+00
  5.6409397e+00  8.4713084e-01  2.5798862e+00  8.9097824e+00
  1.6849433e+00  8.2721701e+00  7.1689720e+00  4.1106219e+00
  5.0035114e+00  5.0525937e+00  6.7575526e+00  5.0666924e+00
  1.0737674e+00  7.3246279e+00  3.6748936e+00  9.9005594e+00
  3.1399076e+00  2.8666408e+00  2.2917109e+00  6.0727320e+00
  8.9826994e+00  1.0254667e+00  2.3594720e+00  5.8431158e+00
  4.7857404e+00  4.9822187e+00  9.7560043e+00  9.7564678e+00
  7.3872032e+00  7.1467991e+00  7.0199881e+00  4.7391133e+00
  2.8807340e+00  6.5354433e+00  5.8123102e+00  8.8130550e+00
  5.5923468e-01  8.7705822e+00  5.3062606e+00 -5.5353129e-01
  3.3890393e+00  6.6493964e+00  5.6508560e+00  1.1202197e+01
  2.9007564e+00  4.1648722e+00  8.0866766e+00  5.7469816e+00
  6.0169592e+00  7.9898276e+00  6.7868557e+00  4.6625357e+00
  1.8561345e+00  4.7718992e+00  9.0640554e+00  2.8347461e+00
  9.0894794e+00  4.9920583e+00  6.2453890e+00  8.4376707e+00
  6.6637831e+00  9.0843725e-01  1.1660665e+01  3.0299661e+00
  7.1950002e+00  4.9914813e+00  4.6181540e+00  2.0924745e+00
  6.6776500e+00  1.1897669e+00  1.0171629e+01  8.9001732e+00
  5.4110227e+00  4.4263487e+00  1.9841802e+00  5.8741927e+00
  7.4450426e+00  2.7903662e+00  5.3433504e+00  3.0894151e+00
  3.5106759e+00  2.0232074e+00  8.7810221e+00  7.4130011e+00
  7.7140536e+00  1.0643046e+00  2.2568712e+00  8.9773560e+00
  9.1454113e-01  2.0186009e+00  6.6545320e+00  6.1442413e+00
  8.3838558e+00  1.0342110e+01  1.1413314e+00  2.9067490e+00
  4.2822914e+00  1.6280425e-01  4.8523468e-01  1.7549541e+00
  4.8714666e+00  2.4402614e+00  8.8096838e+00  3.3564243e+00
  8.6558800e+00  9.9410667e+00  4.1167269e+00  4.1255851e+00
  2.9219372e+00  7.6535926e+00  2.6355727e+00  4.2073326e+00
  4.6453238e+00  6.3675723e+00  9.8485737e+00  2.3905480e+00
  4.2612143e+00  3.0223722e+00  4.4770617e+00  1.6278577e+00
  1.9609565e+00  1.0535285e+01  2.1226084e+00  3.3767881e+00
  9.3117971e+00  7.5796928e+00  5.1729965e+00  7.8598866e+00
  2.2923901e+00  4.4321613e+00  1.4233030e+00  2.3377724e+00
  1.0065243e+00  3.5456564e+00  4.1227231e+00  9.7088566e+00
  2.8547204e+00  4.8324518e+00  7.3718147e+00  7.8783011e+00
  7.8638425e+00  2.9144902e+00  4.9608488e+00  1.9803571e+00
  1.5853404e+00  4.3583817e+00  7.4199648e+00  4.9660010e+00
  1.0190818e+01  8.1329041e+00 -2.8760788e-01  6.0783772e+00
  6.2244406e+00  9.6368160e+00 -1.8620440e-01  4.6117182e+00
  2.4262204e+00  5.2838308e-01  5.2863402e+00  8.3399858e+00
  7.2275910e+00  1.4571591e-01  4.0400686e+00  4.7784710e-01
  2.4910257e+00  1.9078600e+00  1.8931444e+00  4.1630411e+00
  2.4404171e+00  1.0232767e+01  3.5966356e+00  8.9379129e+00
  2.7483301e+00  7.5434041e+00  5.1080024e-01  4.3487568e+00
 -5.6165874e-01  7.8553061e+00  9.3950367e+00  4.2870969e-01
  9.0893593e+00  7.4221296e+00  4.3771329e+00  9.0878849e+00
  2.0663891e+00  3.6473520e+00  9.9999070e-01  5.3237896e+00
 -5.4919869e-01  4.6365285e+00  6.3283267e+00  9.2732124e+00
  1.5552996e+00  7.1880951e+00  6.2271900e+00  5.5313535e+00
  5.8510990e+00  1.5392092e+00  5.0540504e+00  4.4184580e+00
  9.2081413e+00  3.3830662e+00  5.7834134e+00  4.1243353e+00
  8.2975473e+00  2.5386639e+00  7.4207544e-01  6.3304234e+00
  4.2580895e+00  5.8122301e+00 -2.1570435e-01  9.1932268e+00
  8.6552963e+00  7.5708799e+00 -1.9443253e-01  3.7197874e+00
  9.8743181e+00  6.0263009e+00  1.6372683e+00  4.4369135e+00
  9.7833328e+00  3.9919968e+00  4.3431216e-01  6.2208257e+00
  2.6909373e+00  9.8785430e-01  4.0438128e+00  5.6429973e+00
 -6.1843389e-01  7.0297160e+00  5.4341021e+00  8.4860640e+00
  3.4566321e+00  5.7342043e+00  7.6334524e+00  9.0038929e+00
  3.7223434e+00  6.0845118e+00  6.5700788e+00  1.2773844e+00
  7.2991037e+00  3.5088022e+00  5.9354343e+00  5.4178343e+00
  7.0744524e+00  7.4856811e+00  2.1827915e+00  6.6159654e+00
  8.7040949e+00  3.2422276e+00  7.5529513e+00  6.1994824e+00
  8.9478226e+00  1.9693226e+00  1.8612541e+00  3.6062963e+00
  2.4518812e+00  7.2348843e+00  7.3313851e+00  6.7718916e+00
  2.0351167e+00  8.5850897e+00  1.0028167e+01  5.4356914e+00
  6.5442944e+00  4.6184292e+00  3.1283219e+00  2.8901393e+00
  1.9263090e+00  1.1836938e+00  9.0665789e+00  8.3283224e+00
  4.7910285e+00  4.4820914e+00  9.3514023e+00  2.1546044e+00
  5.8805389e+00  4.3903738e-01  2.2247140e+00  5.9088330e+00
  4.9584928e+00  6.2537398e+00  8.6888113e+00  1.0049231e+01
  7.0398006e+00  3.3480191e+00  8.1408710e+00  1.6674470e+00
  6.3164220e+00  4.5155339e+00  1.6805238e+00  9.9202871e+00
  3.6205506e+00  6.6217365e+00  8.2141304e+00  5.4286766e+00
  1.6267601e+00  7.6978483e+00  2.9924681e+00  9.8932133e+00
  8.2759218e+00  8.5018530e+00  3.1936233e+00  7.7755070e+00
  9.2961884e+00  5.5356965e+00  5.6126337e+00 -2.4630746e-01
  7.0293112e+00  8.5386944e+00  3.1930950e+00  2.9155462e+00
  4.4143424e+00  4.8297209e-01  8.3480543e-01  3.1123245e+00
  2.8908000e+00  4.0284605e+00  5.0597181e+00  1.0072316e+01
  9.4932566e+00  2.0429962e+00  8.8413343e+00  8.5795814e-01
  4.9350314e+00  9.1259652e-01  3.0097170e+00  7.9393425e+00
  4.2711453e+00  7.9923959e+00  6.9667177e+00  9.4181356e+00
  6.2381592e+00  8.0183401e+00  3.3664768e+00  3.8059332e+00
  4.0490818e+00  4.4446468e+00  1.5931008e+00  5.2192802e+00
  2.2268224e+00  8.7408686e+00 -1.8847406e-02  4.7761617e+00
  2.8030579e+00 -1.7537308e-01  8.0809097e+00  7.4201512e+00
  2.0487733e+00  5.8333006e+00  8.1179962e+00  8.2311239e+00
  3.1251311e-03  6.8018031e+00  7.6883483e+00  8.7783766e+00
  5.6571703e+00  4.3441300e+00  8.5495930e+00  8.1494551e+00
  5.4555826e+00  3.7778921e+00  7.5609541e+00  4.7757192e+00
  1.0579890e+00  4.9656453e+00  8.1120148e+00  4.6216760e+00
 -3.6158833e-01  6.1140265e+00  9.9045410e+00  7.3525467e+00
  6.7491326e+00  9.5566406e+00  8.2156792e+00  4.7660222e+00
  4.5311189e+00  7.3306732e+00  8.8040447e+00  7.3644763e-01
  7.6932001e+00  3.3370900e+00  7.2479844e+00  7.7876225e+00
  5.2245250e+00  1.5497253e+00  4.2543964e+00  7.5986636e-01
  6.9275203e+00  9.7100849e+00  9.2669239e+00  3.2498708e+00
  5.3699269e+00  2.9394846e+00  5.2818131e+00  3.1141367e+00
  6.7859550e+00  2.3411243e+00  9.6228018e+00  6.6613054e+00
  7.2898990e-01  4.3681574e-01  1.0014687e+01  2.1311550e+00
  4.1314063e+00  2.9794476e+00  9.2801170e+00  6.6272178e+00
  4.6284294e+00  4.4226980e+00  3.2610605e+00  7.1748915e+00
  4.2616057e+00  1.8846684e+00  6.1376987e+00  3.2141211e+00
  6.5789557e+00  4.3955379e+00  4.7454882e+00  7.8042412e+00
  3.6127386e+00  9.4474850e+00  6.7431140e+00  8.3214483e+00
  9.2100325e+00  3.6113214e+00  5.5930977e+00  7.0742140e+00
  6.1205058e+00  9.8638229e+00  2.9179771e+00  2.7246852e+00
  9.3549395e+00  8.8796721e+00  1.6090376e+00  5.6067185e+00
  7.3421440e+00  2.9167426e+00  4.6086540e+00  5.7381048e+00
  3.9722662e+00  7.5606790e+00  2.8954959e-01  3.4192889e+00
  4.4652858e+00  5.4416647e+00  8.0514631e+00  7.3737478e-01
  1.2092665e+00  7.6465349e+00  6.0600023e+00  6.4349012e+00
  9.8435640e+00  5.0693870e+00  1.9410030e+00  6.8551364e+00
  7.0065575e+00  5.7280030e+00  5.5391960e+00  9.0428343e+00
  1.1960058e+00  1.9434166e-01  1.5483990e+00  2.8434887e+00
  5.9591789e+00  6.2908230e+00  2.2022102e+00  4.9054837e+00
  4.8644104e+00  4.7432480e+00  8.4029760e+00  7.0184464e+00
  7.8207922e+00  1.6498985e+00  5.3174524e+00  8.2454939e+00
  1.6656384e+00  4.4284911e+00  5.6130257e+00  1.6552075e+00
  7.8019342e+00  7.7151400e-01  8.2651939e+00  4.7385693e+00
  4.0336847e+00  1.3455848e+00  4.3968825e+00  6.8709621e+00
 -8.9248103e-01  3.0087178e+00  8.5056953e+00  6.4442968e+00
  2.0375299e+00  2.2298772e+00  7.4714904e+00  2.7371094e+00
  1.4103539e+00  1.0038497e+01  2.5699127e+00  8.2954264e-01
  3.7012458e+00  4.3842115e+00  9.4881525e+00  1.6746927e+00
  1.6582866e+00  8.3782732e-01  9.4319105e+00  5.1845031e+00
  2.1975474e+00  5.9735575e+00  5.1665258e+00  1.4493895e+00
  6.2677717e+00  4.1860600e+00  2.3406126e+00  4.6920013e+00
  2.4602680e+00  1.8382215e-01  8.1364155e+00  2.1282406e+00
  4.3198166e+00  3.8556888e+00  3.0953395e+00  2.0980477e+00
  7.0904751e+00  4.5792561e+00  1.5180420e+00  5.2322884e+00
  1.6906004e+00  3.4523757e+00  9.2397833e+00  3.1109934e+00
  5.6705618e+00  4.6444817e+00  3.1917083e-01  6.5684190e+00
  8.3731728e+00  8.2005799e-01  7.7472749e+00  6.1426744e+00
  9.0258293e+00  1.9026639e+00  5.0159683e+00  7.1628571e+00
  3.6749166e-01  8.1895790e+00  9.1702557e+00  4.7452621e+00
  5.5244966e+00  3.4259295e+00 -3.8653705e-01  1.5138631e+00
  9.9786110e+00  9.1852169e+00  4.4421902e+00  9.3922234e+00
  3.7232175e+00  3.2589843e+00  1.1038141e+00  9.0240583e+00
  9.8830881e+00  3.2112100e+00  6.7645154e+00  2.0692959e+00
  1.9215341e+00  4.1629920e+00  5.6662488e+00  1.0191170e+00
  8.1997690e+00  4.2267418e+00  2.4319410e+00  5.7200866e+00
  8.0722961e+00  2.4742532e+00  7.0894265e+00  5.3278923e+00
  1.0044619e+01  8.3759117e+00  3.2966197e+00  4.7162291e-01
  6.4246984e+00  2.2235727e+00  1.1463121e+00  9.4869194e+00
  1.0011403e+01  2.7460160e+00  3.0180008e+00  1.6303557e+00
  4.6730700e+00  8.5651293e+00  7.2287169e+00  8.3861351e+00
  1.8064102e+00  8.7352962e+00  9.3292542e+00  2.5777404e+00
  6.9082699e+00  4.0963984e+00  3.0394080e+00  9.2868967e+00
  2.7766879e+00  1.1135416e+00  3.6845717e+00  4.4099135e+00
  7.6844082e+00  9.2539692e+00  7.1119728e+00  9.6887465e+00
  4.6995382e+00  6.9757147e+00  2.5853448e+00  1.0008667e+00
  3.5760036e+00  7.2550845e+00  3.6595097e+00  8.5721102e+00
  7.2344618e+00  1.7795635e+00  3.6952438e+00  8.6415672e+00
  2.8796201e+00  3.2917647e+00  3.5199354e+00  8.5686522e+00
  3.2441050e-01 -1.1358352e+00  7.5423007e+00  4.0973043e+00
  6.3126531e+00  3.1274986e+00  9.0573235e+00  8.3398399e+00
  6.6881323e-01  9.2919168e+00  9.8882742e+00  1.0048504e+01
  6.7430601e+00  1.2440386e+00  6.1340365e+00  2.8771806e+00
  1.0491127e+01  7.2151108e+00  5.7920051e+00  6.8901110e+00
  4.4353824e+00  4.2684994e+00  9.8386202e+00  3.4906733e+00
  2.9775652e-01  3.9917080e+00  7.7165737e+00  7.2466598e+00
  9.1334028e+00  7.1222425e-01  8.8155575e+00  3.3764515e+00
  6.1065130e+00  2.9506960e+00  1.5282468e+00  1.4799273e+00
  7.3071499e+00  2.5342268e-01  1.4136052e+00  2.5270680e-01
  9.0039234e+00  5.8526845e+00  1.5248462e+00  7.5311608e+00
  9.0430193e+00  9.7280302e+00  9.5882683e+00  1.8279113e+00
  5.8205943e+00  1.7759073e+00  5.7603154e+00  1.6760135e+00
  1.3962649e+00  4.5643988e+00  5.0343242e+00  9.7944088e+00
 -4.4693673e-01  2.0349143e+00  8.5404758e+00  5.6370296e+00
  7.6596098e+00  5.5074930e+00  7.6983194e+00  4.7940998e+00
  2.0215008e+00  7.7181168e+00  5.6737719e+00  1.4210645e+00
  7.9060745e+00  9.2988052e+00  3.9135573e+00  3.8081381e+00
  5.2250624e-01  6.9113393e+00  9.2877436e+00  5.9380788e-01
  4.5916810e+00  7.4059099e-01  2.8883523e-01  6.5858841e+00
  9.1256752e+00  3.1024201e+00  9.1136103e+00  1.0505362e+00
  1.7518139e+00  9.2434750e+00  4.9485741e+00  6.5655003e+00
  4.4320902e-01  1.2777932e+00  5.3725511e-01  8.7559137e+00
  1.4405452e+00  8.4731607e+00  3.5494101e+00  8.0048971e+00
  2.6287205e+00  3.0197628e+00  8.6806625e-01  2.5819857e+00
  5.3563967e+00  7.7403202e+00  4.1256256e+00  9.8598423e+00
  5.5676341e+00 -6.0696220e-01  3.9227915e+00  6.5759339e+00
  2.7735803e+00  6.7311745e+00  4.7392740e+00  2.8594899e-01
  8.0733051e+00  6.5364513e+00  7.4538708e-01  4.6062713e+00
  3.3927956e+00  9.1859818e+00  2.4066575e+00  9.4343519e+00
  5.3255687e+00  3.2345712e+00  8.2187128e+00  7.3158340e+00]
Epoch 1/1000
2023-10-03 01:12:03.685 
Epoch 1/1000 
	 loss: 1866.2371, MinusLogProbMetric: 1866.2371, val_loss: 630.1202, val_MinusLogProbMetric: 630.1202

Epoch 1: val_loss improved from inf to 630.12024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 34s - loss: 1866.2371 - MinusLogProbMetric: 1866.2371 - val_loss: 630.1202 - val_MinusLogProbMetric: 630.1202 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 2/1000
2023-10-03 01:12:14.464 
Epoch 2/1000 
	 loss: 557.4061, MinusLogProbMetric: 557.4061, val_loss: 525.8283, val_MinusLogProbMetric: 525.8283

Epoch 2: val_loss improved from 630.12024 to 525.82831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 557.4061 - MinusLogProbMetric: 557.4061 - val_loss: 525.8283 - val_MinusLogProbMetric: 525.8283 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 3/1000
2023-10-03 01:12:25.284 
Epoch 3/1000 
	 loss: 510.1457, MinusLogProbMetric: 510.1457, val_loss: 499.4075, val_MinusLogProbMetric: 499.4075

Epoch 3: val_loss improved from 525.82831 to 499.40753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 510.1457 - MinusLogProbMetric: 510.1457 - val_loss: 499.4075 - val_MinusLogProbMetric: 499.4075 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 4/1000
2023-10-03 01:12:35.829 
Epoch 4/1000 
	 loss: 489.8230, MinusLogProbMetric: 489.8230, val_loss: 489.7421, val_MinusLogProbMetric: 489.7421

Epoch 4: val_loss improved from 499.40753 to 489.74207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 489.8230 - MinusLogProbMetric: 489.8230 - val_loss: 489.7421 - val_MinusLogProbMetric: 489.7421 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 5/1000
2023-10-03 01:12:46.774 
Epoch 5/1000 
	 loss: 476.6364, MinusLogProbMetric: 476.6364, val_loss: 479.9482, val_MinusLogProbMetric: 479.9482

Epoch 5: val_loss improved from 489.74207 to 479.94821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 476.6364 - MinusLogProbMetric: 476.6364 - val_loss: 479.9482 - val_MinusLogProbMetric: 479.9482 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 6/1000
2023-10-03 01:12:57.798 
Epoch 6/1000 
	 loss: 468.3004, MinusLogProbMetric: 468.3004, val_loss: 472.9848, val_MinusLogProbMetric: 472.9848

Epoch 6: val_loss improved from 479.94821 to 472.98477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 468.3004 - MinusLogProbMetric: 468.3004 - val_loss: 472.9848 - val_MinusLogProbMetric: 472.9848 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 7/1000
2023-10-03 01:13:09.188 
Epoch 7/1000 
	 loss: 461.3144, MinusLogProbMetric: 461.3144, val_loss: 456.2972, val_MinusLogProbMetric: 456.2972

Epoch 7: val_loss improved from 472.98477 to 456.29724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 461.3144 - MinusLogProbMetric: 461.3144 - val_loss: 456.2972 - val_MinusLogProbMetric: 456.2972 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 8/1000
2023-10-03 01:13:20.284 
Epoch 8/1000 
	 loss: 458.5381, MinusLogProbMetric: 458.5381, val_loss: 459.3054, val_MinusLogProbMetric: 459.3054

Epoch 8: val_loss did not improve from 456.29724
196/196 - 11s - loss: 458.5381 - MinusLogProbMetric: 458.5381 - val_loss: 459.3054 - val_MinusLogProbMetric: 459.3054 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 9/1000
2023-10-03 01:13:30.941 
Epoch 9/1000 
	 loss: 452.5079, MinusLogProbMetric: 452.5079, val_loss: 445.7397, val_MinusLogProbMetric: 445.7397

Epoch 9: val_loss improved from 456.29724 to 445.73975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 452.5079 - MinusLogProbMetric: 452.5079 - val_loss: 445.7397 - val_MinusLogProbMetric: 445.7397 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 10/1000
2023-10-03 01:13:41.573 
Epoch 10/1000 
	 loss: 452.3582, MinusLogProbMetric: 452.3582, val_loss: 449.6639, val_MinusLogProbMetric: 449.6639

Epoch 10: val_loss did not improve from 445.73975
196/196 - 10s - loss: 452.3582 - MinusLogProbMetric: 452.3582 - val_loss: 449.6639 - val_MinusLogProbMetric: 449.6639 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 11/1000
2023-10-03 01:13:52.234 
Epoch 11/1000 
	 loss: 445.3220, MinusLogProbMetric: 445.3220, val_loss: 442.3300, val_MinusLogProbMetric: 442.3300

Epoch 11: val_loss improved from 445.73975 to 442.32999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 12s - loss: 445.3220 - MinusLogProbMetric: 445.3220 - val_loss: 442.3300 - val_MinusLogProbMetric: 442.3300 - lr: 0.0010 - 12s/epoch - 62ms/step
Epoch 12/1000
2023-10-03 01:14:04.531 
Epoch 12/1000 
	 loss: 445.2965, MinusLogProbMetric: 445.2965, val_loss: 443.0666, val_MinusLogProbMetric: 443.0666

Epoch 12: val_loss did not improve from 442.32999
196/196 - 11s - loss: 445.2965 - MinusLogProbMetric: 445.2965 - val_loss: 443.0666 - val_MinusLogProbMetric: 443.0666 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 13/1000
2023-10-03 01:14:14.976 
Epoch 13/1000 
	 loss: 439.7247, MinusLogProbMetric: 439.7247, val_loss: 436.9601, val_MinusLogProbMetric: 436.9601

Epoch 13: val_loss improved from 442.32999 to 436.96011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 439.7247 - MinusLogProbMetric: 439.7247 - val_loss: 436.9601 - val_MinusLogProbMetric: 436.9601 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 14/1000
2023-10-03 01:14:25.750 
Epoch 14/1000 
	 loss: 439.4242, MinusLogProbMetric: 439.4242, val_loss: 431.9013, val_MinusLogProbMetric: 431.9013

Epoch 14: val_loss improved from 436.96011 to 431.90128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 439.4242 - MinusLogProbMetric: 439.4242 - val_loss: 431.9013 - val_MinusLogProbMetric: 431.9013 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 15/1000
2023-10-03 01:14:36.485 
Epoch 15/1000 
	 loss: 439.0759, MinusLogProbMetric: 439.0759, val_loss: 462.1051, val_MinusLogProbMetric: 462.1051

Epoch 15: val_loss did not improve from 431.90128
196/196 - 10s - loss: 439.0759 - MinusLogProbMetric: 439.0759 - val_loss: 462.1051 - val_MinusLogProbMetric: 462.1051 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 16/1000
2023-10-03 01:14:47.009 
Epoch 16/1000 
	 loss: 440.7886, MinusLogProbMetric: 440.7886, val_loss: 432.2384, val_MinusLogProbMetric: 432.2384

Epoch 16: val_loss did not improve from 431.90128
196/196 - 11s - loss: 440.7886 - MinusLogProbMetric: 440.7886 - val_loss: 432.2384 - val_MinusLogProbMetric: 432.2384 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 17/1000
2023-10-03 01:14:57.321 
Epoch 17/1000 
	 loss: 432.0638, MinusLogProbMetric: 432.0638, val_loss: 432.8001, val_MinusLogProbMetric: 432.8001

Epoch 17: val_loss did not improve from 431.90128
196/196 - 10s - loss: 432.0638 - MinusLogProbMetric: 432.0638 - val_loss: 432.8001 - val_MinusLogProbMetric: 432.8001 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 18/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 77: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-03 01:15:02.292 
Epoch 18/1000 
	 loss: nan, MinusLogProbMetric: 1970028930907766784.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 18: val_loss did not improve from 431.90128
196/196 - 5s - loss: nan - MinusLogProbMetric: 1970028930907766784.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 5s/epoch - 25ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 344.
===========
Train data generated in 0.63 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.8975353,  7.5487056,  5.3180313, ...,  7.979155 ,  2.5296934,
         6.754205 ],
       [ 7.704416 ,  4.391162 ,  5.173328 , ...,  4.413081 ,  7.8835597,
         6.9377937],
       [ 8.109386 ,  4.318234 ,  5.3090944, ...,  3.0852916,  8.464321 ,
         6.8667717],
       ...,
       [ 5.740096 ,  5.972522 ,  6.2622604, ..., 10.652689 ,  2.5040557,
         6.768634 ],
       [ 6.2015314,  0.5028701,  4.7167926, ...,  4.818747 ,  6.309916 ,
         4.885091 ],
       [ 5.1562834,  7.5026083,  5.1422505, ...,  8.459682 ,  2.4006338,
         6.775316 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_344
self.data_kwargs: {'seed': 541}
self.x_data: [[ 8.944346    5.071281    5.301635   ...  3.2345712   8.218713
   7.315834  ]
 [ 7.6530247   4.23377     5.229719   ...  2.9577737   7.8780494
   6.7679396 ]
 [ 8.655344    4.436833    5.191878   ...  4.1375995   8.791849
   7.4483323 ]
 ...
 [ 6.3798065  -0.45195007  4.9196353  ...  4.8821535   6.055903
   6.0125017 ]
 [ 7.756657    4.538881    5.3200464  ...  2.765904    9.36033
   7.000672  ]
 [ 6.0584593   7.6354427   6.779557   ...  9.598122    2.6918387
   6.656139  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_56 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7fbb349a3f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb349dc220>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb349dc220>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb349a0c40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb76c7553f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb76c756290>, <keras.callbacks.ModelCheckpoint object at 0x7fb76c757a30>, <keras.callbacks.EarlyStopping object at 0x7fb76c754ee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb76c756680>, <keras.callbacks.TerminateOnNaN object at 0x7fb76c756e90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.8975353,  7.5487056,  5.3180313, ...,  7.979155 ,  2.5296934,
         6.754205 ],
       [ 7.704416 ,  4.391162 ,  5.173328 , ...,  4.413081 ,  7.8835597,
         6.9377937],
       [ 8.109386 ,  4.318234 ,  5.3090944, ...,  3.0852916,  8.464321 ,
         6.8667717],
       ...,
       [ 5.740096 ,  5.972522 ,  6.2622604, ..., 10.652689 ,  2.5040557,
         6.768634 ],
       [ 6.2015314,  0.5028701,  4.7167926, ...,  4.818747 ,  6.309916 ,
         4.885091 ],
       [ 5.1562834,  7.5026083,  5.1422505, ...,  8.459682 ,  2.4006338,
         6.775316 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 344/360 with hyperparameters:
timestamp = 2023-10-03 01:15:05.320550
ndims = 1000
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 8.9443464e+00  5.0712810e+00  5.3016348e+00  2.5552166e+00
  5.8691235e+00  1.8351557e+00  5.6839700e+00  1.7029351e+00
  1.6637374e+00  4.2301202e+00  5.1187129e+00  1.5429424e+00
  8.5022074e-01  5.4076612e-03  6.4514995e-02  2.8798556e+00
  4.5537715e+00  8.2742701e+00  5.2343698e+00  9.2561474e+00
 -6.3965023e-01  1.9017107e+00  3.2529631e+00  5.8927112e+00
  8.1674862e+00  3.4725866e+00  2.8051417e+00  5.7039076e-01
  8.7409286e+00  4.2301049e+00  6.3036456e+00  1.0560735e+01
 -7.2515529e-01  1.4195669e+00  7.2751231e+00  8.1002569e+00
  8.2396307e+00  4.4850531e+00  9.8329144e+00  3.6185634e+00
  5.9540081e+00  1.3118193e+00  4.2508869e+00  7.3485479e+00
  2.4983881e+00  9.7614374e+00  7.1622887e+00  2.8269064e+00
  7.6845107e+00  8.6501408e-01  9.8061056e+00 -3.0315861e-01
  8.5370369e+00  4.0063114e+00  7.6050367e+00  1.2078162e+00
  1.8064864e+00  7.4926944e+00  6.1466155e+00  1.8077860e+00
  4.0056338e+00  3.7202733e+00  5.6709218e+00  7.4053359e+00
  8.8831940e+00  4.4600101e+00  3.8643827e+00  2.1389792e+00
  7.8185973e+00  6.8649607e+00  5.1795917e+00  5.0133357e+00
  6.3993440e+00  1.7259912e+00 -5.3211361e-02  2.7235508e+00
  6.2138977e+00  4.8363299e+00  8.0572672e+00  8.1861324e+00
  5.0370812e+00  9.4596857e-01  8.6785088e+00  6.3420782e+00
  8.5438375e+00  7.7141809e+00  9.4851608e+00  3.6518431e+00
  7.0112877e+00  3.4836280e+00  5.5378804e+00  3.1639743e+00
  3.5213878e+00  9.4826164e+00  2.2823873e+00  2.0409453e+00
  4.4045577e+00  2.1437628e+00  4.6174788e+00  8.6706190e+00
  1.3972132e+00  4.1895037e+00  4.3430986e+00  5.4401283e+00
  9.7749367e+00  6.8143382e+00  6.8082824e+00  1.0939198e+01
  4.4761267e+00  2.8583031e+00  8.0687547e-01  1.6127855e+00
  3.0704210e+00  8.1066191e-01  7.7518625e+00  5.3930078e+00
  6.5707617e+00  7.7133188e+00  2.3429046e+00  2.8573308e+00
  7.0358224e+00  3.3356471e+00  1.1319630e-01  3.9435978e+00
  9.2751169e+00  4.1005511e+00  2.7870104e+00  9.8699589e+00
  4.6794405e+00  3.2451954e+00  8.7666917e-01  8.2957745e+00
  7.3025146e+00  9.4455490e+00  2.2730801e+00  1.3725048e+00
  7.2411389e+00  1.6464136e+00  2.7701192e+00  2.3040595e+00
  8.2030325e+00  6.8112426e+00  5.2378827e-01  4.6661735e+00
  8.6955223e+00  5.4876552e+00  5.7896667e+00  1.8971643e+00
  8.5775633e+00  5.5975456e+00  7.5397387e+00  1.0131255e+01
  3.9538219e+00  8.4712620e+00  1.9050634e-01  1.1027322e+00
  5.7999635e+00  3.1462212e+00  2.6874499e+00  3.1894717e+00
  3.2590761e+00  7.7746787e+00  6.2896113e+00  7.8990769e+00
  9.0200062e+00  8.5058842e+00  5.2181306e+00  4.2223444e+00
  8.4943619e+00  6.3309250e+00  2.2411201e+00  5.3274450e+00
  5.1067290e+00  9.8373957e+00  8.6932826e-01  6.0806484e+00
  6.2697802e+00  2.7620327e+00  9.9388742e+00  9.1199150e+00
  9.0355330e+00  2.3999453e-01  6.0671291e+00  4.1604033e+00
  9.0283689e+00  6.2536740e+00  5.2465343e+00  1.1988592e+00
  4.4932823e+00  6.3174534e+00  9.6449385e+00  6.4151430e+00
  9.6748543e+00  1.0681762e+01  6.5413651e+00  4.8880482e+00
  4.6189990e+00  8.0417252e+00  1.5292343e-01  1.1392257e+00
  1.2850125e+00  7.6761832e+00  3.2401102e+00  6.3397503e+00
  5.6845374e+00  3.4693668e+00  5.5460801e+00  5.9178486e+00
  9.4156847e+00  5.1198149e+00  6.1204705e+00  4.3263590e-01
  8.7820969e+00  5.6329536e+00  7.2442002e+00 -2.2960544e-02
  4.9573855e+00  2.2288504e+00  2.6121609e+00  7.4535289e+00
  5.4845848e+00  4.9085340e+00 -2.4640819e-01  9.6421938e+00
  8.9332962e+00  2.0278876e+00 -1.9537413e-01  5.4330840e+00
  9.6458216e+00  9.3649035e+00  5.6065726e+00  3.7719359e+00
  3.5931334e+00  9.0328960e+00  6.7254982e+00  1.4752012e+00
  1.9177732e+00  2.7959094e+00  1.3787949e+00  4.5886636e+00
  1.1515390e+01  7.3938780e+00  8.5164428e-01  1.2635899e-01
  3.6772094e+00  3.8027265e+00  9.1101971e+00  9.5930605e+00
  4.0264854e+00  6.9286065e+00  9.9545469e+00  7.8195210e+00
  5.6409397e+00  8.4713084e-01  2.5798862e+00  8.9097824e+00
  1.6849433e+00  8.2721701e+00  7.1689720e+00  4.1106219e+00
  5.0035114e+00  5.0525937e+00  6.7575526e+00  5.0666924e+00
  1.0737674e+00  7.3246279e+00  3.6748936e+00  9.9005594e+00
  3.1399076e+00  2.8666408e+00  2.2917109e+00  6.0727320e+00
  8.9826994e+00  1.0254667e+00  2.3594720e+00  5.8431158e+00
  4.7857404e+00  4.9822187e+00  9.7560043e+00  9.7564678e+00
  7.3872032e+00  7.1467991e+00  7.0199881e+00  4.7391133e+00
  2.8807340e+00  6.5354433e+00  5.8123102e+00  8.8130550e+00
  5.5923468e-01  8.7705822e+00  5.3062606e+00 -5.5353129e-01
  3.3890393e+00  6.6493964e+00  5.6508560e+00  1.1202197e+01
  2.9007564e+00  4.1648722e+00  8.0866766e+00  5.7469816e+00
  6.0169592e+00  7.9898276e+00  6.7868557e+00  4.6625357e+00
  1.8561345e+00  4.7718992e+00  9.0640554e+00  2.8347461e+00
  9.0894794e+00  4.9920583e+00  6.2453890e+00  8.4376707e+00
  6.6637831e+00  9.0843725e-01  1.1660665e+01  3.0299661e+00
  7.1950002e+00  4.9914813e+00  4.6181540e+00  2.0924745e+00
  6.6776500e+00  1.1897669e+00  1.0171629e+01  8.9001732e+00
  5.4110227e+00  4.4263487e+00  1.9841802e+00  5.8741927e+00
  7.4450426e+00  2.7903662e+00  5.3433504e+00  3.0894151e+00
  3.5106759e+00  2.0232074e+00  8.7810221e+00  7.4130011e+00
  7.7140536e+00  1.0643046e+00  2.2568712e+00  8.9773560e+00
  9.1454113e-01  2.0186009e+00  6.6545320e+00  6.1442413e+00
  8.3838558e+00  1.0342110e+01  1.1413314e+00  2.9067490e+00
  4.2822914e+00  1.6280425e-01  4.8523468e-01  1.7549541e+00
  4.8714666e+00  2.4402614e+00  8.8096838e+00  3.3564243e+00
  8.6558800e+00  9.9410667e+00  4.1167269e+00  4.1255851e+00
  2.9219372e+00  7.6535926e+00  2.6355727e+00  4.2073326e+00
  4.6453238e+00  6.3675723e+00  9.8485737e+00  2.3905480e+00
  4.2612143e+00  3.0223722e+00  4.4770617e+00  1.6278577e+00
  1.9609565e+00  1.0535285e+01  2.1226084e+00  3.3767881e+00
  9.3117971e+00  7.5796928e+00  5.1729965e+00  7.8598866e+00
  2.2923901e+00  4.4321613e+00  1.4233030e+00  2.3377724e+00
  1.0065243e+00  3.5456564e+00  4.1227231e+00  9.7088566e+00
  2.8547204e+00  4.8324518e+00  7.3718147e+00  7.8783011e+00
  7.8638425e+00  2.9144902e+00  4.9608488e+00  1.9803571e+00
  1.5853404e+00  4.3583817e+00  7.4199648e+00  4.9660010e+00
  1.0190818e+01  8.1329041e+00 -2.8760788e-01  6.0783772e+00
  6.2244406e+00  9.6368160e+00 -1.8620440e-01  4.6117182e+00
  2.4262204e+00  5.2838308e-01  5.2863402e+00  8.3399858e+00
  7.2275910e+00  1.4571591e-01  4.0400686e+00  4.7784710e-01
  2.4910257e+00  1.9078600e+00  1.8931444e+00  4.1630411e+00
  2.4404171e+00  1.0232767e+01  3.5966356e+00  8.9379129e+00
  2.7483301e+00  7.5434041e+00  5.1080024e-01  4.3487568e+00
 -5.6165874e-01  7.8553061e+00  9.3950367e+00  4.2870969e-01
  9.0893593e+00  7.4221296e+00  4.3771329e+00  9.0878849e+00
  2.0663891e+00  3.6473520e+00  9.9999070e-01  5.3237896e+00
 -5.4919869e-01  4.6365285e+00  6.3283267e+00  9.2732124e+00
  1.5552996e+00  7.1880951e+00  6.2271900e+00  5.5313535e+00
  5.8510990e+00  1.5392092e+00  5.0540504e+00  4.4184580e+00
  9.2081413e+00  3.3830662e+00  5.7834134e+00  4.1243353e+00
  8.2975473e+00  2.5386639e+00  7.4207544e-01  6.3304234e+00
  4.2580895e+00  5.8122301e+00 -2.1570435e-01  9.1932268e+00
  8.6552963e+00  7.5708799e+00 -1.9443253e-01  3.7197874e+00
  9.8743181e+00  6.0263009e+00  1.6372683e+00  4.4369135e+00
  9.7833328e+00  3.9919968e+00  4.3431216e-01  6.2208257e+00
  2.6909373e+00  9.8785430e-01  4.0438128e+00  5.6429973e+00
 -6.1843389e-01  7.0297160e+00  5.4341021e+00  8.4860640e+00
  3.4566321e+00  5.7342043e+00  7.6334524e+00  9.0038929e+00
  3.7223434e+00  6.0845118e+00  6.5700788e+00  1.2773844e+00
  7.2991037e+00  3.5088022e+00  5.9354343e+00  5.4178343e+00
  7.0744524e+00  7.4856811e+00  2.1827915e+00  6.6159654e+00
  8.7040949e+00  3.2422276e+00  7.5529513e+00  6.1994824e+00
  8.9478226e+00  1.9693226e+00  1.8612541e+00  3.6062963e+00
  2.4518812e+00  7.2348843e+00  7.3313851e+00  6.7718916e+00
  2.0351167e+00  8.5850897e+00  1.0028167e+01  5.4356914e+00
  6.5442944e+00  4.6184292e+00  3.1283219e+00  2.8901393e+00
  1.9263090e+00  1.1836938e+00  9.0665789e+00  8.3283224e+00
  4.7910285e+00  4.4820914e+00  9.3514023e+00  2.1546044e+00
  5.8805389e+00  4.3903738e-01  2.2247140e+00  5.9088330e+00
  4.9584928e+00  6.2537398e+00  8.6888113e+00  1.0049231e+01
  7.0398006e+00  3.3480191e+00  8.1408710e+00  1.6674470e+00
  6.3164220e+00  4.5155339e+00  1.6805238e+00  9.9202871e+00
  3.6205506e+00  6.6217365e+00  8.2141304e+00  5.4286766e+00
  1.6267601e+00  7.6978483e+00  2.9924681e+00  9.8932133e+00
  8.2759218e+00  8.5018530e+00  3.1936233e+00  7.7755070e+00
  9.2961884e+00  5.5356965e+00  5.6126337e+00 -2.4630746e-01
  7.0293112e+00  8.5386944e+00  3.1930950e+00  2.9155462e+00
  4.4143424e+00  4.8297209e-01  8.3480543e-01  3.1123245e+00
  2.8908000e+00  4.0284605e+00  5.0597181e+00  1.0072316e+01
  9.4932566e+00  2.0429962e+00  8.8413343e+00  8.5795814e-01
  4.9350314e+00  9.1259652e-01  3.0097170e+00  7.9393425e+00
  4.2711453e+00  7.9923959e+00  6.9667177e+00  9.4181356e+00
  6.2381592e+00  8.0183401e+00  3.3664768e+00  3.8059332e+00
  4.0490818e+00  4.4446468e+00  1.5931008e+00  5.2192802e+00
  2.2268224e+00  8.7408686e+00 -1.8847406e-02  4.7761617e+00
  2.8030579e+00 -1.7537308e-01  8.0809097e+00  7.4201512e+00
  2.0487733e+00  5.8333006e+00  8.1179962e+00  8.2311239e+00
  3.1251311e-03  6.8018031e+00  7.6883483e+00  8.7783766e+00
  5.6571703e+00  4.3441300e+00  8.5495930e+00  8.1494551e+00
  5.4555826e+00  3.7778921e+00  7.5609541e+00  4.7757192e+00
  1.0579890e+00  4.9656453e+00  8.1120148e+00  4.6216760e+00
 -3.6158833e-01  6.1140265e+00  9.9045410e+00  7.3525467e+00
  6.7491326e+00  9.5566406e+00  8.2156792e+00  4.7660222e+00
  4.5311189e+00  7.3306732e+00  8.8040447e+00  7.3644763e-01
  7.6932001e+00  3.3370900e+00  7.2479844e+00  7.7876225e+00
  5.2245250e+00  1.5497253e+00  4.2543964e+00  7.5986636e-01
  6.9275203e+00  9.7100849e+00  9.2669239e+00  3.2498708e+00
  5.3699269e+00  2.9394846e+00  5.2818131e+00  3.1141367e+00
  6.7859550e+00  2.3411243e+00  9.6228018e+00  6.6613054e+00
  7.2898990e-01  4.3681574e-01  1.0014687e+01  2.1311550e+00
  4.1314063e+00  2.9794476e+00  9.2801170e+00  6.6272178e+00
  4.6284294e+00  4.4226980e+00  3.2610605e+00  7.1748915e+00
  4.2616057e+00  1.8846684e+00  6.1376987e+00  3.2141211e+00
  6.5789557e+00  4.3955379e+00  4.7454882e+00  7.8042412e+00
  3.6127386e+00  9.4474850e+00  6.7431140e+00  8.3214483e+00
  9.2100325e+00  3.6113214e+00  5.5930977e+00  7.0742140e+00
  6.1205058e+00  9.8638229e+00  2.9179771e+00  2.7246852e+00
  9.3549395e+00  8.8796721e+00  1.6090376e+00  5.6067185e+00
  7.3421440e+00  2.9167426e+00  4.6086540e+00  5.7381048e+00
  3.9722662e+00  7.5606790e+00  2.8954959e-01  3.4192889e+00
  4.4652858e+00  5.4416647e+00  8.0514631e+00  7.3737478e-01
  1.2092665e+00  7.6465349e+00  6.0600023e+00  6.4349012e+00
  9.8435640e+00  5.0693870e+00  1.9410030e+00  6.8551364e+00
  7.0065575e+00  5.7280030e+00  5.5391960e+00  9.0428343e+00
  1.1960058e+00  1.9434166e-01  1.5483990e+00  2.8434887e+00
  5.9591789e+00  6.2908230e+00  2.2022102e+00  4.9054837e+00
  4.8644104e+00  4.7432480e+00  8.4029760e+00  7.0184464e+00
  7.8207922e+00  1.6498985e+00  5.3174524e+00  8.2454939e+00
  1.6656384e+00  4.4284911e+00  5.6130257e+00  1.6552075e+00
  7.8019342e+00  7.7151400e-01  8.2651939e+00  4.7385693e+00
  4.0336847e+00  1.3455848e+00  4.3968825e+00  6.8709621e+00
 -8.9248103e-01  3.0087178e+00  8.5056953e+00  6.4442968e+00
  2.0375299e+00  2.2298772e+00  7.4714904e+00  2.7371094e+00
  1.4103539e+00  1.0038497e+01  2.5699127e+00  8.2954264e-01
  3.7012458e+00  4.3842115e+00  9.4881525e+00  1.6746927e+00
  1.6582866e+00  8.3782732e-01  9.4319105e+00  5.1845031e+00
  2.1975474e+00  5.9735575e+00  5.1665258e+00  1.4493895e+00
  6.2677717e+00  4.1860600e+00  2.3406126e+00  4.6920013e+00
  2.4602680e+00  1.8382215e-01  8.1364155e+00  2.1282406e+00
  4.3198166e+00  3.8556888e+00  3.0953395e+00  2.0980477e+00
  7.0904751e+00  4.5792561e+00  1.5180420e+00  5.2322884e+00
  1.6906004e+00  3.4523757e+00  9.2397833e+00  3.1109934e+00
  5.6705618e+00  4.6444817e+00  3.1917083e-01  6.5684190e+00
  8.3731728e+00  8.2005799e-01  7.7472749e+00  6.1426744e+00
  9.0258293e+00  1.9026639e+00  5.0159683e+00  7.1628571e+00
  3.6749166e-01  8.1895790e+00  9.1702557e+00  4.7452621e+00
  5.5244966e+00  3.4259295e+00 -3.8653705e-01  1.5138631e+00
  9.9786110e+00  9.1852169e+00  4.4421902e+00  9.3922234e+00
  3.7232175e+00  3.2589843e+00  1.1038141e+00  9.0240583e+00
  9.8830881e+00  3.2112100e+00  6.7645154e+00  2.0692959e+00
  1.9215341e+00  4.1629920e+00  5.6662488e+00  1.0191170e+00
  8.1997690e+00  4.2267418e+00  2.4319410e+00  5.7200866e+00
  8.0722961e+00  2.4742532e+00  7.0894265e+00  5.3278923e+00
  1.0044619e+01  8.3759117e+00  3.2966197e+00  4.7162291e-01
  6.4246984e+00  2.2235727e+00  1.1463121e+00  9.4869194e+00
  1.0011403e+01  2.7460160e+00  3.0180008e+00  1.6303557e+00
  4.6730700e+00  8.5651293e+00  7.2287169e+00  8.3861351e+00
  1.8064102e+00  8.7352962e+00  9.3292542e+00  2.5777404e+00
  6.9082699e+00  4.0963984e+00  3.0394080e+00  9.2868967e+00
  2.7766879e+00  1.1135416e+00  3.6845717e+00  4.4099135e+00
  7.6844082e+00  9.2539692e+00  7.1119728e+00  9.6887465e+00
  4.6995382e+00  6.9757147e+00  2.5853448e+00  1.0008667e+00
  3.5760036e+00  7.2550845e+00  3.6595097e+00  8.5721102e+00
  7.2344618e+00  1.7795635e+00  3.6952438e+00  8.6415672e+00
  2.8796201e+00  3.2917647e+00  3.5199354e+00  8.5686522e+00
  3.2441050e-01 -1.1358352e+00  7.5423007e+00  4.0973043e+00
  6.3126531e+00  3.1274986e+00  9.0573235e+00  8.3398399e+00
  6.6881323e-01  9.2919168e+00  9.8882742e+00  1.0048504e+01
  6.7430601e+00  1.2440386e+00  6.1340365e+00  2.8771806e+00
  1.0491127e+01  7.2151108e+00  5.7920051e+00  6.8901110e+00
  4.4353824e+00  4.2684994e+00  9.8386202e+00  3.4906733e+00
  2.9775652e-01  3.9917080e+00  7.7165737e+00  7.2466598e+00
  9.1334028e+00  7.1222425e-01  8.8155575e+00  3.3764515e+00
  6.1065130e+00  2.9506960e+00  1.5282468e+00  1.4799273e+00
  7.3071499e+00  2.5342268e-01  1.4136052e+00  2.5270680e-01
  9.0039234e+00  5.8526845e+00  1.5248462e+00  7.5311608e+00
  9.0430193e+00  9.7280302e+00  9.5882683e+00  1.8279113e+00
  5.8205943e+00  1.7759073e+00  5.7603154e+00  1.6760135e+00
  1.3962649e+00  4.5643988e+00  5.0343242e+00  9.7944088e+00
 -4.4693673e-01  2.0349143e+00  8.5404758e+00  5.6370296e+00
  7.6596098e+00  5.5074930e+00  7.6983194e+00  4.7940998e+00
  2.0215008e+00  7.7181168e+00  5.6737719e+00  1.4210645e+00
  7.9060745e+00  9.2988052e+00  3.9135573e+00  3.8081381e+00
  5.2250624e-01  6.9113393e+00  9.2877436e+00  5.9380788e-01
  4.5916810e+00  7.4059099e-01  2.8883523e-01  6.5858841e+00
  9.1256752e+00  3.1024201e+00  9.1136103e+00  1.0505362e+00
  1.7518139e+00  9.2434750e+00  4.9485741e+00  6.5655003e+00
  4.4320902e-01  1.2777932e+00  5.3725511e-01  8.7559137e+00
  1.4405452e+00  8.4731607e+00  3.5494101e+00  8.0048971e+00
  2.6287205e+00  3.0197628e+00  8.6806625e-01  2.5819857e+00
  5.3563967e+00  7.7403202e+00  4.1256256e+00  9.8598423e+00
  5.5676341e+00 -6.0696220e-01  3.9227915e+00  6.5759339e+00
  2.7735803e+00  6.7311745e+00  4.7392740e+00  2.8594899e-01
  8.0733051e+00  6.5364513e+00  7.4538708e-01  4.6062713e+00
  3.3927956e+00  9.1859818e+00  2.4066575e+00  9.4343519e+00
  5.3255687e+00  3.2345712e+00  8.2187128e+00  7.3158340e+00]
Epoch 1/1000
2023-10-03 01:15:40.714 
Epoch 1/1000 
	 loss: 2437.6807, MinusLogProbMetric: 2437.6807, val_loss: 447.3501, val_MinusLogProbMetric: 447.3501

Epoch 1: val_loss improved from inf to 447.35010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 36s - loss: 2437.6807 - MinusLogProbMetric: 2437.6807 - val_loss: 447.3501 - val_MinusLogProbMetric: 447.3501 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 2/1000
2023-10-03 01:15:51.912 
Epoch 2/1000 
	 loss: 434.5158, MinusLogProbMetric: 434.5158, val_loss: 429.0881, val_MinusLogProbMetric: 429.0881

Epoch 2: val_loss improved from 447.35010 to 429.08807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 434.5158 - MinusLogProbMetric: 434.5158 - val_loss: 429.0881 - val_MinusLogProbMetric: 429.0881 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 3/1000
2023-10-03 01:16:02.623 
Epoch 3/1000 
	 loss: 426.4386, MinusLogProbMetric: 426.4386, val_loss: 424.9639, val_MinusLogProbMetric: 424.9639

Epoch 3: val_loss improved from 429.08807 to 424.96387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 426.4386 - MinusLogProbMetric: 426.4386 - val_loss: 424.9639 - val_MinusLogProbMetric: 424.9639 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 4/1000
2023-10-03 01:16:13.535 
Epoch 4/1000 
	 loss: 422.7836, MinusLogProbMetric: 422.7836, val_loss: 421.6450, val_MinusLogProbMetric: 421.6450

Epoch 4: val_loss improved from 424.96387 to 421.64499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 422.7836 - MinusLogProbMetric: 422.7836 - val_loss: 421.6450 - val_MinusLogProbMetric: 421.6450 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 5/1000
2023-10-03 01:16:24.854 
Epoch 5/1000 
	 loss: 420.2268, MinusLogProbMetric: 420.2268, val_loss: 419.6824, val_MinusLogProbMetric: 419.6824

Epoch 5: val_loss improved from 421.64499 to 419.68243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 12s - loss: 420.2268 - MinusLogProbMetric: 420.2268 - val_loss: 419.6824 - val_MinusLogProbMetric: 419.6824 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 6/1000
2023-10-03 01:16:36.033 
Epoch 6/1000 
	 loss: 418.2258, MinusLogProbMetric: 418.2258, val_loss: 418.0341, val_MinusLogProbMetric: 418.0341

Epoch 6: val_loss improved from 419.68243 to 418.03406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 418.2258 - MinusLogProbMetric: 418.2258 - val_loss: 418.0341 - val_MinusLogProbMetric: 418.0341 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 7/1000
2023-10-03 01:16:46.798 
Epoch 7/1000 
	 loss: 416.6515, MinusLogProbMetric: 416.6515, val_loss: 416.0109, val_MinusLogProbMetric: 416.0109

Epoch 7: val_loss improved from 418.03406 to 416.01089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 416.6515 - MinusLogProbMetric: 416.6515 - val_loss: 416.0109 - val_MinusLogProbMetric: 416.0109 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 8/1000
2023-10-03 01:16:57.479 
Epoch 8/1000 
	 loss: 415.2784, MinusLogProbMetric: 415.2784, val_loss: 414.6579, val_MinusLogProbMetric: 414.6579

Epoch 8: val_loss improved from 416.01089 to 414.65790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 415.2784 - MinusLogProbMetric: 415.2784 - val_loss: 414.6579 - val_MinusLogProbMetric: 414.6579 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 9/1000
2023-10-03 01:17:07.997 
Epoch 9/1000 
	 loss: 414.1511, MinusLogProbMetric: 414.1511, val_loss: 414.3160, val_MinusLogProbMetric: 414.3160

Epoch 9: val_loss improved from 414.65790 to 414.31601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 414.1511 - MinusLogProbMetric: 414.1511 - val_loss: 414.3160 - val_MinusLogProbMetric: 414.3160 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 10/1000
2023-10-03 01:17:18.971 
Epoch 10/1000 
	 loss: 413.1021, MinusLogProbMetric: 413.1021, val_loss: 414.2629, val_MinusLogProbMetric: 414.2629

Epoch 10: val_loss improved from 414.31601 to 414.26288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 413.1021 - MinusLogProbMetric: 413.1021 - val_loss: 414.2629 - val_MinusLogProbMetric: 414.2629 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 11/1000
2023-10-03 01:17:29.892 
Epoch 11/1000 
	 loss: 412.3237, MinusLogProbMetric: 412.3237, val_loss: 412.6148, val_MinusLogProbMetric: 412.6148

Epoch 11: val_loss improved from 414.26288 to 412.61481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 412.3237 - MinusLogProbMetric: 412.3237 - val_loss: 412.6148 - val_MinusLogProbMetric: 412.6148 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 12/1000
2023-10-03 01:17:40.701 
Epoch 12/1000 
	 loss: 411.4594, MinusLogProbMetric: 411.4594, val_loss: 411.5521, val_MinusLogProbMetric: 411.5521

Epoch 12: val_loss improved from 412.61481 to 411.55206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 411.4594 - MinusLogProbMetric: 411.4594 - val_loss: 411.5521 - val_MinusLogProbMetric: 411.5521 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 13/1000
2023-10-03 01:17:51.371 
Epoch 13/1000 
	 loss: 410.8729, MinusLogProbMetric: 410.8729, val_loss: 410.5099, val_MinusLogProbMetric: 410.5099

Epoch 13: val_loss improved from 411.55206 to 410.50989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 410.8729 - MinusLogProbMetric: 410.8729 - val_loss: 410.5099 - val_MinusLogProbMetric: 410.5099 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 14/1000
2023-10-03 01:18:02.022 
Epoch 14/1000 
	 loss: 410.2781, MinusLogProbMetric: 410.2781, val_loss: 410.5551, val_MinusLogProbMetric: 410.5551

Epoch 14: val_loss did not improve from 410.50989
196/196 - 10s - loss: 410.2781 - MinusLogProbMetric: 410.2781 - val_loss: 410.5551 - val_MinusLogProbMetric: 410.5551 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 15/1000
2023-10-03 01:18:12.535 
Epoch 15/1000 
	 loss: 409.5620, MinusLogProbMetric: 409.5620, val_loss: 410.0794, val_MinusLogProbMetric: 410.0794

Epoch 15: val_loss improved from 410.50989 to 410.07941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 409.5620 - MinusLogProbMetric: 409.5620 - val_loss: 410.0794 - val_MinusLogProbMetric: 410.0794 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 16/1000
2023-10-03 01:18:23.441 
Epoch 16/1000 
	 loss: 409.0626, MinusLogProbMetric: 409.0626, val_loss: 409.0359, val_MinusLogProbMetric: 409.0359

Epoch 16: val_loss improved from 410.07941 to 409.03589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 409.0626 - MinusLogProbMetric: 409.0626 - val_loss: 409.0359 - val_MinusLogProbMetric: 409.0359 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 17/1000
2023-10-03 01:18:34.738 
Epoch 17/1000 
	 loss: 408.6344, MinusLogProbMetric: 408.6344, val_loss: 409.3058, val_MinusLogProbMetric: 409.3058

Epoch 17: val_loss did not improve from 409.03589
196/196 - 11s - loss: 408.6344 - MinusLogProbMetric: 408.6344 - val_loss: 409.3058 - val_MinusLogProbMetric: 409.3058 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 18/1000
2023-10-03 01:18:45.697 
Epoch 18/1000 
	 loss: 408.1381, MinusLogProbMetric: 408.1381, val_loss: 408.6454, val_MinusLogProbMetric: 408.6454

Epoch 18: val_loss improved from 409.03589 to 408.64545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 408.1381 - MinusLogProbMetric: 408.1381 - val_loss: 408.6454 - val_MinusLogProbMetric: 408.6454 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 19/1000
2023-10-03 01:18:57.170 
Epoch 19/1000 
	 loss: 407.6823, MinusLogProbMetric: 407.6823, val_loss: 408.7229, val_MinusLogProbMetric: 408.7229

Epoch 19: val_loss did not improve from 408.64545
196/196 - 11s - loss: 407.6823 - MinusLogProbMetric: 407.6823 - val_loss: 408.7229 - val_MinusLogProbMetric: 408.7229 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 20/1000
2023-10-03 01:19:07.876 
Epoch 20/1000 
	 loss: 407.3992, MinusLogProbMetric: 407.3992, val_loss: 407.8403, val_MinusLogProbMetric: 407.8403

Epoch 20: val_loss improved from 408.64545 to 407.84033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 407.3992 - MinusLogProbMetric: 407.3992 - val_loss: 407.8403 - val_MinusLogProbMetric: 407.8403 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 21/1000
2023-10-03 01:19:19.033 
Epoch 21/1000 
	 loss: 407.3644, MinusLogProbMetric: 407.3644, val_loss: 408.0121, val_MinusLogProbMetric: 408.0121

Epoch 21: val_loss did not improve from 407.84033
196/196 - 11s - loss: 407.3644 - MinusLogProbMetric: 407.3644 - val_loss: 408.0121 - val_MinusLogProbMetric: 408.0121 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 22/1000
2023-10-03 01:19:29.346 
Epoch 22/1000 
	 loss: 406.6367, MinusLogProbMetric: 406.6367, val_loss: 408.1570, val_MinusLogProbMetric: 408.1570

Epoch 22: val_loss did not improve from 407.84033
196/196 - 10s - loss: 406.6367 - MinusLogProbMetric: 406.6367 - val_loss: 408.1570 - val_MinusLogProbMetric: 408.1570 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 23/1000
2023-10-03 01:19:40.173 
Epoch 23/1000 
	 loss: 406.3159, MinusLogProbMetric: 406.3159, val_loss: 407.7161, val_MinusLogProbMetric: 407.7161

Epoch 23: val_loss improved from 407.84033 to 407.71613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 406.3159 - MinusLogProbMetric: 406.3159 - val_loss: 407.7161 - val_MinusLogProbMetric: 407.7161 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 24/1000
2023-10-03 01:19:51.310 
Epoch 24/1000 
	 loss: 406.0896, MinusLogProbMetric: 406.0896, val_loss: 406.8262, val_MinusLogProbMetric: 406.8262

Epoch 24: val_loss improved from 407.71613 to 406.82617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 406.0896 - MinusLogProbMetric: 406.0896 - val_loss: 406.8262 - val_MinusLogProbMetric: 406.8262 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 25/1000
2023-10-03 01:20:01.905 
Epoch 25/1000 
	 loss: 405.6698, MinusLogProbMetric: 405.6698, val_loss: 406.7386, val_MinusLogProbMetric: 406.7386

Epoch 25: val_loss improved from 406.82617 to 406.73859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 405.6698 - MinusLogProbMetric: 405.6698 - val_loss: 406.7386 - val_MinusLogProbMetric: 406.7386 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 26/1000
2023-10-03 01:20:12.753 
Epoch 26/1000 
	 loss: 405.6403, MinusLogProbMetric: 405.6403, val_loss: 405.4168, val_MinusLogProbMetric: 405.4168

Epoch 26: val_loss improved from 406.73859 to 405.41678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 405.6403 - MinusLogProbMetric: 405.6403 - val_loss: 405.4168 - val_MinusLogProbMetric: 405.4168 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 27/1000
2023-10-03 01:20:23.526 
Epoch 27/1000 
	 loss: 405.1314, MinusLogProbMetric: 405.1314, val_loss: 405.9093, val_MinusLogProbMetric: 405.9093

Epoch 27: val_loss did not improve from 405.41678
196/196 - 10s - loss: 405.1314 - MinusLogProbMetric: 405.1314 - val_loss: 405.9093 - val_MinusLogProbMetric: 405.9093 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 28/1000
2023-10-03 01:20:34.009 
Epoch 28/1000 
	 loss: 404.8314, MinusLogProbMetric: 404.8314, val_loss: 405.4165, val_MinusLogProbMetric: 405.4165

Epoch 28: val_loss improved from 405.41678 to 405.41647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 404.8314 - MinusLogProbMetric: 404.8314 - val_loss: 405.4165 - val_MinusLogProbMetric: 405.4165 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 29/1000
2023-10-03 01:20:44.810 
Epoch 29/1000 
	 loss: 404.8638, MinusLogProbMetric: 404.8638, val_loss: 405.1070, val_MinusLogProbMetric: 405.1070

Epoch 29: val_loss improved from 405.41647 to 405.10696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 404.8638 - MinusLogProbMetric: 404.8638 - val_loss: 405.1070 - val_MinusLogProbMetric: 405.1070 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 30/1000
2023-10-03 01:20:55.691 
Epoch 30/1000 
	 loss: 404.2535, MinusLogProbMetric: 404.2535, val_loss: 405.2783, val_MinusLogProbMetric: 405.2783

Epoch 30: val_loss did not improve from 405.10696
196/196 - 10s - loss: 404.2535 - MinusLogProbMetric: 404.2535 - val_loss: 405.2783 - val_MinusLogProbMetric: 405.2783 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 31/1000
2023-10-03 01:21:06.112 
Epoch 31/1000 
	 loss: 404.2244, MinusLogProbMetric: 404.2244, val_loss: 404.5974, val_MinusLogProbMetric: 404.5974

Epoch 31: val_loss improved from 405.10696 to 404.59741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 404.2244 - MinusLogProbMetric: 404.2244 - val_loss: 404.5974 - val_MinusLogProbMetric: 404.5974 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 32/1000
2023-10-03 01:21:16.974 
Epoch 32/1000 
	 loss: 404.0686, MinusLogProbMetric: 404.0686, val_loss: 405.3458, val_MinusLogProbMetric: 405.3458

Epoch 32: val_loss did not improve from 404.59741
196/196 - 10s - loss: 404.0686 - MinusLogProbMetric: 404.0686 - val_loss: 405.3458 - val_MinusLogProbMetric: 405.3458 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 33/1000
2023-10-03 01:21:27.664 
Epoch 33/1000 
	 loss: 403.5950, MinusLogProbMetric: 403.5950, val_loss: 408.5523, val_MinusLogProbMetric: 408.5523

Epoch 33: val_loss did not improve from 404.59741
196/196 - 11s - loss: 403.5950 - MinusLogProbMetric: 403.5950 - val_loss: 408.5523 - val_MinusLogProbMetric: 408.5523 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 34/1000
2023-10-03 01:21:38.596 
Epoch 34/1000 
	 loss: 403.6136, MinusLogProbMetric: 403.6136, val_loss: 404.1195, val_MinusLogProbMetric: 404.1195

Epoch 34: val_loss improved from 404.59741 to 404.11951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 12s - loss: 403.6136 - MinusLogProbMetric: 403.6136 - val_loss: 404.1195 - val_MinusLogProbMetric: 404.1195 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 35/1000
2023-10-03 01:21:49.905 
Epoch 35/1000 
	 loss: 403.3990, MinusLogProbMetric: 403.3990, val_loss: 403.8563, val_MinusLogProbMetric: 403.8563

Epoch 35: val_loss improved from 404.11951 to 403.85626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 403.3990 - MinusLogProbMetric: 403.3990 - val_loss: 403.8563 - val_MinusLogProbMetric: 403.8563 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 36/1000
2023-10-03 01:22:00.841 
Epoch 36/1000 
	 loss: 403.1065, MinusLogProbMetric: 403.1065, val_loss: 403.5426, val_MinusLogProbMetric: 403.5426

Epoch 36: val_loss improved from 403.85626 to 403.54260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 403.1065 - MinusLogProbMetric: 403.1065 - val_loss: 403.5426 - val_MinusLogProbMetric: 403.5426 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 37/1000
2023-10-03 01:22:11.989 
Epoch 37/1000 
	 loss: 403.0222, MinusLogProbMetric: 403.0222, val_loss: 403.3719, val_MinusLogProbMetric: 403.3719

Epoch 37: val_loss improved from 403.54260 to 403.37186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 403.0222 - MinusLogProbMetric: 403.0222 - val_loss: 403.3719 - val_MinusLogProbMetric: 403.3719 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 38/1000
2023-10-03 01:22:23.473 
Epoch 38/1000 
	 loss: 402.8460, MinusLogProbMetric: 402.8460, val_loss: 402.9472, val_MinusLogProbMetric: 402.9472

Epoch 38: val_loss improved from 403.37186 to 402.94717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 402.8460 - MinusLogProbMetric: 402.8460 - val_loss: 402.9472 - val_MinusLogProbMetric: 402.9472 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 39/1000
2023-10-03 01:22:35.406 
Epoch 39/1000 
	 loss: 402.6447, MinusLogProbMetric: 402.6447, val_loss: 403.0928, val_MinusLogProbMetric: 403.0928

Epoch 39: val_loss did not improve from 402.94717
196/196 - 11s - loss: 402.6447 - MinusLogProbMetric: 402.6447 - val_loss: 403.0928 - val_MinusLogProbMetric: 403.0928 - lr: 3.3333e-04 - 11s/epoch - 59ms/step
Epoch 40/1000
2023-10-03 01:22:46.118 
Epoch 40/1000 
	 loss: 402.6106, MinusLogProbMetric: 402.6106, val_loss: 406.5016, val_MinusLogProbMetric: 406.5016

Epoch 40: val_loss did not improve from 402.94717
196/196 - 11s - loss: 402.6106 - MinusLogProbMetric: 402.6106 - val_loss: 406.5016 - val_MinusLogProbMetric: 406.5016 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 41/1000
2023-10-03 01:22:56.087 
Epoch 41/1000 
	 loss: 402.4525, MinusLogProbMetric: 402.4525, val_loss: 403.2967, val_MinusLogProbMetric: 403.2967

Epoch 41: val_loss did not improve from 402.94717
196/196 - 10s - loss: 402.4525 - MinusLogProbMetric: 402.4525 - val_loss: 403.2967 - val_MinusLogProbMetric: 403.2967 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 42/1000
2023-10-03 01:23:06.341 
Epoch 42/1000 
	 loss: 401.8885, MinusLogProbMetric: 401.8885, val_loss: 402.8549, val_MinusLogProbMetric: 402.8549

Epoch 42: val_loss improved from 402.94717 to 402.85486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 401.8885 - MinusLogProbMetric: 401.8885 - val_loss: 402.8549 - val_MinusLogProbMetric: 402.8549 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 43/1000
2023-10-03 01:23:16.981 
Epoch 43/1000 
	 loss: 402.0056, MinusLogProbMetric: 402.0056, val_loss: 402.7627, val_MinusLogProbMetric: 402.7627

Epoch 43: val_loss improved from 402.85486 to 402.76266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 402.0056 - MinusLogProbMetric: 402.0056 - val_loss: 402.7627 - val_MinusLogProbMetric: 402.7627 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 44/1000
2023-10-03 01:23:27.513 
Epoch 44/1000 
	 loss: 401.7688, MinusLogProbMetric: 401.7688, val_loss: 403.1042, val_MinusLogProbMetric: 403.1042

Epoch 44: val_loss did not improve from 402.76266
196/196 - 10s - loss: 401.7688 - MinusLogProbMetric: 401.7688 - val_loss: 403.1042 - val_MinusLogProbMetric: 403.1042 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 45/1000
2023-10-03 01:23:37.885 
Epoch 45/1000 
	 loss: 401.5775, MinusLogProbMetric: 401.5775, val_loss: 401.6232, val_MinusLogProbMetric: 401.6232

Epoch 45: val_loss improved from 402.76266 to 401.62320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 401.5775 - MinusLogProbMetric: 401.5775 - val_loss: 401.6232 - val_MinusLogProbMetric: 401.6232 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 46/1000
2023-10-03 01:23:48.894 
Epoch 46/1000 
	 loss: 401.3884, MinusLogProbMetric: 401.3884, val_loss: 403.2927, val_MinusLogProbMetric: 403.2927

Epoch 46: val_loss did not improve from 401.62320
196/196 - 10s - loss: 401.3884 - MinusLogProbMetric: 401.3884 - val_loss: 403.2927 - val_MinusLogProbMetric: 403.2927 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 47/1000
2023-10-03 01:23:59.278 
Epoch 47/1000 
	 loss: 401.5730, MinusLogProbMetric: 401.5730, val_loss: 401.8632, val_MinusLogProbMetric: 401.8632

Epoch 47: val_loss did not improve from 401.62320
196/196 - 10s - loss: 401.5730 - MinusLogProbMetric: 401.5730 - val_loss: 401.8632 - val_MinusLogProbMetric: 401.8632 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 48/1000
2023-10-03 01:24:09.864 
Epoch 48/1000 
	 loss: 401.2106, MinusLogProbMetric: 401.2106, val_loss: 402.1720, val_MinusLogProbMetric: 402.1720

Epoch 48: val_loss did not improve from 401.62320
196/196 - 11s - loss: 401.2106 - MinusLogProbMetric: 401.2106 - val_loss: 402.1720 - val_MinusLogProbMetric: 402.1720 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 49/1000
2023-10-03 01:24:20.753 
Epoch 49/1000 
	 loss: 401.1332, MinusLogProbMetric: 401.1332, val_loss: 402.6236, val_MinusLogProbMetric: 402.6236

Epoch 49: val_loss did not improve from 401.62320
196/196 - 11s - loss: 401.1332 - MinusLogProbMetric: 401.1332 - val_loss: 402.6236 - val_MinusLogProbMetric: 402.6236 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 50/1000
2023-10-03 01:24:31.421 
Epoch 50/1000 
	 loss: 400.9914, MinusLogProbMetric: 400.9914, val_loss: 401.6421, val_MinusLogProbMetric: 401.6421

Epoch 50: val_loss did not improve from 401.62320
196/196 - 11s - loss: 400.9914 - MinusLogProbMetric: 400.9914 - val_loss: 401.6421 - val_MinusLogProbMetric: 401.6421 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 51/1000
2023-10-03 01:24:41.891 
Epoch 51/1000 
	 loss: 401.0126, MinusLogProbMetric: 401.0126, val_loss: 401.3055, val_MinusLogProbMetric: 401.3055

Epoch 51: val_loss improved from 401.62320 to 401.30554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 401.0126 - MinusLogProbMetric: 401.0126 - val_loss: 401.3055 - val_MinusLogProbMetric: 401.3055 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 52/1000
2023-10-03 01:24:53.345 
Epoch 52/1000 
	 loss: 401.3490, MinusLogProbMetric: 401.3490, val_loss: 405.5050, val_MinusLogProbMetric: 405.5050

Epoch 52: val_loss did not improve from 401.30554
196/196 - 11s - loss: 401.3490 - MinusLogProbMetric: 401.3490 - val_loss: 405.5050 - val_MinusLogProbMetric: 405.5050 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 53/1000
2023-10-03 01:25:04.240 
Epoch 53/1000 
	 loss: 400.5678, MinusLogProbMetric: 400.5678, val_loss: 401.4853, val_MinusLogProbMetric: 401.4853

Epoch 53: val_loss did not improve from 401.30554
196/196 - 11s - loss: 400.5678 - MinusLogProbMetric: 400.5678 - val_loss: 401.4853 - val_MinusLogProbMetric: 401.4853 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 54/1000
2023-10-03 01:25:15.006 
Epoch 54/1000 
	 loss: 400.4660, MinusLogProbMetric: 400.4660, val_loss: 401.8537, val_MinusLogProbMetric: 401.8537

Epoch 54: val_loss did not improve from 401.30554
196/196 - 11s - loss: 400.4660 - MinusLogProbMetric: 400.4660 - val_loss: 401.8537 - val_MinusLogProbMetric: 401.8537 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 55/1000
2023-10-03 01:25:25.821 
Epoch 55/1000 
	 loss: 400.2638, MinusLogProbMetric: 400.2638, val_loss: 401.1895, val_MinusLogProbMetric: 401.1895

Epoch 55: val_loss improved from 401.30554 to 401.18951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 400.2638 - MinusLogProbMetric: 400.2638 - val_loss: 401.1895 - val_MinusLogProbMetric: 401.1895 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 56/1000
2023-10-03 01:25:36.748 
Epoch 56/1000 
	 loss: 400.4774, MinusLogProbMetric: 400.4774, val_loss: 401.5970, val_MinusLogProbMetric: 401.5970

Epoch 56: val_loss did not improve from 401.18951
196/196 - 11s - loss: 400.4774 - MinusLogProbMetric: 400.4774 - val_loss: 401.5970 - val_MinusLogProbMetric: 401.5970 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 57/1000
2023-10-03 01:25:47.146 
Epoch 57/1000 
	 loss: 400.1731, MinusLogProbMetric: 400.1731, val_loss: 401.1227, val_MinusLogProbMetric: 401.1227

Epoch 57: val_loss improved from 401.18951 to 401.12274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 400.1731 - MinusLogProbMetric: 400.1731 - val_loss: 401.1227 - val_MinusLogProbMetric: 401.1227 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 58/1000
2023-10-03 01:25:57.692 
Epoch 58/1000 
	 loss: 400.0694, MinusLogProbMetric: 400.0694, val_loss: 400.5188, val_MinusLogProbMetric: 400.5188

Epoch 58: val_loss improved from 401.12274 to 400.51883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 400.0694 - MinusLogProbMetric: 400.0694 - val_loss: 400.5188 - val_MinusLogProbMetric: 400.5188 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 59/1000
2023-10-03 01:26:08.526 
Epoch 59/1000 
	 loss: 399.9868, MinusLogProbMetric: 399.9868, val_loss: 400.0763, val_MinusLogProbMetric: 400.0763

Epoch 59: val_loss improved from 400.51883 to 400.07629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 399.9868 - MinusLogProbMetric: 399.9868 - val_loss: 400.0763 - val_MinusLogProbMetric: 400.0763 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 60/1000
2023-10-03 01:26:19.384 
Epoch 60/1000 
	 loss: 399.9276, MinusLogProbMetric: 399.9276, val_loss: 401.4914, val_MinusLogProbMetric: 401.4914

Epoch 60: val_loss did not improve from 400.07629
196/196 - 10s - loss: 399.9276 - MinusLogProbMetric: 399.9276 - val_loss: 401.4914 - val_MinusLogProbMetric: 401.4914 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 61/1000
2023-10-03 01:26:29.840 
Epoch 61/1000 
	 loss: 399.8677, MinusLogProbMetric: 399.8677, val_loss: 400.2833, val_MinusLogProbMetric: 400.2833

Epoch 61: val_loss did not improve from 400.07629
196/196 - 10s - loss: 399.8677 - MinusLogProbMetric: 399.8677 - val_loss: 400.2833 - val_MinusLogProbMetric: 400.2833 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 62/1000
2023-10-03 01:26:40.244 
Epoch 62/1000 
	 loss: 399.7336, MinusLogProbMetric: 399.7336, val_loss: 400.1127, val_MinusLogProbMetric: 400.1127

Epoch 62: val_loss did not improve from 400.07629
196/196 - 10s - loss: 399.7336 - MinusLogProbMetric: 399.7336 - val_loss: 400.1127 - val_MinusLogProbMetric: 400.1127 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 63/1000
2023-10-03 01:26:50.771 
Epoch 63/1000 
	 loss: 399.8342, MinusLogProbMetric: 399.8342, val_loss: 400.4085, val_MinusLogProbMetric: 400.4085

Epoch 63: val_loss did not improve from 400.07629
196/196 - 11s - loss: 399.8342 - MinusLogProbMetric: 399.8342 - val_loss: 400.4085 - val_MinusLogProbMetric: 400.4085 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 64/1000
2023-10-03 01:27:01.091 
Epoch 64/1000 
	 loss: 399.6003, MinusLogProbMetric: 399.6003, val_loss: 401.6739, val_MinusLogProbMetric: 401.6739

Epoch 64: val_loss did not improve from 400.07629
196/196 - 10s - loss: 399.6003 - MinusLogProbMetric: 399.6003 - val_loss: 401.6739 - val_MinusLogProbMetric: 401.6739 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 65/1000
2023-10-03 01:27:11.996 
Epoch 65/1000 
	 loss: 399.3364, MinusLogProbMetric: 399.3364, val_loss: 399.8383, val_MinusLogProbMetric: 399.8383

Epoch 65: val_loss improved from 400.07629 to 399.83829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 399.3364 - MinusLogProbMetric: 399.3364 - val_loss: 399.8383 - val_MinusLogProbMetric: 399.8383 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 66/1000
2023-10-03 01:27:23.166 
Epoch 66/1000 
	 loss: 399.3905, MinusLogProbMetric: 399.3905, val_loss: 400.6212, val_MinusLogProbMetric: 400.6212

Epoch 66: val_loss did not improve from 399.83829
196/196 - 11s - loss: 399.3905 - MinusLogProbMetric: 399.3905 - val_loss: 400.6212 - val_MinusLogProbMetric: 400.6212 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 67/1000
2023-10-03 01:27:33.498 
Epoch 67/1000 
	 loss: 399.6778, MinusLogProbMetric: 399.6778, val_loss: 399.7241, val_MinusLogProbMetric: 399.7241

Epoch 67: val_loss improved from 399.83829 to 399.72406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 399.6778 - MinusLogProbMetric: 399.6778 - val_loss: 399.7241 - val_MinusLogProbMetric: 399.7241 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 68/1000
2023-10-03 01:27:44.752 
Epoch 68/1000 
	 loss: 398.9349, MinusLogProbMetric: 398.9349, val_loss: 400.8974, val_MinusLogProbMetric: 400.8974

Epoch 68: val_loss did not improve from 399.72406
196/196 - 11s - loss: 398.9349 - MinusLogProbMetric: 398.9349 - val_loss: 400.8974 - val_MinusLogProbMetric: 400.8974 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 69/1000
2023-10-03 01:27:55.642 
Epoch 69/1000 
	 loss: 399.1473, MinusLogProbMetric: 399.1473, val_loss: 400.4809, val_MinusLogProbMetric: 400.4809

Epoch 69: val_loss did not improve from 399.72406
196/196 - 11s - loss: 399.1473 - MinusLogProbMetric: 399.1473 - val_loss: 400.4809 - val_MinusLogProbMetric: 400.4809 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 70/1000
2023-10-03 01:28:06.360 
Epoch 70/1000 
	 loss: 399.0596, MinusLogProbMetric: 399.0596, val_loss: 399.3646, val_MinusLogProbMetric: 399.3646

Epoch 70: val_loss improved from 399.72406 to 399.36456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 399.0596 - MinusLogProbMetric: 399.0596 - val_loss: 399.3646 - val_MinusLogProbMetric: 399.3646 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 71/1000
2023-10-03 01:28:17.542 
Epoch 71/1000 
	 loss: 398.9056, MinusLogProbMetric: 398.9056, val_loss: 400.4469, val_MinusLogProbMetric: 400.4469

Epoch 71: val_loss did not improve from 399.36456
196/196 - 11s - loss: 398.9056 - MinusLogProbMetric: 398.9056 - val_loss: 400.4469 - val_MinusLogProbMetric: 400.4469 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 72/1000
2023-10-03 01:28:27.946 
Epoch 72/1000 
	 loss: 398.8836, MinusLogProbMetric: 398.8836, val_loss: 400.8468, val_MinusLogProbMetric: 400.8468

Epoch 72: val_loss did not improve from 399.36456
196/196 - 10s - loss: 398.8836 - MinusLogProbMetric: 398.8836 - val_loss: 400.8468 - val_MinusLogProbMetric: 400.8468 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 73/1000
2023-10-03 01:28:38.585 
Epoch 73/1000 
	 loss: 398.6057, MinusLogProbMetric: 398.6057, val_loss: 399.8206, val_MinusLogProbMetric: 399.8206

Epoch 73: val_loss did not improve from 399.36456
196/196 - 11s - loss: 398.6057 - MinusLogProbMetric: 398.6057 - val_loss: 399.8206 - val_MinusLogProbMetric: 399.8206 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 74/1000
2023-10-03 01:28:49.060 
Epoch 74/1000 
	 loss: 399.2871, MinusLogProbMetric: 399.2871, val_loss: 404.9582, val_MinusLogProbMetric: 404.9582

Epoch 74: val_loss did not improve from 399.36456
196/196 - 10s - loss: 399.2871 - MinusLogProbMetric: 399.2871 - val_loss: 404.9582 - val_MinusLogProbMetric: 404.9582 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 75/1000
2023-10-03 01:28:59.380 
Epoch 75/1000 
	 loss: 398.5648, MinusLogProbMetric: 398.5648, val_loss: 400.2080, val_MinusLogProbMetric: 400.2080

Epoch 75: val_loss did not improve from 399.36456
196/196 - 10s - loss: 398.5648 - MinusLogProbMetric: 398.5648 - val_loss: 400.2080 - val_MinusLogProbMetric: 400.2080 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 76/1000
2023-10-03 01:29:09.795 
Epoch 76/1000 
	 loss: 398.5959, MinusLogProbMetric: 398.5959, val_loss: 401.6821, val_MinusLogProbMetric: 401.6821

Epoch 76: val_loss did not improve from 399.36456
196/196 - 10s - loss: 398.5959 - MinusLogProbMetric: 398.5959 - val_loss: 401.6821 - val_MinusLogProbMetric: 401.6821 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 77/1000
2023-10-03 01:29:20.255 
Epoch 77/1000 
	 loss: 398.5502, MinusLogProbMetric: 398.5502, val_loss: 401.1707, val_MinusLogProbMetric: 401.1707

Epoch 77: val_loss did not improve from 399.36456
196/196 - 10s - loss: 398.5502 - MinusLogProbMetric: 398.5502 - val_loss: 401.1707 - val_MinusLogProbMetric: 401.1707 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 78/1000
2023-10-03 01:29:30.728 
Epoch 78/1000 
	 loss: 398.4442, MinusLogProbMetric: 398.4442, val_loss: 400.1315, val_MinusLogProbMetric: 400.1315

Epoch 78: val_loss did not improve from 399.36456
196/196 - 10s - loss: 398.4442 - MinusLogProbMetric: 398.4442 - val_loss: 400.1315 - val_MinusLogProbMetric: 400.1315 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 79/1000
2023-10-03 01:29:41.109 
Epoch 79/1000 
	 loss: 398.3290, MinusLogProbMetric: 398.3290, val_loss: 401.1963, val_MinusLogProbMetric: 401.1963

Epoch 79: val_loss did not improve from 399.36456
196/196 - 10s - loss: 398.3290 - MinusLogProbMetric: 398.3290 - val_loss: 401.1963 - val_MinusLogProbMetric: 401.1963 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 80/1000
2023-10-03 01:29:51.391 
Epoch 80/1000 
	 loss: 398.3983, MinusLogProbMetric: 398.3983, val_loss: 401.5707, val_MinusLogProbMetric: 401.5707

Epoch 80: val_loss did not improve from 399.36456
196/196 - 10s - loss: 398.3983 - MinusLogProbMetric: 398.3983 - val_loss: 401.5707 - val_MinusLogProbMetric: 401.5707 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 81/1000
2023-10-03 01:30:02.249 
Epoch 81/1000 
	 loss: 398.2910, MinusLogProbMetric: 398.2910, val_loss: 400.1313, val_MinusLogProbMetric: 400.1313

Epoch 81: val_loss did not improve from 399.36456
196/196 - 11s - loss: 398.2910 - MinusLogProbMetric: 398.2910 - val_loss: 400.1313 - val_MinusLogProbMetric: 400.1313 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 82/1000
2023-10-03 01:30:12.937 
Epoch 82/1000 
	 loss: 397.9229, MinusLogProbMetric: 397.9229, val_loss: 399.3056, val_MinusLogProbMetric: 399.3056

Epoch 82: val_loss improved from 399.36456 to 399.30563, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 397.9229 - MinusLogProbMetric: 397.9229 - val_loss: 399.3056 - val_MinusLogProbMetric: 399.3056 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 83/1000
2023-10-03 01:30:24.238 
Epoch 83/1000 
	 loss: 398.4645, MinusLogProbMetric: 398.4645, val_loss: 400.0966, val_MinusLogProbMetric: 400.0966

Epoch 83: val_loss did not improve from 399.30563
196/196 - 11s - loss: 398.4645 - MinusLogProbMetric: 398.4645 - val_loss: 400.0966 - val_MinusLogProbMetric: 400.0966 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 84/1000
2023-10-03 01:30:35.267 
Epoch 84/1000 
	 loss: 397.7006, MinusLogProbMetric: 397.7006, val_loss: 399.1252, val_MinusLogProbMetric: 399.1252

Epoch 84: val_loss improved from 399.30563 to 399.12515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 397.7006 - MinusLogProbMetric: 397.7006 - val_loss: 399.1252 - val_MinusLogProbMetric: 399.1252 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 85/1000
2023-10-03 01:30:45.976 
Epoch 85/1000 
	 loss: 397.9820, MinusLogProbMetric: 397.9820, val_loss: 399.9023, val_MinusLogProbMetric: 399.9023

Epoch 85: val_loss did not improve from 399.12515
196/196 - 10s - loss: 397.9820 - MinusLogProbMetric: 397.9820 - val_loss: 399.9023 - val_MinusLogProbMetric: 399.9023 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 86/1000
2023-10-03 01:30:56.685 
Epoch 86/1000 
	 loss: 397.9015, MinusLogProbMetric: 397.9015, val_loss: 407.4743, val_MinusLogProbMetric: 407.4743

Epoch 86: val_loss did not improve from 399.12515
196/196 - 11s - loss: 397.9015 - MinusLogProbMetric: 397.9015 - val_loss: 407.4743 - val_MinusLogProbMetric: 407.4743 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 87/1000
2023-10-03 01:31:07.086 
Epoch 87/1000 
	 loss: 398.0477, MinusLogProbMetric: 398.0477, val_loss: 400.8492, val_MinusLogProbMetric: 400.8492

Epoch 87: val_loss did not improve from 399.12515
196/196 - 10s - loss: 398.0477 - MinusLogProbMetric: 398.0477 - val_loss: 400.8492 - val_MinusLogProbMetric: 400.8492 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 88/1000
2023-10-03 01:31:17.782 
Epoch 88/1000 
	 loss: 397.8215, MinusLogProbMetric: 397.8215, val_loss: 398.3063, val_MinusLogProbMetric: 398.3063

Epoch 88: val_loss improved from 399.12515 to 398.30634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 397.8215 - MinusLogProbMetric: 397.8215 - val_loss: 398.3063 - val_MinusLogProbMetric: 398.3063 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 89/1000
2023-10-03 01:31:28.981 
Epoch 89/1000 
	 loss: 398.9966, MinusLogProbMetric: 398.9966, val_loss: 398.7373, val_MinusLogProbMetric: 398.7373

Epoch 89: val_loss did not improve from 398.30634
196/196 - 11s - loss: 398.9966 - MinusLogProbMetric: 398.9966 - val_loss: 398.7373 - val_MinusLogProbMetric: 398.7373 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 90/1000
2023-10-03 01:31:39.347 
Epoch 90/1000 
	 loss: 397.2536, MinusLogProbMetric: 397.2536, val_loss: 399.2238, val_MinusLogProbMetric: 399.2238

Epoch 90: val_loss did not improve from 398.30634
196/196 - 10s - loss: 397.2536 - MinusLogProbMetric: 397.2536 - val_loss: 399.2238 - val_MinusLogProbMetric: 399.2238 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 91/1000
2023-10-03 01:31:49.835 
Epoch 91/1000 
	 loss: 397.4952, MinusLogProbMetric: 397.4952, val_loss: 400.7206, val_MinusLogProbMetric: 400.7206

Epoch 91: val_loss did not improve from 398.30634
196/196 - 10s - loss: 397.4952 - MinusLogProbMetric: 397.4952 - val_loss: 400.7206 - val_MinusLogProbMetric: 400.7206 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 92/1000
2023-10-03 01:32:00.182 
Epoch 92/1000 
	 loss: 397.4081, MinusLogProbMetric: 397.4081, val_loss: 398.6921, val_MinusLogProbMetric: 398.6921

Epoch 92: val_loss did not improve from 398.30634
196/196 - 10s - loss: 397.4081 - MinusLogProbMetric: 397.4081 - val_loss: 398.6921 - val_MinusLogProbMetric: 398.6921 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 93/1000
2023-10-03 01:32:10.519 
Epoch 93/1000 
	 loss: 397.5261, MinusLogProbMetric: 397.5261, val_loss: 404.8321, val_MinusLogProbMetric: 404.8321

Epoch 93: val_loss did not improve from 398.30634
196/196 - 10s - loss: 397.5261 - MinusLogProbMetric: 397.5261 - val_loss: 404.8321 - val_MinusLogProbMetric: 404.8321 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-10-03 01:32:21.004 
Epoch 94/1000 
	 loss: 397.4490, MinusLogProbMetric: 397.4490, val_loss: 399.4660, val_MinusLogProbMetric: 399.4660

Epoch 94: val_loss did not improve from 398.30634
196/196 - 10s - loss: 397.4490 - MinusLogProbMetric: 397.4490 - val_loss: 399.4660 - val_MinusLogProbMetric: 399.4660 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 95/1000
2023-10-03 01:32:31.396 
Epoch 95/1000 
	 loss: 397.4230, MinusLogProbMetric: 397.4230, val_loss: 402.2138, val_MinusLogProbMetric: 402.2138

Epoch 95: val_loss did not improve from 398.30634
196/196 - 10s - loss: 397.4230 - MinusLogProbMetric: 397.4230 - val_loss: 402.2138 - val_MinusLogProbMetric: 402.2138 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 96/1000
2023-10-03 01:32:41.930 
Epoch 96/1000 
	 loss: 397.1385, MinusLogProbMetric: 397.1385, val_loss: 399.3649, val_MinusLogProbMetric: 399.3649

Epoch 96: val_loss did not improve from 398.30634
196/196 - 11s - loss: 397.1385 - MinusLogProbMetric: 397.1385 - val_loss: 399.3649 - val_MinusLogProbMetric: 399.3649 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 97/1000
2023-10-03 01:32:52.614 
Epoch 97/1000 
	 loss: 397.2599, MinusLogProbMetric: 397.2599, val_loss: 398.6561, val_MinusLogProbMetric: 398.6561

Epoch 97: val_loss did not improve from 398.30634
196/196 - 11s - loss: 397.2599 - MinusLogProbMetric: 397.2599 - val_loss: 398.6561 - val_MinusLogProbMetric: 398.6561 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 98/1000
2023-10-03 01:33:03.237 
Epoch 98/1000 
	 loss: 397.0856, MinusLogProbMetric: 397.0856, val_loss: 398.0999, val_MinusLogProbMetric: 398.0999

Epoch 98: val_loss improved from 398.30634 to 398.09995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 397.0856 - MinusLogProbMetric: 397.0856 - val_loss: 398.0999 - val_MinusLogProbMetric: 398.0999 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 99/1000
2023-10-03 01:33:13.916 
Epoch 99/1000 
	 loss: 397.0705, MinusLogProbMetric: 397.0705, val_loss: 398.7541, val_MinusLogProbMetric: 398.7541

Epoch 99: val_loss did not improve from 398.09995
196/196 - 10s - loss: 397.0705 - MinusLogProbMetric: 397.0705 - val_loss: 398.7541 - val_MinusLogProbMetric: 398.7541 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 100/1000
2023-10-03 01:33:23.962 
Epoch 100/1000 
	 loss: 396.9984, MinusLogProbMetric: 396.9984, val_loss: 398.8877, val_MinusLogProbMetric: 398.8877

Epoch 100: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.9984 - MinusLogProbMetric: 396.9984 - val_loss: 398.8877 - val_MinusLogProbMetric: 398.8877 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 101/1000
2023-10-03 01:33:34.182 
Epoch 101/1000 
	 loss: 397.2202, MinusLogProbMetric: 397.2202, val_loss: 398.4154, val_MinusLogProbMetric: 398.4154

Epoch 101: val_loss did not improve from 398.09995
196/196 - 10s - loss: 397.2202 - MinusLogProbMetric: 397.2202 - val_loss: 398.4154 - val_MinusLogProbMetric: 398.4154 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 102/1000
2023-10-03 01:33:44.597 
Epoch 102/1000 
	 loss: 396.8513, MinusLogProbMetric: 396.8513, val_loss: 398.2413, val_MinusLogProbMetric: 398.2413

Epoch 102: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.8513 - MinusLogProbMetric: 396.8513 - val_loss: 398.2413 - val_MinusLogProbMetric: 398.2413 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 103/1000
2023-10-03 01:33:54.859 
Epoch 103/1000 
	 loss: 397.3388, MinusLogProbMetric: 397.3388, val_loss: 398.5449, val_MinusLogProbMetric: 398.5449

Epoch 103: val_loss did not improve from 398.09995
196/196 - 10s - loss: 397.3388 - MinusLogProbMetric: 397.3388 - val_loss: 398.5449 - val_MinusLogProbMetric: 398.5449 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 104/1000
2023-10-03 01:34:05.153 
Epoch 104/1000 
	 loss: 396.7249, MinusLogProbMetric: 396.7249, val_loss: 398.2895, val_MinusLogProbMetric: 398.2895

Epoch 104: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.7249 - MinusLogProbMetric: 396.7249 - val_loss: 398.2895 - val_MinusLogProbMetric: 398.2895 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 105/1000
2023-10-03 01:34:15.367 
Epoch 105/1000 
	 loss: 396.7829, MinusLogProbMetric: 396.7829, val_loss: 398.8733, val_MinusLogProbMetric: 398.8733

Epoch 105: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.7829 - MinusLogProbMetric: 396.7829 - val_loss: 398.8733 - val_MinusLogProbMetric: 398.8733 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 106/1000
2023-10-03 01:34:25.594 
Epoch 106/1000 
	 loss: 396.7078, MinusLogProbMetric: 396.7078, val_loss: 398.6173, val_MinusLogProbMetric: 398.6173

Epoch 106: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.7078 - MinusLogProbMetric: 396.7078 - val_loss: 398.6173 - val_MinusLogProbMetric: 398.6173 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 107/1000
2023-10-03 01:34:35.911 
Epoch 107/1000 
	 loss: 396.7153, MinusLogProbMetric: 396.7153, val_loss: 402.8659, val_MinusLogProbMetric: 402.8659

Epoch 107: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.7153 - MinusLogProbMetric: 396.7153 - val_loss: 402.8659 - val_MinusLogProbMetric: 402.8659 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 108/1000
2023-10-03 01:34:46.515 
Epoch 108/1000 
	 loss: 396.7164, MinusLogProbMetric: 396.7164, val_loss: 398.9988, val_MinusLogProbMetric: 398.9988

Epoch 108: val_loss did not improve from 398.09995
196/196 - 11s - loss: 396.7164 - MinusLogProbMetric: 396.7164 - val_loss: 398.9988 - val_MinusLogProbMetric: 398.9988 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 109/1000
2023-10-03 01:34:57.004 
Epoch 109/1000 
	 loss: 396.6764, MinusLogProbMetric: 396.6764, val_loss: 398.3554, val_MinusLogProbMetric: 398.3554

Epoch 109: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.6764 - MinusLogProbMetric: 396.6764 - val_loss: 398.3554 - val_MinusLogProbMetric: 398.3554 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 110/1000
2023-10-03 01:35:07.430 
Epoch 110/1000 
	 loss: 396.9353, MinusLogProbMetric: 396.9353, val_loss: 399.6782, val_MinusLogProbMetric: 399.6782

Epoch 110: val_loss did not improve from 398.09995
196/196 - 10s - loss: 396.9353 - MinusLogProbMetric: 396.9353 - val_loss: 399.6782 - val_MinusLogProbMetric: 399.6782 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 111/1000
2023-10-03 01:35:17.842 
Epoch 111/1000 
	 loss: 396.6178, MinusLogProbMetric: 396.6178, val_loss: 397.4582, val_MinusLogProbMetric: 397.4582

Epoch 111: val_loss improved from 398.09995 to 397.45819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 396.6178 - MinusLogProbMetric: 396.6178 - val_loss: 397.4582 - val_MinusLogProbMetric: 397.4582 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 112/1000
2023-10-03 01:35:28.428 
Epoch 112/1000 
	 loss: 396.8011, MinusLogProbMetric: 396.8011, val_loss: 397.5900, val_MinusLogProbMetric: 397.5900

Epoch 112: val_loss did not improve from 397.45819
196/196 - 10s - loss: 396.8011 - MinusLogProbMetric: 396.8011 - val_loss: 397.5900 - val_MinusLogProbMetric: 397.5900 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 113/1000
2023-10-03 01:35:38.564 
Epoch 113/1000 
	 loss: 396.3554, MinusLogProbMetric: 396.3554, val_loss: 398.4344, val_MinusLogProbMetric: 398.4344

Epoch 113: val_loss did not improve from 397.45819
196/196 - 10s - loss: 396.3554 - MinusLogProbMetric: 396.3554 - val_loss: 398.4344 - val_MinusLogProbMetric: 398.4344 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 114/1000
2023-10-03 01:35:49.476 
Epoch 114/1000 
	 loss: 396.4273, MinusLogProbMetric: 396.4273, val_loss: 397.3820, val_MinusLogProbMetric: 397.3820

Epoch 114: val_loss improved from 397.45819 to 397.38199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 396.4273 - MinusLogProbMetric: 396.4273 - val_loss: 397.3820 - val_MinusLogProbMetric: 397.3820 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 115/1000
2023-10-03 01:36:00.742 
Epoch 115/1000 
	 loss: 396.2992, MinusLogProbMetric: 396.2992, val_loss: 397.5125, val_MinusLogProbMetric: 397.5125

Epoch 115: val_loss did not improve from 397.38199
196/196 - 11s - loss: 396.2992 - MinusLogProbMetric: 396.2992 - val_loss: 397.5125 - val_MinusLogProbMetric: 397.5125 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 116/1000
2023-10-03 01:36:11.069 
Epoch 116/1000 
	 loss: 396.2985, MinusLogProbMetric: 396.2985, val_loss: 397.8129, val_MinusLogProbMetric: 397.8129

Epoch 116: val_loss did not improve from 397.38199
196/196 - 10s - loss: 396.2985 - MinusLogProbMetric: 396.2985 - val_loss: 397.8129 - val_MinusLogProbMetric: 397.8129 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 117/1000
2023-10-03 01:36:21.342 
Epoch 117/1000 
	 loss: 396.4383, MinusLogProbMetric: 396.4383, val_loss: 397.3169, val_MinusLogProbMetric: 397.3169

Epoch 117: val_loss improved from 397.38199 to 397.31689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 396.4383 - MinusLogProbMetric: 396.4383 - val_loss: 397.3169 - val_MinusLogProbMetric: 397.3169 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 118/1000
2023-10-03 01:36:31.918 
Epoch 118/1000 
	 loss: 396.4573, MinusLogProbMetric: 396.4573, val_loss: 397.6124, val_MinusLogProbMetric: 397.6124

Epoch 118: val_loss did not improve from 397.31689
196/196 - 10s - loss: 396.4573 - MinusLogProbMetric: 396.4573 - val_loss: 397.6124 - val_MinusLogProbMetric: 397.6124 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 119/1000
2023-10-03 01:36:41.886 
Epoch 119/1000 
	 loss: 396.1595, MinusLogProbMetric: 396.1595, val_loss: 399.0319, val_MinusLogProbMetric: 399.0319

Epoch 119: val_loss did not improve from 397.31689
196/196 - 10s - loss: 396.1595 - MinusLogProbMetric: 396.1595 - val_loss: 399.0319 - val_MinusLogProbMetric: 399.0319 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 120/1000
2023-10-03 01:36:52.190 
Epoch 120/1000 
	 loss: 396.0286, MinusLogProbMetric: 396.0286, val_loss: 403.7469, val_MinusLogProbMetric: 403.7469

Epoch 120: val_loss did not improve from 397.31689
196/196 - 10s - loss: 396.0286 - MinusLogProbMetric: 396.0286 - val_loss: 403.7469 - val_MinusLogProbMetric: 403.7469 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 121/1000
2023-10-03 01:37:02.333 
Epoch 121/1000 
	 loss: 396.0027, MinusLogProbMetric: 396.0027, val_loss: 397.3657, val_MinusLogProbMetric: 397.3657

Epoch 121: val_loss did not improve from 397.31689
196/196 - 10s - loss: 396.0027 - MinusLogProbMetric: 396.0027 - val_loss: 397.3657 - val_MinusLogProbMetric: 397.3657 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 122/1000
2023-10-03 01:37:12.543 
Epoch 122/1000 
	 loss: 396.1066, MinusLogProbMetric: 396.1066, val_loss: 405.0020, val_MinusLogProbMetric: 405.0020

Epoch 122: val_loss did not improve from 397.31689
196/196 - 10s - loss: 396.1066 - MinusLogProbMetric: 396.1066 - val_loss: 405.0020 - val_MinusLogProbMetric: 405.0020 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 123/1000
2023-10-03 01:37:22.907 
Epoch 123/1000 
	 loss: 396.0827, MinusLogProbMetric: 396.0827, val_loss: 398.3507, val_MinusLogProbMetric: 398.3507

Epoch 123: val_loss did not improve from 397.31689
196/196 - 10s - loss: 396.0827 - MinusLogProbMetric: 396.0827 - val_loss: 398.3507 - val_MinusLogProbMetric: 398.3507 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 124/1000
2023-10-03 01:37:33.093 
Epoch 124/1000 
	 loss: 395.9377, MinusLogProbMetric: 395.9377, val_loss: 398.3693, val_MinusLogProbMetric: 398.3693

Epoch 124: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.9377 - MinusLogProbMetric: 395.9377 - val_loss: 398.3693 - val_MinusLogProbMetric: 398.3693 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 125/1000
2023-10-03 01:37:43.603 
Epoch 125/1000 
	 loss: 395.9111, MinusLogProbMetric: 395.9111, val_loss: 397.4595, val_MinusLogProbMetric: 397.4595

Epoch 125: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.9111 - MinusLogProbMetric: 395.9111 - val_loss: 397.4595 - val_MinusLogProbMetric: 397.4595 - lr: 3.3333e-04 - 10s/epoch - 54ms/step
Epoch 126/1000
2023-10-03 01:37:53.981 
Epoch 126/1000 
	 loss: 395.8626, MinusLogProbMetric: 395.8626, val_loss: 398.2613, val_MinusLogProbMetric: 398.2613

Epoch 126: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.8626 - MinusLogProbMetric: 395.8626 - val_loss: 398.2613 - val_MinusLogProbMetric: 398.2613 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 127/1000
2023-10-03 01:38:04.489 
Epoch 127/1000 
	 loss: 395.7688, MinusLogProbMetric: 395.7688, val_loss: 399.1546, val_MinusLogProbMetric: 399.1546

Epoch 127: val_loss did not improve from 397.31689
196/196 - 11s - loss: 395.7688 - MinusLogProbMetric: 395.7688 - val_loss: 399.1546 - val_MinusLogProbMetric: 399.1546 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 128/1000
2023-10-03 01:38:14.908 
Epoch 128/1000 
	 loss: 395.8136, MinusLogProbMetric: 395.8136, val_loss: 401.4586, val_MinusLogProbMetric: 401.4586

Epoch 128: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.8136 - MinusLogProbMetric: 395.8136 - val_loss: 401.4586 - val_MinusLogProbMetric: 401.4586 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 129/1000
2023-10-03 01:38:25.129 
Epoch 129/1000 
	 loss: 395.7872, MinusLogProbMetric: 395.7872, val_loss: 397.5005, val_MinusLogProbMetric: 397.5005

Epoch 129: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.7872 - MinusLogProbMetric: 395.7872 - val_loss: 397.5005 - val_MinusLogProbMetric: 397.5005 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 130/1000
2023-10-03 01:38:35.303 
Epoch 130/1000 
	 loss: 396.2635, MinusLogProbMetric: 396.2635, val_loss: 398.1138, val_MinusLogProbMetric: 398.1138

Epoch 130: val_loss did not improve from 397.31689
196/196 - 10s - loss: 396.2635 - MinusLogProbMetric: 396.2635 - val_loss: 398.1138 - val_MinusLogProbMetric: 398.1138 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 131/1000
2023-10-03 01:38:46.304 
Epoch 131/1000 
	 loss: 395.5367, MinusLogProbMetric: 395.5367, val_loss: 406.2675, val_MinusLogProbMetric: 406.2675

Epoch 131: val_loss did not improve from 397.31689
196/196 - 11s - loss: 395.5367 - MinusLogProbMetric: 395.5367 - val_loss: 406.2675 - val_MinusLogProbMetric: 406.2675 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 132/1000
2023-10-03 01:38:56.813 
Epoch 132/1000 
	 loss: 395.6257, MinusLogProbMetric: 395.6257, val_loss: 398.3751, val_MinusLogProbMetric: 398.3751

Epoch 132: val_loss did not improve from 397.31689
196/196 - 11s - loss: 395.6257 - MinusLogProbMetric: 395.6257 - val_loss: 398.3751 - val_MinusLogProbMetric: 398.3751 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 133/1000
2023-10-03 01:39:07.204 
Epoch 133/1000 
	 loss: 395.7803, MinusLogProbMetric: 395.7803, val_loss: 398.3050, val_MinusLogProbMetric: 398.3050

Epoch 133: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.7803 - MinusLogProbMetric: 395.7803 - val_loss: 398.3050 - val_MinusLogProbMetric: 398.3050 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 134/1000
2023-10-03 01:39:17.414 
Epoch 134/1000 
	 loss: 395.5265, MinusLogProbMetric: 395.5265, val_loss: 397.6656, val_MinusLogProbMetric: 397.6656

Epoch 134: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.5265 - MinusLogProbMetric: 395.5265 - val_loss: 397.6656 - val_MinusLogProbMetric: 397.6656 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 135/1000
2023-10-03 01:39:27.824 
Epoch 135/1000 
	 loss: 395.6172, MinusLogProbMetric: 395.6172, val_loss: 397.3977, val_MinusLogProbMetric: 397.3977

Epoch 135: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.6172 - MinusLogProbMetric: 395.6172 - val_loss: 397.3977 - val_MinusLogProbMetric: 397.3977 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 136/1000
2023-10-03 01:39:38.218 
Epoch 136/1000 
	 loss: 395.8524, MinusLogProbMetric: 395.8524, val_loss: 401.6872, val_MinusLogProbMetric: 401.6872

Epoch 136: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.8524 - MinusLogProbMetric: 395.8524 - val_loss: 401.6872 - val_MinusLogProbMetric: 401.6872 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 137/1000
2023-10-03 01:39:48.544 
Epoch 137/1000 
	 loss: 395.6940, MinusLogProbMetric: 395.6940, val_loss: 398.7695, val_MinusLogProbMetric: 398.7695

Epoch 137: val_loss did not improve from 397.31689
196/196 - 10s - loss: 395.6940 - MinusLogProbMetric: 395.6940 - val_loss: 398.7695 - val_MinusLogProbMetric: 398.7695 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 138/1000
2023-10-03 01:39:58.947 
Epoch 138/1000 
	 loss: 395.5440, MinusLogProbMetric: 395.5440, val_loss: 397.0555, val_MinusLogProbMetric: 397.0555

Epoch 138: val_loss improved from 397.31689 to 397.05551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 395.5440 - MinusLogProbMetric: 395.5440 - val_loss: 397.0555 - val_MinusLogProbMetric: 397.0555 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 139/1000
2023-10-03 01:40:09.675 
Epoch 139/1000 
	 loss: 395.3435, MinusLogProbMetric: 395.3435, val_loss: 399.2666, val_MinusLogProbMetric: 399.2666

Epoch 139: val_loss did not improve from 397.05551
196/196 - 10s - loss: 395.3435 - MinusLogProbMetric: 395.3435 - val_loss: 399.2666 - val_MinusLogProbMetric: 399.2666 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 140/1000
2023-10-03 01:40:20.174 
Epoch 140/1000 
	 loss: 395.3788, MinusLogProbMetric: 395.3788, val_loss: 397.0033, val_MinusLogProbMetric: 397.0033

Epoch 140: val_loss improved from 397.05551 to 397.00333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 395.3788 - MinusLogProbMetric: 395.3788 - val_loss: 397.0033 - val_MinusLogProbMetric: 397.0033 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 141/1000
2023-10-03 01:40:30.809 
Epoch 141/1000 
	 loss: 396.8997, MinusLogProbMetric: 396.8997, val_loss: 398.0574, val_MinusLogProbMetric: 398.0574

Epoch 141: val_loss did not improve from 397.00333
196/196 - 10s - loss: 396.8997 - MinusLogProbMetric: 396.8997 - val_loss: 398.0574 - val_MinusLogProbMetric: 398.0574 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 142/1000
2023-10-03 01:40:41.179 
Epoch 142/1000 
	 loss: 396.1181, MinusLogProbMetric: 396.1181, val_loss: 397.5193, val_MinusLogProbMetric: 397.5193

Epoch 142: val_loss did not improve from 397.00333
196/196 - 10s - loss: 396.1181 - MinusLogProbMetric: 396.1181 - val_loss: 397.5193 - val_MinusLogProbMetric: 397.5193 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 143/1000
2023-10-03 01:40:51.388 
Epoch 143/1000 
	 loss: 395.1348, MinusLogProbMetric: 395.1348, val_loss: 397.0052, val_MinusLogProbMetric: 397.0052

Epoch 143: val_loss did not improve from 397.00333
196/196 - 10s - loss: 395.1348 - MinusLogProbMetric: 395.1348 - val_loss: 397.0052 - val_MinusLogProbMetric: 397.0052 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 144/1000
2023-10-03 01:41:01.523 
Epoch 144/1000 
	 loss: 395.4216, MinusLogProbMetric: 395.4216, val_loss: 398.5665, val_MinusLogProbMetric: 398.5665

Epoch 144: val_loss did not improve from 397.00333
196/196 - 10s - loss: 395.4216 - MinusLogProbMetric: 395.4216 - val_loss: 398.5665 - val_MinusLogProbMetric: 398.5665 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 145/1000
2023-10-03 01:41:11.450 
Epoch 145/1000 
	 loss: 394.9383, MinusLogProbMetric: 394.9383, val_loss: 396.8575, val_MinusLogProbMetric: 396.8575

Epoch 145: val_loss improved from 397.00333 to 396.85754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 10s - loss: 394.9383 - MinusLogProbMetric: 394.9383 - val_loss: 396.8575 - val_MinusLogProbMetric: 396.8575 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 146/1000
2023-10-03 01:41:21.897 
Epoch 146/1000 
	 loss: 395.3902, MinusLogProbMetric: 395.3902, val_loss: 397.3953, val_MinusLogProbMetric: 397.3953

Epoch 146: val_loss did not improve from 396.85754
196/196 - 10s - loss: 395.3902 - MinusLogProbMetric: 395.3902 - val_loss: 397.3953 - val_MinusLogProbMetric: 397.3953 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 147/1000
2023-10-03 01:41:32.207 
Epoch 147/1000 
	 loss: 395.1090, MinusLogProbMetric: 395.1090, val_loss: 398.1374, val_MinusLogProbMetric: 398.1374

Epoch 147: val_loss did not improve from 396.85754
196/196 - 10s - loss: 395.1090 - MinusLogProbMetric: 395.1090 - val_loss: 398.1374 - val_MinusLogProbMetric: 398.1374 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 148/1000
2023-10-03 01:41:43.517 
Epoch 148/1000 
	 loss: 395.0457, MinusLogProbMetric: 395.0457, val_loss: 397.2226, val_MinusLogProbMetric: 397.2226

Epoch 148: val_loss did not improve from 396.85754
196/196 - 11s - loss: 395.0457 - MinusLogProbMetric: 395.0457 - val_loss: 397.2226 - val_MinusLogProbMetric: 397.2226 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 149/1000
2023-10-03 01:41:53.518 
Epoch 149/1000 
	 loss: 394.9794, MinusLogProbMetric: 394.9794, val_loss: 398.1284, val_MinusLogProbMetric: 398.1284

Epoch 149: val_loss did not improve from 396.85754
196/196 - 10s - loss: 394.9794 - MinusLogProbMetric: 394.9794 - val_loss: 398.1284 - val_MinusLogProbMetric: 398.1284 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 150/1000
2023-10-03 01:42:03.889 
Epoch 150/1000 
	 loss: 394.8989, MinusLogProbMetric: 394.8989, val_loss: 397.7589, val_MinusLogProbMetric: 397.7589

Epoch 150: val_loss did not improve from 396.85754
196/196 - 10s - loss: 394.8989 - MinusLogProbMetric: 394.8989 - val_loss: 397.7589 - val_MinusLogProbMetric: 397.7589 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 151/1000
2023-10-03 01:42:14.385 
Epoch 151/1000 
	 loss: 395.0154, MinusLogProbMetric: 395.0154, val_loss: 397.1828, val_MinusLogProbMetric: 397.1828

Epoch 151: val_loss did not improve from 396.85754
196/196 - 10s - loss: 395.0154 - MinusLogProbMetric: 395.0154 - val_loss: 397.1828 - val_MinusLogProbMetric: 397.1828 - lr: 3.3333e-04 - 10s/epoch - 54ms/step
Epoch 152/1000
2023-10-03 01:42:24.695 
Epoch 152/1000 
	 loss: 394.7791, MinusLogProbMetric: 394.7791, val_loss: 398.8214, val_MinusLogProbMetric: 398.8214

Epoch 152: val_loss did not improve from 396.85754
196/196 - 10s - loss: 394.7791 - MinusLogProbMetric: 394.7791 - val_loss: 398.8214 - val_MinusLogProbMetric: 398.8214 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 153/1000
2023-10-03 01:42:35.012 
Epoch 153/1000 
	 loss: 395.2358, MinusLogProbMetric: 395.2358, val_loss: 397.6116, val_MinusLogProbMetric: 397.6116

Epoch 153: val_loss did not improve from 396.85754
196/196 - 10s - loss: 395.2358 - MinusLogProbMetric: 395.2358 - val_loss: 397.6116 - val_MinusLogProbMetric: 397.6116 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 154/1000
2023-10-03 01:42:45.434 
Epoch 154/1000 
	 loss: 394.6094, MinusLogProbMetric: 394.6094, val_loss: 397.3702, val_MinusLogProbMetric: 397.3702

Epoch 154: val_loss did not improve from 396.85754
196/196 - 10s - loss: 394.6094 - MinusLogProbMetric: 394.6094 - val_loss: 397.3702 - val_MinusLogProbMetric: 397.3702 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 155/1000
2023-10-03 01:42:55.659 
Epoch 155/1000 
	 loss: 394.9358, MinusLogProbMetric: 394.9358, val_loss: 397.1127, val_MinusLogProbMetric: 397.1127

Epoch 155: val_loss did not improve from 396.85754
196/196 - 10s - loss: 394.9358 - MinusLogProbMetric: 394.9358 - val_loss: 397.1127 - val_MinusLogProbMetric: 397.1127 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 156/1000
2023-10-03 01:43:05.820 
Epoch 156/1000 
	 loss: 394.8423, MinusLogProbMetric: 394.8423, val_loss: 399.6623, val_MinusLogProbMetric: 399.6623

Epoch 156: val_loss did not improve from 396.85754
196/196 - 10s - loss: 394.8423 - MinusLogProbMetric: 394.8423 - val_loss: 399.6623 - val_MinusLogProbMetric: 399.6623 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 157/1000
2023-10-03 01:43:16.137 
Epoch 157/1000 
	 loss: 394.6957, MinusLogProbMetric: 394.6957, val_loss: 396.6118, val_MinusLogProbMetric: 396.6118

Epoch 157: val_loss improved from 396.85754 to 396.61176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 394.6957 - MinusLogProbMetric: 394.6957 - val_loss: 396.6118 - val_MinusLogProbMetric: 396.6118 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 158/1000
2023-10-03 01:43:26.617 
Epoch 158/1000 
	 loss: 394.7837, MinusLogProbMetric: 394.7837, val_loss: 396.6756, val_MinusLogProbMetric: 396.6756

Epoch 158: val_loss did not improve from 396.61176
196/196 - 10s - loss: 394.7837 - MinusLogProbMetric: 394.7837 - val_loss: 396.6756 - val_MinusLogProbMetric: 396.6756 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 159/1000
2023-10-03 01:43:36.984 
Epoch 159/1000 
	 loss: 394.8107, MinusLogProbMetric: 394.8107, val_loss: 397.9975, val_MinusLogProbMetric: 397.9975

Epoch 159: val_loss did not improve from 396.61176
196/196 - 10s - loss: 394.8107 - MinusLogProbMetric: 394.8107 - val_loss: 397.9975 - val_MinusLogProbMetric: 397.9975 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 160/1000
2023-10-03 01:43:47.180 
Epoch 160/1000 
	 loss: 394.6339, MinusLogProbMetric: 394.6339, val_loss: 396.1980, val_MinusLogProbMetric: 396.1980

Epoch 160: val_loss improved from 396.61176 to 396.19797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 394.6339 - MinusLogProbMetric: 394.6339 - val_loss: 396.1980 - val_MinusLogProbMetric: 396.1980 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 161/1000
2023-10-03 01:43:57.712 
Epoch 161/1000 
	 loss: 394.8747, MinusLogProbMetric: 394.8747, val_loss: 396.8444, val_MinusLogProbMetric: 396.8444

Epoch 161: val_loss did not improve from 396.19797
196/196 - 10s - loss: 394.8747 - MinusLogProbMetric: 394.8747 - val_loss: 396.8444 - val_MinusLogProbMetric: 396.8444 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 162/1000
2023-10-03 01:44:08.081 
Epoch 162/1000 
	 loss: 394.5013, MinusLogProbMetric: 394.5013, val_loss: 396.7336, val_MinusLogProbMetric: 396.7336

Epoch 162: val_loss did not improve from 396.19797
196/196 - 10s - loss: 394.5013 - MinusLogProbMetric: 394.5013 - val_loss: 396.7336 - val_MinusLogProbMetric: 396.7336 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 163/1000
2023-10-03 01:44:18.225 
Epoch 163/1000 
	 loss: 394.4572, MinusLogProbMetric: 394.4572, val_loss: 396.9955, val_MinusLogProbMetric: 396.9955

Epoch 163: val_loss did not improve from 396.19797
196/196 - 10s - loss: 394.4572 - MinusLogProbMetric: 394.4572 - val_loss: 396.9955 - val_MinusLogProbMetric: 396.9955 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 164/1000
2023-10-03 01:44:28.722 
Epoch 164/1000 
	 loss: 394.6560, MinusLogProbMetric: 394.6560, val_loss: 396.2455, val_MinusLogProbMetric: 396.2455

Epoch 164: val_loss did not improve from 396.19797
196/196 - 10s - loss: 394.6560 - MinusLogProbMetric: 394.6560 - val_loss: 396.2455 - val_MinusLogProbMetric: 396.2455 - lr: 3.3333e-04 - 10s/epoch - 54ms/step
Epoch 165/1000
2023-10-03 01:44:39.846 
Epoch 165/1000 
	 loss: 394.3640, MinusLogProbMetric: 394.3640, val_loss: 396.1033, val_MinusLogProbMetric: 396.1033

Epoch 165: val_loss improved from 396.19797 to 396.10327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 12s - loss: 394.3640 - MinusLogProbMetric: 394.3640 - val_loss: 396.1033 - val_MinusLogProbMetric: 396.1033 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 166/1000
2023-10-03 01:44:50.334 
Epoch 166/1000 
	 loss: 394.6840, MinusLogProbMetric: 394.6840, val_loss: 398.7822, val_MinusLogProbMetric: 398.7822

Epoch 166: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.6840 - MinusLogProbMetric: 394.6840 - val_loss: 398.7822 - val_MinusLogProbMetric: 398.7822 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 167/1000
2023-10-03 01:45:00.580 
Epoch 167/1000 
	 loss: 394.3073, MinusLogProbMetric: 394.3073, val_loss: 410.2738, val_MinusLogProbMetric: 410.2738

Epoch 167: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.3073 - MinusLogProbMetric: 394.3073 - val_loss: 410.2738 - val_MinusLogProbMetric: 410.2738 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 168/1000
2023-10-03 01:45:11.022 
Epoch 168/1000 
	 loss: 394.5758, MinusLogProbMetric: 394.5758, val_loss: 396.7619, val_MinusLogProbMetric: 396.7619

Epoch 168: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.5758 - MinusLogProbMetric: 394.5758 - val_loss: 396.7619 - val_MinusLogProbMetric: 396.7619 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 169/1000
2023-10-03 01:45:21.370 
Epoch 169/1000 
	 loss: 394.4000, MinusLogProbMetric: 394.4000, val_loss: 398.7712, val_MinusLogProbMetric: 398.7712

Epoch 169: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.4000 - MinusLogProbMetric: 394.4000 - val_loss: 398.7712 - val_MinusLogProbMetric: 398.7712 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 170/1000
2023-10-03 01:45:31.828 
Epoch 170/1000 
	 loss: 394.6002, MinusLogProbMetric: 394.6002, val_loss: 396.9873, val_MinusLogProbMetric: 396.9873

Epoch 170: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.6002 - MinusLogProbMetric: 394.6002 - val_loss: 396.9873 - val_MinusLogProbMetric: 396.9873 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 171/1000
2023-10-03 01:45:42.297 
Epoch 171/1000 
	 loss: 394.3135, MinusLogProbMetric: 394.3135, val_loss: 397.3640, val_MinusLogProbMetric: 397.3640

Epoch 171: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.3135 - MinusLogProbMetric: 394.3135 - val_loss: 397.3640 - val_MinusLogProbMetric: 397.3640 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 172/1000
2023-10-03 01:45:52.683 
Epoch 172/1000 
	 loss: 394.3632, MinusLogProbMetric: 394.3632, val_loss: 397.4676, val_MinusLogProbMetric: 397.4676

Epoch 172: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.3632 - MinusLogProbMetric: 394.3632 - val_loss: 397.4676 - val_MinusLogProbMetric: 397.4676 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 173/1000
2023-10-03 01:46:02.890 
Epoch 173/1000 
	 loss: 394.1878, MinusLogProbMetric: 394.1878, val_loss: 398.4743, val_MinusLogProbMetric: 398.4743

Epoch 173: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.1878 - MinusLogProbMetric: 394.1878 - val_loss: 398.4743 - val_MinusLogProbMetric: 398.4743 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 174/1000
2023-10-03 01:46:13.269 
Epoch 174/1000 
	 loss: 394.4567, MinusLogProbMetric: 394.4567, val_loss: 400.0002, val_MinusLogProbMetric: 400.0002

Epoch 174: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.4567 - MinusLogProbMetric: 394.4567 - val_loss: 400.0002 - val_MinusLogProbMetric: 400.0002 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 175/1000
2023-10-03 01:46:23.761 
Epoch 175/1000 
	 loss: 394.0897, MinusLogProbMetric: 394.0897, val_loss: 397.3640, val_MinusLogProbMetric: 397.3640

Epoch 175: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.0897 - MinusLogProbMetric: 394.0897 - val_loss: 397.3640 - val_MinusLogProbMetric: 397.3640 - lr: 3.3333e-04 - 10s/epoch - 54ms/step
Epoch 176/1000
2023-10-03 01:46:34.203 
Epoch 176/1000 
	 loss: 394.2209, MinusLogProbMetric: 394.2209, val_loss: 397.4280, val_MinusLogProbMetric: 397.4280

Epoch 176: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.2209 - MinusLogProbMetric: 394.2209 - val_loss: 397.4280 - val_MinusLogProbMetric: 397.4280 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 177/1000
2023-10-03 01:46:44.494 
Epoch 177/1000 
	 loss: 394.2454, MinusLogProbMetric: 394.2454, val_loss: 396.7168, val_MinusLogProbMetric: 396.7168

Epoch 177: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.2454 - MinusLogProbMetric: 394.2454 - val_loss: 396.7168 - val_MinusLogProbMetric: 396.7168 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 178/1000
2023-10-03 01:46:54.812 
Epoch 178/1000 
	 loss: 393.9854, MinusLogProbMetric: 393.9854, val_loss: 396.4942, val_MinusLogProbMetric: 396.4942

Epoch 178: val_loss did not improve from 396.10327
196/196 - 10s - loss: 393.9854 - MinusLogProbMetric: 393.9854 - val_loss: 396.4942 - val_MinusLogProbMetric: 396.4942 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 179/1000
2023-10-03 01:47:05.223 
Epoch 179/1000 
	 loss: 394.0319, MinusLogProbMetric: 394.0319, val_loss: 397.7401, val_MinusLogProbMetric: 397.7401

Epoch 179: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.0319 - MinusLogProbMetric: 394.0319 - val_loss: 397.7401 - val_MinusLogProbMetric: 397.7401 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 180/1000
2023-10-03 01:47:15.627 
Epoch 180/1000 
	 loss: 394.3372, MinusLogProbMetric: 394.3372, val_loss: 397.0312, val_MinusLogProbMetric: 397.0312

Epoch 180: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.3372 - MinusLogProbMetric: 394.3372 - val_loss: 397.0312 - val_MinusLogProbMetric: 397.0312 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 181/1000
2023-10-03 01:47:26.419 
Epoch 181/1000 
	 loss: 393.9620, MinusLogProbMetric: 393.9620, val_loss: 400.9051, val_MinusLogProbMetric: 400.9051

Epoch 181: val_loss did not improve from 396.10327
196/196 - 11s - loss: 393.9620 - MinusLogProbMetric: 393.9620 - val_loss: 400.9051 - val_MinusLogProbMetric: 400.9051 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 182/1000
2023-10-03 01:47:37.044 
Epoch 182/1000 
	 loss: 394.2328, MinusLogProbMetric: 394.2328, val_loss: 397.2020, val_MinusLogProbMetric: 397.2020

Epoch 182: val_loss did not improve from 396.10327
196/196 - 11s - loss: 394.2328 - MinusLogProbMetric: 394.2328 - val_loss: 397.2020 - val_MinusLogProbMetric: 397.2020 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 183/1000
2023-10-03 01:47:47.599 
Epoch 183/1000 
	 loss: 394.0564, MinusLogProbMetric: 394.0564, val_loss: 396.7009, val_MinusLogProbMetric: 396.7009

Epoch 183: val_loss did not improve from 396.10327
196/196 - 11s - loss: 394.0564 - MinusLogProbMetric: 394.0564 - val_loss: 396.7009 - val_MinusLogProbMetric: 396.7009 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 184/1000
2023-10-03 01:47:57.765 
Epoch 184/1000 
	 loss: 394.1746, MinusLogProbMetric: 394.1746, val_loss: 397.2119, val_MinusLogProbMetric: 397.2119

Epoch 184: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.1746 - MinusLogProbMetric: 394.1746 - val_loss: 397.2119 - val_MinusLogProbMetric: 397.2119 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 185/1000
2023-10-03 01:48:08.005 
Epoch 185/1000 
	 loss: 393.8197, MinusLogProbMetric: 393.8197, val_loss: 396.1313, val_MinusLogProbMetric: 396.1313

Epoch 185: val_loss did not improve from 396.10327
196/196 - 10s - loss: 393.8197 - MinusLogProbMetric: 393.8197 - val_loss: 396.1313 - val_MinusLogProbMetric: 396.1313 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 186/1000
2023-10-03 01:48:18.387 
Epoch 186/1000 
	 loss: 393.9410, MinusLogProbMetric: 393.9410, val_loss: 397.0050, val_MinusLogProbMetric: 397.0050

Epoch 186: val_loss did not improve from 396.10327
196/196 - 10s - loss: 393.9410 - MinusLogProbMetric: 393.9410 - val_loss: 397.0050 - val_MinusLogProbMetric: 397.0050 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 187/1000
2023-10-03 01:48:28.582 
Epoch 187/1000 
	 loss: 394.5299, MinusLogProbMetric: 394.5299, val_loss: 397.0166, val_MinusLogProbMetric: 397.0166

Epoch 187: val_loss did not improve from 396.10327
196/196 - 10s - loss: 394.5299 - MinusLogProbMetric: 394.5299 - val_loss: 397.0166 - val_MinusLogProbMetric: 397.0166 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 188/1000
2023-10-03 01:48:38.717 
Epoch 188/1000 
	 loss: 393.7567, MinusLogProbMetric: 393.7567, val_loss: 396.5743, val_MinusLogProbMetric: 396.5743

Epoch 188: val_loss did not improve from 396.10327
196/196 - 10s - loss: 393.7567 - MinusLogProbMetric: 393.7567 - val_loss: 396.5743 - val_MinusLogProbMetric: 396.5743 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 189/1000
2023-10-03 01:48:49.138 
Epoch 189/1000 
	 loss: 393.7577, MinusLogProbMetric: 393.7577, val_loss: 396.5411, val_MinusLogProbMetric: 396.5411

Epoch 189: val_loss did not improve from 396.10327
196/196 - 10s - loss: 393.7577 - MinusLogProbMetric: 393.7577 - val_loss: 396.5411 - val_MinusLogProbMetric: 396.5411 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 190/1000
2023-10-03 01:48:59.586 
Epoch 190/1000 
	 loss: 393.7368, MinusLogProbMetric: 393.7368, val_loss: 395.7632, val_MinusLogProbMetric: 395.7632

Epoch 190: val_loss improved from 396.10327 to 395.76324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 393.7368 - MinusLogProbMetric: 393.7368 - val_loss: 395.7632 - val_MinusLogProbMetric: 395.7632 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 191/1000
2023-10-03 01:49:10.341 
Epoch 191/1000 
	 loss: 393.7031, MinusLogProbMetric: 393.7031, val_loss: 398.9834, val_MinusLogProbMetric: 398.9834

Epoch 191: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.7031 - MinusLogProbMetric: 393.7031 - val_loss: 398.9834 - val_MinusLogProbMetric: 398.9834 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 192/1000
2023-10-03 01:49:20.592 
Epoch 192/1000 
	 loss: 393.8361, MinusLogProbMetric: 393.8361, val_loss: 396.6416, val_MinusLogProbMetric: 396.6416

Epoch 192: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.8361 - MinusLogProbMetric: 393.8361 - val_loss: 396.6416 - val_MinusLogProbMetric: 396.6416 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 193/1000
2023-10-03 01:49:31.015 
Epoch 193/1000 
	 loss: 393.7593, MinusLogProbMetric: 393.7593, val_loss: 396.5110, val_MinusLogProbMetric: 396.5110

Epoch 193: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.7593 - MinusLogProbMetric: 393.7593 - val_loss: 396.5110 - val_MinusLogProbMetric: 396.5110 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 194/1000
2023-10-03 01:49:41.352 
Epoch 194/1000 
	 loss: 393.9307, MinusLogProbMetric: 393.9307, val_loss: 397.2210, val_MinusLogProbMetric: 397.2210

Epoch 194: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.9307 - MinusLogProbMetric: 393.9307 - val_loss: 397.2210 - val_MinusLogProbMetric: 397.2210 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 195/1000
2023-10-03 01:49:51.747 
Epoch 195/1000 
	 loss: 393.4767, MinusLogProbMetric: 393.4767, val_loss: 400.7531, val_MinusLogProbMetric: 400.7531

Epoch 195: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.4767 - MinusLogProbMetric: 393.4767 - val_loss: 400.7531 - val_MinusLogProbMetric: 400.7531 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 196/1000
2023-10-03 01:50:02.177 
Epoch 196/1000 
	 loss: 393.5862, MinusLogProbMetric: 393.5862, val_loss: 396.7928, val_MinusLogProbMetric: 396.7928

Epoch 196: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.5862 - MinusLogProbMetric: 393.5862 - val_loss: 396.7928 - val_MinusLogProbMetric: 396.7928 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 197/1000
2023-10-03 01:50:12.526 
Epoch 197/1000 
	 loss: 393.5488, MinusLogProbMetric: 393.5488, val_loss: 428.7101, val_MinusLogProbMetric: 428.7101

Epoch 197: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.5488 - MinusLogProbMetric: 393.5488 - val_loss: 428.7101 - val_MinusLogProbMetric: 428.7101 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 198/1000
2023-10-03 01:50:23.529 
Epoch 198/1000 
	 loss: 394.3489, MinusLogProbMetric: 394.3489, val_loss: 396.4833, val_MinusLogProbMetric: 396.4833

Epoch 198: val_loss did not improve from 395.76324
196/196 - 11s - loss: 394.3489 - MinusLogProbMetric: 394.3489 - val_loss: 396.4833 - val_MinusLogProbMetric: 396.4833 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 199/1000
2023-10-03 01:50:33.926 
Epoch 199/1000 
	 loss: 393.4548, MinusLogProbMetric: 393.4548, val_loss: 396.6296, val_MinusLogProbMetric: 396.6296

Epoch 199: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.4548 - MinusLogProbMetric: 393.4548 - val_loss: 396.6296 - val_MinusLogProbMetric: 396.6296 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 200/1000
2023-10-03 01:50:44.306 
Epoch 200/1000 
	 loss: 393.4914, MinusLogProbMetric: 393.4914, val_loss: 397.2292, val_MinusLogProbMetric: 397.2292

Epoch 200: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.4914 - MinusLogProbMetric: 393.4914 - val_loss: 397.2292 - val_MinusLogProbMetric: 397.2292 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 201/1000
2023-10-03 01:50:54.640 
Epoch 201/1000 
	 loss: 393.5006, MinusLogProbMetric: 393.5006, val_loss: 396.5207, val_MinusLogProbMetric: 396.5207

Epoch 201: val_loss did not improve from 395.76324
196/196 - 10s - loss: 393.5006 - MinusLogProbMetric: 393.5006 - val_loss: 396.5207 - val_MinusLogProbMetric: 396.5207 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 202/1000
2023-10-03 01:51:04.977 
Epoch 202/1000 
	 loss: 393.7271, MinusLogProbMetric: 393.7271, val_loss: 395.6992, val_MinusLogProbMetric: 395.6992

Epoch 202: val_loss improved from 395.76324 to 395.69922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 393.7271 - MinusLogProbMetric: 393.7271 - val_loss: 395.6992 - val_MinusLogProbMetric: 395.6992 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 203/1000
2023-10-03 01:51:15.743 
Epoch 203/1000 
	 loss: 393.2739, MinusLogProbMetric: 393.2739, val_loss: 398.0399, val_MinusLogProbMetric: 398.0399

Epoch 203: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.2739 - MinusLogProbMetric: 393.2739 - val_loss: 398.0399 - val_MinusLogProbMetric: 398.0399 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 204/1000
2023-10-03 01:51:26.070 
Epoch 204/1000 
	 loss: 393.9358, MinusLogProbMetric: 393.9358, val_loss: 396.6037, val_MinusLogProbMetric: 396.6037

Epoch 204: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.9358 - MinusLogProbMetric: 393.9358 - val_loss: 396.6037 - val_MinusLogProbMetric: 396.6037 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 205/1000
2023-10-03 01:51:36.329 
Epoch 205/1000 
	 loss: 393.4146, MinusLogProbMetric: 393.4146, val_loss: 397.3593, val_MinusLogProbMetric: 397.3593

Epoch 205: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.4146 - MinusLogProbMetric: 393.4146 - val_loss: 397.3593 - val_MinusLogProbMetric: 397.3593 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 206/1000
2023-10-03 01:51:46.718 
Epoch 206/1000 
	 loss: 393.4006, MinusLogProbMetric: 393.4006, val_loss: 396.2653, val_MinusLogProbMetric: 396.2653

Epoch 206: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.4006 - MinusLogProbMetric: 393.4006 - val_loss: 396.2653 - val_MinusLogProbMetric: 396.2653 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 207/1000
2023-10-03 01:51:56.965 
Epoch 207/1000 
	 loss: 393.2988, MinusLogProbMetric: 393.2988, val_loss: 396.0895, val_MinusLogProbMetric: 396.0895

Epoch 207: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.2988 - MinusLogProbMetric: 393.2988 - val_loss: 396.0895 - val_MinusLogProbMetric: 396.0895 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 208/1000
2023-10-03 01:52:07.107 
Epoch 208/1000 
	 loss: 393.3995, MinusLogProbMetric: 393.3995, val_loss: 396.1852, val_MinusLogProbMetric: 396.1852

Epoch 208: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.3995 - MinusLogProbMetric: 393.3995 - val_loss: 396.1852 - val_MinusLogProbMetric: 396.1852 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 209/1000
2023-10-03 01:52:17.514 
Epoch 209/1000 
	 loss: 393.3036, MinusLogProbMetric: 393.3036, val_loss: 396.2955, val_MinusLogProbMetric: 396.2955

Epoch 209: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.3036 - MinusLogProbMetric: 393.3036 - val_loss: 396.2955 - val_MinusLogProbMetric: 396.2955 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 210/1000
2023-10-03 01:52:27.985 
Epoch 210/1000 
	 loss: 393.0308, MinusLogProbMetric: 393.0308, val_loss: 396.4642, val_MinusLogProbMetric: 396.4642

Epoch 210: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.0308 - MinusLogProbMetric: 393.0308 - val_loss: 396.4642 - val_MinusLogProbMetric: 396.4642 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 211/1000
2023-10-03 01:52:38.385 
Epoch 211/1000 
	 loss: 393.6345, MinusLogProbMetric: 393.6345, val_loss: 397.7830, val_MinusLogProbMetric: 397.7830

Epoch 211: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.6345 - MinusLogProbMetric: 393.6345 - val_loss: 397.7830 - val_MinusLogProbMetric: 397.7830 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 212/1000
2023-10-03 01:52:48.746 
Epoch 212/1000 
	 loss: 393.1504, MinusLogProbMetric: 393.1504, val_loss: 397.7914, val_MinusLogProbMetric: 397.7914

Epoch 212: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.1504 - MinusLogProbMetric: 393.1504 - val_loss: 397.7914 - val_MinusLogProbMetric: 397.7914 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 213/1000
2023-10-03 01:52:58.958 
Epoch 213/1000 
	 loss: 393.1675, MinusLogProbMetric: 393.1675, val_loss: 397.0639, val_MinusLogProbMetric: 397.0639

Epoch 213: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.1675 - MinusLogProbMetric: 393.1675 - val_loss: 397.0639 - val_MinusLogProbMetric: 397.0639 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 214/1000
2023-10-03 01:53:09.171 
Epoch 214/1000 
	 loss: 393.2807, MinusLogProbMetric: 393.2807, val_loss: 397.6113, val_MinusLogProbMetric: 397.6113

Epoch 214: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.2807 - MinusLogProbMetric: 393.2807 - val_loss: 397.6113 - val_MinusLogProbMetric: 397.6113 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 215/1000
2023-10-03 01:53:20.160 
Epoch 215/1000 
	 loss: 393.0692, MinusLogProbMetric: 393.0692, val_loss: 401.7948, val_MinusLogProbMetric: 401.7948

Epoch 215: val_loss did not improve from 395.69922
196/196 - 11s - loss: 393.0692 - MinusLogProbMetric: 393.0692 - val_loss: 401.7948 - val_MinusLogProbMetric: 401.7948 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 216/1000
2023-10-03 01:53:30.589 
Epoch 216/1000 
	 loss: 393.2297, MinusLogProbMetric: 393.2297, val_loss: 396.6096, val_MinusLogProbMetric: 396.6096

Epoch 216: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.2297 - MinusLogProbMetric: 393.2297 - val_loss: 396.6096 - val_MinusLogProbMetric: 396.6096 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 217/1000
2023-10-03 01:53:40.902 
Epoch 217/1000 
	 loss: 393.1711, MinusLogProbMetric: 393.1711, val_loss: 396.0935, val_MinusLogProbMetric: 396.0935

Epoch 217: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.1711 - MinusLogProbMetric: 393.1711 - val_loss: 396.0935 - val_MinusLogProbMetric: 396.0935 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 218/1000
2023-10-03 01:53:51.283 
Epoch 218/1000 
	 loss: 393.0520, MinusLogProbMetric: 393.0520, val_loss: 396.2098, val_MinusLogProbMetric: 396.2098

Epoch 218: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.0520 - MinusLogProbMetric: 393.0520 - val_loss: 396.2098 - val_MinusLogProbMetric: 396.2098 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 219/1000
2023-10-03 01:54:01.735 
Epoch 219/1000 
	 loss: 393.3094, MinusLogProbMetric: 393.3094, val_loss: 396.8157, val_MinusLogProbMetric: 396.8157

Epoch 219: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.3094 - MinusLogProbMetric: 393.3094 - val_loss: 396.8157 - val_MinusLogProbMetric: 396.8157 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 220/1000
2023-10-03 01:54:12.154 
Epoch 220/1000 
	 loss: 393.0673, MinusLogProbMetric: 393.0673, val_loss: 396.8161, val_MinusLogProbMetric: 396.8161

Epoch 220: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.0673 - MinusLogProbMetric: 393.0673 - val_loss: 396.8161 - val_MinusLogProbMetric: 396.8161 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 221/1000
2023-10-03 01:54:22.451 
Epoch 221/1000 
	 loss: 392.9692, MinusLogProbMetric: 392.9692, val_loss: 396.1129, val_MinusLogProbMetric: 396.1129

Epoch 221: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.9692 - MinusLogProbMetric: 392.9692 - val_loss: 396.1129 - val_MinusLogProbMetric: 396.1129 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 222/1000
2023-10-03 01:54:32.793 
Epoch 222/1000 
	 loss: 393.3416, MinusLogProbMetric: 393.3416, val_loss: 400.3848, val_MinusLogProbMetric: 400.3848

Epoch 222: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.3416 - MinusLogProbMetric: 393.3416 - val_loss: 400.3848 - val_MinusLogProbMetric: 400.3848 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 223/1000
2023-10-03 01:54:43.118 
Epoch 223/1000 
	 loss: 393.5192, MinusLogProbMetric: 393.5192, val_loss: 398.3499, val_MinusLogProbMetric: 398.3499

Epoch 223: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.5192 - MinusLogProbMetric: 393.5192 - val_loss: 398.3499 - val_MinusLogProbMetric: 398.3499 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 224/1000
2023-10-03 01:54:53.497 
Epoch 224/1000 
	 loss: 392.8477, MinusLogProbMetric: 392.8477, val_loss: 395.9552, val_MinusLogProbMetric: 395.9552

Epoch 224: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.8477 - MinusLogProbMetric: 392.8477 - val_loss: 395.9552 - val_MinusLogProbMetric: 395.9552 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 225/1000
2023-10-03 01:55:03.832 
Epoch 225/1000 
	 loss: 393.1234, MinusLogProbMetric: 393.1234, val_loss: 396.2353, val_MinusLogProbMetric: 396.2353

Epoch 225: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.1234 - MinusLogProbMetric: 393.1234 - val_loss: 396.2353 - val_MinusLogProbMetric: 396.2353 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 226/1000
2023-10-03 01:55:14.137 
Epoch 226/1000 
	 loss: 392.8749, MinusLogProbMetric: 392.8749, val_loss: 397.9386, val_MinusLogProbMetric: 397.9386

Epoch 226: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.8749 - MinusLogProbMetric: 392.8749 - val_loss: 397.9386 - val_MinusLogProbMetric: 397.9386 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 227/1000
2023-10-03 01:55:24.377 
Epoch 227/1000 
	 loss: 392.9424, MinusLogProbMetric: 392.9424, val_loss: 395.9663, val_MinusLogProbMetric: 395.9663

Epoch 227: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.9424 - MinusLogProbMetric: 392.9424 - val_loss: 395.9663 - val_MinusLogProbMetric: 395.9663 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 228/1000
2023-10-03 01:55:34.628 
Epoch 228/1000 
	 loss: 392.9133, MinusLogProbMetric: 392.9133, val_loss: 398.6142, val_MinusLogProbMetric: 398.6142

Epoch 228: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.9133 - MinusLogProbMetric: 392.9133 - val_loss: 398.6142 - val_MinusLogProbMetric: 398.6142 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 229/1000
2023-10-03 01:55:44.849 
Epoch 229/1000 
	 loss: 392.8963, MinusLogProbMetric: 392.8963, val_loss: 396.5682, val_MinusLogProbMetric: 396.5682

Epoch 229: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.8963 - MinusLogProbMetric: 392.8963 - val_loss: 396.5682 - val_MinusLogProbMetric: 396.5682 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 230/1000
2023-10-03 01:55:55.052 
Epoch 230/1000 
	 loss: 393.5683, MinusLogProbMetric: 393.5683, val_loss: 396.3148, val_MinusLogProbMetric: 396.3148

Epoch 230: val_loss did not improve from 395.69922
196/196 - 10s - loss: 393.5683 - MinusLogProbMetric: 393.5683 - val_loss: 396.3148 - val_MinusLogProbMetric: 396.3148 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 231/1000
2023-10-03 01:56:05.943 
Epoch 231/1000 
	 loss: 392.7092, MinusLogProbMetric: 392.7092, val_loss: 397.4396, val_MinusLogProbMetric: 397.4396

Epoch 231: val_loss did not improve from 395.69922
196/196 - 11s - loss: 392.7092 - MinusLogProbMetric: 392.7092 - val_loss: 397.4396 - val_MinusLogProbMetric: 397.4396 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 232/1000
2023-10-03 01:56:16.399 
Epoch 232/1000 
	 loss: 392.6487, MinusLogProbMetric: 392.6487, val_loss: 396.7830, val_MinusLogProbMetric: 396.7830

Epoch 232: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.6487 - MinusLogProbMetric: 392.6487 - val_loss: 396.7830 - val_MinusLogProbMetric: 396.7830 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 233/1000
2023-10-03 01:56:26.464 
Epoch 233/1000 
	 loss: 392.7663, MinusLogProbMetric: 392.7663, val_loss: 396.9292, val_MinusLogProbMetric: 396.9292

Epoch 233: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.7663 - MinusLogProbMetric: 392.7663 - val_loss: 396.9292 - val_MinusLogProbMetric: 396.9292 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 234/1000
2023-10-03 01:56:36.756 
Epoch 234/1000 
	 loss: 392.8281, MinusLogProbMetric: 392.8281, val_loss: 398.5049, val_MinusLogProbMetric: 398.5049

Epoch 234: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.8281 - MinusLogProbMetric: 392.8281 - val_loss: 398.5049 - val_MinusLogProbMetric: 398.5049 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 235/1000
2023-10-03 01:56:46.992 
Epoch 235/1000 
	 loss: 392.6413, MinusLogProbMetric: 392.6413, val_loss: 397.2617, val_MinusLogProbMetric: 397.2617

Epoch 235: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.6413 - MinusLogProbMetric: 392.6413 - val_loss: 397.2617 - val_MinusLogProbMetric: 397.2617 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 236/1000
2023-10-03 01:56:57.352 
Epoch 236/1000 
	 loss: 392.5760, MinusLogProbMetric: 392.5760, val_loss: 395.9209, val_MinusLogProbMetric: 395.9209

Epoch 236: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.5760 - MinusLogProbMetric: 392.5760 - val_loss: 395.9209 - val_MinusLogProbMetric: 395.9209 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 237/1000
2023-10-03 01:57:07.738 
Epoch 237/1000 
	 loss: 392.7212, MinusLogProbMetric: 392.7212, val_loss: 396.0838, val_MinusLogProbMetric: 396.0838

Epoch 237: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.7212 - MinusLogProbMetric: 392.7212 - val_loss: 396.0838 - val_MinusLogProbMetric: 396.0838 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 238/1000
2023-10-03 01:57:18.031 
Epoch 238/1000 
	 loss: 392.7968, MinusLogProbMetric: 392.7968, val_loss: 396.3227, val_MinusLogProbMetric: 396.3227

Epoch 238: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.7968 - MinusLogProbMetric: 392.7968 - val_loss: 396.3227 - val_MinusLogProbMetric: 396.3227 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 239/1000
2023-10-03 01:57:28.209 
Epoch 239/1000 
	 loss: 392.6217, MinusLogProbMetric: 392.6217, val_loss: 398.8185, val_MinusLogProbMetric: 398.8185

Epoch 239: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.6217 - MinusLogProbMetric: 392.6217 - val_loss: 398.8185 - val_MinusLogProbMetric: 398.8185 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 240/1000
2023-10-03 01:57:38.529 
Epoch 240/1000 
	 loss: 392.5815, MinusLogProbMetric: 392.5815, val_loss: 396.1373, val_MinusLogProbMetric: 396.1373

Epoch 240: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.5815 - MinusLogProbMetric: 392.5815 - val_loss: 396.1373 - val_MinusLogProbMetric: 396.1373 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 241/1000
2023-10-03 01:57:48.627 
Epoch 241/1000 
	 loss: 392.8288, MinusLogProbMetric: 392.8288, val_loss: 395.7067, val_MinusLogProbMetric: 395.7067

Epoch 241: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.8288 - MinusLogProbMetric: 392.8288 - val_loss: 395.7067 - val_MinusLogProbMetric: 395.7067 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 242/1000
2023-10-03 01:57:58.997 
Epoch 242/1000 
	 loss: 392.8051, MinusLogProbMetric: 392.8051, val_loss: 396.0375, val_MinusLogProbMetric: 396.0375

Epoch 242: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.8051 - MinusLogProbMetric: 392.8051 - val_loss: 396.0375 - val_MinusLogProbMetric: 396.0375 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 243/1000
2023-10-03 01:58:09.201 
Epoch 243/1000 
	 loss: 392.4609, MinusLogProbMetric: 392.4609, val_loss: 398.0108, val_MinusLogProbMetric: 398.0108

Epoch 243: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.4609 - MinusLogProbMetric: 392.4609 - val_loss: 398.0108 - val_MinusLogProbMetric: 398.0108 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 244/1000
2023-10-03 01:58:19.542 
Epoch 244/1000 
	 loss: 392.3412, MinusLogProbMetric: 392.3412, val_loss: 396.5384, val_MinusLogProbMetric: 396.5384

Epoch 244: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.3412 - MinusLogProbMetric: 392.3412 - val_loss: 396.5384 - val_MinusLogProbMetric: 396.5384 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 245/1000
2023-10-03 01:58:29.925 
Epoch 245/1000 
	 loss: 392.5255, MinusLogProbMetric: 392.5255, val_loss: 396.1692, val_MinusLogProbMetric: 396.1692

Epoch 245: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.5255 - MinusLogProbMetric: 392.5255 - val_loss: 396.1692 - val_MinusLogProbMetric: 396.1692 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 246/1000
2023-10-03 01:58:40.246 
Epoch 246/1000 
	 loss: 392.5239, MinusLogProbMetric: 392.5239, val_loss: 395.9114, val_MinusLogProbMetric: 395.9114

Epoch 246: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.5239 - MinusLogProbMetric: 392.5239 - val_loss: 395.9114 - val_MinusLogProbMetric: 395.9114 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 247/1000
2023-10-03 01:58:50.667 
Epoch 247/1000 
	 loss: 392.3135, MinusLogProbMetric: 392.3135, val_loss: 396.4915, val_MinusLogProbMetric: 396.4915

Epoch 247: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.3135 - MinusLogProbMetric: 392.3135 - val_loss: 396.4915 - val_MinusLogProbMetric: 396.4915 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 248/1000
2023-10-03 01:59:01.500 
Epoch 248/1000 
	 loss: 392.4314, MinusLogProbMetric: 392.4314, val_loss: 396.2075, val_MinusLogProbMetric: 396.2075

Epoch 248: val_loss did not improve from 395.69922
196/196 - 11s - loss: 392.4314 - MinusLogProbMetric: 392.4314 - val_loss: 396.2075 - val_MinusLogProbMetric: 396.2075 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 249/1000
2023-10-03 01:59:11.933 
Epoch 249/1000 
	 loss: 392.4962, MinusLogProbMetric: 392.4962, val_loss: 396.8540, val_MinusLogProbMetric: 396.8540

Epoch 249: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.4962 - MinusLogProbMetric: 392.4962 - val_loss: 396.8540 - val_MinusLogProbMetric: 396.8540 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 250/1000
2023-10-03 01:59:22.273 
Epoch 250/1000 
	 loss: 392.1351, MinusLogProbMetric: 392.1351, val_loss: 396.2374, val_MinusLogProbMetric: 396.2374

Epoch 250: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.1351 - MinusLogProbMetric: 392.1351 - val_loss: 396.2374 - val_MinusLogProbMetric: 396.2374 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 251/1000
2023-10-03 01:59:32.627 
Epoch 251/1000 
	 loss: 392.4541, MinusLogProbMetric: 392.4541, val_loss: 396.7623, val_MinusLogProbMetric: 396.7623

Epoch 251: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.4541 - MinusLogProbMetric: 392.4541 - val_loss: 396.7623 - val_MinusLogProbMetric: 396.7623 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 252/1000
2023-10-03 01:59:42.892 
Epoch 252/1000 
	 loss: 392.4159, MinusLogProbMetric: 392.4159, val_loss: 395.7713, val_MinusLogProbMetric: 395.7713

Epoch 252: val_loss did not improve from 395.69922
196/196 - 10s - loss: 392.4159 - MinusLogProbMetric: 392.4159 - val_loss: 395.7713 - val_MinusLogProbMetric: 395.7713 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 253/1000
2023-10-03 01:59:53.250 
Epoch 253/1000 
	 loss: 390.2451, MinusLogProbMetric: 390.2451, val_loss: 394.5354, val_MinusLogProbMetric: 394.5354

Epoch 253: val_loss improved from 395.69922 to 394.53543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 390.2451 - MinusLogProbMetric: 390.2451 - val_loss: 394.5354 - val_MinusLogProbMetric: 394.5354 - lr: 1.6667e-04 - 11s/epoch - 55ms/step
Epoch 254/1000
2023-10-03 02:00:03.802 
Epoch 254/1000 
	 loss: 390.2356, MinusLogProbMetric: 390.2356, val_loss: 394.5701, val_MinusLogProbMetric: 394.5701

Epoch 254: val_loss did not improve from 394.53543
196/196 - 10s - loss: 390.2356 - MinusLogProbMetric: 390.2356 - val_loss: 394.5701 - val_MinusLogProbMetric: 394.5701 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 255/1000
2023-10-03 02:00:14.028 
Epoch 255/1000 
	 loss: 390.2997, MinusLogProbMetric: 390.2997, val_loss: 395.5338, val_MinusLogProbMetric: 395.5338

Epoch 255: val_loss did not improve from 394.53543
196/196 - 10s - loss: 390.2997 - MinusLogProbMetric: 390.2997 - val_loss: 395.5338 - val_MinusLogProbMetric: 395.5338 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 256/1000
2023-10-03 02:00:24.418 
Epoch 256/1000 
	 loss: 390.3454, MinusLogProbMetric: 390.3454, val_loss: 394.4564, val_MinusLogProbMetric: 394.4564

Epoch 256: val_loss improved from 394.53543 to 394.45636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 390.3454 - MinusLogProbMetric: 390.3454 - val_loss: 394.4564 - val_MinusLogProbMetric: 394.4564 - lr: 1.6667e-04 - 11s/epoch - 57ms/step
Epoch 257/1000
2023-10-03 02:00:35.218 
Epoch 257/1000 
	 loss: 390.2903, MinusLogProbMetric: 390.2903, val_loss: 394.9841, val_MinusLogProbMetric: 394.9841

Epoch 257: val_loss did not improve from 394.45636
196/196 - 10s - loss: 390.2903 - MinusLogProbMetric: 390.2903 - val_loss: 394.9841 - val_MinusLogProbMetric: 394.9841 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 258/1000
2023-10-03 02:00:45.416 
Epoch 258/1000 
	 loss: 390.4235, MinusLogProbMetric: 390.4235, val_loss: 394.7451, val_MinusLogProbMetric: 394.7451

Epoch 258: val_loss did not improve from 394.45636
196/196 - 10s - loss: 390.4235 - MinusLogProbMetric: 390.4235 - val_loss: 394.7451 - val_MinusLogProbMetric: 394.7451 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 259/1000
2023-10-03 02:00:55.641 
Epoch 259/1000 
	 loss: 390.3879, MinusLogProbMetric: 390.3879, val_loss: 396.8214, val_MinusLogProbMetric: 396.8214

Epoch 259: val_loss did not improve from 394.45636
196/196 - 10s - loss: 390.3879 - MinusLogProbMetric: 390.3879 - val_loss: 396.8214 - val_MinusLogProbMetric: 396.8214 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 260/1000
2023-10-03 02:01:05.993 
Epoch 260/1000 
	 loss: 390.2169, MinusLogProbMetric: 390.2169, val_loss: 394.8855, val_MinusLogProbMetric: 394.8855

Epoch 260: val_loss did not improve from 394.45636
196/196 - 10s - loss: 390.2169 - MinusLogProbMetric: 390.2169 - val_loss: 394.8855 - val_MinusLogProbMetric: 394.8855 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 261/1000
2023-10-03 02:01:16.236 
Epoch 261/1000 
	 loss: 390.3430, MinusLogProbMetric: 390.3430, val_loss: 394.7240, val_MinusLogProbMetric: 394.7240

Epoch 261: val_loss did not improve from 394.45636
196/196 - 10s - loss: 390.3430 - MinusLogProbMetric: 390.3430 - val_loss: 394.7240 - val_MinusLogProbMetric: 394.7240 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 262/1000
2023-10-03 02:01:26.575 
Epoch 262/1000 
	 loss: 390.2297, MinusLogProbMetric: 390.2297, val_loss: 394.3375, val_MinusLogProbMetric: 394.3375

Epoch 262: val_loss improved from 394.45636 to 394.33749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 390.2297 - MinusLogProbMetric: 390.2297 - val_loss: 394.3375 - val_MinusLogProbMetric: 394.3375 - lr: 1.6667e-04 - 11s/epoch - 57ms/step
Epoch 263/1000
2023-10-03 02:01:37.783 
Epoch 263/1000 
	 loss: 390.1743, MinusLogProbMetric: 390.1743, val_loss: 395.1954, val_MinusLogProbMetric: 395.1954

Epoch 263: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1743 - MinusLogProbMetric: 390.1743 - val_loss: 395.1954 - val_MinusLogProbMetric: 395.1954 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 264/1000
2023-10-03 02:01:48.409 
Epoch 264/1000 
	 loss: 390.3066, MinusLogProbMetric: 390.3066, val_loss: 395.3945, val_MinusLogProbMetric: 395.3945

Epoch 264: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.3066 - MinusLogProbMetric: 390.3066 - val_loss: 395.3945 - val_MinusLogProbMetric: 395.3945 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 265/1000
2023-10-03 02:01:58.992 
Epoch 265/1000 
	 loss: 390.3195, MinusLogProbMetric: 390.3195, val_loss: 394.8704, val_MinusLogProbMetric: 394.8704

Epoch 265: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.3195 - MinusLogProbMetric: 390.3195 - val_loss: 394.8704 - val_MinusLogProbMetric: 394.8704 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 266/1000
2023-10-03 02:02:09.320 
Epoch 266/1000 
	 loss: 390.2890, MinusLogProbMetric: 390.2890, val_loss: 395.1849, val_MinusLogProbMetric: 395.1849

Epoch 266: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.2890 - MinusLogProbMetric: 390.2890 - val_loss: 395.1849 - val_MinusLogProbMetric: 395.1849 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 267/1000
2023-10-03 02:02:19.703 
Epoch 267/1000 
	 loss: 390.3741, MinusLogProbMetric: 390.3741, val_loss: 395.9918, val_MinusLogProbMetric: 395.9918

Epoch 267: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.3741 - MinusLogProbMetric: 390.3741 - val_loss: 395.9918 - val_MinusLogProbMetric: 395.9918 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 268/1000
2023-10-03 02:02:30.159 
Epoch 268/1000 
	 loss: 390.2904, MinusLogProbMetric: 390.2904, val_loss: 395.2385, val_MinusLogProbMetric: 395.2385

Epoch 268: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.2904 - MinusLogProbMetric: 390.2904 - val_loss: 395.2385 - val_MinusLogProbMetric: 395.2385 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 269/1000
2023-10-03 02:02:40.592 
Epoch 269/1000 
	 loss: 390.2471, MinusLogProbMetric: 390.2471, val_loss: 394.7062, val_MinusLogProbMetric: 394.7062

Epoch 269: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.2471 - MinusLogProbMetric: 390.2471 - val_loss: 394.7062 - val_MinusLogProbMetric: 394.7062 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 270/1000
2023-10-03 02:02:51.121 
Epoch 270/1000 
	 loss: 390.2014, MinusLogProbMetric: 390.2014, val_loss: 394.7116, val_MinusLogProbMetric: 394.7116

Epoch 270: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.2014 - MinusLogProbMetric: 390.2014 - val_loss: 394.7116 - val_MinusLogProbMetric: 394.7116 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 271/1000
2023-10-03 02:03:01.737 
Epoch 271/1000 
	 loss: 390.2687, MinusLogProbMetric: 390.2687, val_loss: 394.7437, val_MinusLogProbMetric: 394.7437

Epoch 271: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.2687 - MinusLogProbMetric: 390.2687 - val_loss: 394.7437 - val_MinusLogProbMetric: 394.7437 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 272/1000
2023-10-03 02:03:12.082 
Epoch 272/1000 
	 loss: 390.1113, MinusLogProbMetric: 390.1113, val_loss: 394.6059, val_MinusLogProbMetric: 394.6059

Epoch 272: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1113 - MinusLogProbMetric: 390.1113 - val_loss: 394.6059 - val_MinusLogProbMetric: 394.6059 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 273/1000
2023-10-03 02:03:22.581 
Epoch 273/1000 
	 loss: 390.1671, MinusLogProbMetric: 390.1671, val_loss: 395.1004, val_MinusLogProbMetric: 395.1004

Epoch 273: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1671 - MinusLogProbMetric: 390.1671 - val_loss: 395.1004 - val_MinusLogProbMetric: 395.1004 - lr: 1.6667e-04 - 10s/epoch - 54ms/step
Epoch 274/1000
2023-10-03 02:03:33.069 
Epoch 274/1000 
	 loss: 390.2398, MinusLogProbMetric: 390.2398, val_loss: 394.6659, val_MinusLogProbMetric: 394.6659

Epoch 274: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.2398 - MinusLogProbMetric: 390.2398 - val_loss: 394.6659 - val_MinusLogProbMetric: 394.6659 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 275/1000
2023-10-03 02:03:43.592 
Epoch 275/1000 
	 loss: 390.3104, MinusLogProbMetric: 390.3104, val_loss: 394.7358, val_MinusLogProbMetric: 394.7358

Epoch 275: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.3104 - MinusLogProbMetric: 390.3104 - val_loss: 394.7358 - val_MinusLogProbMetric: 394.7358 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 276/1000
2023-10-03 02:03:54.087 
Epoch 276/1000 
	 loss: 390.1407, MinusLogProbMetric: 390.1407, val_loss: 394.4992, val_MinusLogProbMetric: 394.4992

Epoch 276: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1407 - MinusLogProbMetric: 390.1407 - val_loss: 394.4992 - val_MinusLogProbMetric: 394.4992 - lr: 1.6667e-04 - 10s/epoch - 54ms/step
Epoch 277/1000
2023-10-03 02:04:04.408 
Epoch 277/1000 
	 loss: 390.1085, MinusLogProbMetric: 390.1085, val_loss: 394.8999, val_MinusLogProbMetric: 394.8999

Epoch 277: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1085 - MinusLogProbMetric: 390.1085 - val_loss: 394.8999 - val_MinusLogProbMetric: 394.8999 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 278/1000
2023-10-03 02:04:14.885 
Epoch 278/1000 
	 loss: 390.2039, MinusLogProbMetric: 390.2039, val_loss: 394.6186, val_MinusLogProbMetric: 394.6186

Epoch 278: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.2039 - MinusLogProbMetric: 390.2039 - val_loss: 394.6186 - val_MinusLogProbMetric: 394.6186 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 279/1000
2023-10-03 02:04:25.392 
Epoch 279/1000 
	 loss: 390.0660, MinusLogProbMetric: 390.0660, val_loss: 395.4697, val_MinusLogProbMetric: 395.4697

Epoch 279: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.0660 - MinusLogProbMetric: 390.0660 - val_loss: 395.4697 - val_MinusLogProbMetric: 395.4697 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 280/1000
2023-10-03 02:04:35.812 
Epoch 280/1000 
	 loss: 390.2851, MinusLogProbMetric: 390.2851, val_loss: 395.1452, val_MinusLogProbMetric: 395.1452

Epoch 280: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.2851 - MinusLogProbMetric: 390.2851 - val_loss: 395.1452 - val_MinusLogProbMetric: 395.1452 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 281/1000
2023-10-03 02:04:46.362 
Epoch 281/1000 
	 loss: 390.1442, MinusLogProbMetric: 390.1442, val_loss: 394.8668, val_MinusLogProbMetric: 394.8668

Epoch 281: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.1442 - MinusLogProbMetric: 390.1442 - val_loss: 394.8668 - val_MinusLogProbMetric: 394.8668 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 282/1000
2023-10-03 02:04:56.666 
Epoch 282/1000 
	 loss: 390.0896, MinusLogProbMetric: 390.0896, val_loss: 394.5976, val_MinusLogProbMetric: 394.5976

Epoch 282: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.0896 - MinusLogProbMetric: 390.0896 - val_loss: 394.5976 - val_MinusLogProbMetric: 394.5976 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 283/1000
2023-10-03 02:05:07.160 
Epoch 283/1000 
	 loss: 390.0979, MinusLogProbMetric: 390.0979, val_loss: 394.7761, val_MinusLogProbMetric: 394.7761

Epoch 283: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.0979 - MinusLogProbMetric: 390.0979 - val_loss: 394.7761 - val_MinusLogProbMetric: 394.7761 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 284/1000
2023-10-03 02:05:17.479 
Epoch 284/1000 
	 loss: 390.0545, MinusLogProbMetric: 390.0545, val_loss: 394.7974, val_MinusLogProbMetric: 394.7974

Epoch 284: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.0545 - MinusLogProbMetric: 390.0545 - val_loss: 394.7974 - val_MinusLogProbMetric: 394.7974 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 285/1000
2023-10-03 02:05:27.844 
Epoch 285/1000 
	 loss: 390.0927, MinusLogProbMetric: 390.0927, val_loss: 394.8939, val_MinusLogProbMetric: 394.8939

Epoch 285: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.0927 - MinusLogProbMetric: 390.0927 - val_loss: 394.8939 - val_MinusLogProbMetric: 394.8939 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 286/1000
2023-10-03 02:05:38.232 
Epoch 286/1000 
	 loss: 389.9670, MinusLogProbMetric: 389.9670, val_loss: 394.9860, val_MinusLogProbMetric: 394.9860

Epoch 286: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.9670 - MinusLogProbMetric: 389.9670 - val_loss: 394.9860 - val_MinusLogProbMetric: 394.9860 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 287/1000
2023-10-03 02:05:48.517 
Epoch 287/1000 
	 loss: 390.1454, MinusLogProbMetric: 390.1454, val_loss: 394.7102, val_MinusLogProbMetric: 394.7102

Epoch 287: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1454 - MinusLogProbMetric: 390.1454 - val_loss: 394.7102 - val_MinusLogProbMetric: 394.7102 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 288/1000
2023-10-03 02:05:59.058 
Epoch 288/1000 
	 loss: 390.1773, MinusLogProbMetric: 390.1773, val_loss: 394.9920, val_MinusLogProbMetric: 394.9920

Epoch 288: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.1773 - MinusLogProbMetric: 390.1773 - val_loss: 394.9920 - val_MinusLogProbMetric: 394.9920 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 289/1000
2023-10-03 02:06:09.469 
Epoch 289/1000 
	 loss: 389.9797, MinusLogProbMetric: 389.9797, val_loss: 394.4788, val_MinusLogProbMetric: 394.4788

Epoch 289: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.9797 - MinusLogProbMetric: 389.9797 - val_loss: 394.4788 - val_MinusLogProbMetric: 394.4788 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 290/1000
2023-10-03 02:06:19.889 
Epoch 290/1000 
	 loss: 390.1824, MinusLogProbMetric: 390.1824, val_loss: 394.7657, val_MinusLogProbMetric: 394.7657

Epoch 290: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1824 - MinusLogProbMetric: 390.1824 - val_loss: 394.7657 - val_MinusLogProbMetric: 394.7657 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 291/1000
2023-10-03 02:06:30.435 
Epoch 291/1000 
	 loss: 390.2701, MinusLogProbMetric: 390.2701, val_loss: 394.3726, val_MinusLogProbMetric: 394.3726

Epoch 291: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.2701 - MinusLogProbMetric: 390.2701 - val_loss: 394.3726 - val_MinusLogProbMetric: 394.3726 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 292/1000
2023-10-03 02:06:40.803 
Epoch 292/1000 
	 loss: 389.9675, MinusLogProbMetric: 389.9675, val_loss: 395.4886, val_MinusLogProbMetric: 395.4886

Epoch 292: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.9675 - MinusLogProbMetric: 389.9675 - val_loss: 395.4886 - val_MinusLogProbMetric: 395.4886 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 293/1000
2023-10-03 02:06:51.029 
Epoch 293/1000 
	 loss: 390.1667, MinusLogProbMetric: 390.1667, val_loss: 394.6250, val_MinusLogProbMetric: 394.6250

Epoch 293: val_loss did not improve from 394.33749
196/196 - 10s - loss: 390.1667 - MinusLogProbMetric: 390.1667 - val_loss: 394.6250 - val_MinusLogProbMetric: 394.6250 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 294/1000
2023-10-03 02:07:01.344 
Epoch 294/1000 
	 loss: 389.9172, MinusLogProbMetric: 389.9172, val_loss: 395.1139, val_MinusLogProbMetric: 395.1139

Epoch 294: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.9172 - MinusLogProbMetric: 389.9172 - val_loss: 395.1139 - val_MinusLogProbMetric: 395.1139 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 295/1000
2023-10-03 02:07:11.724 
Epoch 295/1000 
	 loss: 389.9859, MinusLogProbMetric: 389.9859, val_loss: 395.6181, val_MinusLogProbMetric: 395.6181

Epoch 295: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.9859 - MinusLogProbMetric: 389.9859 - val_loss: 395.6181 - val_MinusLogProbMetric: 395.6181 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 296/1000
2023-10-03 02:07:21.966 
Epoch 296/1000 
	 loss: 389.9403, MinusLogProbMetric: 389.9403, val_loss: 394.5738, val_MinusLogProbMetric: 394.5738

Epoch 296: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.9403 - MinusLogProbMetric: 389.9403 - val_loss: 394.5738 - val_MinusLogProbMetric: 394.5738 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 297/1000
2023-10-03 02:07:32.505 
Epoch 297/1000 
	 loss: 389.9740, MinusLogProbMetric: 389.9740, val_loss: 394.6761, val_MinusLogProbMetric: 394.6761

Epoch 297: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.9740 - MinusLogProbMetric: 389.9740 - val_loss: 394.6761 - val_MinusLogProbMetric: 394.6761 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 298/1000
2023-10-03 02:07:43.062 
Epoch 298/1000 
	 loss: 389.9688, MinusLogProbMetric: 389.9688, val_loss: 394.8517, val_MinusLogProbMetric: 394.8517

Epoch 298: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.9688 - MinusLogProbMetric: 389.9688 - val_loss: 394.8517 - val_MinusLogProbMetric: 394.8517 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 299/1000
2023-10-03 02:07:53.538 
Epoch 299/1000 
	 loss: 389.9778, MinusLogProbMetric: 389.9778, val_loss: 394.5342, val_MinusLogProbMetric: 394.5342

Epoch 299: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.9778 - MinusLogProbMetric: 389.9778 - val_loss: 394.5342 - val_MinusLogProbMetric: 394.5342 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 300/1000
2023-10-03 02:08:04.505 
Epoch 300/1000 
	 loss: 389.9915, MinusLogProbMetric: 389.9915, val_loss: 394.8145, val_MinusLogProbMetric: 394.8145

Epoch 300: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.9915 - MinusLogProbMetric: 389.9915 - val_loss: 394.8145 - val_MinusLogProbMetric: 394.8145 - lr: 1.6667e-04 - 11s/epoch - 56ms/step
Epoch 301/1000
2023-10-03 02:08:15.219 
Epoch 301/1000 
	 loss: 389.8671, MinusLogProbMetric: 389.8671, val_loss: 396.1050, val_MinusLogProbMetric: 396.1050

Epoch 301: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.8671 - MinusLogProbMetric: 389.8671 - val_loss: 396.1050 - val_MinusLogProbMetric: 396.1050 - lr: 1.6667e-04 - 11s/epoch - 55ms/step
Epoch 302/1000
2023-10-03 02:08:25.785 
Epoch 302/1000 
	 loss: 389.9179, MinusLogProbMetric: 389.9179, val_loss: 395.8741, val_MinusLogProbMetric: 395.8741

Epoch 302: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.9179 - MinusLogProbMetric: 389.9179 - val_loss: 395.8741 - val_MinusLogProbMetric: 395.8741 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 303/1000
2023-10-03 02:08:36.596 
Epoch 303/1000 
	 loss: 389.8480, MinusLogProbMetric: 389.8480, val_loss: 394.8316, val_MinusLogProbMetric: 394.8316

Epoch 303: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.8480 - MinusLogProbMetric: 389.8480 - val_loss: 394.8316 - val_MinusLogProbMetric: 394.8316 - lr: 1.6667e-04 - 11s/epoch - 55ms/step
Epoch 304/1000
2023-10-03 02:08:46.965 
Epoch 304/1000 
	 loss: 389.7484, MinusLogProbMetric: 389.7484, val_loss: 395.6717, val_MinusLogProbMetric: 395.6717

Epoch 304: val_loss did not improve from 394.33749
196/196 - 10s - loss: 389.7484 - MinusLogProbMetric: 389.7484 - val_loss: 395.6717 - val_MinusLogProbMetric: 395.6717 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 305/1000
2023-10-03 02:08:57.525 
Epoch 305/1000 
	 loss: 390.0082, MinusLogProbMetric: 390.0082, val_loss: 395.5928, val_MinusLogProbMetric: 395.5928

Epoch 305: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.0082 - MinusLogProbMetric: 390.0082 - val_loss: 395.5928 - val_MinusLogProbMetric: 395.5928 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 306/1000
2023-10-03 02:09:08.133 
Epoch 306/1000 
	 loss: 389.8071, MinusLogProbMetric: 389.8071, val_loss: 394.5758, val_MinusLogProbMetric: 394.5758

Epoch 306: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.8071 - MinusLogProbMetric: 389.8071 - val_loss: 394.5758 - val_MinusLogProbMetric: 394.5758 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 307/1000
2023-10-03 02:09:18.685 
Epoch 307/1000 
	 loss: 389.9444, MinusLogProbMetric: 389.9444, val_loss: 394.6678, val_MinusLogProbMetric: 394.6678

Epoch 307: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.9444 - MinusLogProbMetric: 389.9444 - val_loss: 394.6678 - val_MinusLogProbMetric: 394.6678 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 308/1000
2023-10-03 02:09:29.298 
Epoch 308/1000 
	 loss: 389.8994, MinusLogProbMetric: 389.8994, val_loss: 395.4338, val_MinusLogProbMetric: 395.4338

Epoch 308: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.8994 - MinusLogProbMetric: 389.8994 - val_loss: 395.4338 - val_MinusLogProbMetric: 395.4338 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 309/1000
2023-10-03 02:09:39.860 
Epoch 309/1000 
	 loss: 389.8575, MinusLogProbMetric: 389.8575, val_loss: 394.7145, val_MinusLogProbMetric: 394.7145

Epoch 309: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.8575 - MinusLogProbMetric: 389.8575 - val_loss: 394.7145 - val_MinusLogProbMetric: 394.7145 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 310/1000
2023-10-03 02:09:50.399 
Epoch 310/1000 
	 loss: 389.7061, MinusLogProbMetric: 389.7061, val_loss: 397.6569, val_MinusLogProbMetric: 397.6569

Epoch 310: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.7061 - MinusLogProbMetric: 389.7061 - val_loss: 397.6569 - val_MinusLogProbMetric: 397.6569 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 311/1000
2023-10-03 02:10:01.101 
Epoch 311/1000 
	 loss: 390.0378, MinusLogProbMetric: 390.0378, val_loss: 394.7327, val_MinusLogProbMetric: 394.7327

Epoch 311: val_loss did not improve from 394.33749
196/196 - 11s - loss: 390.0378 - MinusLogProbMetric: 390.0378 - val_loss: 394.7327 - val_MinusLogProbMetric: 394.7327 - lr: 1.6667e-04 - 11s/epoch - 55ms/step
Epoch 312/1000
2023-10-03 02:10:11.873 
Epoch 312/1000 
	 loss: 389.6780, MinusLogProbMetric: 389.6780, val_loss: 394.8478, val_MinusLogProbMetric: 394.8478

Epoch 312: val_loss did not improve from 394.33749
196/196 - 11s - loss: 389.6780 - MinusLogProbMetric: 389.6780 - val_loss: 394.8478 - val_MinusLogProbMetric: 394.8478 - lr: 1.6667e-04 - 11s/epoch - 55ms/step
Epoch 313/1000
2023-10-03 02:10:22.315 
Epoch 313/1000 
	 loss: 388.7464, MinusLogProbMetric: 388.7464, val_loss: 393.9674, val_MinusLogProbMetric: 393.9674

Epoch 313: val_loss improved from 394.33749 to 393.96741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 388.7464 - MinusLogProbMetric: 388.7464 - val_loss: 393.9674 - val_MinusLogProbMetric: 393.9674 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 314/1000
2023-10-03 02:10:33.266 
Epoch 314/1000 
	 loss: 388.6817, MinusLogProbMetric: 388.6817, val_loss: 394.3311, val_MinusLogProbMetric: 394.3311

Epoch 314: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.6817 - MinusLogProbMetric: 388.6817 - val_loss: 394.3311 - val_MinusLogProbMetric: 394.3311 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 315/1000
2023-10-03 02:10:43.762 
Epoch 315/1000 
	 loss: 388.7084, MinusLogProbMetric: 388.7084, val_loss: 394.0657, val_MinusLogProbMetric: 394.0657

Epoch 315: val_loss did not improve from 393.96741
196/196 - 10s - loss: 388.7084 - MinusLogProbMetric: 388.7084 - val_loss: 394.0657 - val_MinusLogProbMetric: 394.0657 - lr: 8.3333e-05 - 10s/epoch - 54ms/step
Epoch 316/1000
2023-10-03 02:10:54.523 
Epoch 316/1000 
	 loss: 388.6852, MinusLogProbMetric: 388.6852, val_loss: 394.0673, val_MinusLogProbMetric: 394.0673

Epoch 316: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.6852 - MinusLogProbMetric: 388.6852 - val_loss: 394.0673 - val_MinusLogProbMetric: 394.0673 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 317/1000
2023-10-03 02:11:05.089 
Epoch 317/1000 
	 loss: 388.6720, MinusLogProbMetric: 388.6720, val_loss: 394.4777, val_MinusLogProbMetric: 394.4777

Epoch 317: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.6720 - MinusLogProbMetric: 388.6720 - val_loss: 394.4777 - val_MinusLogProbMetric: 394.4777 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 318/1000
2023-10-03 02:11:15.629 
Epoch 318/1000 
	 loss: 388.7150, MinusLogProbMetric: 388.7150, val_loss: 394.0248, val_MinusLogProbMetric: 394.0248

Epoch 318: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.7150 - MinusLogProbMetric: 388.7150 - val_loss: 394.0248 - val_MinusLogProbMetric: 394.0248 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 319/1000
2023-10-03 02:11:26.313 
Epoch 319/1000 
	 loss: 388.6471, MinusLogProbMetric: 388.6471, val_loss: 394.0510, val_MinusLogProbMetric: 394.0510

Epoch 319: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.6471 - MinusLogProbMetric: 388.6471 - val_loss: 394.0510 - val_MinusLogProbMetric: 394.0510 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 320/1000
2023-10-03 02:11:36.806 
Epoch 320/1000 
	 loss: 388.6740, MinusLogProbMetric: 388.6740, val_loss: 394.1933, val_MinusLogProbMetric: 394.1933

Epoch 320: val_loss did not improve from 393.96741
196/196 - 10s - loss: 388.6740 - MinusLogProbMetric: 388.6740 - val_loss: 394.1933 - val_MinusLogProbMetric: 394.1933 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 321/1000
2023-10-03 02:11:47.453 
Epoch 321/1000 
	 loss: 388.7120, MinusLogProbMetric: 388.7120, val_loss: 394.0339, val_MinusLogProbMetric: 394.0339

Epoch 321: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.7120 - MinusLogProbMetric: 388.7120 - val_loss: 394.0339 - val_MinusLogProbMetric: 394.0339 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 322/1000
2023-10-03 02:11:58.094 
Epoch 322/1000 
	 loss: 388.6493, MinusLogProbMetric: 388.6493, val_loss: 394.2635, val_MinusLogProbMetric: 394.2635

Epoch 322: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.6493 - MinusLogProbMetric: 388.6493 - val_loss: 394.2635 - val_MinusLogProbMetric: 394.2635 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 323/1000
2023-10-03 02:12:08.627 
Epoch 323/1000 
	 loss: 388.6932, MinusLogProbMetric: 388.6932, val_loss: 394.2676, val_MinusLogProbMetric: 394.2676

Epoch 323: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.6932 - MinusLogProbMetric: 388.6932 - val_loss: 394.2676 - val_MinusLogProbMetric: 394.2676 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 324/1000
2023-10-03 02:12:19.329 
Epoch 324/1000 
	 loss: 388.6252, MinusLogProbMetric: 388.6252, val_loss: 394.1102, val_MinusLogProbMetric: 394.1102

Epoch 324: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.6252 - MinusLogProbMetric: 388.6252 - val_loss: 394.1102 - val_MinusLogProbMetric: 394.1102 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 325/1000
2023-10-03 02:12:29.852 
Epoch 325/1000 
	 loss: 388.7102, MinusLogProbMetric: 388.7102, val_loss: 394.2167, val_MinusLogProbMetric: 394.2167

Epoch 325: val_loss did not improve from 393.96741
196/196 - 11s - loss: 388.7102 - MinusLogProbMetric: 388.7102 - val_loss: 394.2167 - val_MinusLogProbMetric: 394.2167 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 326/1000
2023-10-03 02:12:40.375 
Epoch 326/1000 
	 loss: 388.6847, MinusLogProbMetric: 388.6847, val_loss: 393.9097, val_MinusLogProbMetric: 393.9097

Epoch 326: val_loss improved from 393.96741 to 393.90970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_344/weights/best_weights.h5
196/196 - 11s - loss: 388.6847 - MinusLogProbMetric: 388.6847 - val_loss: 393.9097 - val_MinusLogProbMetric: 393.9097 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 327/1000
2023-10-03 02:12:51.218 
Epoch 327/1000 
	 loss: 388.6480, MinusLogProbMetric: 388.6480, val_loss: 394.3039, val_MinusLogProbMetric: 394.3039

Epoch 327: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.6480 - MinusLogProbMetric: 388.6480 - val_loss: 394.3039 - val_MinusLogProbMetric: 394.3039 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 328/1000
2023-10-03 02:13:01.737 
Epoch 328/1000 
	 loss: 388.6449, MinusLogProbMetric: 388.6449, val_loss: 394.3606, val_MinusLogProbMetric: 394.3606

Epoch 328: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6449 - MinusLogProbMetric: 388.6449 - val_loss: 394.3606 - val_MinusLogProbMetric: 394.3606 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 329/1000
2023-10-03 02:13:12.461 
Epoch 329/1000 
	 loss: 388.6576, MinusLogProbMetric: 388.6576, val_loss: 394.1603, val_MinusLogProbMetric: 394.1603

Epoch 329: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6576 - MinusLogProbMetric: 388.6576 - val_loss: 394.1603 - val_MinusLogProbMetric: 394.1603 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 330/1000
2023-10-03 02:13:23.135 
Epoch 330/1000 
	 loss: 388.6534, MinusLogProbMetric: 388.6534, val_loss: 394.0408, val_MinusLogProbMetric: 394.0408

Epoch 330: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6534 - MinusLogProbMetric: 388.6534 - val_loss: 394.0408 - val_MinusLogProbMetric: 394.0408 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 331/1000
2023-10-03 02:13:33.894 
Epoch 331/1000 
	 loss: 388.6038, MinusLogProbMetric: 388.6038, val_loss: 394.1687, val_MinusLogProbMetric: 394.1687

Epoch 331: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6038 - MinusLogProbMetric: 388.6038 - val_loss: 394.1687 - val_MinusLogProbMetric: 394.1687 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 332/1000
2023-10-03 02:13:44.518 
Epoch 332/1000 
	 loss: 388.6275, MinusLogProbMetric: 388.6275, val_loss: 394.2357, val_MinusLogProbMetric: 394.2357

Epoch 332: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6275 - MinusLogProbMetric: 388.6275 - val_loss: 394.2357 - val_MinusLogProbMetric: 394.2357 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 333/1000
2023-10-03 02:13:55.039 
Epoch 333/1000 
	 loss: 388.7356, MinusLogProbMetric: 388.7356, val_loss: 393.9711, val_MinusLogProbMetric: 393.9711

Epoch 333: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.7356 - MinusLogProbMetric: 388.7356 - val_loss: 393.9711 - val_MinusLogProbMetric: 393.9711 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 334/1000
2023-10-03 02:14:05.573 
Epoch 334/1000 
	 loss: 388.5567, MinusLogProbMetric: 388.5567, val_loss: 394.1877, val_MinusLogProbMetric: 394.1877

Epoch 334: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5567 - MinusLogProbMetric: 388.5567 - val_loss: 394.1877 - val_MinusLogProbMetric: 394.1877 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 335/1000
2023-10-03 02:14:16.214 
Epoch 335/1000 
	 loss: 388.6757, MinusLogProbMetric: 388.6757, val_loss: 394.8106, val_MinusLogProbMetric: 394.8106

Epoch 335: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6757 - MinusLogProbMetric: 388.6757 - val_loss: 394.8106 - val_MinusLogProbMetric: 394.8106 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 336/1000
2023-10-03 02:14:26.604 
Epoch 336/1000 
	 loss: 388.6203, MinusLogProbMetric: 388.6203, val_loss: 394.1823, val_MinusLogProbMetric: 394.1823

Epoch 336: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.6203 - MinusLogProbMetric: 388.6203 - val_loss: 394.1823 - val_MinusLogProbMetric: 394.1823 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 337/1000
2023-10-03 02:14:37.201 
Epoch 337/1000 
	 loss: 388.6018, MinusLogProbMetric: 388.6018, val_loss: 394.1065, val_MinusLogProbMetric: 394.1065

Epoch 337: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6018 - MinusLogProbMetric: 388.6018 - val_loss: 394.1065 - val_MinusLogProbMetric: 394.1065 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 338/1000
2023-10-03 02:14:47.735 
Epoch 338/1000 
	 loss: 388.5705, MinusLogProbMetric: 388.5705, val_loss: 394.0466, val_MinusLogProbMetric: 394.0466

Epoch 338: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5705 - MinusLogProbMetric: 388.5705 - val_loss: 394.0466 - val_MinusLogProbMetric: 394.0466 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 339/1000
2023-10-03 02:14:58.384 
Epoch 339/1000 
	 loss: 388.6048, MinusLogProbMetric: 388.6048, val_loss: 394.1146, val_MinusLogProbMetric: 394.1146

Epoch 339: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6048 - MinusLogProbMetric: 388.6048 - val_loss: 394.1146 - val_MinusLogProbMetric: 394.1146 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 340/1000
2023-10-03 02:15:09.078 
Epoch 340/1000 
	 loss: 388.5896, MinusLogProbMetric: 388.5896, val_loss: 394.1463, val_MinusLogProbMetric: 394.1463

Epoch 340: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5896 - MinusLogProbMetric: 388.5896 - val_loss: 394.1463 - val_MinusLogProbMetric: 394.1463 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 341/1000
2023-10-03 02:15:19.510 
Epoch 341/1000 
	 loss: 388.6025, MinusLogProbMetric: 388.6025, val_loss: 394.3558, val_MinusLogProbMetric: 394.3558

Epoch 341: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.6025 - MinusLogProbMetric: 388.6025 - val_loss: 394.3558 - val_MinusLogProbMetric: 394.3558 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 342/1000
2023-10-03 02:15:30.169 
Epoch 342/1000 
	 loss: 388.5843, MinusLogProbMetric: 388.5843, val_loss: 394.2808, val_MinusLogProbMetric: 394.2808

Epoch 342: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5843 - MinusLogProbMetric: 388.5843 - val_loss: 394.2808 - val_MinusLogProbMetric: 394.2808 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 343/1000
2023-10-03 02:15:40.910 
Epoch 343/1000 
	 loss: 388.5434, MinusLogProbMetric: 388.5434, val_loss: 394.2623, val_MinusLogProbMetric: 394.2623

Epoch 343: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5434 - MinusLogProbMetric: 388.5434 - val_loss: 394.2623 - val_MinusLogProbMetric: 394.2623 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 344/1000
2023-10-03 02:15:51.428 
Epoch 344/1000 
	 loss: 388.5652, MinusLogProbMetric: 388.5652, val_loss: 394.1609, val_MinusLogProbMetric: 394.1609

Epoch 344: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5652 - MinusLogProbMetric: 388.5652 - val_loss: 394.1609 - val_MinusLogProbMetric: 394.1609 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 345/1000
2023-10-03 02:16:02.025 
Epoch 345/1000 
	 loss: 388.6107, MinusLogProbMetric: 388.6107, val_loss: 393.9570, val_MinusLogProbMetric: 393.9570

Epoch 345: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6107 - MinusLogProbMetric: 388.6107 - val_loss: 393.9570 - val_MinusLogProbMetric: 393.9570 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 346/1000
2023-10-03 02:16:12.420 
Epoch 346/1000 
	 loss: 388.5829, MinusLogProbMetric: 388.5829, val_loss: 394.6485, val_MinusLogProbMetric: 394.6485

Epoch 346: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.5829 - MinusLogProbMetric: 388.5829 - val_loss: 394.6485 - val_MinusLogProbMetric: 394.6485 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 347/1000
2023-10-03 02:16:23.209 
Epoch 347/1000 
	 loss: 388.5143, MinusLogProbMetric: 388.5143, val_loss: 394.1126, val_MinusLogProbMetric: 394.1126

Epoch 347: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5143 - MinusLogProbMetric: 388.5143 - val_loss: 394.1126 - val_MinusLogProbMetric: 394.1126 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 348/1000
2023-10-03 02:16:33.797 
Epoch 348/1000 
	 loss: 388.6099, MinusLogProbMetric: 388.6099, val_loss: 394.6469, val_MinusLogProbMetric: 394.6469

Epoch 348: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.6099 - MinusLogProbMetric: 388.6099 - val_loss: 394.6469 - val_MinusLogProbMetric: 394.6469 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 349/1000
2023-10-03 02:16:44.531 
Epoch 349/1000 
	 loss: 388.5639, MinusLogProbMetric: 388.5639, val_loss: 394.3207, val_MinusLogProbMetric: 394.3207

Epoch 349: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5639 - MinusLogProbMetric: 388.5639 - val_loss: 394.3207 - val_MinusLogProbMetric: 394.3207 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 350/1000
2023-10-03 02:16:55.401 
Epoch 350/1000 
	 loss: 388.5768, MinusLogProbMetric: 388.5768, val_loss: 394.2493, val_MinusLogProbMetric: 394.2493

Epoch 350: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5768 - MinusLogProbMetric: 388.5768 - val_loss: 394.2493 - val_MinusLogProbMetric: 394.2493 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 351/1000
2023-10-03 02:17:06.063 
Epoch 351/1000 
	 loss: 388.5327, MinusLogProbMetric: 388.5327, val_loss: 394.4080, val_MinusLogProbMetric: 394.4080

Epoch 351: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5327 - MinusLogProbMetric: 388.5327 - val_loss: 394.4080 - val_MinusLogProbMetric: 394.4080 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 352/1000
2023-10-03 02:17:16.595 
Epoch 352/1000 
	 loss: 388.5563, MinusLogProbMetric: 388.5563, val_loss: 394.0497, val_MinusLogProbMetric: 394.0497

Epoch 352: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5563 - MinusLogProbMetric: 388.5563 - val_loss: 394.0497 - val_MinusLogProbMetric: 394.0497 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 353/1000
2023-10-03 02:17:27.195 
Epoch 353/1000 
	 loss: 388.4734, MinusLogProbMetric: 388.4734, val_loss: 394.1960, val_MinusLogProbMetric: 394.1960

Epoch 353: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.4734 - MinusLogProbMetric: 388.4734 - val_loss: 394.1960 - val_MinusLogProbMetric: 394.1960 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 354/1000
2023-10-03 02:17:37.669 
Epoch 354/1000 
	 loss: 388.3848, MinusLogProbMetric: 388.3848, val_loss: 394.2659, val_MinusLogProbMetric: 394.2659

Epoch 354: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.3848 - MinusLogProbMetric: 388.3848 - val_loss: 394.2659 - val_MinusLogProbMetric: 394.2659 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 355/1000
2023-10-03 02:17:48.343 
Epoch 355/1000 
	 loss: 388.5205, MinusLogProbMetric: 388.5205, val_loss: 394.1038, val_MinusLogProbMetric: 394.1038

Epoch 355: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5205 - MinusLogProbMetric: 388.5205 - val_loss: 394.1038 - val_MinusLogProbMetric: 394.1038 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 356/1000
2023-10-03 02:17:59.066 
Epoch 356/1000 
	 loss: 388.4517, MinusLogProbMetric: 388.4517, val_loss: 394.3604, val_MinusLogProbMetric: 394.3604

Epoch 356: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.4517 - MinusLogProbMetric: 388.4517 - val_loss: 394.3604 - val_MinusLogProbMetric: 394.3604 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 357/1000
2023-10-03 02:18:09.647 
Epoch 357/1000 
	 loss: 388.4777, MinusLogProbMetric: 388.4777, val_loss: 394.3010, val_MinusLogProbMetric: 394.3010

Epoch 357: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.4777 - MinusLogProbMetric: 388.4777 - val_loss: 394.3010 - val_MinusLogProbMetric: 394.3010 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 358/1000
2023-10-03 02:18:20.292 
Epoch 358/1000 
	 loss: 388.4668, MinusLogProbMetric: 388.4668, val_loss: 394.2158, val_MinusLogProbMetric: 394.2158

Epoch 358: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.4668 - MinusLogProbMetric: 388.4668 - val_loss: 394.2158 - val_MinusLogProbMetric: 394.2158 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 359/1000
2023-10-03 02:18:30.840 
Epoch 359/1000 
	 loss: 388.4576, MinusLogProbMetric: 388.4576, val_loss: 394.2881, val_MinusLogProbMetric: 394.2881

Epoch 359: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.4576 - MinusLogProbMetric: 388.4576 - val_loss: 394.2881 - val_MinusLogProbMetric: 394.2881 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 360/1000
2023-10-03 02:18:41.492 
Epoch 360/1000 
	 loss: 388.4742, MinusLogProbMetric: 388.4742, val_loss: 394.1559, val_MinusLogProbMetric: 394.1559

Epoch 360: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.4742 - MinusLogProbMetric: 388.4742 - val_loss: 394.1559 - val_MinusLogProbMetric: 394.1559 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 361/1000
2023-10-03 02:18:51.959 
Epoch 361/1000 
	 loss: 388.3689, MinusLogProbMetric: 388.3689, val_loss: 394.2540, val_MinusLogProbMetric: 394.2540

Epoch 361: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.3689 - MinusLogProbMetric: 388.3689 - val_loss: 394.2540 - val_MinusLogProbMetric: 394.2540 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 362/1000
2023-10-03 02:19:02.518 
Epoch 362/1000 
	 loss: 388.5032, MinusLogProbMetric: 388.5032, val_loss: 394.2232, val_MinusLogProbMetric: 394.2232

Epoch 362: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5032 - MinusLogProbMetric: 388.5032 - val_loss: 394.2232 - val_MinusLogProbMetric: 394.2232 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 363/1000
2023-10-03 02:19:13.411 
Epoch 363/1000 
	 loss: 388.5761, MinusLogProbMetric: 388.5761, val_loss: 394.6732, val_MinusLogProbMetric: 394.6732

Epoch 363: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5761 - MinusLogProbMetric: 388.5761 - val_loss: 394.6732 - val_MinusLogProbMetric: 394.6732 - lr: 8.3333e-05 - 11s/epoch - 56ms/step
Epoch 364/1000
2023-10-03 02:19:23.939 
Epoch 364/1000 
	 loss: 388.5296, MinusLogProbMetric: 388.5296, val_loss: 394.4273, val_MinusLogProbMetric: 394.4273

Epoch 364: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5296 - MinusLogProbMetric: 388.5296 - val_loss: 394.4273 - val_MinusLogProbMetric: 394.4273 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 365/1000
2023-10-03 02:19:34.469 
Epoch 365/1000 
	 loss: 388.4968, MinusLogProbMetric: 388.4968, val_loss: 394.4132, val_MinusLogProbMetric: 394.4132

Epoch 365: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.4968 - MinusLogProbMetric: 388.4968 - val_loss: 394.4132 - val_MinusLogProbMetric: 394.4132 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 366/1000
2023-10-03 02:19:44.931 
Epoch 366/1000 
	 loss: 388.4616, MinusLogProbMetric: 388.4616, val_loss: 394.5391, val_MinusLogProbMetric: 394.5391

Epoch 366: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.4616 - MinusLogProbMetric: 388.4616 - val_loss: 394.5391 - val_MinusLogProbMetric: 394.5391 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 367/1000
2023-10-03 02:19:55.396 
Epoch 367/1000 
	 loss: 388.4780, MinusLogProbMetric: 388.4780, val_loss: 394.6747, val_MinusLogProbMetric: 394.6747

Epoch 367: val_loss did not improve from 393.90970
196/196 - 10s - loss: 388.4780 - MinusLogProbMetric: 388.4780 - val_loss: 394.6747 - val_MinusLogProbMetric: 394.6747 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 368/1000
2023-10-03 02:20:06.022 
Epoch 368/1000 
	 loss: 388.5560, MinusLogProbMetric: 388.5560, val_loss: 394.5360, val_MinusLogProbMetric: 394.5360

Epoch 368: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5560 - MinusLogProbMetric: 388.5560 - val_loss: 394.5360 - val_MinusLogProbMetric: 394.5360 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 369/1000
2023-10-03 02:20:16.587 
Epoch 369/1000 
	 loss: 388.5471, MinusLogProbMetric: 388.5471, val_loss: 394.3569, val_MinusLogProbMetric: 394.3569

Epoch 369: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5471 - MinusLogProbMetric: 388.5471 - val_loss: 394.3569 - val_MinusLogProbMetric: 394.3569 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 370/1000
2023-10-03 02:20:27.256 
Epoch 370/1000 
	 loss: 388.3364, MinusLogProbMetric: 388.3364, val_loss: 394.3435, val_MinusLogProbMetric: 394.3435

Epoch 370: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.3364 - MinusLogProbMetric: 388.3364 - val_loss: 394.3435 - val_MinusLogProbMetric: 394.3435 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 371/1000
2023-10-03 02:20:37.886 
Epoch 371/1000 
	 loss: 388.3173, MinusLogProbMetric: 388.3173, val_loss: 394.5194, val_MinusLogProbMetric: 394.5194

Epoch 371: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.3173 - MinusLogProbMetric: 388.3173 - val_loss: 394.5194 - val_MinusLogProbMetric: 394.5194 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 372/1000
2023-10-03 02:20:48.468 
Epoch 372/1000 
	 loss: 388.5444, MinusLogProbMetric: 388.5444, val_loss: 394.4945, val_MinusLogProbMetric: 394.4945

Epoch 372: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5444 - MinusLogProbMetric: 388.5444 - val_loss: 394.4945 - val_MinusLogProbMetric: 394.4945 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 373/1000
2023-10-03 02:20:59.167 
Epoch 373/1000 
	 loss: 388.3889, MinusLogProbMetric: 388.3889, val_loss: 394.4491, val_MinusLogProbMetric: 394.4491

Epoch 373: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.3889 - MinusLogProbMetric: 388.3889 - val_loss: 394.4491 - val_MinusLogProbMetric: 394.4491 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 374/1000
2023-10-03 02:21:09.942 
Epoch 374/1000 
	 loss: 388.5413, MinusLogProbMetric: 388.5413, val_loss: 394.3497, val_MinusLogProbMetric: 394.3497

Epoch 374: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5413 - MinusLogProbMetric: 388.5413 - val_loss: 394.3497 - val_MinusLogProbMetric: 394.3497 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 375/1000
2023-10-03 02:21:20.543 
Epoch 375/1000 
	 loss: 388.3650, MinusLogProbMetric: 388.3650, val_loss: 394.3732, val_MinusLogProbMetric: 394.3732

Epoch 375: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.3650 - MinusLogProbMetric: 388.3650 - val_loss: 394.3732 - val_MinusLogProbMetric: 394.3732 - lr: 8.3333e-05 - 11s/epoch - 54ms/step
Epoch 376/1000
2023-10-03 02:21:31.417 
Epoch 376/1000 
	 loss: 388.5109, MinusLogProbMetric: 388.5109, val_loss: 394.7177, val_MinusLogProbMetric: 394.7177

Epoch 376: val_loss did not improve from 393.90970
196/196 - 11s - loss: 388.5109 - MinusLogProbMetric: 388.5109 - val_loss: 394.7177 - val_MinusLogProbMetric: 394.7177 - lr: 8.3333e-05 - 11s/epoch - 55ms/step
Epoch 377/1000
2023-10-03 02:21:41.881 
Epoch 377/1000 
	 loss: 387.9529, MinusLogProbMetric: 387.9529, val_loss: 394.1247, val_MinusLogProbMetric: 394.1247

Epoch 377: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.9529 - MinusLogProbMetric: 387.9529 - val_loss: 394.1247 - val_MinusLogProbMetric: 394.1247 - lr: 4.1667e-05 - 10s/epoch - 53ms/step
Epoch 378/1000
2023-10-03 02:21:52.498 
Epoch 378/1000 
	 loss: 387.9286, MinusLogProbMetric: 387.9286, val_loss: 394.0217, val_MinusLogProbMetric: 394.0217

Epoch 378: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9286 - MinusLogProbMetric: 387.9286 - val_loss: 394.0217 - val_MinusLogProbMetric: 394.0217 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 379/1000
2023-10-03 02:22:03.264 
Epoch 379/1000 
	 loss: 387.9104, MinusLogProbMetric: 387.9104, val_loss: 394.0778, val_MinusLogProbMetric: 394.0778

Epoch 379: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9104 - MinusLogProbMetric: 387.9104 - val_loss: 394.0778 - val_MinusLogProbMetric: 394.0778 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 380/1000
2023-10-03 02:22:14.289 
Epoch 380/1000 
	 loss: 387.8959, MinusLogProbMetric: 387.8959, val_loss: 393.9633, val_MinusLogProbMetric: 393.9633

Epoch 380: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8959 - MinusLogProbMetric: 387.8959 - val_loss: 393.9633 - val_MinusLogProbMetric: 393.9633 - lr: 4.1667e-05 - 11s/epoch - 56ms/step
Epoch 381/1000
2023-10-03 02:22:24.871 
Epoch 381/1000 
	 loss: 387.9162, MinusLogProbMetric: 387.9162, val_loss: 394.2568, val_MinusLogProbMetric: 394.2568

Epoch 381: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9162 - MinusLogProbMetric: 387.9162 - val_loss: 394.2568 - val_MinusLogProbMetric: 394.2568 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 382/1000
2023-10-03 02:22:35.524 
Epoch 382/1000 
	 loss: 387.8919, MinusLogProbMetric: 387.8919, val_loss: 394.1283, val_MinusLogProbMetric: 394.1283

Epoch 382: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8919 - MinusLogProbMetric: 387.8919 - val_loss: 394.1283 - val_MinusLogProbMetric: 394.1283 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 383/1000
2023-10-03 02:22:46.097 
Epoch 383/1000 
	 loss: 387.8780, MinusLogProbMetric: 387.8780, val_loss: 394.0585, val_MinusLogProbMetric: 394.0585

Epoch 383: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8780 - MinusLogProbMetric: 387.8780 - val_loss: 394.0585 - val_MinusLogProbMetric: 394.0585 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 384/1000
2023-10-03 02:22:56.612 
Epoch 384/1000 
	 loss: 387.9072, MinusLogProbMetric: 387.9072, val_loss: 394.1141, val_MinusLogProbMetric: 394.1141

Epoch 384: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9072 - MinusLogProbMetric: 387.9072 - val_loss: 394.1141 - val_MinusLogProbMetric: 394.1141 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 385/1000
2023-10-03 02:23:07.097 
Epoch 385/1000 
	 loss: 387.8812, MinusLogProbMetric: 387.8812, val_loss: 393.9689, val_MinusLogProbMetric: 393.9689

Epoch 385: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.8812 - MinusLogProbMetric: 387.8812 - val_loss: 393.9689 - val_MinusLogProbMetric: 393.9689 - lr: 4.1667e-05 - 10s/epoch - 53ms/step
Epoch 386/1000
2023-10-03 02:23:17.848 
Epoch 386/1000 
	 loss: 387.8814, MinusLogProbMetric: 387.8814, val_loss: 394.1690, val_MinusLogProbMetric: 394.1690

Epoch 386: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8814 - MinusLogProbMetric: 387.8814 - val_loss: 394.1690 - val_MinusLogProbMetric: 394.1690 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 387/1000
2023-10-03 02:23:28.564 
Epoch 387/1000 
	 loss: 387.8894, MinusLogProbMetric: 387.8894, val_loss: 393.9896, val_MinusLogProbMetric: 393.9896

Epoch 387: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8894 - MinusLogProbMetric: 387.8894 - val_loss: 393.9896 - val_MinusLogProbMetric: 393.9896 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 388/1000
2023-10-03 02:23:39.149 
Epoch 388/1000 
	 loss: 387.9054, MinusLogProbMetric: 387.9054, val_loss: 394.1832, val_MinusLogProbMetric: 394.1832

Epoch 388: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9054 - MinusLogProbMetric: 387.9054 - val_loss: 394.1832 - val_MinusLogProbMetric: 394.1832 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 389/1000
2023-10-03 02:23:49.889 
Epoch 389/1000 
	 loss: 387.9046, MinusLogProbMetric: 387.9046, val_loss: 394.1674, val_MinusLogProbMetric: 394.1674

Epoch 389: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9046 - MinusLogProbMetric: 387.9046 - val_loss: 394.1674 - val_MinusLogProbMetric: 394.1674 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 390/1000
2023-10-03 02:24:00.612 
Epoch 390/1000 
	 loss: 387.8748, MinusLogProbMetric: 387.8748, val_loss: 394.1262, val_MinusLogProbMetric: 394.1262

Epoch 390: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8748 - MinusLogProbMetric: 387.8748 - val_loss: 394.1262 - val_MinusLogProbMetric: 394.1262 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 391/1000
2023-10-03 02:24:11.443 
Epoch 391/1000 
	 loss: 387.8739, MinusLogProbMetric: 387.8739, val_loss: 394.2209, val_MinusLogProbMetric: 394.2209

Epoch 391: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8739 - MinusLogProbMetric: 387.8739 - val_loss: 394.2209 - val_MinusLogProbMetric: 394.2209 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 392/1000
2023-10-03 02:24:22.127 
Epoch 392/1000 
	 loss: 387.8775, MinusLogProbMetric: 387.8775, val_loss: 394.1978, val_MinusLogProbMetric: 394.1978

Epoch 392: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8775 - MinusLogProbMetric: 387.8775 - val_loss: 394.1978 - val_MinusLogProbMetric: 394.1978 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 393/1000
2023-10-03 02:24:32.688 
Epoch 393/1000 
	 loss: 387.8831, MinusLogProbMetric: 387.8831, val_loss: 394.1904, val_MinusLogProbMetric: 394.1904

Epoch 393: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8831 - MinusLogProbMetric: 387.8831 - val_loss: 394.1904 - val_MinusLogProbMetric: 394.1904 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 394/1000
2023-10-03 02:24:43.087 
Epoch 394/1000 
	 loss: 387.8504, MinusLogProbMetric: 387.8504, val_loss: 394.1801, val_MinusLogProbMetric: 394.1801

Epoch 394: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.8504 - MinusLogProbMetric: 387.8504 - val_loss: 394.1801 - val_MinusLogProbMetric: 394.1801 - lr: 4.1667e-05 - 10s/epoch - 53ms/step
Epoch 395/1000
2023-10-03 02:24:53.698 
Epoch 395/1000 
	 loss: 387.8950, MinusLogProbMetric: 387.8950, val_loss: 394.1881, val_MinusLogProbMetric: 394.1881

Epoch 395: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8950 - MinusLogProbMetric: 387.8950 - val_loss: 394.1881 - val_MinusLogProbMetric: 394.1881 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 396/1000
2023-10-03 02:25:04.338 
Epoch 396/1000 
	 loss: 387.8452, MinusLogProbMetric: 387.8452, val_loss: 394.0955, val_MinusLogProbMetric: 394.0955

Epoch 396: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8452 - MinusLogProbMetric: 387.8452 - val_loss: 394.0955 - val_MinusLogProbMetric: 394.0955 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 397/1000
2023-10-03 02:25:14.861 
Epoch 397/1000 
	 loss: 387.9256, MinusLogProbMetric: 387.9256, val_loss: 394.3634, val_MinusLogProbMetric: 394.3634

Epoch 397: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9256 - MinusLogProbMetric: 387.9256 - val_loss: 394.3634 - val_MinusLogProbMetric: 394.3634 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 398/1000
2023-10-03 02:25:25.567 
Epoch 398/1000 
	 loss: 387.9048, MinusLogProbMetric: 387.9048, val_loss: 394.3870, val_MinusLogProbMetric: 394.3870

Epoch 398: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.9048 - MinusLogProbMetric: 387.9048 - val_loss: 394.3870 - val_MinusLogProbMetric: 394.3870 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 399/1000
2023-10-03 02:25:36.286 
Epoch 399/1000 
	 loss: 387.8803, MinusLogProbMetric: 387.8803, val_loss: 394.1990, val_MinusLogProbMetric: 394.1990

Epoch 399: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8803 - MinusLogProbMetric: 387.8803 - val_loss: 394.1990 - val_MinusLogProbMetric: 394.1990 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 400/1000
2023-10-03 02:25:46.951 
Epoch 400/1000 
	 loss: 387.8558, MinusLogProbMetric: 387.8558, val_loss: 394.0685, val_MinusLogProbMetric: 394.0685

Epoch 400: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8558 - MinusLogProbMetric: 387.8558 - val_loss: 394.0685 - val_MinusLogProbMetric: 394.0685 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 401/1000
2023-10-03 02:25:57.328 
Epoch 401/1000 
	 loss: 387.8173, MinusLogProbMetric: 387.8173, val_loss: 394.3096, val_MinusLogProbMetric: 394.3096

Epoch 401: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.8173 - MinusLogProbMetric: 387.8173 - val_loss: 394.3096 - val_MinusLogProbMetric: 394.3096 - lr: 4.1667e-05 - 10s/epoch - 53ms/step
Epoch 402/1000
2023-10-03 02:26:07.993 
Epoch 402/1000 
	 loss: 387.8745, MinusLogProbMetric: 387.8745, val_loss: 394.2578, val_MinusLogProbMetric: 394.2578

Epoch 402: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8745 - MinusLogProbMetric: 387.8745 - val_loss: 394.2578 - val_MinusLogProbMetric: 394.2578 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 403/1000
2023-10-03 02:26:18.405 
Epoch 403/1000 
	 loss: 387.8589, MinusLogProbMetric: 387.8589, val_loss: 394.1860, val_MinusLogProbMetric: 394.1860

Epoch 403: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.8589 - MinusLogProbMetric: 387.8589 - val_loss: 394.1860 - val_MinusLogProbMetric: 394.1860 - lr: 4.1667e-05 - 10s/epoch - 53ms/step
Epoch 404/1000
2023-10-03 02:26:28.901 
Epoch 404/1000 
	 loss: 387.8274, MinusLogProbMetric: 387.8274, val_loss: 394.1519, val_MinusLogProbMetric: 394.1519

Epoch 404: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.8274 - MinusLogProbMetric: 387.8274 - val_loss: 394.1519 - val_MinusLogProbMetric: 394.1519 - lr: 4.1667e-05 - 10s/epoch - 54ms/step
Epoch 405/1000
2023-10-03 02:26:39.495 
Epoch 405/1000 
	 loss: 387.8908, MinusLogProbMetric: 387.8908, val_loss: 394.2153, val_MinusLogProbMetric: 394.2153

Epoch 405: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8908 - MinusLogProbMetric: 387.8908 - val_loss: 394.2153 - val_MinusLogProbMetric: 394.2153 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 406/1000
2023-10-03 02:26:50.022 
Epoch 406/1000 
	 loss: 387.8317, MinusLogProbMetric: 387.8317, val_loss: 394.1397, val_MinusLogProbMetric: 394.1397

Epoch 406: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8317 - MinusLogProbMetric: 387.8317 - val_loss: 394.1397 - val_MinusLogProbMetric: 394.1397 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 407/1000
2023-10-03 02:27:00.666 
Epoch 407/1000 
	 loss: 387.8322, MinusLogProbMetric: 387.8322, val_loss: 394.2466, val_MinusLogProbMetric: 394.2466

Epoch 407: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8322 - MinusLogProbMetric: 387.8322 - val_loss: 394.2466 - val_MinusLogProbMetric: 394.2466 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 408/1000
2023-10-03 02:27:11.235 
Epoch 408/1000 
	 loss: 387.8970, MinusLogProbMetric: 387.8970, val_loss: 394.2449, val_MinusLogProbMetric: 394.2449

Epoch 408: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8970 - MinusLogProbMetric: 387.8970 - val_loss: 394.2449 - val_MinusLogProbMetric: 394.2449 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 409/1000
2023-10-03 02:27:21.896 
Epoch 409/1000 
	 loss: 387.8672, MinusLogProbMetric: 387.8672, val_loss: 394.3620, val_MinusLogProbMetric: 394.3620

Epoch 409: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8672 - MinusLogProbMetric: 387.8672 - val_loss: 394.3620 - val_MinusLogProbMetric: 394.3620 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 410/1000
2023-10-03 02:27:32.654 
Epoch 410/1000 
	 loss: 387.8595, MinusLogProbMetric: 387.8595, val_loss: 394.3166, val_MinusLogProbMetric: 394.3166

Epoch 410: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8595 - MinusLogProbMetric: 387.8595 - val_loss: 394.3166 - val_MinusLogProbMetric: 394.3166 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 411/1000
2023-10-03 02:27:43.279 
Epoch 411/1000 
	 loss: 387.8541, MinusLogProbMetric: 387.8541, val_loss: 394.3085, val_MinusLogProbMetric: 394.3085

Epoch 411: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8541 - MinusLogProbMetric: 387.8541 - val_loss: 394.3085 - val_MinusLogProbMetric: 394.3085 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 412/1000
2023-10-03 02:27:53.786 
Epoch 412/1000 
	 loss: 387.8857, MinusLogProbMetric: 387.8857, val_loss: 394.4838, val_MinusLogProbMetric: 394.4838

Epoch 412: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8857 - MinusLogProbMetric: 387.8857 - val_loss: 394.4838 - val_MinusLogProbMetric: 394.4838 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 413/1000
2023-10-03 02:28:04.231 
Epoch 413/1000 
	 loss: 387.8688, MinusLogProbMetric: 387.8688, val_loss: 394.3372, val_MinusLogProbMetric: 394.3372

Epoch 413: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.8688 - MinusLogProbMetric: 387.8688 - val_loss: 394.3372 - val_MinusLogProbMetric: 394.3372 - lr: 4.1667e-05 - 10s/epoch - 53ms/step
Epoch 414/1000
2023-10-03 02:28:14.881 
Epoch 414/1000 
	 loss: 387.8985, MinusLogProbMetric: 387.8985, val_loss: 394.2752, val_MinusLogProbMetric: 394.2752

Epoch 414: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8985 - MinusLogProbMetric: 387.8985 - val_loss: 394.2752 - val_MinusLogProbMetric: 394.2752 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 415/1000
2023-10-03 02:28:25.752 
Epoch 415/1000 
	 loss: 387.8382, MinusLogProbMetric: 387.8382, val_loss: 394.3025, val_MinusLogProbMetric: 394.3025

Epoch 415: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8382 - MinusLogProbMetric: 387.8382 - val_loss: 394.3025 - val_MinusLogProbMetric: 394.3025 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 416/1000
2023-10-03 02:28:35.469 
Epoch 416/1000 
	 loss: 387.8471, MinusLogProbMetric: 387.8471, val_loss: 394.2195, val_MinusLogProbMetric: 394.2195

Epoch 416: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.8471 - MinusLogProbMetric: 387.8471 - val_loss: 394.2195 - val_MinusLogProbMetric: 394.2195 - lr: 4.1667e-05 - 10s/epoch - 50ms/step
Epoch 417/1000
2023-10-03 02:28:45.571 
Epoch 417/1000 
	 loss: 387.7750, MinusLogProbMetric: 387.7750, val_loss: 394.2130, val_MinusLogProbMetric: 394.2130

Epoch 417: val_loss did not improve from 393.90970
196/196 - 10s - loss: 387.7750 - MinusLogProbMetric: 387.7750 - val_loss: 394.2130 - val_MinusLogProbMetric: 394.2130 - lr: 4.1667e-05 - 10s/epoch - 52ms/step
Epoch 418/1000
2023-10-03 02:28:56.132 
Epoch 418/1000 
	 loss: 387.7983, MinusLogProbMetric: 387.7983, val_loss: 394.3836, val_MinusLogProbMetric: 394.3836

Epoch 418: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.7983 - MinusLogProbMetric: 387.7983 - val_loss: 394.3836 - val_MinusLogProbMetric: 394.3836 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 419/1000
2023-10-03 02:29:06.664 
Epoch 419/1000 
	 loss: 387.7837, MinusLogProbMetric: 387.7837, val_loss: 394.5437, val_MinusLogProbMetric: 394.5437

Epoch 419: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.7837 - MinusLogProbMetric: 387.7837 - val_loss: 394.5437 - val_MinusLogProbMetric: 394.5437 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 420/1000
2023-10-03 02:29:17.292 
Epoch 420/1000 
	 loss: 387.8165, MinusLogProbMetric: 387.8165, val_loss: 394.5585, val_MinusLogProbMetric: 394.5585

Epoch 420: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8165 - MinusLogProbMetric: 387.8165 - val_loss: 394.5585 - val_MinusLogProbMetric: 394.5585 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 421/1000
2023-10-03 02:29:27.967 
Epoch 421/1000 
	 loss: 387.8519, MinusLogProbMetric: 387.8519, val_loss: 394.2421, val_MinusLogProbMetric: 394.2421

Epoch 421: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8519 - MinusLogProbMetric: 387.8519 - val_loss: 394.2421 - val_MinusLogProbMetric: 394.2421 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 422/1000
2023-10-03 02:29:38.721 
Epoch 422/1000 
	 loss: 387.8233, MinusLogProbMetric: 387.8233, val_loss: 394.3278, val_MinusLogProbMetric: 394.3278

Epoch 422: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8233 - MinusLogProbMetric: 387.8233 - val_loss: 394.3278 - val_MinusLogProbMetric: 394.3278 - lr: 4.1667e-05 - 11s/epoch - 55ms/step
Epoch 423/1000
2023-10-03 02:29:49.404 
Epoch 423/1000 
	 loss: 387.8034, MinusLogProbMetric: 387.8034, val_loss: 394.3200, val_MinusLogProbMetric: 394.3200

Epoch 423: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.8034 - MinusLogProbMetric: 387.8034 - val_loss: 394.3200 - val_MinusLogProbMetric: 394.3200 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 424/1000
2023-10-03 02:29:59.930 
Epoch 424/1000 
	 loss: 387.7862, MinusLogProbMetric: 387.7862, val_loss: 394.1890, val_MinusLogProbMetric: 394.1890

Epoch 424: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.7862 - MinusLogProbMetric: 387.7862 - val_loss: 394.1890 - val_MinusLogProbMetric: 394.1890 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 425/1000
2023-10-03 02:30:10.618 
Epoch 425/1000 
	 loss: 387.7798, MinusLogProbMetric: 387.7798, val_loss: 394.2944, val_MinusLogProbMetric: 394.2944

Epoch 425: val_loss did not improve from 393.90970
196/196 - 11s - loss: 387.7798 - MinusLogProbMetric: 387.7798 - val_loss: 394.2944 - val_MinusLogProbMetric: 394.2944 - lr: 4.1667e-05 - 11s/epoch - 54ms/step
Epoch 426/1000
2023-10-03 02:30:21.284 
Epoch 426/1000 
	 loss: 387.8046, MinusLogProbMetric: 387.8046, val_loss: 394.3267, val_MinusLogProbMetric: 394.3267

Epoch 426: val_loss did not improve from 393.90970
Restoring model weights from the end of the best epoch: 326.
196/196 - 11s - loss: 387.8046 - MinusLogProbMetric: 387.8046 - val_loss: 394.3267 - val_MinusLogProbMetric: 394.3267 - lr: 4.1667e-05 - 11s/epoch - 56ms/step
Epoch 426: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 5498.059901991044 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 4384.742785860901 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 2884.269189466024 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 2907.5639042970724 seconds.
Training succeeded with seed 541.
Model trained in 4516.15 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 15750.80 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 15751.03 s.
===========
Run 344/360 done in 20484.49 s.
===========

Directory ../../results/MAFN_new/run_345/ already exists.
Skipping it.
===========
Run 345/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_346/ already exists.
Skipping it.
===========
Run 346/360 already exists. Skipping it.
===========

===========
Generating train data for run 347.
===========
Train data generated in 0.59 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_347/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3948703 ,  0.07333986,  4.824906  , ...,  4.8231826 ,
         6.7160444 ,  5.1919713 ],
       [ 7.527156  ,  4.892852  ,  5.1847315 , ...,  3.2493904 ,
         7.69857   ,  7.0736637 ],
       [ 8.031752  ,  5.095358  ,  5.2594748 , ...,  3.4701555 ,
         7.370471  ,  6.9199452 ],
       ...,
       [ 4.686364  ,  7.541738  ,  5.587613  , ..., 11.703795  ,
         1.091339  ,  6.8638806 ],
       [ 6.2255516 ,  0.770295  ,  4.788709  , ...,  4.377611  ,
         6.3573318 ,  5.7629805 ],
       [ 5.9599223 ,  7.873254  ,  6.7175126 , ...,  9.16981   ,
         2.2715137 ,  6.9812255 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_347/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_347
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4597425  7.4750543  6.6229253 ... 10.50112    2.3549712  7.263988 ]
 [ 8.090607   4.4556823  5.302927  ...  4.338169   8.2116     6.803887 ]
 [ 8.792193   4.87096    5.2850995 ...  3.0267322  9.017456   6.6197205]
 ...
 [ 5.637016   7.1742353  5.1522284 ...  9.028178   2.8305635  6.6811485]
 [ 5.43644    6.4432745  4.307521  ... 11.6883335  1.2334509  6.240246 ]
 [ 5.473915  -0.1999113  4.7921906 ...  4.5114264  6.3191977  4.607543 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_67 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  4191520   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7fbb2c14d930>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb76c5e90c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb76c5e90c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb10c7b040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb10c6b1f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb76c5afa30>, <keras.callbacks.ModelCheckpoint object at 0x7fbb2c1294b0>, <keras.callbacks.EarlyStopping object at 0x7fbb10c28d60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb76c71cd00>, <keras.callbacks.TerminateOnNaN object at 0x7fb76c5af340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3948703 ,  0.07333986,  4.824906  , ...,  4.8231826 ,
         6.7160444 ,  5.1919713 ],
       [ 7.527156  ,  4.892852  ,  5.1847315 , ...,  3.2493904 ,
         7.69857   ,  7.0736637 ],
       [ 8.031752  ,  5.095358  ,  5.2594748 , ...,  3.4701555 ,
         7.370471  ,  6.9199452 ],
       ...,
       [ 4.686364  ,  7.541738  ,  5.587613  , ..., 11.703795  ,
         1.091339  ,  6.8638806 ],
       [ 6.2255516 ,  0.770295  ,  4.788709  , ...,  4.377611  ,
         6.3573318 ,  5.7629805 ],
       [ 5.9599223 ,  7.873254  ,  6.7175126 , ...,  9.16981   ,
         2.2715137 ,  6.9812255 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_347/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 347/360 with hyperparameters:
timestamp = 2023-10-03 06:52:54.879627
ndims = 1000
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.45974255e+00  7.47505426e+00  6.62292528e+00  5.05389071e+00
  4.76988316e+00  6.51706886e+00  4.61368465e+00  8.87996101e+00
  9.47148323e+00  3.13882399e+00  8.15863609e+00  4.04901981e+00
  5.94622993e+00  9.59517097e+00  5.27031362e-01  9.14469540e-01
 -4.25633848e-01  8.85004520e+00  8.86627579e+00  8.10460377e+00
  8.33615780e+00  7.79615164e+00  5.63374424e+00  7.47048998e+00
  2.67367125e+00  7.45005274e+00  1.46889400e+00  9.48713875e+00
  4.45138645e+00  3.44658661e+00  1.58531225e+00  7.55554485e+00
  4.54993439e+00  5.62136745e+00  1.87653869e-01  5.38039684e+00
  5.88209438e+00  6.11452675e+00  9.12329006e+00  6.77755785e+00
  3.99515772e+00  3.94083071e+00  6.10655642e+00  3.21156979e-01
  6.36000967e+00  6.49021864e+00  2.01867962e+00  1.28013515e+00
  3.13630557e+00  3.42113018e+00  5.20442581e+00  3.83941078e+00
  9.61144161e+00  1.18064260e+00  1.86078823e+00  1.39309692e+00
  6.26731920e+00  1.24294591e+00  4.54989815e+00  2.37354779e+00
  1.59302151e+00  8.57221782e-01  8.70844173e+00  9.19696510e-01
  2.42238069e+00  2.51755524e+00  9.30487442e+00  7.17357993e-01
  1.01590328e+01  8.79718781e-01  9.74404812e+00  4.70245838e+00
  1.04586010e+01  5.84102201e+00  7.06233120e+00  6.08054399e-02
  2.87881780e+00  1.23544180e+00  3.24556351e+00  1.98769331e+00
  2.80081964e+00  3.94175434e+00  8.52806807e-01  6.88022709e+00
  6.25178528e+00  2.62791204e+00  5.12537289e+00  8.85466695e-01
  4.60176182e+00  9.75250530e+00  3.30948663e+00  5.90994501e+00
  3.94674838e-01  5.89646006e+00  1.92416787e+00  1.81163979e+00
  6.37247372e+00 -3.15570325e-01  8.20219803e+00  1.28168628e-01
  7.22887707e+00  3.14081120e+00  7.60052204e+00  9.29899693e+00
  2.11811924e+00  5.41208363e+00  5.78380585e+00  6.18891096e+00
  7.72233963e-01  9.45670033e+00  4.67343092e+00  8.87408257e+00
  7.51366615e+00  2.70754457e+00  8.02781963e+00  3.81814051e+00
  8.82700539e+00  6.33719015e+00  9.25009441e+00  7.09183693e+00
  7.94002724e+00  5.46600914e+00  9.63299751e+00  6.13893270e+00
  3.46606255e+00  6.14386940e+00 -2.56343067e-01  3.19621682e+00
  6.56198120e+00  2.83842921e+00  6.16884851e+00  4.85251045e+00
  1.63704062e+00  4.03154516e+00  5.67415953e+00  5.39694262e+00
  6.00454140e+00  6.96711063e+00  8.21759605e+00  4.40230036e+00
  8.91812229e+00  3.90368319e+00  4.31117773e+00  8.84699631e+00
  7.71821785e+00  5.99877834e+00  1.34880948e+00  9.03424835e+00
  7.34347820e+00  9.75238895e+00  1.30448639e+00  8.10313702e+00
  1.39094162e+00  5.52686214e+00  1.39628828e+00  8.36674118e+00
  8.21545887e+00  6.90715551e+00  4.54284096e+00  7.04149008e-02
  7.05558109e+00  4.50027800e+00  7.33852243e+00  8.72509861e+00
  9.65448093e+00  8.68677807e+00 -4.90361631e-01  4.37572622e+00
  7.25791740e+00  9.91550207e-01  4.08981705e+00  2.76633114e-01
  1.60020745e+00  3.36497605e-01  7.31811380e+00  2.10338998e+00
  3.16644406e+00  8.89157963e+00  7.15983629e+00  5.86556315e-01
  1.36882353e+00  5.96005535e+00  5.70284748e+00  2.79882360e+00
  9.13490582e+00  6.36294985e+00  5.37789011e+00  5.81836224e+00
  7.21320534e+00  3.13198853e+00  3.97226763e+00  2.15087318e+00
  2.59190130e+00  9.09463596e+00  6.32991552e+00  5.24338341e+00
  1.83617842e+00  2.83599472e+00  1.27747989e+00  3.64649510e+00
  3.92681050e+00  6.30186605e+00  3.65409207e+00  1.71731174e+00
  4.57058102e-01  6.44787788e-01  6.84724951e+00  5.33881903e+00
  5.50001097e+00  9.11246872e+00  9.46778202e+00  1.21239305e+00
  6.76341343e+00  3.13921571e+00 -1.14767998e-02  6.57409286e+00
  3.70978022e+00  3.60852528e+00  5.72658730e+00  8.33034039e+00
  6.29358435e+00  7.69290590e+00  2.87814116e+00  7.86957026e+00
  1.86863732e+00  8.07503319e+00  6.81132603e+00  1.76750231e+00
  7.73322582e+00  7.78794718e+00  2.71473527e+00  2.22814250e+00
  5.22247171e+00 -1.78495497e-01  2.04993224e+00  4.19815350e+00
  3.59715009e+00  5.19453287e+00  3.03882885e+00  4.10586357e+00
  8.65846729e+00  1.19190323e+00  6.68609619e+00  1.66431999e+00
  7.27933407e+00  4.02774668e+00  5.75244665e+00  1.96100402e+00
  7.97178507e-01  4.87493467e+00  2.94125009e+00  9.41046333e+00
  8.84683418e+00  8.29783726e+00  8.16620922e+00  4.45752352e-01
  5.44015598e+00  5.52257824e+00  9.84137154e+00  2.83907056e+00
  3.48917341e+00  4.82754290e-01 -1.74913824e-01  1.04859200e+01
  6.57606125e+00  7.43873882e+00  2.78162265e+00  5.85191441e+00
  3.05306137e-01  6.27210426e+00  9.80071545e+00  9.59279919e+00
  2.62908721e+00  9.63581371e+00  1.95981288e+00  1.04123640e+01
  9.35608292e+00  8.62648869e+00  6.40449333e+00  9.14164925e+00
  2.92951584e+00  8.30910683e+00  6.14546776e+00  9.93399322e-02
  3.28324938e+00  1.23329997e+00  1.12952938e+01  4.61616898e+00
  4.12088060e+00  6.59266615e+00  3.26731110e+00  1.36949062e+00
  8.32833958e+00  2.11090016e+00  5.21115446e+00  2.09654713e+00
  1.17157912e+00  8.66014004e+00  9.83761024e+00  1.03202505e+01
  8.95115089e+00  7.73069334e+00  3.11215138e+00  5.79455554e-01
  3.35496473e+00  2.48578620e+00  1.17842543e+00  9.28994656e-01
  8.02578545e+00 -3.24027464e-02  7.61129618e+00  1.40131593e+00
  4.49355245e-01 -6.77786291e-01  7.64368153e+00  2.64335203e+00
  4.92156458e+00  5.71151590e+00  8.60295391e+00  7.29667330e+00
  3.11098886e+00  1.71513534e+00 -7.64608085e-01  2.72550774e+00
  2.64428568e+00  4.54984236e+00  6.19169712e+00  6.74699068e+00
  2.22437668e+00  4.10193872e+00  2.40145898e+00  8.30602837e+00
  2.10803837e-01  6.90420532e+00  8.09623623e+00  9.01992607e+00
  4.52709913e+00  3.54713321e+00  5.50466681e+00  2.92401195e+00
  4.02959442e+00  2.48848724e+00  4.68494892e+00 -7.88432956e-02
  7.66714287e+00  7.01911807e-01  5.05187416e+00  2.81661105e+00
  6.31898308e+00  9.79913521e+00  6.45021343e+00  3.90302122e-01
  4.74634457e+00  5.13307381e+00  5.31955719e+00  7.49760199e+00
  3.06803155e+00  1.98823380e+00  3.89623880e+00  9.62421513e+00
  1.78653574e+00  9.61907482e+00  4.92532825e+00  4.15603781e+00
  9.05332851e+00  4.39308786e+00  6.38956928e+00  2.94468784e+00
  9.23341846e+00  7.88347673e+00  6.98528957e+00  3.01824784e+00
  7.58317757e+00  6.56825876e+00  2.06247115e+00  1.76111794e+00
  7.39303112e+00  1.00049753e+01  5.53817558e+00  5.74712086e+00
  8.45420170e+00  4.68851423e+00  7.73527098e+00  5.08631086e+00
  8.37628078e+00  1.00118513e+01  8.22282505e+00  1.58061790e+00
  6.68740225e+00  3.19616055e+00  1.71057057e+00  3.51876926e+00
  2.64559579e+00  8.19935894e+00  2.75622308e-01  7.28872442e+00
  3.45673823e+00  4.31849051e+00 -3.76514673e-01  1.52457750e+00
  3.35252428e+00  8.09101868e+00  1.28042889e+00  9.67739201e+00
  8.88187122e+00  3.58949041e+00  2.56057358e+00  2.65730309e+00
  6.15760183e+00  4.53388333e-01  1.19133520e+00  3.52796531e+00
  1.45740986e+00  2.52151465e+00  2.22465158e+00  2.75870442e+00
  9.92789268e-01  1.08244228e+00  1.37636411e+00  6.03599548e+00
  9.74076843e+00  1.01974096e+01  3.81370091e+00  1.23543000e+00
  6.07923603e+00  4.98835278e+00  9.35852528e+00  6.89330816e-01
  8.42686272e+00  2.06397200e+00  2.33050036e+00  7.12331533e+00
  1.42261195e+00  6.61993694e+00  6.08587694e+00  4.34802675e+00
  6.97639894e+00  2.97784686e+00  7.06420803e+00  3.19350052e+00
  4.11629438e+00  7.84212160e+00  9.06661510e+00  8.12937379e-01
  2.29343772e+00  2.96382618e+00  8.46787167e+00  1.00021868e+01
  9.51810265e+00  9.04873848e+00  3.20166469e+00  1.01189880e+01
  2.24754882e+00  1.05154979e+00  9.08356571e+00  2.33975315e+00
  6.90444136e+00  6.94867551e-01  7.88924408e+00  8.30402374e+00
  2.76679182e+00  3.64484310e+00  3.52496576e+00  5.93992901e+00
  2.54696512e+00  2.45469809e+00  4.20503664e+00  3.06979060e+00
  8.99509621e+00  2.56618452e+00  5.03053999e+00  9.70699501e+00
  4.93447161e+00  8.76536655e+00  9.55170155e+00  7.72051620e+00
  7.85468483e+00  9.45400620e+00  9.85480976e+00  4.51369429e+00
  3.03846806e-01  3.01068687e+00  2.73213625e+00  4.72638273e+00
  6.81997597e-01  6.56227589e+00  3.93374228e+00  8.97276306e+00
  9.94160557e+00  5.36897039e+00  2.00578666e+00  6.65257835e+00
  8.72733593e+00  4.09124517e+00  5.57242870e+00  2.53880763e+00
  4.55094290e+00  4.12450981e+00  2.75871348e+00  6.02112818e+00
  3.06419921e+00  3.84279156e+00  6.32869959e+00  7.48365498e+00
  3.71063519e+00  8.40062141e+00  8.99558163e+00  3.85165393e-01
  2.82937360e+00  4.09715176e+00  1.89032626e+00  3.02826071e+00
  7.12571335e+00  6.77697420e+00  6.25701618e+00  7.27390385e+00
  1.56475198e+00  4.32832050e+00  6.96656275e+00  8.36603642e+00
  5.05712938e+00  2.95036817e+00 -1.28020012e+00  6.88831472e+00
  1.05642639e-01  4.12276030e+00  5.58679533e+00  9.61558628e+00
  1.09499300e+00  4.09365320e+00  1.10851243e-01  3.51607203e+00
  6.18450356e+00  3.02304435e+00  2.13838601e+00  1.26376498e+00
  6.53784370e+00  7.02125883e+00  6.88726664e+00  9.13777447e+00
  1.56519008e+00  6.35245228e+00  5.86918831e+00  3.88383627e+00
  9.53446102e+00  1.15593597e-01  7.62757015e+00  8.56912136e+00
  5.23104286e+00  2.93295860e+00  7.61587143e+00  7.90405989e-01
  4.29664516e+00  5.10346937e+00  7.41640377e+00  1.47214413e+00
  2.04542184e+00  3.92378545e+00  6.27783060e+00  9.62040997e+00
  7.83548498e+00  1.79342461e+00  1.01362457e+01  8.40956879e+00
  5.63717556e+00  9.24737167e+00  5.75774097e+00  9.05656815e+00
  8.62134171e+00  9.59310246e+00  8.82232666e+00  1.76992238e-01
  1.78288305e+00  3.43467426e+00  9.51014900e+00  3.09812832e+00
  6.34806871e-01  7.82289410e+00  4.31624603e+00  9.03288937e+00
  1.06984854e+00  6.71173191e+00  4.39347506e+00  7.48281598e-01
  6.86176300e+00  2.25088453e+00  4.68547344e-02  5.82838058e-01
  6.24903381e-01  9.17707634e+00  7.58213139e+00  8.93102455e+00
  5.94159842e+00  5.39698505e+00  3.23612070e+00  1.02735777e+01
  3.62641549e+00  5.13539696e+00  2.91478133e+00  1.08055391e+01
  1.33100975e+00  4.27372980e+00  7.08601332e+00  1.24942565e+00
  5.21018887e+00  5.17725849e+00  3.76263237e+00  1.72782254e+00
  5.43435860e+00  2.81625223e+00  3.70168638e+00  3.35317802e+00
  1.02380409e+01  1.35640597e+00  1.03041577e+00  3.38672709e+00
  6.08518553e+00  6.60374975e+00  3.25406504e+00  1.05463610e+01
  3.51051950e+00  8.32571411e+00  6.70826721e+00  8.25326157e+00
  9.08365059e+00  8.41726971e+00  7.65193129e+00  6.97491121e+00
  3.53853130e+00  1.47005427e+00  6.12256706e-01  2.91100383e+00
  4.96973658e+00  5.28968763e+00  7.29386091e+00  9.31158447e+00
  8.27733755e-01  2.05611849e+00  5.59662247e+00  6.47477531e+00
  8.73009586e+00  9.35113335e+00  9.73438644e+00  4.55514288e+00
  3.20523143e+00  2.66678190e+00  7.04346943e+00  6.32174110e+00
  7.87006044e+00  1.19646192e+00  6.47826338e+00  6.65509748e+00
  2.40423179e+00  2.57194066e+00  8.34508133e+00  1.96436489e+00
  4.91729784e+00  1.27205122e+00  4.54385710e+00  1.03603392e+01
  3.80113721e+00  8.02379799e+00  8.45891595e-01  3.21020508e+00
  3.94293404e+00  3.71692944e+00  6.80958462e+00  3.45055127e+00
  6.91424274e+00  5.92037964e+00  4.45378399e+00  3.99634552e+00
  6.72196484e+00  9.04918671e-01  8.22484493e+00  6.28523540e+00
  7.25443220e+00  5.98428154e+00  2.13958287e+00  5.96601725e+00
  4.05433273e+00  3.16244817e+00  3.72408926e-01  7.47997046e+00
  1.19348931e+00  6.13060141e+00  6.97031546e+00  6.03174973e+00
  1.03076553e+01  5.27457416e-01  8.94766140e+00  2.44996339e-01
  5.36693811e+00  3.66599870e+00  5.52877283e+00  3.59597015e+00
  1.78932524e+00 -5.45340896e-01  5.64772701e+00  9.19430447e+00
  4.42028522e+00  2.24237919e+00 -1.87708378e-01  1.47937727e+00
  1.46093225e+00  1.63989711e+00  2.42697597e+00  1.09869659e-01
  1.54315722e+00  9.01367188e+00  2.78112841e+00  6.94747066e+00
  7.93844604e+00  5.24745893e+00  1.90384924e+00  2.11306882e+00
  4.33682823e+00  5.24131823e+00  3.43558145e+00  7.75171852e+00
  7.60104418e+00  8.86727524e+00  6.22911334e-01  9.03463936e+00
  3.94151974e+00  8.80514336e+00  7.71844244e+00  9.70440960e+00
  7.63131952e+00  2.17288971e+00  4.79780149e+00  3.77800512e+00
  3.52053547e+00  2.49464464e+00  4.90790939e+00  8.35758305e+00
  2.77332044e+00  5.48718977e+00  1.04915285e+00  9.16396737e-01
  4.49069828e-01  9.94224072e+00  2.06893906e-02  1.96413362e+00
  6.02550983e+00  1.40416789e+00  8.92917919e+00  7.30807924e+00
  8.60238266e+00  5.12743139e+00  3.55479407e+00  5.54666090e+00
  1.00518589e+01  4.52478218e+00  2.58068419e+00  8.04085827e+00
  4.15188026e+00  2.39668220e-01  2.77549386e+00  3.97206068e+00
  8.12393951e+00  3.32974410e+00  4.52871943e+00  3.30285239e+00
  8.56041622e+00  9.34256077e+00  9.43628407e+00  3.80536842e+00
  9.50871277e+00  8.09606647e+00  6.72784805e+00  1.52685547e+00
  2.43745208e+00  1.77858579e+00  8.96306610e+00  2.66003942e+00
  5.01896286e+00  5.19758463e+00  7.27442312e+00  8.21808434e+00
  2.31162047e+00  4.53879166e+00  8.23070431e+00  8.05463409e+00
  7.65232205e-01  1.62057388e+00  1.49363244e+00  9.75412965e-01
  4.64891005e+00  1.02944565e+00  4.65099669e+00 -2.19611645e-01
  5.74669075e+00 -9.49231386e-02  9.73519135e+00  7.67262936e+00
  3.89164162e+00  3.33559227e+00  2.41986966e+00  4.89477873e+00
  1.57034174e-01  7.14383364e+00  3.77591443e+00  3.82858586e+00
  8.50906372e+00  7.14387608e+00  9.58337879e+00  3.36379218e+00
  8.92547989e+00  7.81011152e+00  3.75316739e+00  6.45038795e+00
  3.50248003e+00  8.11350918e+00  1.00943398e+00  1.94219565e+00
  1.89951444e+00  3.69217873e+00  6.67764759e+00  4.96922302e+00
  4.89864302e+00 -2.14187935e-01  5.40712452e+00  1.06854296e+00
  2.24141002e+00  9.07321739e+00  1.96794081e+00  7.29460907e+00
  9.10808849e+00  1.64355171e+00  5.39986610e+00  9.28897202e-01
  7.33023119e+00  6.55509567e+00  7.84478426e+00  4.82920742e+00
  8.80195045e+00  8.31086814e-01  3.01831031e+00  7.18754625e+00
  3.48268604e+00  1.67123210e+00  1.49636817e+00  6.85546112e+00
  4.74163532e+00  8.85940742e+00  7.34411335e+00  4.09574366e+00
  3.72876263e+00  3.94373155e+00  8.57115746e+00  7.89460373e+00
  2.84828901e-01  7.03732681e+00  9.92517471e+00  7.13025379e+00
  1.77010977e+00  4.93009567e+00  1.39545083e+00  3.25098395e+00
  9.61502266e+00  8.99063778e+00  2.72320175e+00  3.34499502e+00
  5.54738092e+00  9.42772484e+00  1.34932077e+00  1.72021163e+00
  7.08262205e+00  4.76709890e+00  8.45615292e+00  1.79831946e+00
  5.11601114e+00  3.42909670e+00  4.07918167e+00  5.18363333e+00
  4.65688467e+00  3.41761994e+00  8.14604378e+00  7.30806351e+00
  7.54988194e+00  3.04881263e+00  6.30283117e+00  8.95013046e+00
  7.91262293e+00  3.46671915e+00  7.21975374e+00  6.99606228e+00
  1.43251359e+00  9.02403545e+00  4.46352100e+00  4.73359394e+00
  3.29469442e+00  5.02395201e+00  6.12846851e+00  4.92078924e+00
  8.51279259e+00  6.32400131e+00  4.29928684e+00  4.10619688e+00
  9.38514519e+00  3.39132428e+00  2.31886411e+00  7.54945278e-01
  8.17271996e+00  8.96825886e+00  5.18892646e-01  3.36510205e+00
  9.62342501e-01  1.18473864e+00  8.69164753e+00 -7.18649447e-01
  1.03209085e+01  1.07548547e+00  8.92701817e+00  5.46921825e+00
  3.66492033e+00  3.53753304e+00  7.18332911e+00  3.00394678e+00
  6.41793966e+00  6.07025099e+00  4.88196707e+00  9.20016479e+00
  5.53983831e+00  6.74924755e+00  8.03201866e+00 -2.02914804e-01
  7.71807957e+00  2.31005144e+00  8.58817387e+00 -1.20083690e-01
  3.78105068e+00  1.32219195e+00  5.04449892e+00  8.16030025e+00
  2.95377922e+00  1.51961601e+00  3.25721335e+00  8.20741177e+00
  1.36698890e+00  2.09527206e+00  7.39970028e-01  7.05854177e+00
  3.41466236e+00  2.68145680e+00  7.22985363e+00  3.32064366e+00
  7.24909449e+00  7.56951714e+00  2.16439462e+00  9.11028576e+00
  1.14862895e+00  3.24883676e+00  1.62877440e-01  4.41763210e+00
  5.04922533e+00  5.14228821e+00  8.94242191e+00  9.02628326e+00
  1.40578067e+00  6.13999844e+00  1.18773472e+00  5.53503847e+00
  3.24991751e+00  3.61237216e+00  5.10492229e+00  7.41919661e+00
  6.97021663e-01  9.25158119e+00  7.16013384e+00  1.22523427e+00
  9.79707336e+00  5.12231731e+00  1.00792189e+01  1.99690437e+00
  6.10989857e+00  3.23191142e+00  3.02221370e+00  5.92228127e+00
  8.91099072e+00  8.43638611e+00  6.30670547e+00  5.66287088e+00
  9.18388176e+00  3.98044872e+00  8.69160557e+00  8.25425911e+00
  1.78963411e+00  7.77448034e+00  1.52245164e-04  3.52915049e+00
  2.29402518e+00  5.27710342e+00  7.10148954e+00  1.18256044e+00
  4.28299618e+00  1.05011196e+01  2.35497117e+00  7.26398802e+00]
Epoch 1/1000
2023-10-03 06:53:24.686 
Epoch 1/1000 
	 loss: 1647.1891, MinusLogProbMetric: 1647.1891, val_loss: 586.8745, val_MinusLogProbMetric: 586.8745

Epoch 1: val_loss improved from inf to 586.87445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 30s - loss: 1647.1891 - MinusLogProbMetric: 1647.1891 - val_loss: 586.8745 - val_MinusLogProbMetric: 586.8745 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 2/1000
2023-10-03 06:53:34.393 
Epoch 2/1000 
	 loss: 544.8544, MinusLogProbMetric: 544.8544, val_loss: 513.0887, val_MinusLogProbMetric: 513.0887

Epoch 2: val_loss improved from 586.87445 to 513.08868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 544.8544 - MinusLogProbMetric: 544.8544 - val_loss: 513.0887 - val_MinusLogProbMetric: 513.0887 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 3/1000
2023-10-03 06:53:44.329 
Epoch 3/1000 
	 loss: 506.7065, MinusLogProbMetric: 506.7065, val_loss: 505.6282, val_MinusLogProbMetric: 505.6282

Epoch 3: val_loss improved from 513.08868 to 505.62823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 506.7065 - MinusLogProbMetric: 506.7065 - val_loss: 505.6282 - val_MinusLogProbMetric: 505.6282 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 4/1000
2023-10-03 06:53:53.784 
Epoch 4/1000 
	 loss: 490.4444, MinusLogProbMetric: 490.4444, val_loss: 485.7534, val_MinusLogProbMetric: 485.7534

Epoch 4: val_loss improved from 505.62823 to 485.75339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 9s - loss: 490.4444 - MinusLogProbMetric: 490.4444 - val_loss: 485.7534 - val_MinusLogProbMetric: 485.7534 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 5/1000
2023-10-03 06:54:03.331 
Epoch 5/1000 
	 loss: 477.8293, MinusLogProbMetric: 477.8293, val_loss: 485.0552, val_MinusLogProbMetric: 485.0552

Epoch 5: val_loss improved from 485.75339 to 485.05521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 477.8293 - MinusLogProbMetric: 477.8293 - val_loss: 485.0552 - val_MinusLogProbMetric: 485.0552 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 6/1000
2023-10-03 06:54:12.822 
Epoch 6/1000 
	 loss: 469.5510, MinusLogProbMetric: 469.5510, val_loss: 461.7690, val_MinusLogProbMetric: 461.7690

Epoch 6: val_loss improved from 485.05521 to 461.76901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 469.5510 - MinusLogProbMetric: 469.5510 - val_loss: 461.7690 - val_MinusLogProbMetric: 461.7690 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 7/1000
2023-10-03 06:54:22.698 
Epoch 7/1000 
	 loss: 469.8774, MinusLogProbMetric: 469.8774, val_loss: 452.8706, val_MinusLogProbMetric: 452.8706

Epoch 7: val_loss improved from 461.76901 to 452.87061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 469.8774 - MinusLogProbMetric: 469.8774 - val_loss: 452.8706 - val_MinusLogProbMetric: 452.8706 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 8/1000
2023-10-03 06:54:32.908 
Epoch 8/1000 
	 loss: 455.7344, MinusLogProbMetric: 455.7344, val_loss: 534.1851, val_MinusLogProbMetric: 534.1851

Epoch 8: val_loss did not improve from 452.87061
196/196 - 10s - loss: 455.7344 - MinusLogProbMetric: 455.7344 - val_loss: 534.1851 - val_MinusLogProbMetric: 534.1851 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 9/1000
2023-10-03 06:54:42.228 
Epoch 9/1000 
	 loss: 454.4300, MinusLogProbMetric: 454.4300, val_loss: 463.2563, val_MinusLogProbMetric: 463.2563

Epoch 9: val_loss did not improve from 452.87061
196/196 - 9s - loss: 454.4300 - MinusLogProbMetric: 454.4300 - val_loss: 463.2563 - val_MinusLogProbMetric: 463.2563 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 10/1000
2023-10-03 06:54:51.496 
Epoch 10/1000 
	 loss: 453.5706, MinusLogProbMetric: 453.5706, val_loss: 440.8019, val_MinusLogProbMetric: 440.8019

Epoch 10: val_loss improved from 452.87061 to 440.80188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 453.5706 - MinusLogProbMetric: 453.5706 - val_loss: 440.8019 - val_MinusLogProbMetric: 440.8019 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 11/1000
2023-10-03 06:55:01.022 
Epoch 11/1000 
	 loss: 447.8907, MinusLogProbMetric: 447.8907, val_loss: 439.0020, val_MinusLogProbMetric: 439.0020

Epoch 11: val_loss improved from 440.80188 to 439.00195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 447.8907 - MinusLogProbMetric: 447.8907 - val_loss: 439.0020 - val_MinusLogProbMetric: 439.0020 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 12/1000
2023-10-03 06:55:10.797 
Epoch 12/1000 
	 loss: 441.8010, MinusLogProbMetric: 441.8010, val_loss: 451.6922, val_MinusLogProbMetric: 451.6922

Epoch 12: val_loss did not improve from 439.00195
196/196 - 9s - loss: 441.8010 - MinusLogProbMetric: 441.8010 - val_loss: 451.6922 - val_MinusLogProbMetric: 451.6922 - lr: 0.0010 - 9s/epoch - 47ms/step
Epoch 13/1000
2023-10-03 06:55:20.154 
Epoch 13/1000 
	 loss: 440.6266, MinusLogProbMetric: 440.6266, val_loss: 436.7089, val_MinusLogProbMetric: 436.7089

Epoch 13: val_loss improved from 439.00195 to 436.70886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 440.6266 - MinusLogProbMetric: 440.6266 - val_loss: 436.7089 - val_MinusLogProbMetric: 436.7089 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 14/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 108: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-03 06:55:26.128 
Epoch 14/1000 
	 loss: inf, MinusLogProbMetric: 58302.3203, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 14: val_loss did not improve from 436.70886
196/196 - 5s - loss: inf - MinusLogProbMetric: 58302.3203 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 5s/epoch - 28ms/step
The loss history contains Inf values.
Training failed: trying again with seed 821433 and lr 0.0003333333333333333.
===========
Generating train data for run 347.
===========
Train data generated in 0.60 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_347/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3948703 ,  0.07333986,  4.824906  , ...,  4.8231826 ,
         6.7160444 ,  5.1919713 ],
       [ 7.527156  ,  4.892852  ,  5.1847315 , ...,  3.2493904 ,
         7.69857   ,  7.0736637 ],
       [ 8.031752  ,  5.095358  ,  5.2594748 , ...,  3.4701555 ,
         7.370471  ,  6.9199452 ],
       ...,
       [ 4.686364  ,  7.541738  ,  5.587613  , ..., 11.703795  ,
         1.091339  ,  6.8638806 ],
       [ 6.2255516 ,  0.770295  ,  4.788709  , ...,  4.377611  ,
         6.3573318 ,  5.7629805 ],
       [ 5.9599223 ,  7.873254  ,  6.7175126 , ...,  9.16981   ,
         2.2715137 ,  6.9812255 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_347/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_347
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.4597425  7.4750543  6.6229253 ... 10.50112    2.3549712  7.263988 ]
 [ 8.090607   4.4556823  5.302927  ...  4.338169   8.2116     6.803887 ]
 [ 8.792193   4.87096    5.2850995 ...  3.0267322  9.017456   6.6197205]
 ...
 [ 5.637016   7.1742353  5.1522284 ...  9.028178   2.8305635  6.6811485]
 [ 5.43644    6.4432745  4.307521  ... 11.6883335  1.2334509  6.240246 ]
 [ 5.473915  -0.1999113  4.7921906 ...  4.5114264  6.3191977  4.607543 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_78 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  4191520   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7fb7506c7b50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb76c5b1ed0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb76c5b1ed0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb7506b4a90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb70c54c0d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb70c5c4f40>, <keras.callbacks.ModelCheckpoint object at 0x7fb70c5c5000>, <keras.callbacks.EarlyStopping object at 0x7fb70c5c5270>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb70c5c52a0>, <keras.callbacks.TerminateOnNaN object at 0x7fb70c5c4ee0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3948703 ,  0.07333986,  4.824906  , ...,  4.8231826 ,
         6.7160444 ,  5.1919713 ],
       [ 7.527156  ,  4.892852  ,  5.1847315 , ...,  3.2493904 ,
         7.69857   ,  7.0736637 ],
       [ 8.031752  ,  5.095358  ,  5.2594748 , ...,  3.4701555 ,
         7.370471  ,  6.9199452 ],
       ...,
       [ 4.686364  ,  7.541738  ,  5.587613  , ..., 11.703795  ,
         1.091339  ,  6.8638806 ],
       [ 6.2255516 ,  0.770295  ,  4.788709  , ...,  4.377611  ,
         6.3573318 ,  5.7629805 ],
       [ 5.9599223 ,  7.873254  ,  6.7175126 , ...,  9.16981   ,
         2.2715137 ,  6.9812255 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 347/360 with hyperparameters:
timestamp = 2023-10-03 06:55:28.841953
ndims = 1000
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.45974255e+00  7.47505426e+00  6.62292528e+00  5.05389071e+00
  4.76988316e+00  6.51706886e+00  4.61368465e+00  8.87996101e+00
  9.47148323e+00  3.13882399e+00  8.15863609e+00  4.04901981e+00
  5.94622993e+00  9.59517097e+00  5.27031362e-01  9.14469540e-01
 -4.25633848e-01  8.85004520e+00  8.86627579e+00  8.10460377e+00
  8.33615780e+00  7.79615164e+00  5.63374424e+00  7.47048998e+00
  2.67367125e+00  7.45005274e+00  1.46889400e+00  9.48713875e+00
  4.45138645e+00  3.44658661e+00  1.58531225e+00  7.55554485e+00
  4.54993439e+00  5.62136745e+00  1.87653869e-01  5.38039684e+00
  5.88209438e+00  6.11452675e+00  9.12329006e+00  6.77755785e+00
  3.99515772e+00  3.94083071e+00  6.10655642e+00  3.21156979e-01
  6.36000967e+00  6.49021864e+00  2.01867962e+00  1.28013515e+00
  3.13630557e+00  3.42113018e+00  5.20442581e+00  3.83941078e+00
  9.61144161e+00  1.18064260e+00  1.86078823e+00  1.39309692e+00
  6.26731920e+00  1.24294591e+00  4.54989815e+00  2.37354779e+00
  1.59302151e+00  8.57221782e-01  8.70844173e+00  9.19696510e-01
  2.42238069e+00  2.51755524e+00  9.30487442e+00  7.17357993e-01
  1.01590328e+01  8.79718781e-01  9.74404812e+00  4.70245838e+00
  1.04586010e+01  5.84102201e+00  7.06233120e+00  6.08054399e-02
  2.87881780e+00  1.23544180e+00  3.24556351e+00  1.98769331e+00
  2.80081964e+00  3.94175434e+00  8.52806807e-01  6.88022709e+00
  6.25178528e+00  2.62791204e+00  5.12537289e+00  8.85466695e-01
  4.60176182e+00  9.75250530e+00  3.30948663e+00  5.90994501e+00
  3.94674838e-01  5.89646006e+00  1.92416787e+00  1.81163979e+00
  6.37247372e+00 -3.15570325e-01  8.20219803e+00  1.28168628e-01
  7.22887707e+00  3.14081120e+00  7.60052204e+00  9.29899693e+00
  2.11811924e+00  5.41208363e+00  5.78380585e+00  6.18891096e+00
  7.72233963e-01  9.45670033e+00  4.67343092e+00  8.87408257e+00
  7.51366615e+00  2.70754457e+00  8.02781963e+00  3.81814051e+00
  8.82700539e+00  6.33719015e+00  9.25009441e+00  7.09183693e+00
  7.94002724e+00  5.46600914e+00  9.63299751e+00  6.13893270e+00
  3.46606255e+00  6.14386940e+00 -2.56343067e-01  3.19621682e+00
  6.56198120e+00  2.83842921e+00  6.16884851e+00  4.85251045e+00
  1.63704062e+00  4.03154516e+00  5.67415953e+00  5.39694262e+00
  6.00454140e+00  6.96711063e+00  8.21759605e+00  4.40230036e+00
  8.91812229e+00  3.90368319e+00  4.31117773e+00  8.84699631e+00
  7.71821785e+00  5.99877834e+00  1.34880948e+00  9.03424835e+00
  7.34347820e+00  9.75238895e+00  1.30448639e+00  8.10313702e+00
  1.39094162e+00  5.52686214e+00  1.39628828e+00  8.36674118e+00
  8.21545887e+00  6.90715551e+00  4.54284096e+00  7.04149008e-02
  7.05558109e+00  4.50027800e+00  7.33852243e+00  8.72509861e+00
  9.65448093e+00  8.68677807e+00 -4.90361631e-01  4.37572622e+00
  7.25791740e+00  9.91550207e-01  4.08981705e+00  2.76633114e-01
  1.60020745e+00  3.36497605e-01  7.31811380e+00  2.10338998e+00
  3.16644406e+00  8.89157963e+00  7.15983629e+00  5.86556315e-01
  1.36882353e+00  5.96005535e+00  5.70284748e+00  2.79882360e+00
  9.13490582e+00  6.36294985e+00  5.37789011e+00  5.81836224e+00
  7.21320534e+00  3.13198853e+00  3.97226763e+00  2.15087318e+00
  2.59190130e+00  9.09463596e+00  6.32991552e+00  5.24338341e+00
  1.83617842e+00  2.83599472e+00  1.27747989e+00  3.64649510e+00
  3.92681050e+00  6.30186605e+00  3.65409207e+00  1.71731174e+00
  4.57058102e-01  6.44787788e-01  6.84724951e+00  5.33881903e+00
  5.50001097e+00  9.11246872e+00  9.46778202e+00  1.21239305e+00
  6.76341343e+00  3.13921571e+00 -1.14767998e-02  6.57409286e+00
  3.70978022e+00  3.60852528e+00  5.72658730e+00  8.33034039e+00
  6.29358435e+00  7.69290590e+00  2.87814116e+00  7.86957026e+00
  1.86863732e+00  8.07503319e+00  6.81132603e+00  1.76750231e+00
  7.73322582e+00  7.78794718e+00  2.71473527e+00  2.22814250e+00
  5.22247171e+00 -1.78495497e-01  2.04993224e+00  4.19815350e+00
  3.59715009e+00  5.19453287e+00  3.03882885e+00  4.10586357e+00
  8.65846729e+00  1.19190323e+00  6.68609619e+00  1.66431999e+00
  7.27933407e+00  4.02774668e+00  5.75244665e+00  1.96100402e+00
  7.97178507e-01  4.87493467e+00  2.94125009e+00  9.41046333e+00
  8.84683418e+00  8.29783726e+00  8.16620922e+00  4.45752352e-01
  5.44015598e+00  5.52257824e+00  9.84137154e+00  2.83907056e+00
  3.48917341e+00  4.82754290e-01 -1.74913824e-01  1.04859200e+01
  6.57606125e+00  7.43873882e+00  2.78162265e+00  5.85191441e+00
  3.05306137e-01  6.27210426e+00  9.80071545e+00  9.59279919e+00
  2.62908721e+00  9.63581371e+00  1.95981288e+00  1.04123640e+01
  9.35608292e+00  8.62648869e+00  6.40449333e+00  9.14164925e+00
  2.92951584e+00  8.30910683e+00  6.14546776e+00  9.93399322e-02
  3.28324938e+00  1.23329997e+00  1.12952938e+01  4.61616898e+00
  4.12088060e+00  6.59266615e+00  3.26731110e+00  1.36949062e+00
  8.32833958e+00  2.11090016e+00  5.21115446e+00  2.09654713e+00
  1.17157912e+00  8.66014004e+00  9.83761024e+00  1.03202505e+01
  8.95115089e+00  7.73069334e+00  3.11215138e+00  5.79455554e-01
  3.35496473e+00  2.48578620e+00  1.17842543e+00  9.28994656e-01
  8.02578545e+00 -3.24027464e-02  7.61129618e+00  1.40131593e+00
  4.49355245e-01 -6.77786291e-01  7.64368153e+00  2.64335203e+00
  4.92156458e+00  5.71151590e+00  8.60295391e+00  7.29667330e+00
  3.11098886e+00  1.71513534e+00 -7.64608085e-01  2.72550774e+00
  2.64428568e+00  4.54984236e+00  6.19169712e+00  6.74699068e+00
  2.22437668e+00  4.10193872e+00  2.40145898e+00  8.30602837e+00
  2.10803837e-01  6.90420532e+00  8.09623623e+00  9.01992607e+00
  4.52709913e+00  3.54713321e+00  5.50466681e+00  2.92401195e+00
  4.02959442e+00  2.48848724e+00  4.68494892e+00 -7.88432956e-02
  7.66714287e+00  7.01911807e-01  5.05187416e+00  2.81661105e+00
  6.31898308e+00  9.79913521e+00  6.45021343e+00  3.90302122e-01
  4.74634457e+00  5.13307381e+00  5.31955719e+00  7.49760199e+00
  3.06803155e+00  1.98823380e+00  3.89623880e+00  9.62421513e+00
  1.78653574e+00  9.61907482e+00  4.92532825e+00  4.15603781e+00
  9.05332851e+00  4.39308786e+00  6.38956928e+00  2.94468784e+00
  9.23341846e+00  7.88347673e+00  6.98528957e+00  3.01824784e+00
  7.58317757e+00  6.56825876e+00  2.06247115e+00  1.76111794e+00
  7.39303112e+00  1.00049753e+01  5.53817558e+00  5.74712086e+00
  8.45420170e+00  4.68851423e+00  7.73527098e+00  5.08631086e+00
  8.37628078e+00  1.00118513e+01  8.22282505e+00  1.58061790e+00
  6.68740225e+00  3.19616055e+00  1.71057057e+00  3.51876926e+00
  2.64559579e+00  8.19935894e+00  2.75622308e-01  7.28872442e+00
  3.45673823e+00  4.31849051e+00 -3.76514673e-01  1.52457750e+00
  3.35252428e+00  8.09101868e+00  1.28042889e+00  9.67739201e+00
  8.88187122e+00  3.58949041e+00  2.56057358e+00  2.65730309e+00
  6.15760183e+00  4.53388333e-01  1.19133520e+00  3.52796531e+00
  1.45740986e+00  2.52151465e+00  2.22465158e+00  2.75870442e+00
  9.92789268e-01  1.08244228e+00  1.37636411e+00  6.03599548e+00
  9.74076843e+00  1.01974096e+01  3.81370091e+00  1.23543000e+00
  6.07923603e+00  4.98835278e+00  9.35852528e+00  6.89330816e-01
  8.42686272e+00  2.06397200e+00  2.33050036e+00  7.12331533e+00
  1.42261195e+00  6.61993694e+00  6.08587694e+00  4.34802675e+00
  6.97639894e+00  2.97784686e+00  7.06420803e+00  3.19350052e+00
  4.11629438e+00  7.84212160e+00  9.06661510e+00  8.12937379e-01
  2.29343772e+00  2.96382618e+00  8.46787167e+00  1.00021868e+01
  9.51810265e+00  9.04873848e+00  3.20166469e+00  1.01189880e+01
  2.24754882e+00  1.05154979e+00  9.08356571e+00  2.33975315e+00
  6.90444136e+00  6.94867551e-01  7.88924408e+00  8.30402374e+00
  2.76679182e+00  3.64484310e+00  3.52496576e+00  5.93992901e+00
  2.54696512e+00  2.45469809e+00  4.20503664e+00  3.06979060e+00
  8.99509621e+00  2.56618452e+00  5.03053999e+00  9.70699501e+00
  4.93447161e+00  8.76536655e+00  9.55170155e+00  7.72051620e+00
  7.85468483e+00  9.45400620e+00  9.85480976e+00  4.51369429e+00
  3.03846806e-01  3.01068687e+00  2.73213625e+00  4.72638273e+00
  6.81997597e-01  6.56227589e+00  3.93374228e+00  8.97276306e+00
  9.94160557e+00  5.36897039e+00  2.00578666e+00  6.65257835e+00
  8.72733593e+00  4.09124517e+00  5.57242870e+00  2.53880763e+00
  4.55094290e+00  4.12450981e+00  2.75871348e+00  6.02112818e+00
  3.06419921e+00  3.84279156e+00  6.32869959e+00  7.48365498e+00
  3.71063519e+00  8.40062141e+00  8.99558163e+00  3.85165393e-01
  2.82937360e+00  4.09715176e+00  1.89032626e+00  3.02826071e+00
  7.12571335e+00  6.77697420e+00  6.25701618e+00  7.27390385e+00
  1.56475198e+00  4.32832050e+00  6.96656275e+00  8.36603642e+00
  5.05712938e+00  2.95036817e+00 -1.28020012e+00  6.88831472e+00
  1.05642639e-01  4.12276030e+00  5.58679533e+00  9.61558628e+00
  1.09499300e+00  4.09365320e+00  1.10851243e-01  3.51607203e+00
  6.18450356e+00  3.02304435e+00  2.13838601e+00  1.26376498e+00
  6.53784370e+00  7.02125883e+00  6.88726664e+00  9.13777447e+00
  1.56519008e+00  6.35245228e+00  5.86918831e+00  3.88383627e+00
  9.53446102e+00  1.15593597e-01  7.62757015e+00  8.56912136e+00
  5.23104286e+00  2.93295860e+00  7.61587143e+00  7.90405989e-01
  4.29664516e+00  5.10346937e+00  7.41640377e+00  1.47214413e+00
  2.04542184e+00  3.92378545e+00  6.27783060e+00  9.62040997e+00
  7.83548498e+00  1.79342461e+00  1.01362457e+01  8.40956879e+00
  5.63717556e+00  9.24737167e+00  5.75774097e+00  9.05656815e+00
  8.62134171e+00  9.59310246e+00  8.82232666e+00  1.76992238e-01
  1.78288305e+00  3.43467426e+00  9.51014900e+00  3.09812832e+00
  6.34806871e-01  7.82289410e+00  4.31624603e+00  9.03288937e+00
  1.06984854e+00  6.71173191e+00  4.39347506e+00  7.48281598e-01
  6.86176300e+00  2.25088453e+00  4.68547344e-02  5.82838058e-01
  6.24903381e-01  9.17707634e+00  7.58213139e+00  8.93102455e+00
  5.94159842e+00  5.39698505e+00  3.23612070e+00  1.02735777e+01
  3.62641549e+00  5.13539696e+00  2.91478133e+00  1.08055391e+01
  1.33100975e+00  4.27372980e+00  7.08601332e+00  1.24942565e+00
  5.21018887e+00  5.17725849e+00  3.76263237e+00  1.72782254e+00
  5.43435860e+00  2.81625223e+00  3.70168638e+00  3.35317802e+00
  1.02380409e+01  1.35640597e+00  1.03041577e+00  3.38672709e+00
  6.08518553e+00  6.60374975e+00  3.25406504e+00  1.05463610e+01
  3.51051950e+00  8.32571411e+00  6.70826721e+00  8.25326157e+00
  9.08365059e+00  8.41726971e+00  7.65193129e+00  6.97491121e+00
  3.53853130e+00  1.47005427e+00  6.12256706e-01  2.91100383e+00
  4.96973658e+00  5.28968763e+00  7.29386091e+00  9.31158447e+00
  8.27733755e-01  2.05611849e+00  5.59662247e+00  6.47477531e+00
  8.73009586e+00  9.35113335e+00  9.73438644e+00  4.55514288e+00
  3.20523143e+00  2.66678190e+00  7.04346943e+00  6.32174110e+00
  7.87006044e+00  1.19646192e+00  6.47826338e+00  6.65509748e+00
  2.40423179e+00  2.57194066e+00  8.34508133e+00  1.96436489e+00
  4.91729784e+00  1.27205122e+00  4.54385710e+00  1.03603392e+01
  3.80113721e+00  8.02379799e+00  8.45891595e-01  3.21020508e+00
  3.94293404e+00  3.71692944e+00  6.80958462e+00  3.45055127e+00
  6.91424274e+00  5.92037964e+00  4.45378399e+00  3.99634552e+00
  6.72196484e+00  9.04918671e-01  8.22484493e+00  6.28523540e+00
  7.25443220e+00  5.98428154e+00  2.13958287e+00  5.96601725e+00
  4.05433273e+00  3.16244817e+00  3.72408926e-01  7.47997046e+00
  1.19348931e+00  6.13060141e+00  6.97031546e+00  6.03174973e+00
  1.03076553e+01  5.27457416e-01  8.94766140e+00  2.44996339e-01
  5.36693811e+00  3.66599870e+00  5.52877283e+00  3.59597015e+00
  1.78932524e+00 -5.45340896e-01  5.64772701e+00  9.19430447e+00
  4.42028522e+00  2.24237919e+00 -1.87708378e-01  1.47937727e+00
  1.46093225e+00  1.63989711e+00  2.42697597e+00  1.09869659e-01
  1.54315722e+00  9.01367188e+00  2.78112841e+00  6.94747066e+00
  7.93844604e+00  5.24745893e+00  1.90384924e+00  2.11306882e+00
  4.33682823e+00  5.24131823e+00  3.43558145e+00  7.75171852e+00
  7.60104418e+00  8.86727524e+00  6.22911334e-01  9.03463936e+00
  3.94151974e+00  8.80514336e+00  7.71844244e+00  9.70440960e+00
  7.63131952e+00  2.17288971e+00  4.79780149e+00  3.77800512e+00
  3.52053547e+00  2.49464464e+00  4.90790939e+00  8.35758305e+00
  2.77332044e+00  5.48718977e+00  1.04915285e+00  9.16396737e-01
  4.49069828e-01  9.94224072e+00  2.06893906e-02  1.96413362e+00
  6.02550983e+00  1.40416789e+00  8.92917919e+00  7.30807924e+00
  8.60238266e+00  5.12743139e+00  3.55479407e+00  5.54666090e+00
  1.00518589e+01  4.52478218e+00  2.58068419e+00  8.04085827e+00
  4.15188026e+00  2.39668220e-01  2.77549386e+00  3.97206068e+00
  8.12393951e+00  3.32974410e+00  4.52871943e+00  3.30285239e+00
  8.56041622e+00  9.34256077e+00  9.43628407e+00  3.80536842e+00
  9.50871277e+00  8.09606647e+00  6.72784805e+00  1.52685547e+00
  2.43745208e+00  1.77858579e+00  8.96306610e+00  2.66003942e+00
  5.01896286e+00  5.19758463e+00  7.27442312e+00  8.21808434e+00
  2.31162047e+00  4.53879166e+00  8.23070431e+00  8.05463409e+00
  7.65232205e-01  1.62057388e+00  1.49363244e+00  9.75412965e-01
  4.64891005e+00  1.02944565e+00  4.65099669e+00 -2.19611645e-01
  5.74669075e+00 -9.49231386e-02  9.73519135e+00  7.67262936e+00
  3.89164162e+00  3.33559227e+00  2.41986966e+00  4.89477873e+00
  1.57034174e-01  7.14383364e+00  3.77591443e+00  3.82858586e+00
  8.50906372e+00  7.14387608e+00  9.58337879e+00  3.36379218e+00
  8.92547989e+00  7.81011152e+00  3.75316739e+00  6.45038795e+00
  3.50248003e+00  8.11350918e+00  1.00943398e+00  1.94219565e+00
  1.89951444e+00  3.69217873e+00  6.67764759e+00  4.96922302e+00
  4.89864302e+00 -2.14187935e-01  5.40712452e+00  1.06854296e+00
  2.24141002e+00  9.07321739e+00  1.96794081e+00  7.29460907e+00
  9.10808849e+00  1.64355171e+00  5.39986610e+00  9.28897202e-01
  7.33023119e+00  6.55509567e+00  7.84478426e+00  4.82920742e+00
  8.80195045e+00  8.31086814e-01  3.01831031e+00  7.18754625e+00
  3.48268604e+00  1.67123210e+00  1.49636817e+00  6.85546112e+00
  4.74163532e+00  8.85940742e+00  7.34411335e+00  4.09574366e+00
  3.72876263e+00  3.94373155e+00  8.57115746e+00  7.89460373e+00
  2.84828901e-01  7.03732681e+00  9.92517471e+00  7.13025379e+00
  1.77010977e+00  4.93009567e+00  1.39545083e+00  3.25098395e+00
  9.61502266e+00  8.99063778e+00  2.72320175e+00  3.34499502e+00
  5.54738092e+00  9.42772484e+00  1.34932077e+00  1.72021163e+00
  7.08262205e+00  4.76709890e+00  8.45615292e+00  1.79831946e+00
  5.11601114e+00  3.42909670e+00  4.07918167e+00  5.18363333e+00
  4.65688467e+00  3.41761994e+00  8.14604378e+00  7.30806351e+00
  7.54988194e+00  3.04881263e+00  6.30283117e+00  8.95013046e+00
  7.91262293e+00  3.46671915e+00  7.21975374e+00  6.99606228e+00
  1.43251359e+00  9.02403545e+00  4.46352100e+00  4.73359394e+00
  3.29469442e+00  5.02395201e+00  6.12846851e+00  4.92078924e+00
  8.51279259e+00  6.32400131e+00  4.29928684e+00  4.10619688e+00
  9.38514519e+00  3.39132428e+00  2.31886411e+00  7.54945278e-01
  8.17271996e+00  8.96825886e+00  5.18892646e-01  3.36510205e+00
  9.62342501e-01  1.18473864e+00  8.69164753e+00 -7.18649447e-01
  1.03209085e+01  1.07548547e+00  8.92701817e+00  5.46921825e+00
  3.66492033e+00  3.53753304e+00  7.18332911e+00  3.00394678e+00
  6.41793966e+00  6.07025099e+00  4.88196707e+00  9.20016479e+00
  5.53983831e+00  6.74924755e+00  8.03201866e+00 -2.02914804e-01
  7.71807957e+00  2.31005144e+00  8.58817387e+00 -1.20083690e-01
  3.78105068e+00  1.32219195e+00  5.04449892e+00  8.16030025e+00
  2.95377922e+00  1.51961601e+00  3.25721335e+00  8.20741177e+00
  1.36698890e+00  2.09527206e+00  7.39970028e-01  7.05854177e+00
  3.41466236e+00  2.68145680e+00  7.22985363e+00  3.32064366e+00
  7.24909449e+00  7.56951714e+00  2.16439462e+00  9.11028576e+00
  1.14862895e+00  3.24883676e+00  1.62877440e-01  4.41763210e+00
  5.04922533e+00  5.14228821e+00  8.94242191e+00  9.02628326e+00
  1.40578067e+00  6.13999844e+00  1.18773472e+00  5.53503847e+00
  3.24991751e+00  3.61237216e+00  5.10492229e+00  7.41919661e+00
  6.97021663e-01  9.25158119e+00  7.16013384e+00  1.22523427e+00
  9.79707336e+00  5.12231731e+00  1.00792189e+01  1.99690437e+00
  6.10989857e+00  3.23191142e+00  3.02221370e+00  5.92228127e+00
  8.91099072e+00  8.43638611e+00  6.30670547e+00  5.66287088e+00
  9.18388176e+00  3.98044872e+00  8.69160557e+00  8.25425911e+00
  1.78963411e+00  7.77448034e+00  1.52245164e-04  3.52915049e+00
  2.29402518e+00  5.27710342e+00  7.10148954e+00  1.18256044e+00
  4.28299618e+00  1.05011196e+01  2.35497117e+00  7.26398802e+00]
Epoch 1/1000
2023-10-03 06:56:00.487 
Epoch 1/1000 
	 loss: 466.7879, MinusLogProbMetric: 466.7879, val_loss: 419.5848, val_MinusLogProbMetric: 419.5848

Epoch 1: val_loss improved from inf to 419.58481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 32s - loss: 466.7879 - MinusLogProbMetric: 466.7879 - val_loss: 419.5848 - val_MinusLogProbMetric: 419.5848 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 2/1000
2023-10-03 06:56:11.087 
Epoch 2/1000 
	 loss: 417.4258, MinusLogProbMetric: 417.4258, val_loss: 418.8170, val_MinusLogProbMetric: 418.8170

Epoch 2: val_loss improved from 419.58481 to 418.81696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 417.4258 - MinusLogProbMetric: 417.4258 - val_loss: 418.8170 - val_MinusLogProbMetric: 418.8170 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 3/1000
2023-10-03 06:56:21.188 
Epoch 3/1000 
	 loss: 416.9980, MinusLogProbMetric: 416.9980, val_loss: 415.7693, val_MinusLogProbMetric: 415.7693

Epoch 3: val_loss improved from 418.81696 to 415.76926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 416.9980 - MinusLogProbMetric: 416.9980 - val_loss: 415.7693 - val_MinusLogProbMetric: 415.7693 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 4/1000
2023-10-03 06:56:31.083 
Epoch 4/1000 
	 loss: 416.3913, MinusLogProbMetric: 416.3913, val_loss: 416.7748, val_MinusLogProbMetric: 416.7748

Epoch 4: val_loss did not improve from 415.76926
196/196 - 9s - loss: 416.3913 - MinusLogProbMetric: 416.3913 - val_loss: 416.7748 - val_MinusLogProbMetric: 416.7748 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 5/1000
2023-10-03 06:56:40.389 
Epoch 5/1000 
	 loss: 416.2626, MinusLogProbMetric: 416.2626, val_loss: 418.3117, val_MinusLogProbMetric: 418.3117

Epoch 5: val_loss did not improve from 415.76926
196/196 - 9s - loss: 416.2626 - MinusLogProbMetric: 416.2626 - val_loss: 418.3117 - val_MinusLogProbMetric: 418.3117 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 6/1000
2023-10-03 06:56:49.637 
Epoch 6/1000 
	 loss: 416.2186, MinusLogProbMetric: 416.2186, val_loss: 415.1890, val_MinusLogProbMetric: 415.1890

Epoch 6: val_loss improved from 415.76926 to 415.18903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 416.2186 - MinusLogProbMetric: 416.2186 - val_loss: 415.1890 - val_MinusLogProbMetric: 415.1890 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 7/1000
2023-10-03 06:56:59.705 
Epoch 7/1000 
	 loss: 415.1995, MinusLogProbMetric: 415.1995, val_loss: 429.0388, val_MinusLogProbMetric: 429.0388

Epoch 7: val_loss did not improve from 415.18903
196/196 - 9s - loss: 415.1995 - MinusLogProbMetric: 415.1995 - val_loss: 429.0388 - val_MinusLogProbMetric: 429.0388 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 8/1000
2023-10-03 06:57:08.994 
Epoch 8/1000 
	 loss: 414.7717, MinusLogProbMetric: 414.7717, val_loss: 414.9222, val_MinusLogProbMetric: 414.9222

Epoch 8: val_loss improved from 415.18903 to 414.92224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 414.7717 - MinusLogProbMetric: 414.7717 - val_loss: 414.9222 - val_MinusLogProbMetric: 414.9222 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 9/1000
2023-10-03 06:57:18.954 
Epoch 9/1000 
	 loss: 413.7216, MinusLogProbMetric: 413.7216, val_loss: 414.2436, val_MinusLogProbMetric: 414.2436

Epoch 9: val_loss improved from 414.92224 to 414.24356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 413.7216 - MinusLogProbMetric: 413.7216 - val_loss: 414.2436 - val_MinusLogProbMetric: 414.2436 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 10/1000
2023-10-03 06:57:28.532 
Epoch 10/1000 
	 loss: 414.2440, MinusLogProbMetric: 414.2440, val_loss: 416.8016, val_MinusLogProbMetric: 416.8016

Epoch 10: val_loss did not improve from 414.24356
196/196 - 9s - loss: 414.2440 - MinusLogProbMetric: 414.2440 - val_loss: 416.8016 - val_MinusLogProbMetric: 416.8016 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 11/1000
2023-10-03 06:57:37.720 
Epoch 11/1000 
	 loss: 413.2180, MinusLogProbMetric: 413.2180, val_loss: 414.4138, val_MinusLogProbMetric: 414.4138

Epoch 11: val_loss did not improve from 414.24356
196/196 - 9s - loss: 413.2180 - MinusLogProbMetric: 413.2180 - val_loss: 414.4138 - val_MinusLogProbMetric: 414.4138 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 12/1000
2023-10-03 06:57:47.826 
Epoch 12/1000 
	 loss: 412.6015, MinusLogProbMetric: 412.6015, val_loss: 414.9483, val_MinusLogProbMetric: 414.9483

Epoch 12: val_loss did not improve from 414.24356
196/196 - 10s - loss: 412.6015 - MinusLogProbMetric: 412.6015 - val_loss: 414.9483 - val_MinusLogProbMetric: 414.9483 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 13/1000
2023-10-03 06:57:57.238 
Epoch 13/1000 
	 loss: 412.3764, MinusLogProbMetric: 412.3764, val_loss: 416.5549, val_MinusLogProbMetric: 416.5549

Epoch 13: val_loss did not improve from 414.24356
196/196 - 9s - loss: 412.3764 - MinusLogProbMetric: 412.3764 - val_loss: 416.5549 - val_MinusLogProbMetric: 416.5549 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 14/1000
2023-10-03 06:58:06.485 
Epoch 14/1000 
	 loss: 411.6930, MinusLogProbMetric: 411.6930, val_loss: 415.0300, val_MinusLogProbMetric: 415.0300

Epoch 14: val_loss did not improve from 414.24356
196/196 - 9s - loss: 411.6930 - MinusLogProbMetric: 411.6930 - val_loss: 415.0300 - val_MinusLogProbMetric: 415.0300 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 15/1000
2023-10-03 06:58:16.009 
Epoch 15/1000 
	 loss: 412.3944, MinusLogProbMetric: 412.3944, val_loss: 411.9421, val_MinusLogProbMetric: 411.9421

Epoch 15: val_loss improved from 414.24356 to 411.94214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 412.3944 - MinusLogProbMetric: 412.3944 - val_loss: 411.9421 - val_MinusLogProbMetric: 411.9421 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 16/1000
2023-10-03 06:58:25.771 
Epoch 16/1000 
	 loss: 410.8475, MinusLogProbMetric: 410.8475, val_loss: 413.0799, val_MinusLogProbMetric: 413.0799

Epoch 16: val_loss did not improve from 411.94214
196/196 - 9s - loss: 410.8475 - MinusLogProbMetric: 410.8475 - val_loss: 413.0799 - val_MinusLogProbMetric: 413.0799 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 17/1000
2023-10-03 06:58:35.033 
Epoch 17/1000 
	 loss: 411.2576, MinusLogProbMetric: 411.2576, val_loss: 414.2752, val_MinusLogProbMetric: 414.2752

Epoch 17: val_loss did not improve from 411.94214
196/196 - 9s - loss: 411.2576 - MinusLogProbMetric: 411.2576 - val_loss: 414.2752 - val_MinusLogProbMetric: 414.2752 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 18/1000
2023-10-03 06:58:44.121 
Epoch 18/1000 
	 loss: 410.1320, MinusLogProbMetric: 410.1320, val_loss: 414.4722, val_MinusLogProbMetric: 414.4722

Epoch 18: val_loss did not improve from 411.94214
196/196 - 9s - loss: 410.1320 - MinusLogProbMetric: 410.1320 - val_loss: 414.4722 - val_MinusLogProbMetric: 414.4722 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 19/1000
2023-10-03 06:58:53.402 
Epoch 19/1000 
	 loss: 410.0403, MinusLogProbMetric: 410.0403, val_loss: 422.1958, val_MinusLogProbMetric: 422.1958

Epoch 19: val_loss did not improve from 411.94214
196/196 - 9s - loss: 410.0403 - MinusLogProbMetric: 410.0403 - val_loss: 422.1958 - val_MinusLogProbMetric: 422.1958 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 20/1000
2023-10-03 06:59:02.514 
Epoch 20/1000 
	 loss: 410.2595, MinusLogProbMetric: 410.2595, val_loss: 409.1730, val_MinusLogProbMetric: 409.1730

Epoch 20: val_loss improved from 411.94214 to 409.17297, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 9s - loss: 410.2595 - MinusLogProbMetric: 410.2595 - val_loss: 409.1730 - val_MinusLogProbMetric: 409.1730 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 21/1000
2023-10-03 06:59:11.967 
Epoch 21/1000 
	 loss: 409.4372, MinusLogProbMetric: 409.4372, val_loss: 409.1908, val_MinusLogProbMetric: 409.1908

Epoch 21: val_loss did not improve from 409.17297
196/196 - 9s - loss: 409.4372 - MinusLogProbMetric: 409.4372 - val_loss: 409.1908 - val_MinusLogProbMetric: 409.1908 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 22/1000
2023-10-03 06:59:21.393 
Epoch 22/1000 
	 loss: 409.3446, MinusLogProbMetric: 409.3446, val_loss: 413.7762, val_MinusLogProbMetric: 413.7762

Epoch 22: val_loss did not improve from 409.17297
196/196 - 9s - loss: 409.3446 - MinusLogProbMetric: 409.3446 - val_loss: 413.7762 - val_MinusLogProbMetric: 413.7762 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 23/1000
2023-10-03 06:59:31.142 
Epoch 23/1000 
	 loss: 409.1001, MinusLogProbMetric: 409.1001, val_loss: 410.1221, val_MinusLogProbMetric: 410.1221

Epoch 23: val_loss did not improve from 409.17297
196/196 - 10s - loss: 409.1001 - MinusLogProbMetric: 409.1001 - val_loss: 410.1221 - val_MinusLogProbMetric: 410.1221 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 24/1000
2023-10-03 06:59:40.764 
Epoch 24/1000 
	 loss: 409.8683, MinusLogProbMetric: 409.8683, val_loss: 408.7048, val_MinusLogProbMetric: 408.7048

Epoch 24: val_loss improved from 409.17297 to 408.70480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 409.8683 - MinusLogProbMetric: 409.8683 - val_loss: 408.7048 - val_MinusLogProbMetric: 408.7048 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 25/1000
2023-10-03 06:59:50.322 
Epoch 25/1000 
	 loss: 409.3492, MinusLogProbMetric: 409.3492, val_loss: 409.1075, val_MinusLogProbMetric: 409.1075

Epoch 25: val_loss did not improve from 408.70480
196/196 - 9s - loss: 409.3492 - MinusLogProbMetric: 409.3492 - val_loss: 409.1075 - val_MinusLogProbMetric: 409.1075 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 26/1000
2023-10-03 06:59:59.396 
Epoch 26/1000 
	 loss: 409.7310, MinusLogProbMetric: 409.7310, val_loss: 437.6714, val_MinusLogProbMetric: 437.6714

Epoch 26: val_loss did not improve from 408.70480
196/196 - 9s - loss: 409.7310 - MinusLogProbMetric: 409.7310 - val_loss: 437.6714 - val_MinusLogProbMetric: 437.6714 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 27/1000
2023-10-03 07:00:08.689 
Epoch 27/1000 
	 loss: 408.4580, MinusLogProbMetric: 408.4580, val_loss: 406.6592, val_MinusLogProbMetric: 406.6592

Epoch 27: val_loss improved from 408.70480 to 406.65918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 408.4580 - MinusLogProbMetric: 408.4580 - val_loss: 406.6592 - val_MinusLogProbMetric: 406.6592 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 28/1000
2023-10-03 07:00:18.279 
Epoch 28/1000 
	 loss: 408.5453, MinusLogProbMetric: 408.5453, val_loss: 406.8835, val_MinusLogProbMetric: 406.8835

Epoch 28: val_loss did not improve from 406.65918
196/196 - 9s - loss: 408.5453 - MinusLogProbMetric: 408.5453 - val_loss: 406.8835 - val_MinusLogProbMetric: 406.8835 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 29/1000
2023-10-03 07:00:27.457 
Epoch 29/1000 
	 loss: 407.9768, MinusLogProbMetric: 407.9768, val_loss: 409.6401, val_MinusLogProbMetric: 409.6401

Epoch 29: val_loss did not improve from 406.65918
196/196 - 9s - loss: 407.9768 - MinusLogProbMetric: 407.9768 - val_loss: 409.6401 - val_MinusLogProbMetric: 409.6401 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 30/1000
2023-10-03 07:00:36.729 
Epoch 30/1000 
	 loss: 407.1217, MinusLogProbMetric: 407.1217, val_loss: 407.6861, val_MinusLogProbMetric: 407.6861

Epoch 30: val_loss did not improve from 406.65918
196/196 - 9s - loss: 407.1217 - MinusLogProbMetric: 407.1217 - val_loss: 407.6861 - val_MinusLogProbMetric: 407.6861 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 31/1000
2023-10-03 07:00:45.961 
Epoch 31/1000 
	 loss: 415.5699, MinusLogProbMetric: 415.5699, val_loss: 407.7941, val_MinusLogProbMetric: 407.7941

Epoch 31: val_loss did not improve from 406.65918
196/196 - 9s - loss: 415.5699 - MinusLogProbMetric: 415.5699 - val_loss: 407.7941 - val_MinusLogProbMetric: 407.7941 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 32/1000
2023-10-03 07:00:55.056 
Epoch 32/1000 
	 loss: 406.3073, MinusLogProbMetric: 406.3073, val_loss: 408.7140, val_MinusLogProbMetric: 408.7140

Epoch 32: val_loss did not improve from 406.65918
196/196 - 9s - loss: 406.3073 - MinusLogProbMetric: 406.3073 - val_loss: 408.7140 - val_MinusLogProbMetric: 408.7140 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 33/1000
2023-10-03 07:01:04.650 
Epoch 33/1000 
	 loss: 406.6518, MinusLogProbMetric: 406.6518, val_loss: 415.3148, val_MinusLogProbMetric: 415.3148

Epoch 33: val_loss did not improve from 406.65918
196/196 - 10s - loss: 406.6518 - MinusLogProbMetric: 406.6518 - val_loss: 415.3148 - val_MinusLogProbMetric: 415.3148 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 34/1000
2023-10-03 07:01:14.712 
Epoch 34/1000 
	 loss: 406.6418, MinusLogProbMetric: 406.6418, val_loss: 410.5362, val_MinusLogProbMetric: 410.5362

Epoch 34: val_loss did not improve from 406.65918
196/196 - 10s - loss: 406.6418 - MinusLogProbMetric: 406.6418 - val_loss: 410.5362 - val_MinusLogProbMetric: 410.5362 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 35/1000
2023-10-03 07:01:23.992 
Epoch 35/1000 
	 loss: 406.9511, MinusLogProbMetric: 406.9511, val_loss: 409.3690, val_MinusLogProbMetric: 409.3690

Epoch 35: val_loss did not improve from 406.65918
196/196 - 9s - loss: 406.9511 - MinusLogProbMetric: 406.9511 - val_loss: 409.3690 - val_MinusLogProbMetric: 409.3690 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 36/1000
2023-10-03 07:01:33.423 
Epoch 36/1000 
	 loss: 417.6250, MinusLogProbMetric: 417.6250, val_loss: 410.2239, val_MinusLogProbMetric: 410.2239

Epoch 36: val_loss did not improve from 406.65918
196/196 - 9s - loss: 417.6250 - MinusLogProbMetric: 417.6250 - val_loss: 410.2239 - val_MinusLogProbMetric: 410.2239 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 37/1000
2023-10-03 07:01:42.746 
Epoch 37/1000 
	 loss: 406.7214, MinusLogProbMetric: 406.7214, val_loss: 408.7690, val_MinusLogProbMetric: 408.7690

Epoch 37: val_loss did not improve from 406.65918
196/196 - 9s - loss: 406.7214 - MinusLogProbMetric: 406.7214 - val_loss: 408.7690 - val_MinusLogProbMetric: 408.7690 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 38/1000
2023-10-03 07:01:52.174 
Epoch 38/1000 
	 loss: 406.1001, MinusLogProbMetric: 406.1001, val_loss: 406.7058, val_MinusLogProbMetric: 406.7058

Epoch 38: val_loss did not improve from 406.65918
196/196 - 9s - loss: 406.1001 - MinusLogProbMetric: 406.1001 - val_loss: 406.7058 - val_MinusLogProbMetric: 406.7058 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 39/1000
2023-10-03 07:02:01.331 
Epoch 39/1000 
	 loss: 405.5051, MinusLogProbMetric: 405.5051, val_loss: 405.5159, val_MinusLogProbMetric: 405.5159

Epoch 39: val_loss improved from 406.65918 to 405.51593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 9s - loss: 405.5051 - MinusLogProbMetric: 405.5051 - val_loss: 405.5159 - val_MinusLogProbMetric: 405.5159 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 40/1000
2023-10-03 07:02:10.770 
Epoch 40/1000 
	 loss: 405.9928, MinusLogProbMetric: 405.9928, val_loss: 416.5579, val_MinusLogProbMetric: 416.5579

Epoch 40: val_loss did not improve from 405.51593
196/196 - 9s - loss: 405.9928 - MinusLogProbMetric: 405.9928 - val_loss: 416.5579 - val_MinusLogProbMetric: 416.5579 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 41/1000
2023-10-03 07:02:20.083 
Epoch 41/1000 
	 loss: 406.6730, MinusLogProbMetric: 406.6730, val_loss: 405.5333, val_MinusLogProbMetric: 405.5333

Epoch 41: val_loss did not improve from 405.51593
196/196 - 9s - loss: 406.6730 - MinusLogProbMetric: 406.6730 - val_loss: 405.5333 - val_MinusLogProbMetric: 405.5333 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 42/1000
2023-10-03 07:02:29.308 
Epoch 42/1000 
	 loss: 405.8378, MinusLogProbMetric: 405.8378, val_loss: 406.1020, val_MinusLogProbMetric: 406.1020

Epoch 42: val_loss did not improve from 405.51593
196/196 - 9s - loss: 405.8378 - MinusLogProbMetric: 405.8378 - val_loss: 406.1020 - val_MinusLogProbMetric: 406.1020 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 43/1000
2023-10-03 07:02:38.556 
Epoch 43/1000 
	 loss: 405.7924, MinusLogProbMetric: 405.7924, val_loss: 406.2621, val_MinusLogProbMetric: 406.2621

Epoch 43: val_loss did not improve from 405.51593
196/196 - 9s - loss: 405.7924 - MinusLogProbMetric: 405.7924 - val_loss: 406.2621 - val_MinusLogProbMetric: 406.2621 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 44/1000
2023-10-03 07:02:48.242 
Epoch 44/1000 
	 loss: 405.9846, MinusLogProbMetric: 405.9846, val_loss: 406.7149, val_MinusLogProbMetric: 406.7149

Epoch 44: val_loss did not improve from 405.51593
196/196 - 10s - loss: 405.9846 - MinusLogProbMetric: 405.9846 - val_loss: 406.7149 - val_MinusLogProbMetric: 406.7149 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 45/1000
2023-10-03 07:02:58.160 
Epoch 45/1000 
	 loss: 405.2628, MinusLogProbMetric: 405.2628, val_loss: 406.1228, val_MinusLogProbMetric: 406.1228

Epoch 45: val_loss did not improve from 405.51593
196/196 - 10s - loss: 405.2628 - MinusLogProbMetric: 405.2628 - val_loss: 406.1228 - val_MinusLogProbMetric: 406.1228 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 46/1000
2023-10-03 07:03:07.468 
Epoch 46/1000 
	 loss: 404.6639, MinusLogProbMetric: 404.6639, val_loss: 406.3078, val_MinusLogProbMetric: 406.3078

Epoch 46: val_loss did not improve from 405.51593
196/196 - 9s - loss: 404.6639 - MinusLogProbMetric: 404.6639 - val_loss: 406.3078 - val_MinusLogProbMetric: 406.3078 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 47/1000
2023-10-03 07:03:16.529 
Epoch 47/1000 
	 loss: 405.2886, MinusLogProbMetric: 405.2886, val_loss: 406.3418, val_MinusLogProbMetric: 406.3418

Epoch 47: val_loss did not improve from 405.51593
196/196 - 9s - loss: 405.2886 - MinusLogProbMetric: 405.2886 - val_loss: 406.3418 - val_MinusLogProbMetric: 406.3418 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 48/1000
2023-10-03 07:03:25.767 
Epoch 48/1000 
	 loss: 405.1984, MinusLogProbMetric: 405.1984, val_loss: 403.8704, val_MinusLogProbMetric: 403.8704

Epoch 48: val_loss improved from 405.51593 to 403.87042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 405.1984 - MinusLogProbMetric: 405.1984 - val_loss: 403.8704 - val_MinusLogProbMetric: 403.8704 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 49/1000
2023-10-03 07:03:35.300 
Epoch 49/1000 
	 loss: 404.2795, MinusLogProbMetric: 404.2795, val_loss: 419.3315, val_MinusLogProbMetric: 419.3315

Epoch 49: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.2795 - MinusLogProbMetric: 404.2795 - val_loss: 419.3315 - val_MinusLogProbMetric: 419.3315 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 50/1000
2023-10-03 07:03:44.554 
Epoch 50/1000 
	 loss: 404.8722, MinusLogProbMetric: 404.8722, val_loss: 408.7821, val_MinusLogProbMetric: 408.7821

Epoch 50: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.8722 - MinusLogProbMetric: 404.8722 - val_loss: 408.7821 - val_MinusLogProbMetric: 408.7821 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 51/1000
2023-10-03 07:03:53.727 
Epoch 51/1000 
	 loss: 404.3196, MinusLogProbMetric: 404.3196, val_loss: 405.6658, val_MinusLogProbMetric: 405.6658

Epoch 51: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.3196 - MinusLogProbMetric: 404.3196 - val_loss: 405.6658 - val_MinusLogProbMetric: 405.6658 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 52/1000
2023-10-03 07:04:02.866 
Epoch 52/1000 
	 loss: 404.7741, MinusLogProbMetric: 404.7741, val_loss: 405.3749, val_MinusLogProbMetric: 405.3749

Epoch 52: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.7741 - MinusLogProbMetric: 404.7741 - val_loss: 405.3749 - val_MinusLogProbMetric: 405.3749 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 53/1000
2023-10-03 07:04:12.019 
Epoch 53/1000 
	 loss: 405.2002, MinusLogProbMetric: 405.2002, val_loss: 404.8940, val_MinusLogProbMetric: 404.8940

Epoch 53: val_loss did not improve from 403.87042
196/196 - 9s - loss: 405.2002 - MinusLogProbMetric: 405.2002 - val_loss: 404.8940 - val_MinusLogProbMetric: 404.8940 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 54/1000
2023-10-03 07:04:21.383 
Epoch 54/1000 
	 loss: 404.8134, MinusLogProbMetric: 404.8134, val_loss: 403.9985, val_MinusLogProbMetric: 403.9985

Epoch 54: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.8134 - MinusLogProbMetric: 404.8134 - val_loss: 403.9985 - val_MinusLogProbMetric: 403.9985 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 55/1000
2023-10-03 07:04:31.618 
Epoch 55/1000 
	 loss: 404.1954, MinusLogProbMetric: 404.1954, val_loss: 404.0003, val_MinusLogProbMetric: 404.0003

Epoch 55: val_loss did not improve from 403.87042
196/196 - 10s - loss: 404.1954 - MinusLogProbMetric: 404.1954 - val_loss: 404.0003 - val_MinusLogProbMetric: 404.0003 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 56/1000
2023-10-03 07:04:41.402 
Epoch 56/1000 
	 loss: 403.7352, MinusLogProbMetric: 403.7352, val_loss: 404.5570, val_MinusLogProbMetric: 404.5570

Epoch 56: val_loss did not improve from 403.87042
196/196 - 10s - loss: 403.7352 - MinusLogProbMetric: 403.7352 - val_loss: 404.5570 - val_MinusLogProbMetric: 404.5570 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 57/1000
2023-10-03 07:04:50.656 
Epoch 57/1000 
	 loss: 442.9043, MinusLogProbMetric: 442.9043, val_loss: 430.7592, val_MinusLogProbMetric: 430.7592

Epoch 57: val_loss did not improve from 403.87042
196/196 - 9s - loss: 442.9043 - MinusLogProbMetric: 442.9043 - val_loss: 430.7592 - val_MinusLogProbMetric: 430.7592 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 58/1000
2023-10-03 07:04:59.732 
Epoch 58/1000 
	 loss: 416.0591, MinusLogProbMetric: 416.0591, val_loss: 416.3681, val_MinusLogProbMetric: 416.3681

Epoch 58: val_loss did not improve from 403.87042
196/196 - 9s - loss: 416.0591 - MinusLogProbMetric: 416.0591 - val_loss: 416.3681 - val_MinusLogProbMetric: 416.3681 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 59/1000
2023-10-03 07:05:08.988 
Epoch 59/1000 
	 loss: 409.9916, MinusLogProbMetric: 409.9916, val_loss: 408.4538, val_MinusLogProbMetric: 408.4538

Epoch 59: val_loss did not improve from 403.87042
196/196 - 9s - loss: 409.9916 - MinusLogProbMetric: 409.9916 - val_loss: 408.4538 - val_MinusLogProbMetric: 408.4538 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 60/1000
2023-10-03 07:05:18.377 
Epoch 60/1000 
	 loss: 407.9940, MinusLogProbMetric: 407.9940, val_loss: 407.5397, val_MinusLogProbMetric: 407.5397

Epoch 60: val_loss did not improve from 403.87042
196/196 - 9s - loss: 407.9940 - MinusLogProbMetric: 407.9940 - val_loss: 407.5397 - val_MinusLogProbMetric: 407.5397 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 61/1000
2023-10-03 07:05:27.593 
Epoch 61/1000 
	 loss: 406.9357, MinusLogProbMetric: 406.9357, val_loss: 433.9119, val_MinusLogProbMetric: 433.9119

Epoch 61: val_loss did not improve from 403.87042
196/196 - 9s - loss: 406.9357 - MinusLogProbMetric: 406.9357 - val_loss: 433.9119 - val_MinusLogProbMetric: 433.9119 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 62/1000
2023-10-03 07:05:36.590 
Epoch 62/1000 
	 loss: 407.2455, MinusLogProbMetric: 407.2455, val_loss: 409.2220, val_MinusLogProbMetric: 409.2220

Epoch 62: val_loss did not improve from 403.87042
196/196 - 9s - loss: 407.2455 - MinusLogProbMetric: 407.2455 - val_loss: 409.2220 - val_MinusLogProbMetric: 409.2220 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 63/1000
2023-10-03 07:05:45.719 
Epoch 63/1000 
	 loss: 405.8380, MinusLogProbMetric: 405.8380, val_loss: 405.7809, val_MinusLogProbMetric: 405.7809

Epoch 63: val_loss did not improve from 403.87042
196/196 - 9s - loss: 405.8380 - MinusLogProbMetric: 405.8380 - val_loss: 405.7809 - val_MinusLogProbMetric: 405.7809 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 64/1000
2023-10-03 07:05:55.041 
Epoch 64/1000 
	 loss: 405.7148, MinusLogProbMetric: 405.7148, val_loss: 406.8275, val_MinusLogProbMetric: 406.8275

Epoch 64: val_loss did not improve from 403.87042
196/196 - 9s - loss: 405.7148 - MinusLogProbMetric: 405.7148 - val_loss: 406.8275 - val_MinusLogProbMetric: 406.8275 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 65/1000
2023-10-03 07:06:04.378 
Epoch 65/1000 
	 loss: 404.5614, MinusLogProbMetric: 404.5614, val_loss: 411.3905, val_MinusLogProbMetric: 411.3905

Epoch 65: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.5614 - MinusLogProbMetric: 404.5614 - val_loss: 411.3905 - val_MinusLogProbMetric: 411.3905 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 66/1000
2023-10-03 07:06:13.844 
Epoch 66/1000 
	 loss: 404.4178, MinusLogProbMetric: 404.4178, val_loss: 410.7616, val_MinusLogProbMetric: 410.7616

Epoch 66: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.4178 - MinusLogProbMetric: 404.4178 - val_loss: 410.7616 - val_MinusLogProbMetric: 410.7616 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 67/1000
2023-10-03 07:06:23.608 
Epoch 67/1000 
	 loss: 404.8725, MinusLogProbMetric: 404.8725, val_loss: 404.1069, val_MinusLogProbMetric: 404.1069

Epoch 67: val_loss did not improve from 403.87042
196/196 - 10s - loss: 404.8725 - MinusLogProbMetric: 404.8725 - val_loss: 404.1069 - val_MinusLogProbMetric: 404.1069 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 68/1000
2023-10-03 07:06:32.911 
Epoch 68/1000 
	 loss: 404.1272, MinusLogProbMetric: 404.1272, val_loss: 407.3222, val_MinusLogProbMetric: 407.3222

Epoch 68: val_loss did not improve from 403.87042
196/196 - 9s - loss: 404.1272 - MinusLogProbMetric: 404.1272 - val_loss: 407.3222 - val_MinusLogProbMetric: 407.3222 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 69/1000
2023-10-03 07:06:42.230 
Epoch 69/1000 
	 loss: 405.7012, MinusLogProbMetric: 405.7012, val_loss: 403.8691, val_MinusLogProbMetric: 403.8691

Epoch 69: val_loss improved from 403.87042 to 403.86908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 405.7012 - MinusLogProbMetric: 405.7012 - val_loss: 403.8691 - val_MinusLogProbMetric: 403.8691 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 70/1000
2023-10-03 07:06:52.001 
Epoch 70/1000 
	 loss: 403.7033, MinusLogProbMetric: 403.7033, val_loss: 410.1258, val_MinusLogProbMetric: 410.1258

Epoch 70: val_loss did not improve from 403.86908
196/196 - 9s - loss: 403.7033 - MinusLogProbMetric: 403.7033 - val_loss: 410.1258 - val_MinusLogProbMetric: 410.1258 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 71/1000
2023-10-03 07:07:01.101 
Epoch 71/1000 
	 loss: 403.5085, MinusLogProbMetric: 403.5085, val_loss: 403.5858, val_MinusLogProbMetric: 403.5858

Epoch 71: val_loss improved from 403.86908 to 403.58578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 9s - loss: 403.5085 - MinusLogProbMetric: 403.5085 - val_loss: 403.5858 - val_MinusLogProbMetric: 403.5858 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 72/1000
2023-10-03 07:07:10.319 
Epoch 72/1000 
	 loss: 404.1676, MinusLogProbMetric: 404.1676, val_loss: 403.8329, val_MinusLogProbMetric: 403.8329

Epoch 72: val_loss did not improve from 403.58578
196/196 - 9s - loss: 404.1676 - MinusLogProbMetric: 404.1676 - val_loss: 403.8329 - val_MinusLogProbMetric: 403.8329 - lr: 3.3333e-04 - 9s/epoch - 45ms/step
Epoch 73/1000
2023-10-03 07:07:19.645 
Epoch 73/1000 
	 loss: 402.8516, MinusLogProbMetric: 402.8516, val_loss: 403.5073, val_MinusLogProbMetric: 403.5073

Epoch 73: val_loss improved from 403.58578 to 403.50726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 402.8516 - MinusLogProbMetric: 402.8516 - val_loss: 403.5073 - val_MinusLogProbMetric: 403.5073 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 74/1000
2023-10-03 07:07:29.342 
Epoch 74/1000 
	 loss: 404.0009, MinusLogProbMetric: 404.0009, val_loss: 403.0784, val_MinusLogProbMetric: 403.0784

Epoch 74: val_loss improved from 403.50726 to 403.07840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 404.0009 - MinusLogProbMetric: 404.0009 - val_loss: 403.0784 - val_MinusLogProbMetric: 403.0784 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 75/1000
2023-10-03 07:07:38.853 
Epoch 75/1000 
	 loss: 402.9812, MinusLogProbMetric: 402.9812, val_loss: 420.3615, val_MinusLogProbMetric: 420.3615

Epoch 75: val_loss did not improve from 403.07840
196/196 - 9s - loss: 402.9812 - MinusLogProbMetric: 402.9812 - val_loss: 420.3615 - val_MinusLogProbMetric: 420.3615 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 76/1000
2023-10-03 07:07:48.092 
Epoch 76/1000 
	 loss: 403.3946, MinusLogProbMetric: 403.3946, val_loss: 402.6560, val_MinusLogProbMetric: 402.6560

Epoch 76: val_loss improved from 403.07840 to 402.65598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 403.3946 - MinusLogProbMetric: 403.3946 - val_loss: 402.6560 - val_MinusLogProbMetric: 402.6560 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 77/1000
2023-10-03 07:07:58.322 
Epoch 77/1000 
	 loss: 402.3831, MinusLogProbMetric: 402.3831, val_loss: 402.6750, val_MinusLogProbMetric: 402.6750

Epoch 77: val_loss did not improve from 402.65598
196/196 - 10s - loss: 402.3831 - MinusLogProbMetric: 402.3831 - val_loss: 402.6750 - val_MinusLogProbMetric: 402.6750 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 78/1000
2023-10-03 07:08:07.636 
Epoch 78/1000 
	 loss: 403.2039, MinusLogProbMetric: 403.2039, val_loss: 404.7634, val_MinusLogProbMetric: 404.7634

Epoch 78: val_loss did not improve from 402.65598
196/196 - 9s - loss: 403.2039 - MinusLogProbMetric: 403.2039 - val_loss: 404.7634 - val_MinusLogProbMetric: 404.7634 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 79/1000
2023-10-03 07:08:17.175 
Epoch 79/1000 
	 loss: 401.8965, MinusLogProbMetric: 401.8965, val_loss: 405.1863, val_MinusLogProbMetric: 405.1863

Epoch 79: val_loss did not improve from 402.65598
196/196 - 10s - loss: 401.8965 - MinusLogProbMetric: 401.8965 - val_loss: 405.1863 - val_MinusLogProbMetric: 405.1863 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 80/1000
2023-10-03 07:08:26.456 
Epoch 80/1000 
	 loss: 402.2909, MinusLogProbMetric: 402.2909, val_loss: 405.5069, val_MinusLogProbMetric: 405.5069

Epoch 80: val_loss did not improve from 402.65598
196/196 - 9s - loss: 402.2909 - MinusLogProbMetric: 402.2909 - val_loss: 405.5069 - val_MinusLogProbMetric: 405.5069 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 81/1000
2023-10-03 07:08:36.078 
Epoch 81/1000 
	 loss: 402.7921, MinusLogProbMetric: 402.7921, val_loss: 404.0189, val_MinusLogProbMetric: 404.0189

Epoch 81: val_loss did not improve from 402.65598
196/196 - 10s - loss: 402.7921 - MinusLogProbMetric: 402.7921 - val_loss: 404.0189 - val_MinusLogProbMetric: 404.0189 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 82/1000
2023-10-03 07:08:45.381 
Epoch 82/1000 
	 loss: 401.3839, MinusLogProbMetric: 401.3839, val_loss: 404.0142, val_MinusLogProbMetric: 404.0142

Epoch 82: val_loss did not improve from 402.65598
196/196 - 9s - loss: 401.3839 - MinusLogProbMetric: 401.3839 - val_loss: 404.0142 - val_MinusLogProbMetric: 404.0142 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 83/1000
2023-10-03 07:08:54.917 
Epoch 83/1000 
	 loss: 401.8020, MinusLogProbMetric: 401.8020, val_loss: 403.5604, val_MinusLogProbMetric: 403.5604

Epoch 83: val_loss did not improve from 402.65598
196/196 - 10s - loss: 401.8020 - MinusLogProbMetric: 401.8020 - val_loss: 403.5604 - val_MinusLogProbMetric: 403.5604 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 84/1000
2023-10-03 07:09:03.995 
Epoch 84/1000 
	 loss: 402.6716, MinusLogProbMetric: 402.6716, val_loss: 408.4658, val_MinusLogProbMetric: 408.4658

Epoch 84: val_loss did not improve from 402.65598
196/196 - 9s - loss: 402.6716 - MinusLogProbMetric: 402.6716 - val_loss: 408.4658 - val_MinusLogProbMetric: 408.4658 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 85/1000
2023-10-03 07:09:13.362 
Epoch 85/1000 
	 loss: 401.3946, MinusLogProbMetric: 401.3946, val_loss: 402.0057, val_MinusLogProbMetric: 402.0057

Epoch 85: val_loss improved from 402.65598 to 402.00571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 401.3946 - MinusLogProbMetric: 401.3946 - val_loss: 402.0057 - val_MinusLogProbMetric: 402.0057 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 86/1000
2023-10-03 07:09:22.922 
Epoch 86/1000 
	 loss: 401.5368, MinusLogProbMetric: 401.5368, val_loss: 402.1530, val_MinusLogProbMetric: 402.1530

Epoch 86: val_loss did not improve from 402.00571
196/196 - 9s - loss: 401.5368 - MinusLogProbMetric: 401.5368 - val_loss: 402.1530 - val_MinusLogProbMetric: 402.1530 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 87/1000
2023-10-03 07:09:32.101 
Epoch 87/1000 
	 loss: 401.3166, MinusLogProbMetric: 401.3166, val_loss: 402.6677, val_MinusLogProbMetric: 402.6677

Epoch 87: val_loss did not improve from 402.00571
196/196 - 9s - loss: 401.3166 - MinusLogProbMetric: 401.3166 - val_loss: 402.6677 - val_MinusLogProbMetric: 402.6677 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 88/1000
2023-10-03 07:09:41.741 
Epoch 88/1000 
	 loss: 401.0431, MinusLogProbMetric: 401.0431, val_loss: 404.0251, val_MinusLogProbMetric: 404.0251

Epoch 88: val_loss did not improve from 402.00571
196/196 - 10s - loss: 401.0431 - MinusLogProbMetric: 401.0431 - val_loss: 404.0251 - val_MinusLogProbMetric: 404.0251 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 89/1000
2023-10-03 07:09:51.072 
Epoch 89/1000 
	 loss: 400.8897, MinusLogProbMetric: 400.8897, val_loss: 401.9175, val_MinusLogProbMetric: 401.9175

Epoch 89: val_loss improved from 402.00571 to 401.91748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 400.8897 - MinusLogProbMetric: 400.8897 - val_loss: 401.9175 - val_MinusLogProbMetric: 401.9175 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 90/1000
2023-10-03 07:10:00.781 
Epoch 90/1000 
	 loss: 402.0613, MinusLogProbMetric: 402.0613, val_loss: 401.1612, val_MinusLogProbMetric: 401.1612

Epoch 90: val_loss improved from 401.91748 to 401.16122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 402.0613 - MinusLogProbMetric: 402.0613 - val_loss: 401.1612 - val_MinusLogProbMetric: 401.1612 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 91/1000
2023-10-03 07:10:10.197 
Epoch 91/1000 
	 loss: 402.5254, MinusLogProbMetric: 402.5254, val_loss: 401.7548, val_MinusLogProbMetric: 401.7548

Epoch 91: val_loss did not improve from 401.16122
196/196 - 9s - loss: 402.5254 - MinusLogProbMetric: 402.5254 - val_loss: 401.7548 - val_MinusLogProbMetric: 401.7548 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 92/1000
2023-10-03 07:10:19.598 
Epoch 92/1000 
	 loss: 400.8403, MinusLogProbMetric: 400.8403, val_loss: 404.2013, val_MinusLogProbMetric: 404.2013

Epoch 92: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.8403 - MinusLogProbMetric: 400.8403 - val_loss: 404.2013 - val_MinusLogProbMetric: 404.2013 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 93/1000
2023-10-03 07:10:28.770 
Epoch 93/1000 
	 loss: 400.7370, MinusLogProbMetric: 400.7370, val_loss: 405.6411, val_MinusLogProbMetric: 405.6411

Epoch 93: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.7370 - MinusLogProbMetric: 400.7370 - val_loss: 405.6411 - val_MinusLogProbMetric: 405.6411 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 94/1000
2023-10-03 07:10:38.193 
Epoch 94/1000 
	 loss: 400.6647, MinusLogProbMetric: 400.6647, val_loss: 401.7163, val_MinusLogProbMetric: 401.7163

Epoch 94: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.6647 - MinusLogProbMetric: 400.6647 - val_loss: 401.7163 - val_MinusLogProbMetric: 401.7163 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 95/1000
2023-10-03 07:10:47.385 
Epoch 95/1000 
	 loss: 400.6937, MinusLogProbMetric: 400.6937, val_loss: 403.8322, val_MinusLogProbMetric: 403.8322

Epoch 95: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.6937 - MinusLogProbMetric: 400.6937 - val_loss: 403.8322 - val_MinusLogProbMetric: 403.8322 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 96/1000
2023-10-03 07:10:56.781 
Epoch 96/1000 
	 loss: 400.6810, MinusLogProbMetric: 400.6810, val_loss: 402.4498, val_MinusLogProbMetric: 402.4498

Epoch 96: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.6810 - MinusLogProbMetric: 400.6810 - val_loss: 402.4498 - val_MinusLogProbMetric: 402.4498 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 97/1000
2023-10-03 07:11:06.079 
Epoch 97/1000 
	 loss: 400.5575, MinusLogProbMetric: 400.5575, val_loss: 406.0057, val_MinusLogProbMetric: 406.0057

Epoch 97: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.5575 - MinusLogProbMetric: 400.5575 - val_loss: 406.0057 - val_MinusLogProbMetric: 406.0057 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 98/1000
2023-10-03 07:11:15.451 
Epoch 98/1000 
	 loss: 400.5267, MinusLogProbMetric: 400.5267, val_loss: 402.1625, val_MinusLogProbMetric: 402.1625

Epoch 98: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.5267 - MinusLogProbMetric: 400.5267 - val_loss: 402.1625 - val_MinusLogProbMetric: 402.1625 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 99/1000
2023-10-03 07:11:24.982 
Epoch 99/1000 
	 loss: 400.4032, MinusLogProbMetric: 400.4032, val_loss: 401.2151, val_MinusLogProbMetric: 401.2151

Epoch 99: val_loss did not improve from 401.16122
196/196 - 10s - loss: 400.4032 - MinusLogProbMetric: 400.4032 - val_loss: 401.2151 - val_MinusLogProbMetric: 401.2151 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 100/1000
2023-10-03 07:11:34.095 
Epoch 100/1000 
	 loss: 400.6410, MinusLogProbMetric: 400.6410, val_loss: 410.5146, val_MinusLogProbMetric: 410.5146

Epoch 100: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.6410 - MinusLogProbMetric: 400.6410 - val_loss: 410.5146 - val_MinusLogProbMetric: 410.5146 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 101/1000
2023-10-03 07:11:43.462 
Epoch 101/1000 
	 loss: 400.7072, MinusLogProbMetric: 400.7072, val_loss: 405.1139, val_MinusLogProbMetric: 405.1139

Epoch 101: val_loss did not improve from 401.16122
196/196 - 9s - loss: 400.7072 - MinusLogProbMetric: 400.7072 - val_loss: 405.1139 - val_MinusLogProbMetric: 405.1139 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 102/1000
2023-10-03 07:11:52.502 
Epoch 102/1000 
	 loss: 400.0725, MinusLogProbMetric: 400.0725, val_loss: 401.0062, val_MinusLogProbMetric: 401.0062

Epoch 102: val_loss improved from 401.16122 to 401.00620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 9s - loss: 400.0725 - MinusLogProbMetric: 400.0725 - val_loss: 401.0062 - val_MinusLogProbMetric: 401.0062 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 103/1000
2023-10-03 07:12:02.155 
Epoch 103/1000 
	 loss: 400.0472, MinusLogProbMetric: 400.0472, val_loss: 404.2123, val_MinusLogProbMetric: 404.2123

Epoch 103: val_loss did not improve from 401.00620
196/196 - 9s - loss: 400.0472 - MinusLogProbMetric: 400.0472 - val_loss: 404.2123 - val_MinusLogProbMetric: 404.2123 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 104/1000
2023-10-03 07:12:11.546 
Epoch 104/1000 
	 loss: 400.1880, MinusLogProbMetric: 400.1880, val_loss: 403.1050, val_MinusLogProbMetric: 403.1050

Epoch 104: val_loss did not improve from 401.00620
196/196 - 9s - loss: 400.1880 - MinusLogProbMetric: 400.1880 - val_loss: 403.1050 - val_MinusLogProbMetric: 403.1050 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 105/1000
2023-10-03 07:12:21.132 
Epoch 105/1000 
	 loss: 399.8590, MinusLogProbMetric: 399.8590, val_loss: 403.1447, val_MinusLogProbMetric: 403.1447

Epoch 105: val_loss did not improve from 401.00620
196/196 - 10s - loss: 399.8590 - MinusLogProbMetric: 399.8590 - val_loss: 403.1447 - val_MinusLogProbMetric: 403.1447 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 106/1000
2023-10-03 07:12:30.541 
Epoch 106/1000 
	 loss: 400.5335, MinusLogProbMetric: 400.5335, val_loss: 404.7771, val_MinusLogProbMetric: 404.7771

Epoch 106: val_loss did not improve from 401.00620
196/196 - 9s - loss: 400.5335 - MinusLogProbMetric: 400.5335 - val_loss: 404.7771 - val_MinusLogProbMetric: 404.7771 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 107/1000
2023-10-03 07:12:39.970 
Epoch 107/1000 
	 loss: 399.6130, MinusLogProbMetric: 399.6130, val_loss: 401.2958, val_MinusLogProbMetric: 401.2958

Epoch 107: val_loss did not improve from 401.00620
196/196 - 9s - loss: 399.6130 - MinusLogProbMetric: 399.6130 - val_loss: 401.2958 - val_MinusLogProbMetric: 401.2958 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 108/1000
2023-10-03 07:12:49.311 
Epoch 108/1000 
	 loss: 400.8300, MinusLogProbMetric: 400.8300, val_loss: 405.5319, val_MinusLogProbMetric: 405.5319

Epoch 108: val_loss did not improve from 401.00620
196/196 - 9s - loss: 400.8300 - MinusLogProbMetric: 400.8300 - val_loss: 405.5319 - val_MinusLogProbMetric: 405.5319 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 109/1000
2023-10-03 07:12:58.928 
Epoch 109/1000 
	 loss: 399.8787, MinusLogProbMetric: 399.8787, val_loss: 401.4949, val_MinusLogProbMetric: 401.4949

Epoch 109: val_loss did not improve from 401.00620
196/196 - 10s - loss: 399.8787 - MinusLogProbMetric: 399.8787 - val_loss: 401.4949 - val_MinusLogProbMetric: 401.4949 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 110/1000
2023-10-03 07:13:08.416 
Epoch 110/1000 
	 loss: 399.8780, MinusLogProbMetric: 399.8780, val_loss: 400.2195, val_MinusLogProbMetric: 400.2195

Epoch 110: val_loss improved from 401.00620 to 400.21951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 399.8780 - MinusLogProbMetric: 399.8780 - val_loss: 400.2195 - val_MinusLogProbMetric: 400.2195 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 111/1000
2023-10-03 07:13:18.271 
Epoch 111/1000 
	 loss: 401.5321, MinusLogProbMetric: 401.5321, val_loss: 401.6665, val_MinusLogProbMetric: 401.6665

Epoch 111: val_loss did not improve from 400.21951
196/196 - 9s - loss: 401.5321 - MinusLogProbMetric: 401.5321 - val_loss: 401.6665 - val_MinusLogProbMetric: 401.6665 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 112/1000
2023-10-03 07:13:27.561 
Epoch 112/1000 
	 loss: 399.1494, MinusLogProbMetric: 399.1494, val_loss: 408.4912, val_MinusLogProbMetric: 408.4912

Epoch 112: val_loss did not improve from 400.21951
196/196 - 9s - loss: 399.1494 - MinusLogProbMetric: 399.1494 - val_loss: 408.4912 - val_MinusLogProbMetric: 408.4912 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 113/1000
2023-10-03 07:13:36.806 
Epoch 113/1000 
	 loss: 399.3656, MinusLogProbMetric: 399.3656, val_loss: 401.7561, val_MinusLogProbMetric: 401.7561

Epoch 113: val_loss did not improve from 400.21951
196/196 - 9s - loss: 399.3656 - MinusLogProbMetric: 399.3656 - val_loss: 401.7561 - val_MinusLogProbMetric: 401.7561 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 114/1000
2023-10-03 07:13:46.239 
Epoch 114/1000 
	 loss: 399.9839, MinusLogProbMetric: 399.9839, val_loss: 405.9155, val_MinusLogProbMetric: 405.9155

Epoch 114: val_loss did not improve from 400.21951
196/196 - 9s - loss: 399.9839 - MinusLogProbMetric: 399.9839 - val_loss: 405.9155 - val_MinusLogProbMetric: 405.9155 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 115/1000
2023-10-03 07:13:55.569 
Epoch 115/1000 
	 loss: 399.1321, MinusLogProbMetric: 399.1321, val_loss: 437.4741, val_MinusLogProbMetric: 437.4741

Epoch 115: val_loss did not improve from 400.21951
196/196 - 9s - loss: 399.1321 - MinusLogProbMetric: 399.1321 - val_loss: 437.4741 - val_MinusLogProbMetric: 437.4741 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 116/1000
2023-10-03 07:14:04.729 
Epoch 116/1000 
	 loss: 401.1483, MinusLogProbMetric: 401.1483, val_loss: 400.2013, val_MinusLogProbMetric: 400.2013

Epoch 116: val_loss improved from 400.21951 to 400.20129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 401.1483 - MinusLogProbMetric: 401.1483 - val_loss: 400.2013 - val_MinusLogProbMetric: 400.2013 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 117/1000
2023-10-03 07:14:14.262 
Epoch 117/1000 
	 loss: 398.8006, MinusLogProbMetric: 398.8006, val_loss: 401.1771, val_MinusLogProbMetric: 401.1771

Epoch 117: val_loss did not improve from 400.20129
196/196 - 9s - loss: 398.8006 - MinusLogProbMetric: 398.8006 - val_loss: 401.1771 - val_MinusLogProbMetric: 401.1771 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 118/1000
2023-10-03 07:14:23.513 
Epoch 118/1000 
	 loss: 399.2282, MinusLogProbMetric: 399.2282, val_loss: 402.3764, val_MinusLogProbMetric: 402.3764

Epoch 118: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.2282 - MinusLogProbMetric: 399.2282 - val_loss: 402.3764 - val_MinusLogProbMetric: 402.3764 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 119/1000
2023-10-03 07:14:32.852 
Epoch 119/1000 
	 loss: 399.2254, MinusLogProbMetric: 399.2254, val_loss: 400.3380, val_MinusLogProbMetric: 400.3380

Epoch 119: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.2254 - MinusLogProbMetric: 399.2254 - val_loss: 400.3380 - val_MinusLogProbMetric: 400.3380 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 120/1000
2023-10-03 07:14:42.287 
Epoch 120/1000 
	 loss: 399.0414, MinusLogProbMetric: 399.0414, val_loss: 401.5555, val_MinusLogProbMetric: 401.5555

Epoch 120: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.0414 - MinusLogProbMetric: 399.0414 - val_loss: 401.5555 - val_MinusLogProbMetric: 401.5555 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 121/1000
2023-10-03 07:14:51.678 
Epoch 121/1000 
	 loss: 399.0948, MinusLogProbMetric: 399.0948, val_loss: 410.4452, val_MinusLogProbMetric: 410.4452

Epoch 121: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.0948 - MinusLogProbMetric: 399.0948 - val_loss: 410.4452 - val_MinusLogProbMetric: 410.4452 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 122/1000
2023-10-03 07:15:01.174 
Epoch 122/1000 
	 loss: 399.1149, MinusLogProbMetric: 399.1149, val_loss: 401.3645, val_MinusLogProbMetric: 401.3645

Epoch 122: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.1149 - MinusLogProbMetric: 399.1149 - val_loss: 401.3645 - val_MinusLogProbMetric: 401.3645 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 123/1000
2023-10-03 07:15:10.424 
Epoch 123/1000 
	 loss: 399.0614, MinusLogProbMetric: 399.0614, val_loss: 402.3790, val_MinusLogProbMetric: 402.3790

Epoch 123: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.0614 - MinusLogProbMetric: 399.0614 - val_loss: 402.3790 - val_MinusLogProbMetric: 402.3790 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 124/1000
2023-10-03 07:15:19.805 
Epoch 124/1000 
	 loss: 399.7999, MinusLogProbMetric: 399.7999, val_loss: 402.7257, val_MinusLogProbMetric: 402.7257

Epoch 124: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.7999 - MinusLogProbMetric: 399.7999 - val_loss: 402.7257 - val_MinusLogProbMetric: 402.7257 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 125/1000
2023-10-03 07:15:28.989 
Epoch 125/1000 
	 loss: 399.2052, MinusLogProbMetric: 399.2052, val_loss: 400.8859, val_MinusLogProbMetric: 400.8859

Epoch 125: val_loss did not improve from 400.20129
196/196 - 9s - loss: 399.2052 - MinusLogProbMetric: 399.2052 - val_loss: 400.8859 - val_MinusLogProbMetric: 400.8859 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 126/1000
2023-10-03 07:15:38.428 
Epoch 126/1000 
	 loss: 398.8115, MinusLogProbMetric: 398.8115, val_loss: 402.0277, val_MinusLogProbMetric: 402.0277

Epoch 126: val_loss did not improve from 400.20129
196/196 - 9s - loss: 398.8115 - MinusLogProbMetric: 398.8115 - val_loss: 402.0277 - val_MinusLogProbMetric: 402.0277 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 127/1000
2023-10-03 07:15:47.845 
Epoch 127/1000 
	 loss: 398.5075, MinusLogProbMetric: 398.5075, val_loss: 410.8092, val_MinusLogProbMetric: 410.8092

Epoch 127: val_loss did not improve from 400.20129
196/196 - 9s - loss: 398.5075 - MinusLogProbMetric: 398.5075 - val_loss: 410.8092 - val_MinusLogProbMetric: 410.8092 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 128/1000
2023-10-03 07:15:56.770 
Epoch 128/1000 
	 loss: 399.0808, MinusLogProbMetric: 399.0808, val_loss: 399.9572, val_MinusLogProbMetric: 399.9572

Epoch 128: val_loss improved from 400.20129 to 399.95721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 9s - loss: 399.0808 - MinusLogProbMetric: 399.0808 - val_loss: 399.9572 - val_MinusLogProbMetric: 399.9572 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 129/1000
2023-10-03 07:16:06.292 
Epoch 129/1000 
	 loss: 398.6089, MinusLogProbMetric: 398.6089, val_loss: 403.2831, val_MinusLogProbMetric: 403.2831

Epoch 129: val_loss did not improve from 399.95721
196/196 - 9s - loss: 398.6089 - MinusLogProbMetric: 398.6089 - val_loss: 403.2831 - val_MinusLogProbMetric: 403.2831 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 130/1000
2023-10-03 07:16:15.485 
Epoch 130/1000 
	 loss: 398.9999, MinusLogProbMetric: 398.9999, val_loss: 399.8150, val_MinusLogProbMetric: 399.8150

Epoch 130: val_loss improved from 399.95721 to 399.81503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 398.9999 - MinusLogProbMetric: 398.9999 - val_loss: 399.8150 - val_MinusLogProbMetric: 399.8150 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 131/1000
2023-10-03 07:16:25.391 
Epoch 131/1000 
	 loss: 399.1210, MinusLogProbMetric: 399.1210, val_loss: 401.0682, val_MinusLogProbMetric: 401.0682

Epoch 131: val_loss did not improve from 399.81503
196/196 - 10s - loss: 399.1210 - MinusLogProbMetric: 399.1210 - val_loss: 401.0682 - val_MinusLogProbMetric: 401.0682 - lr: 3.3333e-04 - 10s/epoch - 48ms/step
Epoch 132/1000
2023-10-03 07:16:34.607 
Epoch 132/1000 
	 loss: 398.5344, MinusLogProbMetric: 398.5344, val_loss: 400.4505, val_MinusLogProbMetric: 400.4505

Epoch 132: val_loss did not improve from 399.81503
196/196 - 9s - loss: 398.5344 - MinusLogProbMetric: 398.5344 - val_loss: 400.4505 - val_MinusLogProbMetric: 400.4505 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 133/1000
2023-10-03 07:16:44.025 
Epoch 133/1000 
	 loss: 399.1229, MinusLogProbMetric: 399.1229, val_loss: 400.4223, val_MinusLogProbMetric: 400.4223

Epoch 133: val_loss did not improve from 399.81503
196/196 - 9s - loss: 399.1229 - MinusLogProbMetric: 399.1229 - val_loss: 400.4223 - val_MinusLogProbMetric: 400.4223 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 134/1000
2023-10-03 07:16:53.307 
Epoch 134/1000 
	 loss: 398.3445, MinusLogProbMetric: 398.3445, val_loss: 399.7500, val_MinusLogProbMetric: 399.7500

Epoch 134: val_loss improved from 399.81503 to 399.74997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 398.3445 - MinusLogProbMetric: 398.3445 - val_loss: 399.7500 - val_MinusLogProbMetric: 399.7500 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 135/1000
2023-10-03 07:17:03.211 
Epoch 135/1000 
	 loss: 398.4944, MinusLogProbMetric: 398.4944, val_loss: 404.0066, val_MinusLogProbMetric: 404.0066

Epoch 135: val_loss did not improve from 399.74997
196/196 - 9s - loss: 398.4944 - MinusLogProbMetric: 398.4944 - val_loss: 404.0066 - val_MinusLogProbMetric: 404.0066 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 136/1000
2023-10-03 07:17:12.468 
Epoch 136/1000 
	 loss: 399.4518, MinusLogProbMetric: 399.4518, val_loss: 399.3892, val_MinusLogProbMetric: 399.3892

Epoch 136: val_loss improved from 399.74997 to 399.38919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 399.4518 - MinusLogProbMetric: 399.4518 - val_loss: 399.3892 - val_MinusLogProbMetric: 399.3892 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 137/1000
2023-10-03 07:17:22.144 
Epoch 137/1000 
	 loss: 398.3362, MinusLogProbMetric: 398.3362, val_loss: 399.6835, val_MinusLogProbMetric: 399.6835

Epoch 137: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.3362 - MinusLogProbMetric: 398.3362 - val_loss: 399.6835 - val_MinusLogProbMetric: 399.6835 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 138/1000
2023-10-03 07:17:31.236 
Epoch 138/1000 
	 loss: 398.3194, MinusLogProbMetric: 398.3194, val_loss: 401.5617, val_MinusLogProbMetric: 401.5617

Epoch 138: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.3194 - MinusLogProbMetric: 398.3194 - val_loss: 401.5617 - val_MinusLogProbMetric: 401.5617 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 139/1000
2023-10-03 07:17:40.554 
Epoch 139/1000 
	 loss: 399.3080, MinusLogProbMetric: 399.3080, val_loss: 400.3224, val_MinusLogProbMetric: 400.3224

Epoch 139: val_loss did not improve from 399.38919
196/196 - 9s - loss: 399.3080 - MinusLogProbMetric: 399.3080 - val_loss: 400.3224 - val_MinusLogProbMetric: 400.3224 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 140/1000
2023-10-03 07:17:49.714 
Epoch 140/1000 
	 loss: 398.8724, MinusLogProbMetric: 398.8724, val_loss: 399.6079, val_MinusLogProbMetric: 399.6079

Epoch 140: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.8724 - MinusLogProbMetric: 398.8724 - val_loss: 399.6079 - val_MinusLogProbMetric: 399.6079 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 141/1000
2023-10-03 07:17:59.152 
Epoch 141/1000 
	 loss: 397.8144, MinusLogProbMetric: 397.8144, val_loss: 402.1325, val_MinusLogProbMetric: 402.1325

Epoch 141: val_loss did not improve from 399.38919
196/196 - 9s - loss: 397.8144 - MinusLogProbMetric: 397.8144 - val_loss: 402.1325 - val_MinusLogProbMetric: 402.1325 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 142/1000
2023-10-03 07:18:08.806 
Epoch 142/1000 
	 loss: 398.5282, MinusLogProbMetric: 398.5282, val_loss: 400.7014, val_MinusLogProbMetric: 400.7014

Epoch 142: val_loss did not improve from 399.38919
196/196 - 10s - loss: 398.5282 - MinusLogProbMetric: 398.5282 - val_loss: 400.7014 - val_MinusLogProbMetric: 400.7014 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 143/1000
2023-10-03 07:18:18.100 
Epoch 143/1000 
	 loss: 398.0975, MinusLogProbMetric: 398.0975, val_loss: 401.6855, val_MinusLogProbMetric: 401.6855

Epoch 143: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.0975 - MinusLogProbMetric: 398.0975 - val_loss: 401.6855 - val_MinusLogProbMetric: 401.6855 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 144/1000
2023-10-03 07:18:27.343 
Epoch 144/1000 
	 loss: 398.0387, MinusLogProbMetric: 398.0387, val_loss: 400.2355, val_MinusLogProbMetric: 400.2355

Epoch 144: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.0387 - MinusLogProbMetric: 398.0387 - val_loss: 400.2355 - val_MinusLogProbMetric: 400.2355 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 145/1000
2023-10-03 07:18:36.600 
Epoch 145/1000 
	 loss: 398.8490, MinusLogProbMetric: 398.8490, val_loss: 401.4672, val_MinusLogProbMetric: 401.4672

Epoch 145: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.8490 - MinusLogProbMetric: 398.8490 - val_loss: 401.4672 - val_MinusLogProbMetric: 401.4672 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 146/1000
2023-10-03 07:18:46.142 
Epoch 146/1000 
	 loss: 397.5597, MinusLogProbMetric: 397.5597, val_loss: 401.2108, val_MinusLogProbMetric: 401.2108

Epoch 146: val_loss did not improve from 399.38919
196/196 - 10s - loss: 397.5597 - MinusLogProbMetric: 397.5597 - val_loss: 401.2108 - val_MinusLogProbMetric: 401.2108 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 147/1000
2023-10-03 07:18:55.288 
Epoch 147/1000 
	 loss: 398.6010, MinusLogProbMetric: 398.6010, val_loss: 410.8921, val_MinusLogProbMetric: 410.8921

Epoch 147: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.6010 - MinusLogProbMetric: 398.6010 - val_loss: 410.8921 - val_MinusLogProbMetric: 410.8921 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 148/1000
2023-10-03 07:19:04.903 
Epoch 148/1000 
	 loss: 767.0670, MinusLogProbMetric: 767.0670, val_loss: 1372.1473, val_MinusLogProbMetric: 1372.1473

Epoch 148: val_loss did not improve from 399.38919
196/196 - 10s - loss: 767.0670 - MinusLogProbMetric: 767.0670 - val_loss: 1372.1473 - val_MinusLogProbMetric: 1372.1473 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 149/1000
2023-10-03 07:19:14.186 
Epoch 149/1000 
	 loss: 590.2103, MinusLogProbMetric: 590.2103, val_loss: 448.4894, val_MinusLogProbMetric: 448.4894

Epoch 149: val_loss did not improve from 399.38919
196/196 - 9s - loss: 590.2103 - MinusLogProbMetric: 590.2103 - val_loss: 448.4894 - val_MinusLogProbMetric: 448.4894 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 150/1000
2023-10-03 07:19:23.451 
Epoch 150/1000 
	 loss: 435.3200, MinusLogProbMetric: 435.3200, val_loss: 428.9844, val_MinusLogProbMetric: 428.9844

Epoch 150: val_loss did not improve from 399.38919
196/196 - 9s - loss: 435.3200 - MinusLogProbMetric: 435.3200 - val_loss: 428.9844 - val_MinusLogProbMetric: 428.9844 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 151/1000
2023-10-03 07:19:32.634 
Epoch 151/1000 
	 loss: 423.4952, MinusLogProbMetric: 423.4952, val_loss: 421.8598, val_MinusLogProbMetric: 421.8598

Epoch 151: val_loss did not improve from 399.38919
196/196 - 9s - loss: 423.4952 - MinusLogProbMetric: 423.4952 - val_loss: 421.8598 - val_MinusLogProbMetric: 421.8598 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 152/1000
2023-10-03 07:19:42.058 
Epoch 152/1000 
	 loss: 417.9704, MinusLogProbMetric: 417.9704, val_loss: 417.4120, val_MinusLogProbMetric: 417.4120

Epoch 152: val_loss did not improve from 399.38919
196/196 - 9s - loss: 417.9704 - MinusLogProbMetric: 417.9704 - val_loss: 417.4120 - val_MinusLogProbMetric: 417.4120 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 153/1000
2023-10-03 07:19:51.413 
Epoch 153/1000 
	 loss: 414.5839, MinusLogProbMetric: 414.5839, val_loss: 415.2383, val_MinusLogProbMetric: 415.2383

Epoch 153: val_loss did not improve from 399.38919
196/196 - 9s - loss: 414.5839 - MinusLogProbMetric: 414.5839 - val_loss: 415.2383 - val_MinusLogProbMetric: 415.2383 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 154/1000
2023-10-03 07:20:00.558 
Epoch 154/1000 
	 loss: 412.1555, MinusLogProbMetric: 412.1555, val_loss: 413.2773, val_MinusLogProbMetric: 413.2773

Epoch 154: val_loss did not improve from 399.38919
196/196 - 9s - loss: 412.1555 - MinusLogProbMetric: 412.1555 - val_loss: 413.2773 - val_MinusLogProbMetric: 413.2773 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 155/1000
2023-10-03 07:20:09.866 
Epoch 155/1000 
	 loss: 410.2303, MinusLogProbMetric: 410.2303, val_loss: 411.1408, val_MinusLogProbMetric: 411.1408

Epoch 155: val_loss did not improve from 399.38919
196/196 - 9s - loss: 410.2303 - MinusLogProbMetric: 410.2303 - val_loss: 411.1408 - val_MinusLogProbMetric: 411.1408 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 156/1000
2023-10-03 07:20:18.913 
Epoch 156/1000 
	 loss: 408.8396, MinusLogProbMetric: 408.8396, val_loss: 410.3630, val_MinusLogProbMetric: 410.3630

Epoch 156: val_loss did not improve from 399.38919
196/196 - 9s - loss: 408.8396 - MinusLogProbMetric: 408.8396 - val_loss: 410.3630 - val_MinusLogProbMetric: 410.3630 - lr: 3.3333e-04 - 9s/epoch - 46ms/step
Epoch 157/1000
2023-10-03 07:20:28.132 
Epoch 157/1000 
	 loss: 407.6129, MinusLogProbMetric: 407.6129, val_loss: 408.3503, val_MinusLogProbMetric: 408.3503

Epoch 157: val_loss did not improve from 399.38919
196/196 - 9s - loss: 407.6129 - MinusLogProbMetric: 407.6129 - val_loss: 408.3503 - val_MinusLogProbMetric: 408.3503 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 158/1000
2023-10-03 07:20:37.405 
Epoch 158/1000 
	 loss: 406.6321, MinusLogProbMetric: 406.6321, val_loss: 407.3808, val_MinusLogProbMetric: 407.3808

Epoch 158: val_loss did not improve from 399.38919
196/196 - 9s - loss: 406.6321 - MinusLogProbMetric: 406.6321 - val_loss: 407.3808 - val_MinusLogProbMetric: 407.3808 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 159/1000
2023-10-03 07:20:46.924 
Epoch 159/1000 
	 loss: 405.6488, MinusLogProbMetric: 405.6488, val_loss: 406.6107, val_MinusLogProbMetric: 406.6107

Epoch 159: val_loss did not improve from 399.38919
196/196 - 10s - loss: 405.6488 - MinusLogProbMetric: 405.6488 - val_loss: 406.6107 - val_MinusLogProbMetric: 406.6107 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 160/1000
2023-10-03 07:20:56.108 
Epoch 160/1000 
	 loss: 404.9372, MinusLogProbMetric: 404.9372, val_loss: 407.5695, val_MinusLogProbMetric: 407.5695

Epoch 160: val_loss did not improve from 399.38919
196/196 - 9s - loss: 404.9372 - MinusLogProbMetric: 404.9372 - val_loss: 407.5695 - val_MinusLogProbMetric: 407.5695 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 161/1000
2023-10-03 07:21:05.338 
Epoch 161/1000 
	 loss: 404.3844, MinusLogProbMetric: 404.3844, val_loss: 405.8533, val_MinusLogProbMetric: 405.8533

Epoch 161: val_loss did not improve from 399.38919
196/196 - 9s - loss: 404.3844 - MinusLogProbMetric: 404.3844 - val_loss: 405.8533 - val_MinusLogProbMetric: 405.8533 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 162/1000
2023-10-03 07:21:14.552 
Epoch 162/1000 
	 loss: 403.6115, MinusLogProbMetric: 403.6115, val_loss: 404.8423, val_MinusLogProbMetric: 404.8423

Epoch 162: val_loss did not improve from 399.38919
196/196 - 9s - loss: 403.6115 - MinusLogProbMetric: 403.6115 - val_loss: 404.8423 - val_MinusLogProbMetric: 404.8423 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 163/1000
2023-10-03 07:21:24.010 
Epoch 163/1000 
	 loss: 403.2713, MinusLogProbMetric: 403.2713, val_loss: 404.2685, val_MinusLogProbMetric: 404.2685

Epoch 163: val_loss did not improve from 399.38919
196/196 - 9s - loss: 403.2713 - MinusLogProbMetric: 403.2713 - val_loss: 404.2685 - val_MinusLogProbMetric: 404.2685 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 164/1000
2023-10-03 07:21:33.340 
Epoch 164/1000 
	 loss: 402.5744, MinusLogProbMetric: 402.5744, val_loss: 403.5553, val_MinusLogProbMetric: 403.5553

Epoch 164: val_loss did not improve from 399.38919
196/196 - 9s - loss: 402.5744 - MinusLogProbMetric: 402.5744 - val_loss: 403.5553 - val_MinusLogProbMetric: 403.5553 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 165/1000
2023-10-03 07:21:42.572 
Epoch 165/1000 
	 loss: 402.4681, MinusLogProbMetric: 402.4681, val_loss: 403.3123, val_MinusLogProbMetric: 403.3123

Epoch 165: val_loss did not improve from 399.38919
196/196 - 9s - loss: 402.4681 - MinusLogProbMetric: 402.4681 - val_loss: 403.3123 - val_MinusLogProbMetric: 403.3123 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 166/1000
2023-10-03 07:21:51.832 
Epoch 166/1000 
	 loss: 401.8719, MinusLogProbMetric: 401.8719, val_loss: 404.4773, val_MinusLogProbMetric: 404.4773

Epoch 166: val_loss did not improve from 399.38919
196/196 - 9s - loss: 401.8719 - MinusLogProbMetric: 401.8719 - val_loss: 404.4773 - val_MinusLogProbMetric: 404.4773 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 167/1000
2023-10-03 07:22:01.046 
Epoch 167/1000 
	 loss: 401.4839, MinusLogProbMetric: 401.4839, val_loss: 403.1257, val_MinusLogProbMetric: 403.1257

Epoch 167: val_loss did not improve from 399.38919
196/196 - 9s - loss: 401.4839 - MinusLogProbMetric: 401.4839 - val_loss: 403.1257 - val_MinusLogProbMetric: 403.1257 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 168/1000
2023-10-03 07:22:10.493 
Epoch 168/1000 
	 loss: 401.4756, MinusLogProbMetric: 401.4756, val_loss: 403.1583, val_MinusLogProbMetric: 403.1583

Epoch 168: val_loss did not improve from 399.38919
196/196 - 9s - loss: 401.4756 - MinusLogProbMetric: 401.4756 - val_loss: 403.1583 - val_MinusLogProbMetric: 403.1583 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 169/1000
2023-10-03 07:22:19.622 
Epoch 169/1000 
	 loss: 400.8830, MinusLogProbMetric: 400.8830, val_loss: 402.3089, val_MinusLogProbMetric: 402.3089

Epoch 169: val_loss did not improve from 399.38919
196/196 - 9s - loss: 400.8830 - MinusLogProbMetric: 400.8830 - val_loss: 402.3089 - val_MinusLogProbMetric: 402.3089 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 170/1000
2023-10-03 07:22:29.009 
Epoch 170/1000 
	 loss: 400.6803, MinusLogProbMetric: 400.6803, val_loss: 402.1635, val_MinusLogProbMetric: 402.1635

Epoch 170: val_loss did not improve from 399.38919
196/196 - 9s - loss: 400.6803 - MinusLogProbMetric: 400.6803 - val_loss: 402.1635 - val_MinusLogProbMetric: 402.1635 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 171/1000
2023-10-03 07:22:38.288 
Epoch 171/1000 
	 loss: 400.5422, MinusLogProbMetric: 400.5422, val_loss: 403.5509, val_MinusLogProbMetric: 403.5509

Epoch 171: val_loss did not improve from 399.38919
196/196 - 9s - loss: 400.5422 - MinusLogProbMetric: 400.5422 - val_loss: 403.5509 - val_MinusLogProbMetric: 403.5509 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 172/1000
2023-10-03 07:22:47.619 
Epoch 172/1000 
	 loss: 400.3618, MinusLogProbMetric: 400.3618, val_loss: 402.6758, val_MinusLogProbMetric: 402.6758

Epoch 172: val_loss did not improve from 399.38919
196/196 - 9s - loss: 400.3618 - MinusLogProbMetric: 400.3618 - val_loss: 402.6758 - val_MinusLogProbMetric: 402.6758 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 173/1000
2023-10-03 07:22:56.835 
Epoch 173/1000 
	 loss: 400.1435, MinusLogProbMetric: 400.1435, val_loss: 402.9122, val_MinusLogProbMetric: 402.9122

Epoch 173: val_loss did not improve from 399.38919
196/196 - 9s - loss: 400.1435 - MinusLogProbMetric: 400.1435 - val_loss: 402.9122 - val_MinusLogProbMetric: 402.9122 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 174/1000
2023-10-03 07:23:06.273 
Epoch 174/1000 
	 loss: 399.8469, MinusLogProbMetric: 399.8469, val_loss: 403.4120, val_MinusLogProbMetric: 403.4120

Epoch 174: val_loss did not improve from 399.38919
196/196 - 9s - loss: 399.8469 - MinusLogProbMetric: 399.8469 - val_loss: 403.4120 - val_MinusLogProbMetric: 403.4120 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 175/1000
2023-10-03 07:23:15.663 
Epoch 175/1000 
	 loss: 399.5590, MinusLogProbMetric: 399.5590, val_loss: 402.9241, val_MinusLogProbMetric: 402.9241

Epoch 175: val_loss did not improve from 399.38919
196/196 - 9s - loss: 399.5590 - MinusLogProbMetric: 399.5590 - val_loss: 402.9241 - val_MinusLogProbMetric: 402.9241 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 176/1000
2023-10-03 07:23:24.957 
Epoch 176/1000 
	 loss: 399.7115, MinusLogProbMetric: 399.7115, val_loss: 403.7789, val_MinusLogProbMetric: 403.7789

Epoch 176: val_loss did not improve from 399.38919
196/196 - 9s - loss: 399.7115 - MinusLogProbMetric: 399.7115 - val_loss: 403.7789 - val_MinusLogProbMetric: 403.7789 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 177/1000
2023-10-03 07:23:34.526 
Epoch 177/1000 
	 loss: 399.4904, MinusLogProbMetric: 399.4904, val_loss: 400.2011, val_MinusLogProbMetric: 400.2011

Epoch 177: val_loss did not improve from 399.38919
196/196 - 10s - loss: 399.4904 - MinusLogProbMetric: 399.4904 - val_loss: 400.2011 - val_MinusLogProbMetric: 400.2011 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 178/1000
2023-10-03 07:23:43.756 
Epoch 178/1000 
	 loss: 399.3183, MinusLogProbMetric: 399.3183, val_loss: 400.4860, val_MinusLogProbMetric: 400.4860

Epoch 178: val_loss did not improve from 399.38919
196/196 - 9s - loss: 399.3183 - MinusLogProbMetric: 399.3183 - val_loss: 400.4860 - val_MinusLogProbMetric: 400.4860 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 179/1000
2023-10-03 07:23:53.322 
Epoch 179/1000 
	 loss: 399.0360, MinusLogProbMetric: 399.0360, val_loss: 401.2772, val_MinusLogProbMetric: 401.2772

Epoch 179: val_loss did not improve from 399.38919
196/196 - 10s - loss: 399.0360 - MinusLogProbMetric: 399.0360 - val_loss: 401.2772 - val_MinusLogProbMetric: 401.2772 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 180/1000
2023-10-03 07:24:02.753 
Epoch 180/1000 
	 loss: 398.9602, MinusLogProbMetric: 398.9602, val_loss: 402.0178, val_MinusLogProbMetric: 402.0178

Epoch 180: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.9602 - MinusLogProbMetric: 398.9602 - val_loss: 402.0178 - val_MinusLogProbMetric: 402.0178 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 181/1000
2023-10-03 07:24:12.131 
Epoch 181/1000 
	 loss: 398.7710, MinusLogProbMetric: 398.7710, val_loss: 401.9366, val_MinusLogProbMetric: 401.9366

Epoch 181: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.7710 - MinusLogProbMetric: 398.7710 - val_loss: 401.9366 - val_MinusLogProbMetric: 401.9366 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 182/1000
2023-10-03 07:24:21.522 
Epoch 182/1000 
	 loss: 398.6524, MinusLogProbMetric: 398.6524, val_loss: 400.2935, val_MinusLogProbMetric: 400.2935

Epoch 182: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.6524 - MinusLogProbMetric: 398.6524 - val_loss: 400.2935 - val_MinusLogProbMetric: 400.2935 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 183/1000
2023-10-03 07:24:30.881 
Epoch 183/1000 
	 loss: 398.5143, MinusLogProbMetric: 398.5143, val_loss: 400.6340, val_MinusLogProbMetric: 400.6340

Epoch 183: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.5143 - MinusLogProbMetric: 398.5143 - val_loss: 400.6340 - val_MinusLogProbMetric: 400.6340 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 184/1000
2023-10-03 07:24:40.114 
Epoch 184/1000 
	 loss: 398.6765, MinusLogProbMetric: 398.6765, val_loss: 401.3502, val_MinusLogProbMetric: 401.3502

Epoch 184: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.6765 - MinusLogProbMetric: 398.6765 - val_loss: 401.3502 - val_MinusLogProbMetric: 401.3502 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 185/1000
2023-10-03 07:24:49.515 
Epoch 185/1000 
	 loss: 398.3517, MinusLogProbMetric: 398.3517, val_loss: 400.4763, val_MinusLogProbMetric: 400.4763

Epoch 185: val_loss did not improve from 399.38919
196/196 - 9s - loss: 398.3517 - MinusLogProbMetric: 398.3517 - val_loss: 400.4763 - val_MinusLogProbMetric: 400.4763 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 186/1000
2023-10-03 07:24:59.003 
Epoch 186/1000 
	 loss: 398.3905, MinusLogProbMetric: 398.3905, val_loss: 399.1721, val_MinusLogProbMetric: 399.1721

Epoch 186: val_loss improved from 399.38919 to 399.17212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 398.3905 - MinusLogProbMetric: 398.3905 - val_loss: 399.1721 - val_MinusLogProbMetric: 399.1721 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 187/1000
2023-10-03 07:25:08.478 
Epoch 187/1000 
	 loss: 398.0495, MinusLogProbMetric: 398.0495, val_loss: 400.2386, val_MinusLogProbMetric: 400.2386

Epoch 187: val_loss did not improve from 399.17212
196/196 - 9s - loss: 398.0495 - MinusLogProbMetric: 398.0495 - val_loss: 400.2386 - val_MinusLogProbMetric: 400.2386 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 188/1000
2023-10-03 07:25:18.145 
Epoch 188/1000 
	 loss: 398.3297, MinusLogProbMetric: 398.3297, val_loss: 400.9148, val_MinusLogProbMetric: 400.9148

Epoch 188: val_loss did not improve from 399.17212
196/196 - 10s - loss: 398.3297 - MinusLogProbMetric: 398.3297 - val_loss: 400.9148 - val_MinusLogProbMetric: 400.9148 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 189/1000
2023-10-03 07:25:28.239 
Epoch 189/1000 
	 loss: 397.7862, MinusLogProbMetric: 397.7862, val_loss: 399.6067, val_MinusLogProbMetric: 399.6067

Epoch 189: val_loss did not improve from 399.17212
196/196 - 10s - loss: 397.7862 - MinusLogProbMetric: 397.7862 - val_loss: 399.6067 - val_MinusLogProbMetric: 399.6067 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 190/1000
2023-10-03 07:25:37.547 
Epoch 190/1000 
	 loss: 398.0480, MinusLogProbMetric: 398.0480, val_loss: 400.2041, val_MinusLogProbMetric: 400.2041

Epoch 190: val_loss did not improve from 399.17212
196/196 - 9s - loss: 398.0480 - MinusLogProbMetric: 398.0480 - val_loss: 400.2041 - val_MinusLogProbMetric: 400.2041 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 191/1000
2023-10-03 07:25:46.968 
Epoch 191/1000 
	 loss: 397.7406, MinusLogProbMetric: 397.7406, val_loss: 401.0565, val_MinusLogProbMetric: 401.0565

Epoch 191: val_loss did not improve from 399.17212
196/196 - 9s - loss: 397.7406 - MinusLogProbMetric: 397.7406 - val_loss: 401.0565 - val_MinusLogProbMetric: 401.0565 - lr: 3.3333e-04 - 9s/epoch - 48ms/step
Epoch 192/1000
2023-10-03 07:25:56.730 
Epoch 192/1000 
	 loss: 397.7201, MinusLogProbMetric: 397.7201, val_loss: 399.3535, val_MinusLogProbMetric: 399.3535

Epoch 192: val_loss did not improve from 399.17212
196/196 - 10s - loss: 397.7201 - MinusLogProbMetric: 397.7201 - val_loss: 399.3535 - val_MinusLogProbMetric: 399.3535 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 193/1000
2023-10-03 07:26:07.020 
Epoch 193/1000 
	 loss: 397.6690, MinusLogProbMetric: 397.6690, val_loss: 399.5933, val_MinusLogProbMetric: 399.5933

Epoch 193: val_loss did not improve from 399.17212
196/196 - 10s - loss: 397.6690 - MinusLogProbMetric: 397.6690 - val_loss: 399.5933 - val_MinusLogProbMetric: 399.5933 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 194/1000
2023-10-03 07:26:17.222 
Epoch 194/1000 
	 loss: 397.5690, MinusLogProbMetric: 397.5690, val_loss: 399.8312, val_MinusLogProbMetric: 399.8312

Epoch 194: val_loss did not improve from 399.17212
196/196 - 10s - loss: 397.5690 - MinusLogProbMetric: 397.5690 - val_loss: 399.8312 - val_MinusLogProbMetric: 399.8312 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 195/1000
2023-10-03 07:26:27.512 
Epoch 195/1000 
	 loss: 397.5331, MinusLogProbMetric: 397.5331, val_loss: 401.0105, val_MinusLogProbMetric: 401.0105

Epoch 195: val_loss did not improve from 399.17212
196/196 - 10s - loss: 397.5331 - MinusLogProbMetric: 397.5331 - val_loss: 401.0105 - val_MinusLogProbMetric: 401.0105 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 196/1000
2023-10-03 07:26:37.816 
Epoch 196/1000 
	 loss: 397.5586, MinusLogProbMetric: 397.5586, val_loss: 398.1743, val_MinusLogProbMetric: 398.1743

Epoch 196: val_loss improved from 399.17212 to 398.17429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 397.5586 - MinusLogProbMetric: 397.5586 - val_loss: 398.1743 - val_MinusLogProbMetric: 398.1743 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 197/1000
2023-10-03 07:26:48.552 
Epoch 197/1000 
	 loss: 397.1740, MinusLogProbMetric: 397.1740, val_loss: 403.0728, val_MinusLogProbMetric: 403.0728

Epoch 197: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.1740 - MinusLogProbMetric: 397.1740 - val_loss: 403.0728 - val_MinusLogProbMetric: 403.0728 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 198/1000
2023-10-03 07:26:58.667 
Epoch 198/1000 
	 loss: 397.3611, MinusLogProbMetric: 397.3611, val_loss: 400.2139, val_MinusLogProbMetric: 400.2139

Epoch 198: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.3611 - MinusLogProbMetric: 397.3611 - val_loss: 400.2139 - val_MinusLogProbMetric: 400.2139 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 199/1000
2023-10-03 07:27:09.019 
Epoch 199/1000 
	 loss: 397.1134, MinusLogProbMetric: 397.1134, val_loss: 400.3211, val_MinusLogProbMetric: 400.3211

Epoch 199: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.1134 - MinusLogProbMetric: 397.1134 - val_loss: 400.3211 - val_MinusLogProbMetric: 400.3211 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 200/1000
2023-10-03 07:27:19.248 
Epoch 200/1000 
	 loss: 397.0348, MinusLogProbMetric: 397.0348, val_loss: 398.8361, val_MinusLogProbMetric: 398.8361

Epoch 200: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.0348 - MinusLogProbMetric: 397.0348 - val_loss: 398.8361 - val_MinusLogProbMetric: 398.8361 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 201/1000
2023-10-03 07:27:29.609 
Epoch 201/1000 
	 loss: 397.1767, MinusLogProbMetric: 397.1767, val_loss: 398.9626, val_MinusLogProbMetric: 398.9626

Epoch 201: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.1767 - MinusLogProbMetric: 397.1767 - val_loss: 398.9626 - val_MinusLogProbMetric: 398.9626 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 202/1000
2023-10-03 07:27:39.815 
Epoch 202/1000 
	 loss: 396.8915, MinusLogProbMetric: 396.8915, val_loss: 399.0902, val_MinusLogProbMetric: 399.0902

Epoch 202: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.8915 - MinusLogProbMetric: 396.8915 - val_loss: 399.0902 - val_MinusLogProbMetric: 399.0902 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 203/1000
2023-10-03 07:27:50.116 
Epoch 203/1000 
	 loss: 397.0155, MinusLogProbMetric: 397.0155, val_loss: 398.8703, val_MinusLogProbMetric: 398.8703

Epoch 203: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.0155 - MinusLogProbMetric: 397.0155 - val_loss: 398.8703 - val_MinusLogProbMetric: 398.8703 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 204/1000
2023-10-03 07:28:00.386 
Epoch 204/1000 
	 loss: 396.7755, MinusLogProbMetric: 396.7755, val_loss: 399.5406, val_MinusLogProbMetric: 399.5406

Epoch 204: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.7755 - MinusLogProbMetric: 396.7755 - val_loss: 399.5406 - val_MinusLogProbMetric: 399.5406 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 205/1000
2023-10-03 07:28:10.626 
Epoch 205/1000 
	 loss: 397.0580, MinusLogProbMetric: 397.0580, val_loss: 401.7097, val_MinusLogProbMetric: 401.7097

Epoch 205: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.0580 - MinusLogProbMetric: 397.0580 - val_loss: 401.7097 - val_MinusLogProbMetric: 401.7097 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 206/1000
2023-10-03 07:28:21.003 
Epoch 206/1000 
	 loss: 396.7937, MinusLogProbMetric: 396.7937, val_loss: 399.8993, val_MinusLogProbMetric: 399.8993

Epoch 206: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.7937 - MinusLogProbMetric: 396.7937 - val_loss: 399.8993 - val_MinusLogProbMetric: 399.8993 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 207/1000
2023-10-03 07:28:31.438 
Epoch 207/1000 
	 loss: 396.5085, MinusLogProbMetric: 396.5085, val_loss: 398.4404, val_MinusLogProbMetric: 398.4404

Epoch 207: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.5085 - MinusLogProbMetric: 396.5085 - val_loss: 398.4404 - val_MinusLogProbMetric: 398.4404 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 208/1000
2023-10-03 07:28:41.764 
Epoch 208/1000 
	 loss: 397.2427, MinusLogProbMetric: 397.2427, val_loss: 398.3415, val_MinusLogProbMetric: 398.3415

Epoch 208: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.2427 - MinusLogProbMetric: 397.2427 - val_loss: 398.3415 - val_MinusLogProbMetric: 398.3415 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 209/1000
2023-10-03 07:28:52.066 
Epoch 209/1000 
	 loss: 396.3886, MinusLogProbMetric: 396.3886, val_loss: 401.3895, val_MinusLogProbMetric: 401.3895

Epoch 209: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.3886 - MinusLogProbMetric: 396.3886 - val_loss: 401.3895 - val_MinusLogProbMetric: 401.3895 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 210/1000
2023-10-03 07:29:02.086 
Epoch 210/1000 
	 loss: 396.4453, MinusLogProbMetric: 396.4453, val_loss: 399.3202, val_MinusLogProbMetric: 399.3202

Epoch 210: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.4453 - MinusLogProbMetric: 396.4453 - val_loss: 399.3202 - val_MinusLogProbMetric: 399.3202 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 211/1000
2023-10-03 07:29:12.514 
Epoch 211/1000 
	 loss: 496.8068, MinusLogProbMetric: 496.8068, val_loss: 434.3604, val_MinusLogProbMetric: 434.3604

Epoch 211: val_loss did not improve from 398.17429
196/196 - 10s - loss: 496.8068 - MinusLogProbMetric: 496.8068 - val_loss: 434.3604 - val_MinusLogProbMetric: 434.3604 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 212/1000
2023-10-03 07:29:22.885 
Epoch 212/1000 
	 loss: 418.3751, MinusLogProbMetric: 418.3751, val_loss: 411.8219, val_MinusLogProbMetric: 411.8219

Epoch 212: val_loss did not improve from 398.17429
196/196 - 10s - loss: 418.3751 - MinusLogProbMetric: 418.3751 - val_loss: 411.8219 - val_MinusLogProbMetric: 411.8219 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 213/1000
2023-10-03 07:29:33.166 
Epoch 213/1000 
	 loss: 407.5584, MinusLogProbMetric: 407.5584, val_loss: 407.9679, val_MinusLogProbMetric: 407.9679

Epoch 213: val_loss did not improve from 398.17429
196/196 - 10s - loss: 407.5584 - MinusLogProbMetric: 407.5584 - val_loss: 407.9679 - val_MinusLogProbMetric: 407.9679 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 214/1000
2023-10-03 07:29:43.517 
Epoch 214/1000 
	 loss: 404.3554, MinusLogProbMetric: 404.3554, val_loss: 405.1823, val_MinusLogProbMetric: 405.1823

Epoch 214: val_loss did not improve from 398.17429
196/196 - 10s - loss: 404.3554 - MinusLogProbMetric: 404.3554 - val_loss: 405.1823 - val_MinusLogProbMetric: 405.1823 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 215/1000
2023-10-03 07:29:53.968 
Epoch 215/1000 
	 loss: 402.5273, MinusLogProbMetric: 402.5273, val_loss: 403.0735, val_MinusLogProbMetric: 403.0735

Epoch 215: val_loss did not improve from 398.17429
196/196 - 10s - loss: 402.5273 - MinusLogProbMetric: 402.5273 - val_loss: 403.0735 - val_MinusLogProbMetric: 403.0735 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 216/1000
2023-10-03 07:30:04.353 
Epoch 216/1000 
	 loss: 401.5072, MinusLogProbMetric: 401.5072, val_loss: 402.4163, val_MinusLogProbMetric: 402.4163

Epoch 216: val_loss did not improve from 398.17429
196/196 - 10s - loss: 401.5072 - MinusLogProbMetric: 401.5072 - val_loss: 402.4163 - val_MinusLogProbMetric: 402.4163 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 217/1000
2023-10-03 07:30:14.822 
Epoch 217/1000 
	 loss: 400.4326, MinusLogProbMetric: 400.4326, val_loss: 404.4015, val_MinusLogProbMetric: 404.4015

Epoch 217: val_loss did not improve from 398.17429
196/196 - 10s - loss: 400.4326 - MinusLogProbMetric: 400.4326 - val_loss: 404.4015 - val_MinusLogProbMetric: 404.4015 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 218/1000
2023-10-03 07:30:25.018 
Epoch 218/1000 
	 loss: 400.2223, MinusLogProbMetric: 400.2223, val_loss: 401.9991, val_MinusLogProbMetric: 401.9991

Epoch 218: val_loss did not improve from 398.17429
196/196 - 10s - loss: 400.2223 - MinusLogProbMetric: 400.2223 - val_loss: 401.9991 - val_MinusLogProbMetric: 401.9991 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 219/1000
2023-10-03 07:30:35.153 
Epoch 219/1000 
	 loss: 399.7513, MinusLogProbMetric: 399.7513, val_loss: 401.8704, val_MinusLogProbMetric: 401.8704

Epoch 219: val_loss did not improve from 398.17429
196/196 - 10s - loss: 399.7513 - MinusLogProbMetric: 399.7513 - val_loss: 401.8704 - val_MinusLogProbMetric: 401.8704 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 220/1000
2023-10-03 07:30:45.333 
Epoch 220/1000 
	 loss: 399.3369, MinusLogProbMetric: 399.3369, val_loss: 400.9945, val_MinusLogProbMetric: 400.9945

Epoch 220: val_loss did not improve from 398.17429
196/196 - 10s - loss: 399.3369 - MinusLogProbMetric: 399.3369 - val_loss: 400.9945 - val_MinusLogProbMetric: 400.9945 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 221/1000
2023-10-03 07:30:55.632 
Epoch 221/1000 
	 loss: 398.9275, MinusLogProbMetric: 398.9275, val_loss: 402.0381, val_MinusLogProbMetric: 402.0381

Epoch 221: val_loss did not improve from 398.17429
196/196 - 10s - loss: 398.9275 - MinusLogProbMetric: 398.9275 - val_loss: 402.0381 - val_MinusLogProbMetric: 402.0381 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 222/1000
2023-10-03 07:31:05.950 
Epoch 222/1000 
	 loss: 399.0085, MinusLogProbMetric: 399.0085, val_loss: 401.2166, val_MinusLogProbMetric: 401.2166

Epoch 222: val_loss did not improve from 398.17429
196/196 - 10s - loss: 399.0085 - MinusLogProbMetric: 399.0085 - val_loss: 401.2166 - val_MinusLogProbMetric: 401.2166 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 223/1000
2023-10-03 07:31:16.271 
Epoch 223/1000 
	 loss: 398.5324, MinusLogProbMetric: 398.5324, val_loss: 402.5334, val_MinusLogProbMetric: 402.5334

Epoch 223: val_loss did not improve from 398.17429
196/196 - 10s - loss: 398.5324 - MinusLogProbMetric: 398.5324 - val_loss: 402.5334 - val_MinusLogProbMetric: 402.5334 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 224/1000
2023-10-03 07:31:26.388 
Epoch 224/1000 
	 loss: 398.6697, MinusLogProbMetric: 398.6697, val_loss: 403.3787, val_MinusLogProbMetric: 403.3787

Epoch 224: val_loss did not improve from 398.17429
196/196 - 10s - loss: 398.6697 - MinusLogProbMetric: 398.6697 - val_loss: 403.3787 - val_MinusLogProbMetric: 403.3787 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 225/1000
2023-10-03 07:31:36.607 
Epoch 225/1000 
	 loss: 398.3208, MinusLogProbMetric: 398.3208, val_loss: 400.6851, val_MinusLogProbMetric: 400.6851

Epoch 225: val_loss did not improve from 398.17429
196/196 - 10s - loss: 398.3208 - MinusLogProbMetric: 398.3208 - val_loss: 400.6851 - val_MinusLogProbMetric: 400.6851 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 226/1000
2023-10-03 07:31:47.082 
Epoch 226/1000 
	 loss: 398.1499, MinusLogProbMetric: 398.1499, val_loss: 399.8523, val_MinusLogProbMetric: 399.8523

Epoch 226: val_loss did not improve from 398.17429
196/196 - 10s - loss: 398.1499 - MinusLogProbMetric: 398.1499 - val_loss: 399.8523 - val_MinusLogProbMetric: 399.8523 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 227/1000
2023-10-03 07:31:57.513 
Epoch 227/1000 
	 loss: 397.9153, MinusLogProbMetric: 397.9153, val_loss: 399.6341, val_MinusLogProbMetric: 399.6341

Epoch 227: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.9153 - MinusLogProbMetric: 397.9153 - val_loss: 399.6341 - val_MinusLogProbMetric: 399.6341 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 228/1000
2023-10-03 07:32:07.970 
Epoch 228/1000 
	 loss: 397.9872, MinusLogProbMetric: 397.9872, val_loss: 400.1540, val_MinusLogProbMetric: 400.1540

Epoch 228: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.9872 - MinusLogProbMetric: 397.9872 - val_loss: 400.1540 - val_MinusLogProbMetric: 400.1540 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 229/1000
2023-10-03 07:32:18.346 
Epoch 229/1000 
	 loss: 397.6577, MinusLogProbMetric: 397.6577, val_loss: 398.9305, val_MinusLogProbMetric: 398.9305

Epoch 229: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.6577 - MinusLogProbMetric: 397.6577 - val_loss: 398.9305 - val_MinusLogProbMetric: 398.9305 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 230/1000
2023-10-03 07:32:28.527 
Epoch 230/1000 
	 loss: 397.6178, MinusLogProbMetric: 397.6178, val_loss: 399.9293, val_MinusLogProbMetric: 399.9293

Epoch 230: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.6178 - MinusLogProbMetric: 397.6178 - val_loss: 399.9293 - val_MinusLogProbMetric: 399.9293 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 231/1000
2023-10-03 07:32:38.748 
Epoch 231/1000 
	 loss: 397.2601, MinusLogProbMetric: 397.2601, val_loss: 403.9712, val_MinusLogProbMetric: 403.9712

Epoch 231: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.2601 - MinusLogProbMetric: 397.2601 - val_loss: 403.9712 - val_MinusLogProbMetric: 403.9712 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 232/1000
2023-10-03 07:32:48.846 
Epoch 232/1000 
	 loss: 397.5063, MinusLogProbMetric: 397.5063, val_loss: 401.8265, val_MinusLogProbMetric: 401.8265

Epoch 232: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.5063 - MinusLogProbMetric: 397.5063 - val_loss: 401.8265 - val_MinusLogProbMetric: 401.8265 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 233/1000
2023-10-03 07:32:58.938 
Epoch 233/1000 
	 loss: 397.1156, MinusLogProbMetric: 397.1156, val_loss: 399.1521, val_MinusLogProbMetric: 399.1521

Epoch 233: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.1156 - MinusLogProbMetric: 397.1156 - val_loss: 399.1521 - val_MinusLogProbMetric: 399.1521 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 234/1000
2023-10-03 07:33:09.143 
Epoch 234/1000 
	 loss: 397.1249, MinusLogProbMetric: 397.1249, val_loss: 398.9613, val_MinusLogProbMetric: 398.9613

Epoch 234: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.1249 - MinusLogProbMetric: 397.1249 - val_loss: 398.9613 - val_MinusLogProbMetric: 398.9613 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 235/1000
2023-10-03 07:33:19.458 
Epoch 235/1000 
	 loss: 396.5652, MinusLogProbMetric: 396.5652, val_loss: 398.9976, val_MinusLogProbMetric: 398.9976

Epoch 235: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.5652 - MinusLogProbMetric: 396.5652 - val_loss: 398.9976 - val_MinusLogProbMetric: 398.9976 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 236/1000
2023-10-03 07:33:29.707 
Epoch 236/1000 
	 loss: 397.0761, MinusLogProbMetric: 397.0761, val_loss: 398.8721, val_MinusLogProbMetric: 398.8721

Epoch 236: val_loss did not improve from 398.17429
196/196 - 10s - loss: 397.0761 - MinusLogProbMetric: 397.0761 - val_loss: 398.8721 - val_MinusLogProbMetric: 398.8721 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 237/1000
2023-10-03 07:33:40.086 
Epoch 237/1000 
	 loss: 396.6368, MinusLogProbMetric: 396.6368, val_loss: 399.7538, val_MinusLogProbMetric: 399.7538

Epoch 237: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.6368 - MinusLogProbMetric: 396.6368 - val_loss: 399.7538 - val_MinusLogProbMetric: 399.7538 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 238/1000
2023-10-03 07:33:50.420 
Epoch 238/1000 
	 loss: 396.6656, MinusLogProbMetric: 396.6656, val_loss: 398.6580, val_MinusLogProbMetric: 398.6580

Epoch 238: val_loss did not improve from 398.17429
196/196 - 10s - loss: 396.6656 - MinusLogProbMetric: 396.6656 - val_loss: 398.6580 - val_MinusLogProbMetric: 398.6580 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 239/1000
2023-10-03 07:34:00.914 
Epoch 239/1000 
	 loss: 396.4982, MinusLogProbMetric: 396.4982, val_loss: 398.0399, val_MinusLogProbMetric: 398.0399

Epoch 239: val_loss improved from 398.17429 to 398.03986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 396.4982 - MinusLogProbMetric: 396.4982 - val_loss: 398.0399 - val_MinusLogProbMetric: 398.0399 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 240/1000
2023-10-03 07:34:11.561 
Epoch 240/1000 
	 loss: 396.3970, MinusLogProbMetric: 396.3970, val_loss: 398.6808, val_MinusLogProbMetric: 398.6808

Epoch 240: val_loss did not improve from 398.03986
196/196 - 10s - loss: 396.3970 - MinusLogProbMetric: 396.3970 - val_loss: 398.6808 - val_MinusLogProbMetric: 398.6808 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 241/1000
2023-10-03 07:34:21.843 
Epoch 241/1000 
	 loss: 396.3032, MinusLogProbMetric: 396.3032, val_loss: 397.9301, val_MinusLogProbMetric: 397.9301

Epoch 241: val_loss improved from 398.03986 to 397.93011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 396.3032 - MinusLogProbMetric: 396.3032 - val_loss: 397.9301 - val_MinusLogProbMetric: 397.9301 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 242/1000
2023-10-03 07:34:32.482 
Epoch 242/1000 
	 loss: 396.2626, MinusLogProbMetric: 396.2626, val_loss: 397.5676, val_MinusLogProbMetric: 397.5676

Epoch 242: val_loss improved from 397.93011 to 397.56763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 396.2626 - MinusLogProbMetric: 396.2626 - val_loss: 397.5676 - val_MinusLogProbMetric: 397.5676 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 243/1000
2023-10-03 07:34:43.101 
Epoch 243/1000 
	 loss: 396.1532, MinusLogProbMetric: 396.1532, val_loss: 398.1130, val_MinusLogProbMetric: 398.1130

Epoch 243: val_loss did not improve from 397.56763
196/196 - 10s - loss: 396.1532 - MinusLogProbMetric: 396.1532 - val_loss: 398.1130 - val_MinusLogProbMetric: 398.1130 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 244/1000
2023-10-03 07:34:53.437 
Epoch 244/1000 
	 loss: 396.3644, MinusLogProbMetric: 396.3644, val_loss: 397.7302, val_MinusLogProbMetric: 397.7302

Epoch 244: val_loss did not improve from 397.56763
196/196 - 10s - loss: 396.3644 - MinusLogProbMetric: 396.3644 - val_loss: 397.7302 - val_MinusLogProbMetric: 397.7302 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 245/1000
2023-10-03 07:35:03.706 
Epoch 245/1000 
	 loss: 396.0372, MinusLogProbMetric: 396.0372, val_loss: 398.8119, val_MinusLogProbMetric: 398.8119

Epoch 245: val_loss did not improve from 397.56763
196/196 - 10s - loss: 396.0372 - MinusLogProbMetric: 396.0372 - val_loss: 398.8119 - val_MinusLogProbMetric: 398.8119 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 246/1000
2023-10-03 07:35:14.124 
Epoch 246/1000 
	 loss: 395.9852, MinusLogProbMetric: 395.9852, val_loss: 399.8030, val_MinusLogProbMetric: 399.8030

Epoch 246: val_loss did not improve from 397.56763
196/196 - 10s - loss: 395.9852 - MinusLogProbMetric: 395.9852 - val_loss: 399.8030 - val_MinusLogProbMetric: 399.8030 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 247/1000
2023-10-03 07:35:24.515 
Epoch 247/1000 
	 loss: 395.9678, MinusLogProbMetric: 395.9678, val_loss: 399.1917, val_MinusLogProbMetric: 399.1917

Epoch 247: val_loss did not improve from 397.56763
196/196 - 10s - loss: 395.9678 - MinusLogProbMetric: 395.9678 - val_loss: 399.1917 - val_MinusLogProbMetric: 399.1917 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 248/1000
2023-10-03 07:35:34.913 
Epoch 248/1000 
	 loss: 395.8180, MinusLogProbMetric: 395.8180, val_loss: 396.9754, val_MinusLogProbMetric: 396.9754

Epoch 248: val_loss improved from 397.56763 to 396.97537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 395.8180 - MinusLogProbMetric: 395.8180 - val_loss: 396.9754 - val_MinusLogProbMetric: 396.9754 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 249/1000
2023-10-03 07:35:45.443 
Epoch 249/1000 
	 loss: 395.9083, MinusLogProbMetric: 395.9083, val_loss: 398.0648, val_MinusLogProbMetric: 398.0648

Epoch 249: val_loss did not improve from 396.97537
196/196 - 10s - loss: 395.9083 - MinusLogProbMetric: 395.9083 - val_loss: 398.0648 - val_MinusLogProbMetric: 398.0648 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 250/1000
2023-10-03 07:35:55.995 
Epoch 250/1000 
	 loss: 395.5130, MinusLogProbMetric: 395.5130, val_loss: 398.0080, val_MinusLogProbMetric: 398.0080

Epoch 250: val_loss did not improve from 396.97537
196/196 - 11s - loss: 395.5130 - MinusLogProbMetric: 395.5130 - val_loss: 398.0080 - val_MinusLogProbMetric: 398.0080 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 251/1000
2023-10-03 07:36:06.368 
Epoch 251/1000 
	 loss: 395.7351, MinusLogProbMetric: 395.7351, val_loss: 396.8938, val_MinusLogProbMetric: 396.8938

Epoch 251: val_loss improved from 396.97537 to 396.89380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 395.7351 - MinusLogProbMetric: 395.7351 - val_loss: 396.8938 - val_MinusLogProbMetric: 396.8938 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 252/1000
2023-10-03 07:36:17.352 
Epoch 252/1000 
	 loss: 395.7316, MinusLogProbMetric: 395.7316, val_loss: 397.0313, val_MinusLogProbMetric: 397.0313

Epoch 252: val_loss did not improve from 396.89380
196/196 - 10s - loss: 395.7316 - MinusLogProbMetric: 395.7316 - val_loss: 397.0313 - val_MinusLogProbMetric: 397.0313 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 253/1000
2023-10-03 07:36:27.564 
Epoch 253/1000 
	 loss: 395.5122, MinusLogProbMetric: 395.5122, val_loss: 399.2428, val_MinusLogProbMetric: 399.2428

Epoch 253: val_loss did not improve from 396.89380
196/196 - 10s - loss: 395.5122 - MinusLogProbMetric: 395.5122 - val_loss: 399.2428 - val_MinusLogProbMetric: 399.2428 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 254/1000
2023-10-03 07:36:37.883 
Epoch 254/1000 
	 loss: 395.7017, MinusLogProbMetric: 395.7017, val_loss: 397.9504, val_MinusLogProbMetric: 397.9504

Epoch 254: val_loss did not improve from 396.89380
196/196 - 10s - loss: 395.7017 - MinusLogProbMetric: 395.7017 - val_loss: 397.9504 - val_MinusLogProbMetric: 397.9504 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 255/1000
2023-10-03 07:36:48.083 
Epoch 255/1000 
	 loss: 395.3949, MinusLogProbMetric: 395.3949, val_loss: 398.2867, val_MinusLogProbMetric: 398.2867

Epoch 255: val_loss did not improve from 396.89380
196/196 - 10s - loss: 395.3949 - MinusLogProbMetric: 395.3949 - val_loss: 398.2867 - val_MinusLogProbMetric: 398.2867 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 256/1000
2023-10-03 07:36:58.525 
Epoch 256/1000 
	 loss: 395.3226, MinusLogProbMetric: 395.3226, val_loss: 400.1787, val_MinusLogProbMetric: 400.1787

Epoch 256: val_loss did not improve from 396.89380
196/196 - 10s - loss: 395.3226 - MinusLogProbMetric: 395.3226 - val_loss: 400.1787 - val_MinusLogProbMetric: 400.1787 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 257/1000
2023-10-03 07:37:08.857 
Epoch 257/1000 
	 loss: 395.5075, MinusLogProbMetric: 395.5075, val_loss: 397.6439, val_MinusLogProbMetric: 397.6439

Epoch 257: val_loss did not improve from 396.89380
196/196 - 10s - loss: 395.5075 - MinusLogProbMetric: 395.5075 - val_loss: 397.6439 - val_MinusLogProbMetric: 397.6439 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 258/1000
2023-10-03 07:37:19.203 
Epoch 258/1000 
	 loss: 395.4294, MinusLogProbMetric: 395.4294, val_loss: 396.7983, val_MinusLogProbMetric: 396.7983

Epoch 258: val_loss improved from 396.89380 to 396.79831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 395.4294 - MinusLogProbMetric: 395.4294 - val_loss: 396.7983 - val_MinusLogProbMetric: 396.7983 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 259/1000
2023-10-03 07:37:29.910 
Epoch 259/1000 
	 loss: 395.1676, MinusLogProbMetric: 395.1676, val_loss: 398.3497, val_MinusLogProbMetric: 398.3497

Epoch 259: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.1676 - MinusLogProbMetric: 395.1676 - val_loss: 398.3497 - val_MinusLogProbMetric: 398.3497 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 260/1000
2023-10-03 07:37:40.145 
Epoch 260/1000 
	 loss: 395.2531, MinusLogProbMetric: 395.2531, val_loss: 399.4762, val_MinusLogProbMetric: 399.4762

Epoch 260: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.2531 - MinusLogProbMetric: 395.2531 - val_loss: 399.4762 - val_MinusLogProbMetric: 399.4762 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 261/1000
2023-10-03 07:37:50.452 
Epoch 261/1000 
	 loss: 395.1595, MinusLogProbMetric: 395.1595, val_loss: 397.2015, val_MinusLogProbMetric: 397.2015

Epoch 261: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.1595 - MinusLogProbMetric: 395.1595 - val_loss: 397.2015 - val_MinusLogProbMetric: 397.2015 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 262/1000
2023-10-03 07:38:00.733 
Epoch 262/1000 
	 loss: 395.1969, MinusLogProbMetric: 395.1969, val_loss: 397.6764, val_MinusLogProbMetric: 397.6764

Epoch 262: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.1969 - MinusLogProbMetric: 395.1969 - val_loss: 397.6764 - val_MinusLogProbMetric: 397.6764 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 263/1000
2023-10-03 07:38:11.295 
Epoch 263/1000 
	 loss: 395.0247, MinusLogProbMetric: 395.0247, val_loss: 396.9779, val_MinusLogProbMetric: 396.9779

Epoch 263: val_loss did not improve from 396.79831
196/196 - 11s - loss: 395.0247 - MinusLogProbMetric: 395.0247 - val_loss: 396.9779 - val_MinusLogProbMetric: 396.9779 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 264/1000
2023-10-03 07:38:21.729 
Epoch 264/1000 
	 loss: 395.0373, MinusLogProbMetric: 395.0373, val_loss: 397.1701, val_MinusLogProbMetric: 397.1701

Epoch 264: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.0373 - MinusLogProbMetric: 395.0373 - val_loss: 397.1701 - val_MinusLogProbMetric: 397.1701 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 265/1000
2023-10-03 07:38:32.092 
Epoch 265/1000 
	 loss: 395.2870, MinusLogProbMetric: 395.2870, val_loss: 397.3082, val_MinusLogProbMetric: 397.3082

Epoch 265: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.2870 - MinusLogProbMetric: 395.2870 - val_loss: 397.3082 - val_MinusLogProbMetric: 397.3082 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 266/1000
2023-10-03 07:38:42.347 
Epoch 266/1000 
	 loss: 394.9745, MinusLogProbMetric: 394.9745, val_loss: 399.0536, val_MinusLogProbMetric: 399.0536

Epoch 266: val_loss did not improve from 396.79831
196/196 - 10s - loss: 394.9745 - MinusLogProbMetric: 394.9745 - val_loss: 399.0536 - val_MinusLogProbMetric: 399.0536 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 267/1000
2023-10-03 07:38:52.473 
Epoch 267/1000 
	 loss: 395.2870, MinusLogProbMetric: 395.2870, val_loss: 397.3806, val_MinusLogProbMetric: 397.3806

Epoch 267: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.2870 - MinusLogProbMetric: 395.2870 - val_loss: 397.3806 - val_MinusLogProbMetric: 397.3806 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 268/1000
2023-10-03 07:39:02.765 
Epoch 268/1000 
	 loss: 394.7046, MinusLogProbMetric: 394.7046, val_loss: 396.8810, val_MinusLogProbMetric: 396.8810

Epoch 268: val_loss did not improve from 396.79831
196/196 - 10s - loss: 394.7046 - MinusLogProbMetric: 394.7046 - val_loss: 396.8810 - val_MinusLogProbMetric: 396.8810 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 269/1000
2023-10-03 07:39:13.133 
Epoch 269/1000 
	 loss: 395.2456, MinusLogProbMetric: 395.2456, val_loss: 397.5100, val_MinusLogProbMetric: 397.5100

Epoch 269: val_loss did not improve from 396.79831
196/196 - 10s - loss: 395.2456 - MinusLogProbMetric: 395.2456 - val_loss: 397.5100 - val_MinusLogProbMetric: 397.5100 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 270/1000
2023-10-03 07:39:23.353 
Epoch 270/1000 
	 loss: 394.5022, MinusLogProbMetric: 394.5022, val_loss: 397.5867, val_MinusLogProbMetric: 397.5867

Epoch 270: val_loss did not improve from 396.79831
196/196 - 10s - loss: 394.5022 - MinusLogProbMetric: 394.5022 - val_loss: 397.5867 - val_MinusLogProbMetric: 397.5867 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 271/1000
2023-10-03 07:39:33.770 
Epoch 271/1000 
	 loss: 394.8283, MinusLogProbMetric: 394.8283, val_loss: 396.4454, val_MinusLogProbMetric: 396.4454

Epoch 271: val_loss improved from 396.79831 to 396.44540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 394.8283 - MinusLogProbMetric: 394.8283 - val_loss: 396.4454 - val_MinusLogProbMetric: 396.4454 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 272/1000
2023-10-03 07:39:44.685 
Epoch 272/1000 
	 loss: 394.6892, MinusLogProbMetric: 394.6892, val_loss: 397.3648, val_MinusLogProbMetric: 397.3648

Epoch 272: val_loss did not improve from 396.44540
196/196 - 10s - loss: 394.6892 - MinusLogProbMetric: 394.6892 - val_loss: 397.3648 - val_MinusLogProbMetric: 397.3648 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 273/1000
2023-10-03 07:39:54.930 
Epoch 273/1000 
	 loss: 394.8494, MinusLogProbMetric: 394.8494, val_loss: 396.7778, val_MinusLogProbMetric: 396.7778

Epoch 273: val_loss did not improve from 396.44540
196/196 - 10s - loss: 394.8494 - MinusLogProbMetric: 394.8494 - val_loss: 396.7778 - val_MinusLogProbMetric: 396.7778 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 274/1000
2023-10-03 07:40:05.308 
Epoch 274/1000 
	 loss: 394.4672, MinusLogProbMetric: 394.4672, val_loss: 396.7124, val_MinusLogProbMetric: 396.7124

Epoch 274: val_loss did not improve from 396.44540
196/196 - 10s - loss: 394.4672 - MinusLogProbMetric: 394.4672 - val_loss: 396.7124 - val_MinusLogProbMetric: 396.7124 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 275/1000
2023-10-03 07:40:15.751 
Epoch 275/1000 
	 loss: 394.8479, MinusLogProbMetric: 394.8479, val_loss: 397.7421, val_MinusLogProbMetric: 397.7421

Epoch 275: val_loss did not improve from 396.44540
196/196 - 10s - loss: 394.8479 - MinusLogProbMetric: 394.8479 - val_loss: 397.7421 - val_MinusLogProbMetric: 397.7421 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 276/1000
2023-10-03 07:40:25.998 
Epoch 276/1000 
	 loss: 394.6389, MinusLogProbMetric: 394.6389, val_loss: 395.7926, val_MinusLogProbMetric: 395.7926

Epoch 276: val_loss improved from 396.44540 to 395.79260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 394.6389 - MinusLogProbMetric: 394.6389 - val_loss: 395.7926 - val_MinusLogProbMetric: 395.7926 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 277/1000
2023-10-03 07:40:36.610 
Epoch 277/1000 
	 loss: 394.7652, MinusLogProbMetric: 394.7652, val_loss: 398.5431, val_MinusLogProbMetric: 398.5431

Epoch 277: val_loss did not improve from 395.79260
196/196 - 10s - loss: 394.7652 - MinusLogProbMetric: 394.7652 - val_loss: 398.5431 - val_MinusLogProbMetric: 398.5431 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 278/1000
2023-10-03 07:40:46.858 
Epoch 278/1000 
	 loss: 394.5967, MinusLogProbMetric: 394.5967, val_loss: 396.9111, val_MinusLogProbMetric: 396.9111

Epoch 278: val_loss did not improve from 395.79260
196/196 - 10s - loss: 394.5967 - MinusLogProbMetric: 394.5967 - val_loss: 396.9111 - val_MinusLogProbMetric: 396.9111 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 279/1000
2023-10-03 07:40:57.470 
Epoch 279/1000 
	 loss: 394.5396, MinusLogProbMetric: 394.5396, val_loss: 395.7325, val_MinusLogProbMetric: 395.7325

Epoch 279: val_loss improved from 395.79260 to 395.73251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 394.5396 - MinusLogProbMetric: 394.5396 - val_loss: 395.7325 - val_MinusLogProbMetric: 395.7325 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 280/1000
2023-10-03 07:41:08.315 
Epoch 280/1000 
	 loss: 394.5520, MinusLogProbMetric: 394.5520, val_loss: 398.2386, val_MinusLogProbMetric: 398.2386

Epoch 280: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.5520 - MinusLogProbMetric: 394.5520 - val_loss: 398.2386 - val_MinusLogProbMetric: 398.2386 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 281/1000
2023-10-03 07:41:18.572 
Epoch 281/1000 
	 loss: 394.4845, MinusLogProbMetric: 394.4845, val_loss: 396.5460, val_MinusLogProbMetric: 396.5460

Epoch 281: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.4845 - MinusLogProbMetric: 394.4845 - val_loss: 396.5460 - val_MinusLogProbMetric: 396.5460 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 282/1000
2023-10-03 07:41:28.964 
Epoch 282/1000 
	 loss: 394.6539, MinusLogProbMetric: 394.6539, val_loss: 396.1908, val_MinusLogProbMetric: 396.1908

Epoch 282: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.6539 - MinusLogProbMetric: 394.6539 - val_loss: 396.1908 - val_MinusLogProbMetric: 396.1908 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 283/1000
2023-10-03 07:41:39.202 
Epoch 283/1000 
	 loss: 394.3863, MinusLogProbMetric: 394.3863, val_loss: 397.3151, val_MinusLogProbMetric: 397.3151

Epoch 283: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.3863 - MinusLogProbMetric: 394.3863 - val_loss: 397.3151 - val_MinusLogProbMetric: 397.3151 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 284/1000
2023-10-03 07:41:49.462 
Epoch 284/1000 
	 loss: 394.3035, MinusLogProbMetric: 394.3035, val_loss: 399.2880, val_MinusLogProbMetric: 399.2880

Epoch 284: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.3035 - MinusLogProbMetric: 394.3035 - val_loss: 399.2880 - val_MinusLogProbMetric: 399.2880 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 285/1000
2023-10-03 07:41:59.933 
Epoch 285/1000 
	 loss: 394.3755, MinusLogProbMetric: 394.3755, val_loss: 420.2115, val_MinusLogProbMetric: 420.2115

Epoch 285: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.3755 - MinusLogProbMetric: 394.3755 - val_loss: 420.2115 - val_MinusLogProbMetric: 420.2115 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 286/1000
2023-10-03 07:42:10.268 
Epoch 286/1000 
	 loss: 394.6674, MinusLogProbMetric: 394.6674, val_loss: 396.2064, val_MinusLogProbMetric: 396.2064

Epoch 286: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.6674 - MinusLogProbMetric: 394.6674 - val_loss: 396.2064 - val_MinusLogProbMetric: 396.2064 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 287/1000
2023-10-03 07:42:20.472 
Epoch 287/1000 
	 loss: 394.3036, MinusLogProbMetric: 394.3036, val_loss: 397.5847, val_MinusLogProbMetric: 397.5847

Epoch 287: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.3036 - MinusLogProbMetric: 394.3036 - val_loss: 397.5847 - val_MinusLogProbMetric: 397.5847 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 288/1000
2023-10-03 07:42:30.832 
Epoch 288/1000 
	 loss: 394.5628, MinusLogProbMetric: 394.5628, val_loss: 396.6080, val_MinusLogProbMetric: 396.6080

Epoch 288: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.5628 - MinusLogProbMetric: 394.5628 - val_loss: 396.6080 - val_MinusLogProbMetric: 396.6080 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 289/1000
2023-10-03 07:42:41.294 
Epoch 289/1000 
	 loss: 394.3584, MinusLogProbMetric: 394.3584, val_loss: 397.6743, val_MinusLogProbMetric: 397.6743

Epoch 289: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.3584 - MinusLogProbMetric: 394.3584 - val_loss: 397.6743 - val_MinusLogProbMetric: 397.6743 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 290/1000
2023-10-03 07:42:51.556 
Epoch 290/1000 
	 loss: 394.0028, MinusLogProbMetric: 394.0028, val_loss: 396.5163, val_MinusLogProbMetric: 396.5163

Epoch 290: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.0028 - MinusLogProbMetric: 394.0028 - val_loss: 396.5163 - val_MinusLogProbMetric: 396.5163 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 291/1000
2023-10-03 07:43:01.927 
Epoch 291/1000 
	 loss: 394.2189, MinusLogProbMetric: 394.2189, val_loss: 398.3458, val_MinusLogProbMetric: 398.3458

Epoch 291: val_loss did not improve from 395.73251
196/196 - 10s - loss: 394.2189 - MinusLogProbMetric: 394.2189 - val_loss: 398.3458 - val_MinusLogProbMetric: 398.3458 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 292/1000
2023-10-03 07:43:12.312 
Epoch 292/1000 
	 loss: 394.1310, MinusLogProbMetric: 394.1310, val_loss: 395.5037, val_MinusLogProbMetric: 395.5037

Epoch 292: val_loss improved from 395.73251 to 395.50366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 394.1310 - MinusLogProbMetric: 394.1310 - val_loss: 395.5037 - val_MinusLogProbMetric: 395.5037 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 293/1000
2023-10-03 07:43:21.891 
Epoch 293/1000 
	 loss: 394.2116, MinusLogProbMetric: 394.2116, val_loss: 397.6103, val_MinusLogProbMetric: 397.6103

Epoch 293: val_loss did not improve from 395.50366
196/196 - 9s - loss: 394.2116 - MinusLogProbMetric: 394.2116 - val_loss: 397.6103 - val_MinusLogProbMetric: 397.6103 - lr: 3.3333e-04 - 9s/epoch - 47ms/step
Epoch 294/1000
2023-10-03 07:43:31.423 
Epoch 294/1000 
	 loss: 394.1201, MinusLogProbMetric: 394.1201, val_loss: 403.5139, val_MinusLogProbMetric: 403.5139

Epoch 294: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.1201 - MinusLogProbMetric: 394.1201 - val_loss: 403.5139 - val_MinusLogProbMetric: 403.5139 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 295/1000
2023-10-03 07:43:41.370 
Epoch 295/1000 
	 loss: 394.3214, MinusLogProbMetric: 394.3214, val_loss: 397.0757, val_MinusLogProbMetric: 397.0757

Epoch 295: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.3214 - MinusLogProbMetric: 394.3214 - val_loss: 397.0757 - val_MinusLogProbMetric: 397.0757 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 296/1000
2023-10-03 07:43:51.677 
Epoch 296/1000 
	 loss: 394.0647, MinusLogProbMetric: 394.0647, val_loss: 396.6129, val_MinusLogProbMetric: 396.6129

Epoch 296: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.0647 - MinusLogProbMetric: 394.0647 - val_loss: 396.6129 - val_MinusLogProbMetric: 396.6129 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 297/1000
2023-10-03 07:44:01.961 
Epoch 297/1000 
	 loss: 393.8436, MinusLogProbMetric: 393.8436, val_loss: 396.3851, val_MinusLogProbMetric: 396.3851

Epoch 297: val_loss did not improve from 395.50366
196/196 - 10s - loss: 393.8436 - MinusLogProbMetric: 393.8436 - val_loss: 396.3851 - val_MinusLogProbMetric: 396.3851 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 298/1000
2023-10-03 07:44:12.366 
Epoch 298/1000 
	 loss: 602.6364, MinusLogProbMetric: 602.6364, val_loss: 641.6407, val_MinusLogProbMetric: 641.6407

Epoch 298: val_loss did not improve from 395.50366
196/196 - 10s - loss: 602.6364 - MinusLogProbMetric: 602.6364 - val_loss: 641.6407 - val_MinusLogProbMetric: 641.6407 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 299/1000
2023-10-03 07:44:22.719 
Epoch 299/1000 
	 loss: 483.8391, MinusLogProbMetric: 483.8391, val_loss: 430.4014, val_MinusLogProbMetric: 430.4014

Epoch 299: val_loss did not improve from 395.50366
196/196 - 10s - loss: 483.8391 - MinusLogProbMetric: 483.8391 - val_loss: 430.4014 - val_MinusLogProbMetric: 430.4014 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 300/1000
2023-10-03 07:44:33.170 
Epoch 300/1000 
	 loss: 419.9740, MinusLogProbMetric: 419.9740, val_loss: 415.5048, val_MinusLogProbMetric: 415.5048

Epoch 300: val_loss did not improve from 395.50366
196/196 - 10s - loss: 419.9740 - MinusLogProbMetric: 419.9740 - val_loss: 415.5048 - val_MinusLogProbMetric: 415.5048 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 301/1000
2023-10-03 07:44:43.445 
Epoch 301/1000 
	 loss: 411.1025, MinusLogProbMetric: 411.1025, val_loss: 410.4101, val_MinusLogProbMetric: 410.4101

Epoch 301: val_loss did not improve from 395.50366
196/196 - 10s - loss: 411.1025 - MinusLogProbMetric: 411.1025 - val_loss: 410.4101 - val_MinusLogProbMetric: 410.4101 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 302/1000
2023-10-03 07:44:53.657 
Epoch 302/1000 
	 loss: 407.3666, MinusLogProbMetric: 407.3666, val_loss: 407.7030, val_MinusLogProbMetric: 407.7030

Epoch 302: val_loss did not improve from 395.50366
196/196 - 10s - loss: 407.3666 - MinusLogProbMetric: 407.3666 - val_loss: 407.7030 - val_MinusLogProbMetric: 407.7030 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 303/1000
2023-10-03 07:45:03.909 
Epoch 303/1000 
	 loss: 405.0414, MinusLogProbMetric: 405.0414, val_loss: 406.2064, val_MinusLogProbMetric: 406.2064

Epoch 303: val_loss did not improve from 395.50366
196/196 - 10s - loss: 405.0414 - MinusLogProbMetric: 405.0414 - val_loss: 406.2064 - val_MinusLogProbMetric: 406.2064 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 304/1000
2023-10-03 07:45:14.361 
Epoch 304/1000 
	 loss: 403.3596, MinusLogProbMetric: 403.3596, val_loss: 404.6426, val_MinusLogProbMetric: 404.6426

Epoch 304: val_loss did not improve from 395.50366
196/196 - 10s - loss: 403.3596 - MinusLogProbMetric: 403.3596 - val_loss: 404.6426 - val_MinusLogProbMetric: 404.6426 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 305/1000
2023-10-03 07:45:24.761 
Epoch 305/1000 
	 loss: 402.1992, MinusLogProbMetric: 402.1992, val_loss: 403.5147, val_MinusLogProbMetric: 403.5147

Epoch 305: val_loss did not improve from 395.50366
196/196 - 10s - loss: 402.1992 - MinusLogProbMetric: 402.1992 - val_loss: 403.5147 - val_MinusLogProbMetric: 403.5147 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 306/1000
2023-10-03 07:45:35.021 
Epoch 306/1000 
	 loss: 401.1322, MinusLogProbMetric: 401.1322, val_loss: 403.4447, val_MinusLogProbMetric: 403.4447

Epoch 306: val_loss did not improve from 395.50366
196/196 - 10s - loss: 401.1322 - MinusLogProbMetric: 401.1322 - val_loss: 403.4447 - val_MinusLogProbMetric: 403.4447 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 307/1000
2023-10-03 07:45:45.480 
Epoch 307/1000 
	 loss: 400.5075, MinusLogProbMetric: 400.5075, val_loss: 401.5220, val_MinusLogProbMetric: 401.5220

Epoch 307: val_loss did not improve from 395.50366
196/196 - 10s - loss: 400.5075 - MinusLogProbMetric: 400.5075 - val_loss: 401.5220 - val_MinusLogProbMetric: 401.5220 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 308/1000
2023-10-03 07:45:55.887 
Epoch 308/1000 
	 loss: 399.8719, MinusLogProbMetric: 399.8719, val_loss: 401.7106, val_MinusLogProbMetric: 401.7106

Epoch 308: val_loss did not improve from 395.50366
196/196 - 10s - loss: 399.8719 - MinusLogProbMetric: 399.8719 - val_loss: 401.7106 - val_MinusLogProbMetric: 401.7106 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 309/1000
2023-10-03 07:46:06.072 
Epoch 309/1000 
	 loss: 399.1974, MinusLogProbMetric: 399.1974, val_loss: 400.7136, val_MinusLogProbMetric: 400.7136

Epoch 309: val_loss did not improve from 395.50366
196/196 - 10s - loss: 399.1974 - MinusLogProbMetric: 399.1974 - val_loss: 400.7136 - val_MinusLogProbMetric: 400.7136 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 310/1000
2023-10-03 07:46:16.238 
Epoch 310/1000 
	 loss: 398.8246, MinusLogProbMetric: 398.8246, val_loss: 401.1488, val_MinusLogProbMetric: 401.1488

Epoch 310: val_loss did not improve from 395.50366
196/196 - 10s - loss: 398.8246 - MinusLogProbMetric: 398.8246 - val_loss: 401.1488 - val_MinusLogProbMetric: 401.1488 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 311/1000
2023-10-03 07:46:26.755 
Epoch 311/1000 
	 loss: 398.5429, MinusLogProbMetric: 398.5429, val_loss: 400.1597, val_MinusLogProbMetric: 400.1597

Epoch 311: val_loss did not improve from 395.50366
196/196 - 11s - loss: 398.5429 - MinusLogProbMetric: 398.5429 - val_loss: 400.1597 - val_MinusLogProbMetric: 400.1597 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 312/1000
2023-10-03 07:46:37.157 
Epoch 312/1000 
	 loss: 398.1505, MinusLogProbMetric: 398.1505, val_loss: 400.0574, val_MinusLogProbMetric: 400.0574

Epoch 312: val_loss did not improve from 395.50366
196/196 - 10s - loss: 398.1505 - MinusLogProbMetric: 398.1505 - val_loss: 400.0574 - val_MinusLogProbMetric: 400.0574 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 313/1000
2023-10-03 07:46:47.480 
Epoch 313/1000 
	 loss: 397.6826, MinusLogProbMetric: 397.6826, val_loss: 399.1938, val_MinusLogProbMetric: 399.1938

Epoch 313: val_loss did not improve from 395.50366
196/196 - 10s - loss: 397.6826 - MinusLogProbMetric: 397.6826 - val_loss: 399.1938 - val_MinusLogProbMetric: 399.1938 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 314/1000
2023-10-03 07:46:57.620 
Epoch 314/1000 
	 loss: 397.4671, MinusLogProbMetric: 397.4671, val_loss: 399.9842, val_MinusLogProbMetric: 399.9842

Epoch 314: val_loss did not improve from 395.50366
196/196 - 10s - loss: 397.4671 - MinusLogProbMetric: 397.4671 - val_loss: 399.9842 - val_MinusLogProbMetric: 399.9842 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 315/1000
2023-10-03 07:47:08.051 
Epoch 315/1000 
	 loss: 397.5806, MinusLogProbMetric: 397.5806, val_loss: 399.7187, val_MinusLogProbMetric: 399.7187

Epoch 315: val_loss did not improve from 395.50366
196/196 - 10s - loss: 397.5806 - MinusLogProbMetric: 397.5806 - val_loss: 399.7187 - val_MinusLogProbMetric: 399.7187 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 316/1000
2023-10-03 07:47:18.542 
Epoch 316/1000 
	 loss: 396.9614, MinusLogProbMetric: 396.9614, val_loss: 398.9180, val_MinusLogProbMetric: 398.9180

Epoch 316: val_loss did not improve from 395.50366
196/196 - 10s - loss: 396.9614 - MinusLogProbMetric: 396.9614 - val_loss: 398.9180 - val_MinusLogProbMetric: 398.9180 - lr: 3.3333e-04 - 10s/epoch - 54ms/step
Epoch 317/1000
2023-10-03 07:47:28.988 
Epoch 317/1000 
	 loss: 396.8889, MinusLogProbMetric: 396.8889, val_loss: 398.9450, val_MinusLogProbMetric: 398.9450

Epoch 317: val_loss did not improve from 395.50366
196/196 - 10s - loss: 396.8889 - MinusLogProbMetric: 396.8889 - val_loss: 398.9450 - val_MinusLogProbMetric: 398.9450 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 318/1000
2023-10-03 07:47:39.360 
Epoch 318/1000 
	 loss: 396.6831, MinusLogProbMetric: 396.6831, val_loss: 399.0824, val_MinusLogProbMetric: 399.0824

Epoch 318: val_loss did not improve from 395.50366
196/196 - 10s - loss: 396.6831 - MinusLogProbMetric: 396.6831 - val_loss: 399.0824 - val_MinusLogProbMetric: 399.0824 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 319/1000
2023-10-03 07:47:49.679 
Epoch 319/1000 
	 loss: 396.4809, MinusLogProbMetric: 396.4809, val_loss: 398.4036, val_MinusLogProbMetric: 398.4036

Epoch 319: val_loss did not improve from 395.50366
196/196 - 10s - loss: 396.4809 - MinusLogProbMetric: 396.4809 - val_loss: 398.4036 - val_MinusLogProbMetric: 398.4036 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 320/1000
2023-10-03 07:48:00.073 
Epoch 320/1000 
	 loss: 396.2903, MinusLogProbMetric: 396.2903, val_loss: 399.8261, val_MinusLogProbMetric: 399.8261

Epoch 320: val_loss did not improve from 395.50366
196/196 - 10s - loss: 396.2903 - MinusLogProbMetric: 396.2903 - val_loss: 399.8261 - val_MinusLogProbMetric: 399.8261 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 321/1000
2023-10-03 07:48:10.535 
Epoch 321/1000 
	 loss: 396.2075, MinusLogProbMetric: 396.2075, val_loss: 398.7221, val_MinusLogProbMetric: 398.7221

Epoch 321: val_loss did not improve from 395.50366
196/196 - 10s - loss: 396.2075 - MinusLogProbMetric: 396.2075 - val_loss: 398.7221 - val_MinusLogProbMetric: 398.7221 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 322/1000
2023-10-03 07:48:20.923 
Epoch 322/1000 
	 loss: 395.8558, MinusLogProbMetric: 395.8558, val_loss: 398.4110, val_MinusLogProbMetric: 398.4110

Epoch 322: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.8558 - MinusLogProbMetric: 395.8558 - val_loss: 398.4110 - val_MinusLogProbMetric: 398.4110 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 323/1000
2023-10-03 07:48:31.299 
Epoch 323/1000 
	 loss: 395.9308, MinusLogProbMetric: 395.9308, val_loss: 398.5080, val_MinusLogProbMetric: 398.5080

Epoch 323: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.9308 - MinusLogProbMetric: 395.9308 - val_loss: 398.5080 - val_MinusLogProbMetric: 398.5080 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 324/1000
2023-10-03 07:48:41.857 
Epoch 324/1000 
	 loss: 395.7867, MinusLogProbMetric: 395.7867, val_loss: 398.9605, val_MinusLogProbMetric: 398.9605

Epoch 324: val_loss did not improve from 395.50366
196/196 - 11s - loss: 395.7867 - MinusLogProbMetric: 395.7867 - val_loss: 398.9605 - val_MinusLogProbMetric: 398.9605 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 325/1000
2023-10-03 07:48:52.228 
Epoch 325/1000 
	 loss: 395.8128, MinusLogProbMetric: 395.8128, val_loss: 397.7529, val_MinusLogProbMetric: 397.7529

Epoch 325: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.8128 - MinusLogProbMetric: 395.8128 - val_loss: 397.7529 - val_MinusLogProbMetric: 397.7529 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 326/1000
2023-10-03 07:49:02.577 
Epoch 326/1000 
	 loss: 395.8170, MinusLogProbMetric: 395.8170, val_loss: 397.0971, val_MinusLogProbMetric: 397.0971

Epoch 326: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.8170 - MinusLogProbMetric: 395.8170 - val_loss: 397.0971 - val_MinusLogProbMetric: 397.0971 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 327/1000
2023-10-03 07:49:12.850 
Epoch 327/1000 
	 loss: 395.4542, MinusLogProbMetric: 395.4542, val_loss: 398.0989, val_MinusLogProbMetric: 398.0989

Epoch 327: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.4542 - MinusLogProbMetric: 395.4542 - val_loss: 398.0989 - val_MinusLogProbMetric: 398.0989 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 328/1000
2023-10-03 07:49:23.230 
Epoch 328/1000 
	 loss: 395.4403, MinusLogProbMetric: 395.4403, val_loss: 397.0676, val_MinusLogProbMetric: 397.0676

Epoch 328: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.4403 - MinusLogProbMetric: 395.4403 - val_loss: 397.0676 - val_MinusLogProbMetric: 397.0676 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 329/1000
2023-10-03 07:49:33.283 
Epoch 329/1000 
	 loss: 395.3097, MinusLogProbMetric: 395.3097, val_loss: 397.9446, val_MinusLogProbMetric: 397.9446

Epoch 329: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.3097 - MinusLogProbMetric: 395.3097 - val_loss: 397.9446 - val_MinusLogProbMetric: 397.9446 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 330/1000
2023-10-03 07:49:43.637 
Epoch 330/1000 
	 loss: 395.0379, MinusLogProbMetric: 395.0379, val_loss: 396.7130, val_MinusLogProbMetric: 396.7130

Epoch 330: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.0379 - MinusLogProbMetric: 395.0379 - val_loss: 396.7130 - val_MinusLogProbMetric: 396.7130 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 331/1000
2023-10-03 07:49:54.096 
Epoch 331/1000 
	 loss: 395.2497, MinusLogProbMetric: 395.2497, val_loss: 397.2599, val_MinusLogProbMetric: 397.2599

Epoch 331: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.2497 - MinusLogProbMetric: 395.2497 - val_loss: 397.2599 - val_MinusLogProbMetric: 397.2599 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 332/1000
2023-10-03 07:50:04.596 
Epoch 332/1000 
	 loss: 394.9294, MinusLogProbMetric: 394.9294, val_loss: 397.2279, val_MinusLogProbMetric: 397.2279

Epoch 332: val_loss did not improve from 395.50366
196/196 - 11s - loss: 394.9294 - MinusLogProbMetric: 394.9294 - val_loss: 397.2279 - val_MinusLogProbMetric: 397.2279 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 333/1000
2023-10-03 07:50:14.968 
Epoch 333/1000 
	 loss: 394.8908, MinusLogProbMetric: 394.8908, val_loss: 402.8749, val_MinusLogProbMetric: 402.8749

Epoch 333: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.8908 - MinusLogProbMetric: 394.8908 - val_loss: 402.8749 - val_MinusLogProbMetric: 402.8749 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 334/1000
2023-10-03 07:50:25.187 
Epoch 334/1000 
	 loss: 395.0431, MinusLogProbMetric: 395.0431, val_loss: 396.6160, val_MinusLogProbMetric: 396.6160

Epoch 334: val_loss did not improve from 395.50366
196/196 - 10s - loss: 395.0431 - MinusLogProbMetric: 395.0431 - val_loss: 396.6160 - val_MinusLogProbMetric: 396.6160 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 335/1000
2023-10-03 07:50:34.795 
Epoch 335/1000 
	 loss: 394.9360, MinusLogProbMetric: 394.9360, val_loss: 397.0977, val_MinusLogProbMetric: 397.0977

Epoch 335: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.9360 - MinusLogProbMetric: 394.9360 - val_loss: 397.0977 - val_MinusLogProbMetric: 397.0977 - lr: 3.3333e-04 - 10s/epoch - 49ms/step
Epoch 336/1000
2023-10-03 07:50:45.171 
Epoch 336/1000 
	 loss: 394.4303, MinusLogProbMetric: 394.4303, val_loss: 396.8608, val_MinusLogProbMetric: 396.8608

Epoch 336: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.4303 - MinusLogProbMetric: 394.4303 - val_loss: 396.8608 - val_MinusLogProbMetric: 396.8608 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 337/1000
2023-10-03 07:50:55.410 
Epoch 337/1000 
	 loss: 394.9386, MinusLogProbMetric: 394.9386, val_loss: 396.5213, val_MinusLogProbMetric: 396.5213

Epoch 337: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.9386 - MinusLogProbMetric: 394.9386 - val_loss: 396.5213 - val_MinusLogProbMetric: 396.5213 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 338/1000
2023-10-03 07:51:05.903 
Epoch 338/1000 
	 loss: 394.7802, MinusLogProbMetric: 394.7802, val_loss: 396.8150, val_MinusLogProbMetric: 396.8150

Epoch 338: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.7802 - MinusLogProbMetric: 394.7802 - val_loss: 396.8150 - val_MinusLogProbMetric: 396.8150 - lr: 3.3333e-04 - 10s/epoch - 54ms/step
Epoch 339/1000
2023-10-03 07:51:16.182 
Epoch 339/1000 
	 loss: 394.5719, MinusLogProbMetric: 394.5719, val_loss: 397.1589, val_MinusLogProbMetric: 397.1589

Epoch 339: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.5719 - MinusLogProbMetric: 394.5719 - val_loss: 397.1589 - val_MinusLogProbMetric: 397.1589 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 340/1000
2023-10-03 07:51:26.536 
Epoch 340/1000 
	 loss: 394.5932, MinusLogProbMetric: 394.5932, val_loss: 396.4024, val_MinusLogProbMetric: 396.4024

Epoch 340: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.5932 - MinusLogProbMetric: 394.5932 - val_loss: 396.4024 - val_MinusLogProbMetric: 396.4024 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 341/1000
2023-10-03 07:51:36.882 
Epoch 341/1000 
	 loss: 394.5789, MinusLogProbMetric: 394.5789, val_loss: 398.1009, val_MinusLogProbMetric: 398.1009

Epoch 341: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.5789 - MinusLogProbMetric: 394.5789 - val_loss: 398.1009 - val_MinusLogProbMetric: 398.1009 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 342/1000
2023-10-03 07:51:47.207 
Epoch 342/1000 
	 loss: 394.5090, MinusLogProbMetric: 394.5090, val_loss: 396.4238, val_MinusLogProbMetric: 396.4238

Epoch 342: val_loss did not improve from 395.50366
196/196 - 10s - loss: 394.5090 - MinusLogProbMetric: 394.5090 - val_loss: 396.4238 - val_MinusLogProbMetric: 396.4238 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 343/1000
2023-10-03 07:51:57.318 
Epoch 343/1000 
	 loss: 391.9859, MinusLogProbMetric: 391.9859, val_loss: 394.8630, val_MinusLogProbMetric: 394.8630

Epoch 343: val_loss improved from 395.50366 to 394.86304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 391.9859 - MinusLogProbMetric: 391.9859 - val_loss: 394.8630 - val_MinusLogProbMetric: 394.8630 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 344/1000
2023-10-03 07:52:07.916 
Epoch 344/1000 
	 loss: 391.8102, MinusLogProbMetric: 391.8102, val_loss: 394.2856, val_MinusLogProbMetric: 394.2856

Epoch 344: val_loss improved from 394.86304 to 394.28561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 391.8102 - MinusLogProbMetric: 391.8102 - val_loss: 394.2856 - val_MinusLogProbMetric: 394.2856 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 345/1000
2023-10-03 07:52:18.554 
Epoch 345/1000 
	 loss: 391.7885, MinusLogProbMetric: 391.7885, val_loss: 394.4034, val_MinusLogProbMetric: 394.4034

Epoch 345: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.7885 - MinusLogProbMetric: 391.7885 - val_loss: 394.4034 - val_MinusLogProbMetric: 394.4034 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 346/1000
2023-10-03 07:52:28.823 
Epoch 346/1000 
	 loss: 391.9015, MinusLogProbMetric: 391.9015, val_loss: 394.8348, val_MinusLogProbMetric: 394.8348

Epoch 346: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.9015 - MinusLogProbMetric: 391.9015 - val_loss: 394.8348 - val_MinusLogProbMetric: 394.8348 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 347/1000
2023-10-03 07:52:38.954 
Epoch 347/1000 
	 loss: 392.0119, MinusLogProbMetric: 392.0119, val_loss: 395.6112, val_MinusLogProbMetric: 395.6112

Epoch 347: val_loss did not improve from 394.28561
196/196 - 10s - loss: 392.0119 - MinusLogProbMetric: 392.0119 - val_loss: 395.6112 - val_MinusLogProbMetric: 395.6112 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 348/1000
2023-10-03 07:52:49.303 
Epoch 348/1000 
	 loss: 391.8088, MinusLogProbMetric: 391.8088, val_loss: 394.7980, val_MinusLogProbMetric: 394.7980

Epoch 348: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.8088 - MinusLogProbMetric: 391.8088 - val_loss: 394.7980 - val_MinusLogProbMetric: 394.7980 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 349/1000
2023-10-03 07:52:59.583 
Epoch 349/1000 
	 loss: 391.6828, MinusLogProbMetric: 391.6828, val_loss: 394.4140, val_MinusLogProbMetric: 394.4140

Epoch 349: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.6828 - MinusLogProbMetric: 391.6828 - val_loss: 394.4140 - val_MinusLogProbMetric: 394.4140 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 350/1000
2023-10-03 07:53:10.037 
Epoch 350/1000 
	 loss: 391.8374, MinusLogProbMetric: 391.8374, val_loss: 394.3679, val_MinusLogProbMetric: 394.3679

Epoch 350: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.8374 - MinusLogProbMetric: 391.8374 - val_loss: 394.3679 - val_MinusLogProbMetric: 394.3679 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 351/1000
2023-10-03 07:53:20.211 
Epoch 351/1000 
	 loss: 391.7688, MinusLogProbMetric: 391.7688, val_loss: 394.6614, val_MinusLogProbMetric: 394.6614

Epoch 351: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.7688 - MinusLogProbMetric: 391.7688 - val_loss: 394.6614 - val_MinusLogProbMetric: 394.6614 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 352/1000
2023-10-03 07:53:30.559 
Epoch 352/1000 
	 loss: 391.8077, MinusLogProbMetric: 391.8077, val_loss: 394.7618, val_MinusLogProbMetric: 394.7618

Epoch 352: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.8077 - MinusLogProbMetric: 391.8077 - val_loss: 394.7618 - val_MinusLogProbMetric: 394.7618 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 353/1000
2023-10-03 07:53:40.843 
Epoch 353/1000 
	 loss: 391.8226, MinusLogProbMetric: 391.8226, val_loss: 394.4438, val_MinusLogProbMetric: 394.4438

Epoch 353: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.8226 - MinusLogProbMetric: 391.8226 - val_loss: 394.4438 - val_MinusLogProbMetric: 394.4438 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 354/1000
2023-10-03 07:53:51.139 
Epoch 354/1000 
	 loss: 391.8542, MinusLogProbMetric: 391.8542, val_loss: 394.7567, val_MinusLogProbMetric: 394.7567

Epoch 354: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.8542 - MinusLogProbMetric: 391.8542 - val_loss: 394.7567 - val_MinusLogProbMetric: 394.7567 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 355/1000
2023-10-03 07:54:01.579 
Epoch 355/1000 
	 loss: 391.6940, MinusLogProbMetric: 391.6940, val_loss: 395.1880, val_MinusLogProbMetric: 395.1880

Epoch 355: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.6940 - MinusLogProbMetric: 391.6940 - val_loss: 395.1880 - val_MinusLogProbMetric: 395.1880 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 356/1000
2023-10-03 07:54:11.961 
Epoch 356/1000 
	 loss: 391.6416, MinusLogProbMetric: 391.6416, val_loss: 395.8276, val_MinusLogProbMetric: 395.8276

Epoch 356: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.6416 - MinusLogProbMetric: 391.6416 - val_loss: 395.8276 - val_MinusLogProbMetric: 395.8276 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 357/1000
2023-10-03 07:54:22.225 
Epoch 357/1000 
	 loss: 391.6702, MinusLogProbMetric: 391.6702, val_loss: 394.7909, val_MinusLogProbMetric: 394.7909

Epoch 357: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.6702 - MinusLogProbMetric: 391.6702 - val_loss: 394.7909 - val_MinusLogProbMetric: 394.7909 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 358/1000
2023-10-03 07:54:32.369 
Epoch 358/1000 
	 loss: 391.5375, MinusLogProbMetric: 391.5375, val_loss: 394.3240, val_MinusLogProbMetric: 394.3240

Epoch 358: val_loss did not improve from 394.28561
196/196 - 10s - loss: 391.5375 - MinusLogProbMetric: 391.5375 - val_loss: 394.3240 - val_MinusLogProbMetric: 394.3240 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 359/1000
2023-10-03 07:54:42.558 
Epoch 359/1000 
	 loss: 391.5894, MinusLogProbMetric: 391.5894, val_loss: 394.0658, val_MinusLogProbMetric: 394.0658

Epoch 359: val_loss improved from 394.28561 to 394.06583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 391.5894 - MinusLogProbMetric: 391.5894 - val_loss: 394.0658 - val_MinusLogProbMetric: 394.0658 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 360/1000
2023-10-03 07:54:53.117 
Epoch 360/1000 
	 loss: 391.8526, MinusLogProbMetric: 391.8526, val_loss: 394.7209, val_MinusLogProbMetric: 394.7209

Epoch 360: val_loss did not improve from 394.06583
196/196 - 10s - loss: 391.8526 - MinusLogProbMetric: 391.8526 - val_loss: 394.7209 - val_MinusLogProbMetric: 394.7209 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 361/1000
2023-10-03 07:55:03.408 
Epoch 361/1000 
	 loss: 391.6479, MinusLogProbMetric: 391.6479, val_loss: 395.7113, val_MinusLogProbMetric: 395.7113

Epoch 361: val_loss did not improve from 394.06583
196/196 - 10s - loss: 391.6479 - MinusLogProbMetric: 391.6479 - val_loss: 395.7113 - val_MinusLogProbMetric: 395.7113 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 362/1000
2023-10-03 07:55:13.582 
Epoch 362/1000 
	 loss: 391.5650, MinusLogProbMetric: 391.5650, val_loss: 393.8337, val_MinusLogProbMetric: 393.8337

Epoch 362: val_loss improved from 394.06583 to 393.83374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 391.5650 - MinusLogProbMetric: 391.5650 - val_loss: 393.8337 - val_MinusLogProbMetric: 393.8337 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 363/1000
2023-10-03 07:55:24.309 
Epoch 363/1000 
	 loss: 391.8692, MinusLogProbMetric: 391.8692, val_loss: 394.9956, val_MinusLogProbMetric: 394.9956

Epoch 363: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.8692 - MinusLogProbMetric: 391.8692 - val_loss: 394.9956 - val_MinusLogProbMetric: 394.9956 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 364/1000
2023-10-03 07:55:34.501 
Epoch 364/1000 
	 loss: 391.4726, MinusLogProbMetric: 391.4726, val_loss: 393.9619, val_MinusLogProbMetric: 393.9619

Epoch 364: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.4726 - MinusLogProbMetric: 391.4726 - val_loss: 393.9619 - val_MinusLogProbMetric: 393.9619 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 365/1000
2023-10-03 07:55:44.504 
Epoch 365/1000 
	 loss: 391.2750, MinusLogProbMetric: 391.2750, val_loss: 394.0063, val_MinusLogProbMetric: 394.0063

Epoch 365: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.2750 - MinusLogProbMetric: 391.2750 - val_loss: 394.0063 - val_MinusLogProbMetric: 394.0063 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 366/1000
2023-10-03 07:55:54.668 
Epoch 366/1000 
	 loss: 391.5349, MinusLogProbMetric: 391.5349, val_loss: 394.5040, val_MinusLogProbMetric: 394.5040

Epoch 366: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.5349 - MinusLogProbMetric: 391.5349 - val_loss: 394.5040 - val_MinusLogProbMetric: 394.5040 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 367/1000
2023-10-03 07:56:04.710 
Epoch 367/1000 
	 loss: 391.6337, MinusLogProbMetric: 391.6337, val_loss: 395.6629, val_MinusLogProbMetric: 395.6629

Epoch 367: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.6337 - MinusLogProbMetric: 391.6337 - val_loss: 395.6629 - val_MinusLogProbMetric: 395.6629 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 368/1000
2023-10-03 07:56:15.013 
Epoch 368/1000 
	 loss: 391.5521, MinusLogProbMetric: 391.5521, val_loss: 394.0600, val_MinusLogProbMetric: 394.0600

Epoch 368: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.5521 - MinusLogProbMetric: 391.5521 - val_loss: 394.0600 - val_MinusLogProbMetric: 394.0600 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 369/1000
2023-10-03 07:56:25.072 
Epoch 369/1000 
	 loss: 391.3049, MinusLogProbMetric: 391.3049, val_loss: 394.6043, val_MinusLogProbMetric: 394.6043

Epoch 369: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.3049 - MinusLogProbMetric: 391.3049 - val_loss: 394.6043 - val_MinusLogProbMetric: 394.6043 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 370/1000
2023-10-03 07:56:35.469 
Epoch 370/1000 
	 loss: 391.3473, MinusLogProbMetric: 391.3473, val_loss: 394.7343, val_MinusLogProbMetric: 394.7343

Epoch 370: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.3473 - MinusLogProbMetric: 391.3473 - val_loss: 394.7343 - val_MinusLogProbMetric: 394.7343 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 371/1000
2023-10-03 07:56:45.853 
Epoch 371/1000 
	 loss: 391.5640, MinusLogProbMetric: 391.5640, val_loss: 394.6923, val_MinusLogProbMetric: 394.6923

Epoch 371: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.5640 - MinusLogProbMetric: 391.5640 - val_loss: 394.6923 - val_MinusLogProbMetric: 394.6923 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 372/1000
2023-10-03 07:56:56.033 
Epoch 372/1000 
	 loss: 391.3896, MinusLogProbMetric: 391.3896, val_loss: 394.4159, val_MinusLogProbMetric: 394.4159

Epoch 372: val_loss did not improve from 393.83374
196/196 - 10s - loss: 391.3896 - MinusLogProbMetric: 391.3896 - val_loss: 394.4159 - val_MinusLogProbMetric: 394.4159 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 373/1000
2023-10-03 07:57:06.316 
Epoch 373/1000 
	 loss: 391.4229, MinusLogProbMetric: 391.4229, val_loss: 393.7997, val_MinusLogProbMetric: 393.7997

Epoch 373: val_loss improved from 393.83374 to 393.79974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 391.4229 - MinusLogProbMetric: 391.4229 - val_loss: 393.7997 - val_MinusLogProbMetric: 393.7997 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 374/1000
2023-10-03 07:57:17.000 
Epoch 374/1000 
	 loss: 391.3554, MinusLogProbMetric: 391.3554, val_loss: 394.2886, val_MinusLogProbMetric: 394.2886

Epoch 374: val_loss did not improve from 393.79974
196/196 - 10s - loss: 391.3554 - MinusLogProbMetric: 391.3554 - val_loss: 394.2886 - val_MinusLogProbMetric: 394.2886 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 375/1000
2023-10-03 07:57:27.223 
Epoch 375/1000 
	 loss: 391.2819, MinusLogProbMetric: 391.2819, val_loss: 393.8639, val_MinusLogProbMetric: 393.8639

Epoch 375: val_loss did not improve from 393.79974
196/196 - 10s - loss: 391.2819 - MinusLogProbMetric: 391.2819 - val_loss: 393.8639 - val_MinusLogProbMetric: 393.8639 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 376/1000
2023-10-03 07:57:37.557 
Epoch 376/1000 
	 loss: 391.2428, MinusLogProbMetric: 391.2428, val_loss: 394.5228, val_MinusLogProbMetric: 394.5228

Epoch 376: val_loss did not improve from 393.79974
196/196 - 10s - loss: 391.2428 - MinusLogProbMetric: 391.2428 - val_loss: 394.5228 - val_MinusLogProbMetric: 394.5228 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 377/1000
2023-10-03 07:57:47.860 
Epoch 377/1000 
	 loss: 391.5019, MinusLogProbMetric: 391.5019, val_loss: 394.4041, val_MinusLogProbMetric: 394.4041

Epoch 377: val_loss did not improve from 393.79974
196/196 - 10s - loss: 391.5019 - MinusLogProbMetric: 391.5019 - val_loss: 394.4041 - val_MinusLogProbMetric: 394.4041 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 378/1000
2023-10-03 07:57:58.284 
Epoch 378/1000 
	 loss: 391.2216, MinusLogProbMetric: 391.2216, val_loss: 395.4430, val_MinusLogProbMetric: 395.4430

Epoch 378: val_loss did not improve from 393.79974
196/196 - 10s - loss: 391.2216 - MinusLogProbMetric: 391.2216 - val_loss: 395.4430 - val_MinusLogProbMetric: 395.4430 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 379/1000
2023-10-03 07:58:08.649 
Epoch 379/1000 
	 loss: 391.4115, MinusLogProbMetric: 391.4115, val_loss: 393.9886, val_MinusLogProbMetric: 393.9886

Epoch 379: val_loss did not improve from 393.79974
196/196 - 10s - loss: 391.4115 - MinusLogProbMetric: 391.4115 - val_loss: 393.9886 - val_MinusLogProbMetric: 393.9886 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 380/1000
2023-10-03 07:58:18.963 
Epoch 380/1000 
	 loss: 391.3587, MinusLogProbMetric: 391.3587, val_loss: 393.7373, val_MinusLogProbMetric: 393.7373

Epoch 380: val_loss improved from 393.79974 to 393.73734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 391.3587 - MinusLogProbMetric: 391.3587 - val_loss: 393.7373 - val_MinusLogProbMetric: 393.7373 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 381/1000
2023-10-03 07:58:29.460 
Epoch 381/1000 
	 loss: 391.3340, MinusLogProbMetric: 391.3340, val_loss: 393.8124, val_MinusLogProbMetric: 393.8124

Epoch 381: val_loss did not improve from 393.73734
196/196 - 10s - loss: 391.3340 - MinusLogProbMetric: 391.3340 - val_loss: 393.8124 - val_MinusLogProbMetric: 393.8124 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 382/1000
2023-10-03 07:58:39.815 
Epoch 382/1000 
	 loss: 391.1852, MinusLogProbMetric: 391.1852, val_loss: 393.5466, val_MinusLogProbMetric: 393.5466

Epoch 382: val_loss improved from 393.73734 to 393.54657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 391.1852 - MinusLogProbMetric: 391.1852 - val_loss: 393.5466 - val_MinusLogProbMetric: 393.5466 - lr: 1.6667e-04 - 11s/epoch - 55ms/step
Epoch 383/1000
2023-10-03 07:58:50.504 
Epoch 383/1000 
	 loss: 391.2227, MinusLogProbMetric: 391.2227, val_loss: 394.5949, val_MinusLogProbMetric: 394.5949

Epoch 383: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.2227 - MinusLogProbMetric: 391.2227 - val_loss: 394.5949 - val_MinusLogProbMetric: 394.5949 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 384/1000
2023-10-03 07:59:00.858 
Epoch 384/1000 
	 loss: 391.1368, MinusLogProbMetric: 391.1368, val_loss: 394.1800, val_MinusLogProbMetric: 394.1800

Epoch 384: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.1368 - MinusLogProbMetric: 391.1368 - val_loss: 394.1800 - val_MinusLogProbMetric: 394.1800 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 385/1000
2023-10-03 07:59:11.223 
Epoch 385/1000 
	 loss: 391.4436, MinusLogProbMetric: 391.4436, val_loss: 394.3658, val_MinusLogProbMetric: 394.3658

Epoch 385: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.4436 - MinusLogProbMetric: 391.4436 - val_loss: 394.3658 - val_MinusLogProbMetric: 394.3658 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 386/1000
2023-10-03 07:59:21.535 
Epoch 386/1000 
	 loss: 391.3131, MinusLogProbMetric: 391.3131, val_loss: 393.6486, val_MinusLogProbMetric: 393.6486

Epoch 386: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.3131 - MinusLogProbMetric: 391.3131 - val_loss: 393.6486 - val_MinusLogProbMetric: 393.6486 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 387/1000
2023-10-03 07:59:31.664 
Epoch 387/1000 
	 loss: 391.1267, MinusLogProbMetric: 391.1267, val_loss: 395.6605, val_MinusLogProbMetric: 395.6605

Epoch 387: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.1267 - MinusLogProbMetric: 391.1267 - val_loss: 395.6605 - val_MinusLogProbMetric: 395.6605 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 388/1000
2023-10-03 07:59:41.872 
Epoch 388/1000 
	 loss: 391.4006, MinusLogProbMetric: 391.4006, val_loss: 394.8582, val_MinusLogProbMetric: 394.8582

Epoch 388: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.4006 - MinusLogProbMetric: 391.4006 - val_loss: 394.8582 - val_MinusLogProbMetric: 394.8582 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 389/1000
2023-10-03 07:59:52.113 
Epoch 389/1000 
	 loss: 391.1581, MinusLogProbMetric: 391.1581, val_loss: 394.4367, val_MinusLogProbMetric: 394.4367

Epoch 389: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.1581 - MinusLogProbMetric: 391.1581 - val_loss: 394.4367 - val_MinusLogProbMetric: 394.4367 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 390/1000
2023-10-03 08:00:02.203 
Epoch 390/1000 
	 loss: 391.4344, MinusLogProbMetric: 391.4344, val_loss: 394.4530, val_MinusLogProbMetric: 394.4530

Epoch 390: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.4344 - MinusLogProbMetric: 391.4344 - val_loss: 394.4530 - val_MinusLogProbMetric: 394.4530 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 391/1000
2023-10-03 08:00:12.192 
Epoch 391/1000 
	 loss: 391.1734, MinusLogProbMetric: 391.1734, val_loss: 394.3002, val_MinusLogProbMetric: 394.3002

Epoch 391: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.1734 - MinusLogProbMetric: 391.1734 - val_loss: 394.3002 - val_MinusLogProbMetric: 394.3002 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 392/1000
2023-10-03 08:00:21.851 
Epoch 392/1000 
	 loss: 391.0788, MinusLogProbMetric: 391.0788, val_loss: 394.3764, val_MinusLogProbMetric: 394.3764

Epoch 392: val_loss did not improve from 393.54657
196/196 - 10s - loss: 391.0788 - MinusLogProbMetric: 391.0788 - val_loss: 394.3764 - val_MinusLogProbMetric: 394.3764 - lr: 1.6667e-04 - 10s/epoch - 49ms/step
Epoch 393/1000
2023-10-03 08:00:31.966 
Epoch 393/1000 
	 loss: 390.8666, MinusLogProbMetric: 390.8666, val_loss: 393.4463, val_MinusLogProbMetric: 393.4463

Epoch 393: val_loss improved from 393.54657 to 393.44632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 390.8666 - MinusLogProbMetric: 390.8666 - val_loss: 393.4463 - val_MinusLogProbMetric: 393.4463 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 394/1000
2023-10-03 08:00:42.304 
Epoch 394/1000 
	 loss: 391.0377, MinusLogProbMetric: 391.0377, val_loss: 394.5171, val_MinusLogProbMetric: 394.5171

Epoch 394: val_loss did not improve from 393.44632
196/196 - 10s - loss: 391.0377 - MinusLogProbMetric: 391.0377 - val_loss: 394.5171 - val_MinusLogProbMetric: 394.5171 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 395/1000
2023-10-03 08:00:52.344 
Epoch 395/1000 
	 loss: 391.0286, MinusLogProbMetric: 391.0286, val_loss: 393.5309, val_MinusLogProbMetric: 393.5309

Epoch 395: val_loss did not improve from 393.44632
196/196 - 10s - loss: 391.0286 - MinusLogProbMetric: 391.0286 - val_loss: 393.5309 - val_MinusLogProbMetric: 393.5309 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 396/1000
2023-10-03 08:01:02.296 
Epoch 396/1000 
	 loss: 390.8880, MinusLogProbMetric: 390.8880, val_loss: 393.3109, val_MinusLogProbMetric: 393.3109

Epoch 396: val_loss improved from 393.44632 to 393.31094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 390.8880 - MinusLogProbMetric: 390.8880 - val_loss: 393.3109 - val_MinusLogProbMetric: 393.3109 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 397/1000
2023-10-03 08:01:12.904 
Epoch 397/1000 
	 loss: 391.0646, MinusLogProbMetric: 391.0646, val_loss: 393.6414, val_MinusLogProbMetric: 393.6414

Epoch 397: val_loss did not improve from 393.31094
196/196 - 10s - loss: 391.0646 - MinusLogProbMetric: 391.0646 - val_loss: 393.6414 - val_MinusLogProbMetric: 393.6414 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 398/1000
2023-10-03 08:01:22.863 
Epoch 398/1000 
	 loss: 391.1573, MinusLogProbMetric: 391.1573, val_loss: 393.8955, val_MinusLogProbMetric: 393.8955

Epoch 398: val_loss did not improve from 393.31094
196/196 - 10s - loss: 391.1573 - MinusLogProbMetric: 391.1573 - val_loss: 393.8955 - val_MinusLogProbMetric: 393.8955 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 399/1000
2023-10-03 08:01:32.632 
Epoch 399/1000 
	 loss: 391.2502, MinusLogProbMetric: 391.2502, val_loss: 394.1665, val_MinusLogProbMetric: 394.1665

Epoch 399: val_loss did not improve from 393.31094
196/196 - 10s - loss: 391.2502 - MinusLogProbMetric: 391.2502 - val_loss: 394.1665 - val_MinusLogProbMetric: 394.1665 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 400/1000
2023-10-03 08:01:42.664 
Epoch 400/1000 
	 loss: 391.0209, MinusLogProbMetric: 391.0209, val_loss: 393.3564, val_MinusLogProbMetric: 393.3564

Epoch 400: val_loss did not improve from 393.31094
196/196 - 10s - loss: 391.0209 - MinusLogProbMetric: 391.0209 - val_loss: 393.3564 - val_MinusLogProbMetric: 393.3564 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 401/1000
2023-10-03 08:01:52.657 
Epoch 401/1000 
	 loss: 391.0475, MinusLogProbMetric: 391.0475, val_loss: 393.3910, val_MinusLogProbMetric: 393.3910

Epoch 401: val_loss did not improve from 393.31094
196/196 - 10s - loss: 391.0475 - MinusLogProbMetric: 391.0475 - val_loss: 393.3910 - val_MinusLogProbMetric: 393.3910 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 402/1000
2023-10-03 08:02:02.699 
Epoch 402/1000 
	 loss: 390.9028, MinusLogProbMetric: 390.9028, val_loss: 393.1776, val_MinusLogProbMetric: 393.1776

Epoch 402: val_loss improved from 393.31094 to 393.17764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 390.9028 - MinusLogProbMetric: 390.9028 - val_loss: 393.1776 - val_MinusLogProbMetric: 393.1776 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 403/1000
2023-10-03 08:02:13.168 
Epoch 403/1000 
	 loss: 390.7359, MinusLogProbMetric: 390.7359, val_loss: 393.4810, val_MinusLogProbMetric: 393.4810

Epoch 403: val_loss did not improve from 393.17764
196/196 - 10s - loss: 390.7359 - MinusLogProbMetric: 390.7359 - val_loss: 393.4810 - val_MinusLogProbMetric: 393.4810 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 404/1000
2023-10-03 08:02:23.423 
Epoch 404/1000 
	 loss: 390.9807, MinusLogProbMetric: 390.9807, val_loss: 392.9922, val_MinusLogProbMetric: 392.9922

Epoch 404: val_loss improved from 393.17764 to 392.99216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 11s - loss: 390.9807 - MinusLogProbMetric: 390.9807 - val_loss: 392.9922 - val_MinusLogProbMetric: 392.9922 - lr: 1.6667e-04 - 11s/epoch - 54ms/step
Epoch 405/1000
2023-10-03 08:02:33.938 
Epoch 405/1000 
	 loss: 390.7944, MinusLogProbMetric: 390.7944, val_loss: 394.0551, val_MinusLogProbMetric: 394.0551

Epoch 405: val_loss did not improve from 392.99216
196/196 - 10s - loss: 390.7944 - MinusLogProbMetric: 390.7944 - val_loss: 394.0551 - val_MinusLogProbMetric: 394.0551 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 406/1000
2023-10-03 08:02:44.006 
Epoch 406/1000 
	 loss: 390.9659, MinusLogProbMetric: 390.9659, val_loss: 393.9750, val_MinusLogProbMetric: 393.9750

Epoch 406: val_loss did not improve from 392.99216
196/196 - 10s - loss: 390.9659 - MinusLogProbMetric: 390.9659 - val_loss: 393.9750 - val_MinusLogProbMetric: 393.9750 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 407/1000
2023-10-03 08:02:53.922 
Epoch 407/1000 
	 loss: 390.9160, MinusLogProbMetric: 390.9160, val_loss: 393.2110, val_MinusLogProbMetric: 393.2110

Epoch 407: val_loss did not improve from 392.99216
196/196 - 10s - loss: 390.9160 - MinusLogProbMetric: 390.9160 - val_loss: 393.2110 - val_MinusLogProbMetric: 393.2110 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 408/1000
2023-10-03 08:03:03.995 
Epoch 408/1000 
	 loss: 391.0609, MinusLogProbMetric: 391.0609, val_loss: 394.4118, val_MinusLogProbMetric: 394.4118

Epoch 408: val_loss did not improve from 392.99216
196/196 - 10s - loss: 391.0609 - MinusLogProbMetric: 391.0609 - val_loss: 394.4118 - val_MinusLogProbMetric: 394.4118 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 409/1000
2023-10-03 08:03:13.865 
Epoch 409/1000 
	 loss: 390.6558, MinusLogProbMetric: 390.6558, val_loss: 392.8527, val_MinusLogProbMetric: 392.8527

Epoch 409: val_loss improved from 392.99216 to 392.85266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 390.6558 - MinusLogProbMetric: 390.6558 - val_loss: 392.8527 - val_MinusLogProbMetric: 392.8527 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 410/1000
2023-10-03 08:03:24.087 
Epoch 410/1000 
	 loss: 390.8916, MinusLogProbMetric: 390.8916, val_loss: 393.5440, val_MinusLogProbMetric: 393.5440

Epoch 410: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8916 - MinusLogProbMetric: 390.8916 - val_loss: 393.5440 - val_MinusLogProbMetric: 393.5440 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 411/1000
2023-10-03 08:03:34.144 
Epoch 411/1000 
	 loss: 390.7580, MinusLogProbMetric: 390.7580, val_loss: 392.9042, val_MinusLogProbMetric: 392.9042

Epoch 411: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7580 - MinusLogProbMetric: 390.7580 - val_loss: 392.9042 - val_MinusLogProbMetric: 392.9042 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 412/1000
2023-10-03 08:03:44.424 
Epoch 412/1000 
	 loss: 390.6885, MinusLogProbMetric: 390.6884, val_loss: 393.4670, val_MinusLogProbMetric: 393.4670

Epoch 412: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6885 - MinusLogProbMetric: 390.6884 - val_loss: 393.4670 - val_MinusLogProbMetric: 393.4670 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 413/1000
2023-10-03 08:03:54.755 
Epoch 413/1000 
	 loss: 390.7595, MinusLogProbMetric: 390.7595, val_loss: 393.4509, val_MinusLogProbMetric: 393.4509

Epoch 413: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7595 - MinusLogProbMetric: 390.7595 - val_loss: 393.4509 - val_MinusLogProbMetric: 393.4509 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 414/1000
2023-10-03 08:04:04.924 
Epoch 414/1000 
	 loss: 390.8862, MinusLogProbMetric: 390.8862, val_loss: 393.9662, val_MinusLogProbMetric: 393.9662

Epoch 414: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8862 - MinusLogProbMetric: 390.8862 - val_loss: 393.9662 - val_MinusLogProbMetric: 393.9662 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 415/1000
2023-10-03 08:04:14.864 
Epoch 415/1000 
	 loss: 390.8741, MinusLogProbMetric: 390.8741, val_loss: 394.7018, val_MinusLogProbMetric: 394.7018

Epoch 415: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8741 - MinusLogProbMetric: 390.8741 - val_loss: 394.7018 - val_MinusLogProbMetric: 394.7018 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 416/1000
2023-10-03 08:04:24.812 
Epoch 416/1000 
	 loss: 390.7870, MinusLogProbMetric: 390.7870, val_loss: 394.1898, val_MinusLogProbMetric: 394.1898

Epoch 416: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7870 - MinusLogProbMetric: 390.7870 - val_loss: 394.1898 - val_MinusLogProbMetric: 394.1898 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 417/1000
2023-10-03 08:04:34.828 
Epoch 417/1000 
	 loss: 390.8295, MinusLogProbMetric: 390.8295, val_loss: 395.0073, val_MinusLogProbMetric: 395.0073

Epoch 417: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8295 - MinusLogProbMetric: 390.8295 - val_loss: 395.0073 - val_MinusLogProbMetric: 395.0073 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 418/1000
2023-10-03 08:04:44.712 
Epoch 418/1000 
	 loss: 390.6993, MinusLogProbMetric: 390.6993, val_loss: 394.1235, val_MinusLogProbMetric: 394.1235

Epoch 418: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6993 - MinusLogProbMetric: 390.6993 - val_loss: 394.1235 - val_MinusLogProbMetric: 394.1235 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 419/1000
2023-10-03 08:04:54.698 
Epoch 419/1000 
	 loss: 390.6522, MinusLogProbMetric: 390.6522, val_loss: 393.5850, val_MinusLogProbMetric: 393.5850

Epoch 419: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6522 - MinusLogProbMetric: 390.6522 - val_loss: 393.5850 - val_MinusLogProbMetric: 393.5850 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 420/1000
2023-10-03 08:05:04.505 
Epoch 420/1000 
	 loss: 390.5847, MinusLogProbMetric: 390.5847, val_loss: 393.6021, val_MinusLogProbMetric: 393.6021

Epoch 420: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5847 - MinusLogProbMetric: 390.5847 - val_loss: 393.6021 - val_MinusLogProbMetric: 393.6021 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 421/1000
2023-10-03 08:05:14.466 
Epoch 421/1000 
	 loss: 390.6834, MinusLogProbMetric: 390.6834, val_loss: 393.7986, val_MinusLogProbMetric: 393.7986

Epoch 421: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6834 - MinusLogProbMetric: 390.6834 - val_loss: 393.7986 - val_MinusLogProbMetric: 393.7986 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 422/1000
2023-10-03 08:05:24.214 
Epoch 422/1000 
	 loss: 391.1757, MinusLogProbMetric: 391.1757, val_loss: 393.1055, val_MinusLogProbMetric: 393.1055

Epoch 422: val_loss did not improve from 392.85266
196/196 - 10s - loss: 391.1757 - MinusLogProbMetric: 391.1757 - val_loss: 393.1055 - val_MinusLogProbMetric: 393.1055 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 423/1000
2023-10-03 08:05:34.262 
Epoch 423/1000 
	 loss: 390.5118, MinusLogProbMetric: 390.5118, val_loss: 393.3309, val_MinusLogProbMetric: 393.3309

Epoch 423: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5118 - MinusLogProbMetric: 390.5118 - val_loss: 393.3309 - val_MinusLogProbMetric: 393.3309 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 424/1000
2023-10-03 08:05:44.616 
Epoch 424/1000 
	 loss: 390.5534, MinusLogProbMetric: 390.5534, val_loss: 393.4538, val_MinusLogProbMetric: 393.4538

Epoch 424: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5534 - MinusLogProbMetric: 390.5534 - val_loss: 393.4538 - val_MinusLogProbMetric: 393.4538 - lr: 1.6667e-04 - 10s/epoch - 53ms/step
Epoch 425/1000
2023-10-03 08:05:54.519 
Epoch 425/1000 
	 loss: 390.6854, MinusLogProbMetric: 390.6854, val_loss: 393.0996, val_MinusLogProbMetric: 393.0996

Epoch 425: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6854 - MinusLogProbMetric: 390.6854 - val_loss: 393.0996 - val_MinusLogProbMetric: 393.0996 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 426/1000
2023-10-03 08:06:04.345 
Epoch 426/1000 
	 loss: 390.9010, MinusLogProbMetric: 390.9010, val_loss: 393.1142, val_MinusLogProbMetric: 393.1142

Epoch 426: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.9010 - MinusLogProbMetric: 390.9010 - val_loss: 393.1142 - val_MinusLogProbMetric: 393.1142 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 427/1000
2023-10-03 08:06:14.249 
Epoch 427/1000 
	 loss: 390.5812, MinusLogProbMetric: 390.5812, val_loss: 394.5316, val_MinusLogProbMetric: 394.5316

Epoch 427: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5812 - MinusLogProbMetric: 390.5812 - val_loss: 394.5316 - val_MinusLogProbMetric: 394.5316 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 428/1000
2023-10-03 08:06:24.104 
Epoch 428/1000 
	 loss: 390.6805, MinusLogProbMetric: 390.6805, val_loss: 393.2195, val_MinusLogProbMetric: 393.2195

Epoch 428: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6805 - MinusLogProbMetric: 390.6805 - val_loss: 393.2195 - val_MinusLogProbMetric: 393.2195 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 429/1000
2023-10-03 08:06:34.351 
Epoch 429/1000 
	 loss: 390.8091, MinusLogProbMetric: 390.8091, val_loss: 393.8144, val_MinusLogProbMetric: 393.8144

Epoch 429: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8091 - MinusLogProbMetric: 390.8091 - val_loss: 393.8144 - val_MinusLogProbMetric: 393.8144 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 430/1000
2023-10-03 08:06:44.269 
Epoch 430/1000 
	 loss: 390.9221, MinusLogProbMetric: 390.9221, val_loss: 394.2977, val_MinusLogProbMetric: 394.2977

Epoch 430: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.9221 - MinusLogProbMetric: 390.9221 - val_loss: 394.2977 - val_MinusLogProbMetric: 394.2977 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 431/1000
2023-10-03 08:06:54.374 
Epoch 431/1000 
	 loss: 390.5891, MinusLogProbMetric: 390.5891, val_loss: 395.1622, val_MinusLogProbMetric: 395.1622

Epoch 431: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5891 - MinusLogProbMetric: 390.5891 - val_loss: 395.1622 - val_MinusLogProbMetric: 395.1622 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 432/1000
2023-10-03 08:07:04.075 
Epoch 432/1000 
	 loss: 390.4579, MinusLogProbMetric: 390.4579, val_loss: 393.7025, val_MinusLogProbMetric: 393.7025

Epoch 432: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.4579 - MinusLogProbMetric: 390.4579 - val_loss: 393.7025 - val_MinusLogProbMetric: 393.7025 - lr: 1.6667e-04 - 10s/epoch - 49ms/step
Epoch 433/1000
2023-10-03 08:07:14.050 
Epoch 433/1000 
	 loss: 390.5769, MinusLogProbMetric: 390.5769, val_loss: 393.4597, val_MinusLogProbMetric: 393.4597

Epoch 433: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5769 - MinusLogProbMetric: 390.5769 - val_loss: 393.4597 - val_MinusLogProbMetric: 393.4597 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 434/1000
2023-10-03 08:07:24.006 
Epoch 434/1000 
	 loss: 390.6443, MinusLogProbMetric: 390.6443, val_loss: 393.5436, val_MinusLogProbMetric: 393.5436

Epoch 434: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6443 - MinusLogProbMetric: 390.6443 - val_loss: 393.5436 - val_MinusLogProbMetric: 393.5436 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 435/1000
2023-10-03 08:07:34.036 
Epoch 435/1000 
	 loss: 390.7670, MinusLogProbMetric: 390.7670, val_loss: 394.3438, val_MinusLogProbMetric: 394.3438

Epoch 435: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7670 - MinusLogProbMetric: 390.7670 - val_loss: 394.3438 - val_MinusLogProbMetric: 394.3438 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 436/1000
2023-10-03 08:07:44.043 
Epoch 436/1000 
	 loss: 390.7162, MinusLogProbMetric: 390.7162, val_loss: 393.4272, val_MinusLogProbMetric: 393.4272

Epoch 436: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7162 - MinusLogProbMetric: 390.7162 - val_loss: 393.4272 - val_MinusLogProbMetric: 393.4272 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 437/1000
2023-10-03 08:07:54.064 
Epoch 437/1000 
	 loss: 390.6034, MinusLogProbMetric: 390.6034, val_loss: 393.2077, val_MinusLogProbMetric: 393.2077

Epoch 437: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6034 - MinusLogProbMetric: 390.6034 - val_loss: 393.2077 - val_MinusLogProbMetric: 393.2077 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 438/1000
2023-10-03 08:08:03.944 
Epoch 438/1000 
	 loss: 390.5782, MinusLogProbMetric: 390.5782, val_loss: 393.4124, val_MinusLogProbMetric: 393.4124

Epoch 438: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5782 - MinusLogProbMetric: 390.5782 - val_loss: 393.4124 - val_MinusLogProbMetric: 393.4124 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 439/1000
2023-10-03 08:08:14.043 
Epoch 439/1000 
	 loss: 390.8808, MinusLogProbMetric: 390.8808, val_loss: 393.3928, val_MinusLogProbMetric: 393.3928

Epoch 439: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8808 - MinusLogProbMetric: 390.8808 - val_loss: 393.3928 - val_MinusLogProbMetric: 393.3928 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 440/1000
2023-10-03 08:08:24.014 
Epoch 440/1000 
	 loss: 390.5084, MinusLogProbMetric: 390.5084, val_loss: 393.8931, val_MinusLogProbMetric: 393.8931

Epoch 440: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5084 - MinusLogProbMetric: 390.5084 - val_loss: 393.8931 - val_MinusLogProbMetric: 393.8931 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 441/1000
2023-10-03 08:08:33.949 
Epoch 441/1000 
	 loss: 390.6270, MinusLogProbMetric: 390.6270, val_loss: 394.1483, val_MinusLogProbMetric: 394.1483

Epoch 441: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.6270 - MinusLogProbMetric: 390.6270 - val_loss: 394.1483 - val_MinusLogProbMetric: 394.1483 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 442/1000
2023-10-03 08:08:43.743 
Epoch 442/1000 
	 loss: 390.7228, MinusLogProbMetric: 390.7228, val_loss: 393.6630, val_MinusLogProbMetric: 393.6630

Epoch 442: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7228 - MinusLogProbMetric: 390.7228 - val_loss: 393.6630 - val_MinusLogProbMetric: 393.6630 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 443/1000
2023-10-03 08:08:53.582 
Epoch 443/1000 
	 loss: 390.4417, MinusLogProbMetric: 390.4417, val_loss: 394.1266, val_MinusLogProbMetric: 394.1266

Epoch 443: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.4417 - MinusLogProbMetric: 390.4417 - val_loss: 394.1266 - val_MinusLogProbMetric: 394.1266 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 444/1000
2023-10-03 08:09:03.332 
Epoch 444/1000 
	 loss: 390.7099, MinusLogProbMetric: 390.7099, val_loss: 393.0872, val_MinusLogProbMetric: 393.0872

Epoch 444: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7099 - MinusLogProbMetric: 390.7099 - val_loss: 393.0872 - val_MinusLogProbMetric: 393.0872 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 445/1000
2023-10-03 08:09:13.141 
Epoch 445/1000 
	 loss: 390.4079, MinusLogProbMetric: 390.4079, val_loss: 392.9872, val_MinusLogProbMetric: 392.9872

Epoch 445: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.4079 - MinusLogProbMetric: 390.4079 - val_loss: 392.9872 - val_MinusLogProbMetric: 392.9872 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 446/1000
2023-10-03 08:09:23.136 
Epoch 446/1000 
	 loss: 390.8653, MinusLogProbMetric: 390.8653, val_loss: 393.0739, val_MinusLogProbMetric: 393.0739

Epoch 446: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8653 - MinusLogProbMetric: 390.8653 - val_loss: 393.0739 - val_MinusLogProbMetric: 393.0739 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 447/1000
2023-10-03 08:09:33.192 
Epoch 447/1000 
	 loss: 390.3524, MinusLogProbMetric: 390.3524, val_loss: 392.8737, val_MinusLogProbMetric: 392.8737

Epoch 447: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.3524 - MinusLogProbMetric: 390.3524 - val_loss: 392.8737 - val_MinusLogProbMetric: 392.8737 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 448/1000
2023-10-03 08:09:43.099 
Epoch 448/1000 
	 loss: 390.4141, MinusLogProbMetric: 390.4141, val_loss: 393.8239, val_MinusLogProbMetric: 393.8239

Epoch 448: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.4141 - MinusLogProbMetric: 390.4141 - val_loss: 393.8239 - val_MinusLogProbMetric: 393.8239 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 449/1000
2023-10-03 08:09:52.836 
Epoch 449/1000 
	 loss: 390.2888, MinusLogProbMetric: 390.2888, val_loss: 393.3035, val_MinusLogProbMetric: 393.3035

Epoch 449: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.2888 - MinusLogProbMetric: 390.2888 - val_loss: 393.3035 - val_MinusLogProbMetric: 393.3035 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 450/1000
2023-10-03 08:10:03.026 
Epoch 450/1000 
	 loss: 390.4487, MinusLogProbMetric: 390.4487, val_loss: 393.3885, val_MinusLogProbMetric: 393.3885

Epoch 450: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.4487 - MinusLogProbMetric: 390.4487 - val_loss: 393.3885 - val_MinusLogProbMetric: 393.3885 - lr: 1.6667e-04 - 10s/epoch - 52ms/step
Epoch 451/1000
2023-10-03 08:10:13.090 
Epoch 451/1000 
	 loss: 390.8674, MinusLogProbMetric: 390.8674, val_loss: 394.5232, val_MinusLogProbMetric: 394.5232

Epoch 451: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.8674 - MinusLogProbMetric: 390.8674 - val_loss: 394.5232 - val_MinusLogProbMetric: 394.5232 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 452/1000
2023-10-03 08:10:23.175 
Epoch 452/1000 
	 loss: 390.5604, MinusLogProbMetric: 390.5604, val_loss: 392.8583, val_MinusLogProbMetric: 392.8583

Epoch 452: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.5604 - MinusLogProbMetric: 390.5604 - val_loss: 392.8583 - val_MinusLogProbMetric: 392.8583 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 453/1000
2023-10-03 08:10:32.906 
Epoch 453/1000 
	 loss: 390.1333, MinusLogProbMetric: 390.1333, val_loss: 393.3190, val_MinusLogProbMetric: 393.3190

Epoch 453: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.1333 - MinusLogProbMetric: 390.1333 - val_loss: 393.3190 - val_MinusLogProbMetric: 393.3190 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 454/1000
2023-10-03 08:10:42.812 
Epoch 454/1000 
	 loss: 390.7101, MinusLogProbMetric: 390.7101, val_loss: 393.0523, val_MinusLogProbMetric: 393.0523

Epoch 454: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7101 - MinusLogProbMetric: 390.7101 - val_loss: 393.0523 - val_MinusLogProbMetric: 393.0523 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 455/1000
2023-10-03 08:10:52.680 
Epoch 455/1000 
	 loss: 390.4841, MinusLogProbMetric: 390.4841, val_loss: 394.3489, val_MinusLogProbMetric: 394.3489

Epoch 455: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.4841 - MinusLogProbMetric: 390.4841 - val_loss: 394.3489 - val_MinusLogProbMetric: 394.3489 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 456/1000
2023-10-03 08:11:02.524 
Epoch 456/1000 
	 loss: 390.1625, MinusLogProbMetric: 390.1625, val_loss: 393.1353, val_MinusLogProbMetric: 393.1353

Epoch 456: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.1625 - MinusLogProbMetric: 390.1625 - val_loss: 393.1353 - val_MinusLogProbMetric: 393.1353 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 457/1000
2023-10-03 08:11:12.566 
Epoch 457/1000 
	 loss: 390.7052, MinusLogProbMetric: 390.7052, val_loss: 393.0626, val_MinusLogProbMetric: 393.0626

Epoch 457: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.7052 - MinusLogProbMetric: 390.7052 - val_loss: 393.0626 - val_MinusLogProbMetric: 393.0626 - lr: 1.6667e-04 - 10s/epoch - 51ms/step
Epoch 458/1000
2023-10-03 08:11:22.298 
Epoch 458/1000 
	 loss: 390.4621, MinusLogProbMetric: 390.4621, val_loss: 393.7965, val_MinusLogProbMetric: 393.7965

Epoch 458: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.4621 - MinusLogProbMetric: 390.4621 - val_loss: 393.7965 - val_MinusLogProbMetric: 393.7965 - lr: 1.6667e-04 - 10s/epoch - 50ms/step
Epoch 459/1000
2023-10-03 08:11:31.983 
Epoch 459/1000 
	 loss: 390.1599, MinusLogProbMetric: 390.1599, val_loss: 393.1354, val_MinusLogProbMetric: 393.1354

Epoch 459: val_loss did not improve from 392.85266
196/196 - 10s - loss: 390.1599 - MinusLogProbMetric: 390.1599 - val_loss: 393.1354 - val_MinusLogProbMetric: 393.1354 - lr: 1.6667e-04 - 10s/epoch - 49ms/step
Epoch 460/1000
2023-10-03 08:11:42.116 
Epoch 460/1000 
	 loss: 388.9382, MinusLogProbMetric: 388.9382, val_loss: 392.0477, val_MinusLogProbMetric: 392.0477

Epoch 460: val_loss improved from 392.85266 to 392.04773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.9382 - MinusLogProbMetric: 388.9382 - val_loss: 392.0477 - val_MinusLogProbMetric: 392.0477 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 461/1000
2023-10-03 08:11:52.367 
Epoch 461/1000 
	 loss: 388.8522, MinusLogProbMetric: 388.8522, val_loss: 392.0849, val_MinusLogProbMetric: 392.0849

Epoch 461: val_loss did not improve from 392.04773
196/196 - 10s - loss: 388.8522 - MinusLogProbMetric: 388.8522 - val_loss: 392.0849 - val_MinusLogProbMetric: 392.0849 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 462/1000
2023-10-03 08:12:02.424 
Epoch 462/1000 
	 loss: 388.9042, MinusLogProbMetric: 388.9042, val_loss: 391.9748, val_MinusLogProbMetric: 391.9748

Epoch 462: val_loss improved from 392.04773 to 391.97482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.9042 - MinusLogProbMetric: 388.9042 - val_loss: 391.9748 - val_MinusLogProbMetric: 391.9748 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 463/1000
2023-10-03 08:12:12.695 
Epoch 463/1000 
	 loss: 388.9646, MinusLogProbMetric: 388.9646, val_loss: 391.9022, val_MinusLogProbMetric: 391.9022

Epoch 463: val_loss improved from 391.97482 to 391.90219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.9646 - MinusLogProbMetric: 388.9646 - val_loss: 391.9022 - val_MinusLogProbMetric: 391.9022 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 464/1000
2023-10-03 08:12:23.058 
Epoch 464/1000 
	 loss: 388.9804, MinusLogProbMetric: 388.9804, val_loss: 392.3193, val_MinusLogProbMetric: 392.3193

Epoch 464: val_loss did not improve from 391.90219
196/196 - 10s - loss: 388.9804 - MinusLogProbMetric: 388.9804 - val_loss: 392.3193 - val_MinusLogProbMetric: 392.3193 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 465/1000
2023-10-03 08:12:33.019 
Epoch 465/1000 
	 loss: 389.0296, MinusLogProbMetric: 389.0296, val_loss: 392.3004, val_MinusLogProbMetric: 392.3004

Epoch 465: val_loss did not improve from 391.90219
196/196 - 10s - loss: 389.0296 - MinusLogProbMetric: 389.0296 - val_loss: 392.3004 - val_MinusLogProbMetric: 392.3004 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 466/1000
2023-10-03 08:12:43.218 
Epoch 466/1000 
	 loss: 388.8745, MinusLogProbMetric: 388.8745, val_loss: 392.1092, val_MinusLogProbMetric: 392.1092

Epoch 466: val_loss did not improve from 391.90219
196/196 - 10s - loss: 388.8745 - MinusLogProbMetric: 388.8745 - val_loss: 392.1092 - val_MinusLogProbMetric: 392.1092 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 467/1000
2023-10-03 08:12:53.219 
Epoch 467/1000 
	 loss: 388.8205, MinusLogProbMetric: 388.8205, val_loss: 392.2858, val_MinusLogProbMetric: 392.2858

Epoch 467: val_loss did not improve from 391.90219
196/196 - 10s - loss: 388.8205 - MinusLogProbMetric: 388.8205 - val_loss: 392.2858 - val_MinusLogProbMetric: 392.2858 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 468/1000
2023-10-03 08:13:03.122 
Epoch 468/1000 
	 loss: 388.9473, MinusLogProbMetric: 388.9473, val_loss: 392.6598, val_MinusLogProbMetric: 392.6598

Epoch 468: val_loss did not improve from 391.90219
196/196 - 10s - loss: 388.9473 - MinusLogProbMetric: 388.9473 - val_loss: 392.6598 - val_MinusLogProbMetric: 392.6598 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 469/1000
2023-10-03 08:13:12.782 
Epoch 469/1000 
	 loss: 388.9077, MinusLogProbMetric: 388.9077, val_loss: 392.1687, val_MinusLogProbMetric: 392.1687

Epoch 469: val_loss did not improve from 391.90219
196/196 - 10s - loss: 388.9077 - MinusLogProbMetric: 388.9077 - val_loss: 392.1687 - val_MinusLogProbMetric: 392.1687 - lr: 8.3333e-05 - 10s/epoch - 49ms/step
Epoch 470/1000
2023-10-03 08:13:22.610 
Epoch 470/1000 
	 loss: 388.8998, MinusLogProbMetric: 388.8998, val_loss: 392.2123, val_MinusLogProbMetric: 392.2123

Epoch 470: val_loss did not improve from 391.90219
196/196 - 10s - loss: 388.8998 - MinusLogProbMetric: 388.8998 - val_loss: 392.2123 - val_MinusLogProbMetric: 392.2123 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 471/1000
2023-10-03 08:13:32.474 
Epoch 471/1000 
	 loss: 388.8581, MinusLogProbMetric: 388.8581, val_loss: 392.1029, val_MinusLogProbMetric: 392.1029

Epoch 471: val_loss did not improve from 391.90219
196/196 - 10s - loss: 388.8581 - MinusLogProbMetric: 388.8581 - val_loss: 392.1029 - val_MinusLogProbMetric: 392.1029 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 472/1000
2023-10-03 08:13:42.427 
Epoch 472/1000 
	 loss: 388.7720, MinusLogProbMetric: 388.7720, val_loss: 391.6928, val_MinusLogProbMetric: 391.6928

Epoch 472: val_loss improved from 391.90219 to 391.69284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.7720 - MinusLogProbMetric: 388.7720 - val_loss: 391.6928 - val_MinusLogProbMetric: 391.6928 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 473/1000
2023-10-03 08:13:52.868 
Epoch 473/1000 
	 loss: 388.9423, MinusLogProbMetric: 388.9423, val_loss: 391.8394, val_MinusLogProbMetric: 391.8394

Epoch 473: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.9423 - MinusLogProbMetric: 388.9423 - val_loss: 391.8394 - val_MinusLogProbMetric: 391.8394 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 474/1000
2023-10-03 08:14:02.533 
Epoch 474/1000 
	 loss: 388.9954, MinusLogProbMetric: 388.9954, val_loss: 392.2443, val_MinusLogProbMetric: 392.2443

Epoch 474: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.9954 - MinusLogProbMetric: 388.9954 - val_loss: 392.2443 - val_MinusLogProbMetric: 392.2443 - lr: 8.3333e-05 - 10s/epoch - 49ms/step
Epoch 475/1000
2023-10-03 08:14:12.595 
Epoch 475/1000 
	 loss: 388.8870, MinusLogProbMetric: 388.8870, val_loss: 392.0427, val_MinusLogProbMetric: 392.0427

Epoch 475: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.8870 - MinusLogProbMetric: 388.8870 - val_loss: 392.0427 - val_MinusLogProbMetric: 392.0427 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 476/1000
2023-10-03 08:14:22.466 
Epoch 476/1000 
	 loss: 388.9828, MinusLogProbMetric: 388.9828, val_loss: 392.4178, val_MinusLogProbMetric: 392.4178

Epoch 476: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.9828 - MinusLogProbMetric: 388.9828 - val_loss: 392.4178 - val_MinusLogProbMetric: 392.4178 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 477/1000
2023-10-03 08:14:32.432 
Epoch 477/1000 
	 loss: 388.8774, MinusLogProbMetric: 388.8774, val_loss: 391.8849, val_MinusLogProbMetric: 391.8849

Epoch 477: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.8774 - MinusLogProbMetric: 388.8774 - val_loss: 391.8849 - val_MinusLogProbMetric: 391.8849 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 478/1000
2023-10-03 08:14:42.391 
Epoch 478/1000 
	 loss: 388.7523, MinusLogProbMetric: 388.7523, val_loss: 391.8274, val_MinusLogProbMetric: 391.8274

Epoch 478: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.7523 - MinusLogProbMetric: 388.7523 - val_loss: 391.8274 - val_MinusLogProbMetric: 391.8274 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 479/1000
2023-10-03 08:14:52.376 
Epoch 479/1000 
	 loss: 388.8604, MinusLogProbMetric: 388.8604, val_loss: 392.7000, val_MinusLogProbMetric: 392.7000

Epoch 479: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.8604 - MinusLogProbMetric: 388.8604 - val_loss: 392.7000 - val_MinusLogProbMetric: 392.7000 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 480/1000
2023-10-03 08:15:02.208 
Epoch 480/1000 
	 loss: 389.1286, MinusLogProbMetric: 389.1286, val_loss: 391.9122, val_MinusLogProbMetric: 391.9122

Epoch 480: val_loss did not improve from 391.69284
196/196 - 10s - loss: 389.1286 - MinusLogProbMetric: 389.1286 - val_loss: 391.9122 - val_MinusLogProbMetric: 391.9122 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 481/1000
2023-10-03 08:15:12.156 
Epoch 481/1000 
	 loss: 388.6937, MinusLogProbMetric: 388.6937, val_loss: 391.8216, val_MinusLogProbMetric: 391.8216

Epoch 481: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.6937 - MinusLogProbMetric: 388.6937 - val_loss: 391.8216 - val_MinusLogProbMetric: 391.8216 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 482/1000
2023-10-03 08:15:22.000 
Epoch 482/1000 
	 loss: 388.7438, MinusLogProbMetric: 388.7438, val_loss: 391.9368, val_MinusLogProbMetric: 391.9368

Epoch 482: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.7438 - MinusLogProbMetric: 388.7438 - val_loss: 391.9368 - val_MinusLogProbMetric: 391.9368 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 483/1000
2023-10-03 08:15:31.960 
Epoch 483/1000 
	 loss: 388.8615, MinusLogProbMetric: 388.8615, val_loss: 392.2672, val_MinusLogProbMetric: 392.2672

Epoch 483: val_loss did not improve from 391.69284
196/196 - 10s - loss: 388.8615 - MinusLogProbMetric: 388.8615 - val_loss: 392.2672 - val_MinusLogProbMetric: 392.2672 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 484/1000
2023-10-03 08:15:41.890 
Epoch 484/1000 
	 loss: 388.8166, MinusLogProbMetric: 388.8166, val_loss: 391.6586, val_MinusLogProbMetric: 391.6586

Epoch 484: val_loss improved from 391.69284 to 391.65863, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.8166 - MinusLogProbMetric: 388.8166 - val_loss: 391.6586 - val_MinusLogProbMetric: 391.6586 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 485/1000
2023-10-03 08:15:52.070 
Epoch 485/1000 
	 loss: 388.8312, MinusLogProbMetric: 388.8312, val_loss: 391.7817, val_MinusLogProbMetric: 391.7817

Epoch 485: val_loss did not improve from 391.65863
196/196 - 10s - loss: 388.8312 - MinusLogProbMetric: 388.8312 - val_loss: 391.7817 - val_MinusLogProbMetric: 391.7817 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 486/1000
2023-10-03 08:16:01.844 
Epoch 486/1000 
	 loss: 388.9193, MinusLogProbMetric: 388.9193, val_loss: 391.9512, val_MinusLogProbMetric: 391.9512

Epoch 486: val_loss did not improve from 391.65863
196/196 - 10s - loss: 388.9193 - MinusLogProbMetric: 388.9193 - val_loss: 391.9512 - val_MinusLogProbMetric: 391.9512 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 487/1000
2023-10-03 08:16:11.769 
Epoch 487/1000 
	 loss: 388.8723, MinusLogProbMetric: 388.8723, val_loss: 391.7192, val_MinusLogProbMetric: 391.7192

Epoch 487: val_loss did not improve from 391.65863
196/196 - 10s - loss: 388.8723 - MinusLogProbMetric: 388.8723 - val_loss: 391.7192 - val_MinusLogProbMetric: 391.7192 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 488/1000
2023-10-03 08:16:21.937 
Epoch 488/1000 
	 loss: 388.9955, MinusLogProbMetric: 388.9955, val_loss: 391.9032, val_MinusLogProbMetric: 391.9032

Epoch 488: val_loss did not improve from 391.65863
196/196 - 10s - loss: 388.9955 - MinusLogProbMetric: 388.9955 - val_loss: 391.9032 - val_MinusLogProbMetric: 391.9032 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 489/1000
2023-10-03 08:16:32.100 
Epoch 489/1000 
	 loss: 388.8007, MinusLogProbMetric: 388.8007, val_loss: 392.2082, val_MinusLogProbMetric: 392.2082

Epoch 489: val_loss did not improve from 391.65863
196/196 - 10s - loss: 388.8007 - MinusLogProbMetric: 388.8007 - val_loss: 392.2082 - val_MinusLogProbMetric: 392.2082 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 490/1000
2023-10-03 08:16:42.132 
Epoch 490/1000 
	 loss: 388.8249, MinusLogProbMetric: 388.8249, val_loss: 391.6492, val_MinusLogProbMetric: 391.6492

Epoch 490: val_loss improved from 391.65863 to 391.64923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.8249 - MinusLogProbMetric: 388.8249 - val_loss: 391.6492 - val_MinusLogProbMetric: 391.6492 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 491/1000
2023-10-03 08:16:52.730 
Epoch 491/1000 
	 loss: 388.9183, MinusLogProbMetric: 388.9183, val_loss: 392.2610, val_MinusLogProbMetric: 392.2610

Epoch 491: val_loss did not improve from 391.64923
196/196 - 10s - loss: 388.9183 - MinusLogProbMetric: 388.9183 - val_loss: 392.2610 - val_MinusLogProbMetric: 392.2610 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 492/1000
2023-10-03 08:17:02.903 
Epoch 492/1000 
	 loss: 388.8176, MinusLogProbMetric: 388.8176, val_loss: 392.0351, val_MinusLogProbMetric: 392.0351

Epoch 492: val_loss did not improve from 391.64923
196/196 - 10s - loss: 388.8176 - MinusLogProbMetric: 388.8176 - val_loss: 392.0351 - val_MinusLogProbMetric: 392.0351 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 493/1000
2023-10-03 08:17:12.994 
Epoch 493/1000 
	 loss: 388.9532, MinusLogProbMetric: 388.9532, val_loss: 391.5611, val_MinusLogProbMetric: 391.5611

Epoch 493: val_loss improved from 391.64923 to 391.56110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.9532 - MinusLogProbMetric: 388.9532 - val_loss: 391.5611 - val_MinusLogProbMetric: 391.5611 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 494/1000
2023-10-03 08:17:23.216 
Epoch 494/1000 
	 loss: 388.8595, MinusLogProbMetric: 388.8595, val_loss: 391.6586, val_MinusLogProbMetric: 391.6586

Epoch 494: val_loss did not improve from 391.56110
196/196 - 10s - loss: 388.8595 - MinusLogProbMetric: 388.8595 - val_loss: 391.6586 - val_MinusLogProbMetric: 391.6586 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 495/1000
2023-10-03 08:17:33.348 
Epoch 495/1000 
	 loss: 388.7639, MinusLogProbMetric: 388.7639, val_loss: 392.1005, val_MinusLogProbMetric: 392.1005

Epoch 495: val_loss did not improve from 391.56110
196/196 - 10s - loss: 388.7639 - MinusLogProbMetric: 388.7639 - val_loss: 392.1005 - val_MinusLogProbMetric: 392.1005 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 496/1000
2023-10-03 08:17:43.456 
Epoch 496/1000 
	 loss: 388.8217, MinusLogProbMetric: 388.8217, val_loss: 391.7961, val_MinusLogProbMetric: 391.7961

Epoch 496: val_loss did not improve from 391.56110
196/196 - 10s - loss: 388.8217 - MinusLogProbMetric: 388.8217 - val_loss: 391.7961 - val_MinusLogProbMetric: 391.7961 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 497/1000
2023-10-03 08:17:53.497 
Epoch 497/1000 
	 loss: 388.7665, MinusLogProbMetric: 388.7665, val_loss: 391.5252, val_MinusLogProbMetric: 391.5252

Epoch 497: val_loss improved from 391.56110 to 391.52524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.7665 - MinusLogProbMetric: 388.7665 - val_loss: 391.5252 - val_MinusLogProbMetric: 391.5252 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 498/1000
2023-10-03 08:18:04.049 
Epoch 498/1000 
	 loss: 388.6518, MinusLogProbMetric: 388.6518, val_loss: 391.7906, val_MinusLogProbMetric: 391.7906

Epoch 498: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.6518 - MinusLogProbMetric: 388.6518 - val_loss: 391.7906 - val_MinusLogProbMetric: 391.7906 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 499/1000
2023-10-03 08:18:14.123 
Epoch 499/1000 
	 loss: 388.6689, MinusLogProbMetric: 388.6689, val_loss: 391.6332, val_MinusLogProbMetric: 391.6332

Epoch 499: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.6689 - MinusLogProbMetric: 388.6689 - val_loss: 391.6332 - val_MinusLogProbMetric: 391.6332 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 500/1000
2023-10-03 08:18:24.279 
Epoch 500/1000 
	 loss: 388.8184, MinusLogProbMetric: 388.8184, val_loss: 391.7620, val_MinusLogProbMetric: 391.7620

Epoch 500: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.8184 - MinusLogProbMetric: 388.8184 - val_loss: 391.7620 - val_MinusLogProbMetric: 391.7620 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 501/1000
2023-10-03 08:18:34.112 
Epoch 501/1000 
	 loss: 389.0816, MinusLogProbMetric: 389.0816, val_loss: 392.0947, val_MinusLogProbMetric: 392.0947

Epoch 501: val_loss did not improve from 391.52524
196/196 - 10s - loss: 389.0816 - MinusLogProbMetric: 389.0816 - val_loss: 392.0947 - val_MinusLogProbMetric: 392.0947 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 502/1000
2023-10-03 08:18:44.086 
Epoch 502/1000 
	 loss: 388.8599, MinusLogProbMetric: 388.8599, val_loss: 391.8275, val_MinusLogProbMetric: 391.8275

Epoch 502: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.8599 - MinusLogProbMetric: 388.8599 - val_loss: 391.8275 - val_MinusLogProbMetric: 391.8275 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 503/1000
2023-10-03 08:18:54.093 
Epoch 503/1000 
	 loss: 388.8075, MinusLogProbMetric: 388.8075, val_loss: 392.4622, val_MinusLogProbMetric: 392.4622

Epoch 503: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.8075 - MinusLogProbMetric: 388.8075 - val_loss: 392.4622 - val_MinusLogProbMetric: 392.4622 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 504/1000
2023-10-03 08:19:04.128 
Epoch 504/1000 
	 loss: 388.8167, MinusLogProbMetric: 388.8167, val_loss: 392.3359, val_MinusLogProbMetric: 392.3359

Epoch 504: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.8167 - MinusLogProbMetric: 388.8167 - val_loss: 392.3359 - val_MinusLogProbMetric: 392.3359 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 505/1000
2023-10-03 08:19:13.992 
Epoch 505/1000 
	 loss: 388.8372, MinusLogProbMetric: 388.8372, val_loss: 391.8026, val_MinusLogProbMetric: 391.8026

Epoch 505: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.8372 - MinusLogProbMetric: 388.8372 - val_loss: 391.8026 - val_MinusLogProbMetric: 391.8026 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 506/1000
2023-10-03 08:19:24.010 
Epoch 506/1000 
	 loss: 388.7048, MinusLogProbMetric: 388.7048, val_loss: 391.9565, val_MinusLogProbMetric: 391.9565

Epoch 506: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.7048 - MinusLogProbMetric: 388.7048 - val_loss: 391.9565 - val_MinusLogProbMetric: 391.9565 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 507/1000
2023-10-03 08:19:33.842 
Epoch 507/1000 
	 loss: 388.7802, MinusLogProbMetric: 388.7802, val_loss: 391.6533, val_MinusLogProbMetric: 391.6533

Epoch 507: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.7802 - MinusLogProbMetric: 388.7802 - val_loss: 391.6533 - val_MinusLogProbMetric: 391.6533 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 508/1000
2023-10-03 08:19:43.431 
Epoch 508/1000 
	 loss: 388.8591, MinusLogProbMetric: 388.8591, val_loss: 392.1675, val_MinusLogProbMetric: 392.1675

Epoch 508: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.8591 - MinusLogProbMetric: 388.8591 - val_loss: 392.1675 - val_MinusLogProbMetric: 392.1675 - lr: 8.3333e-05 - 10s/epoch - 49ms/step
Epoch 509/1000
2023-10-03 08:19:53.190 
Epoch 509/1000 
	 loss: 388.9435, MinusLogProbMetric: 388.9435, val_loss: 391.9108, val_MinusLogProbMetric: 391.9108

Epoch 509: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.9435 - MinusLogProbMetric: 388.9435 - val_loss: 391.9108 - val_MinusLogProbMetric: 391.9108 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 510/1000
2023-10-03 08:20:03.062 
Epoch 510/1000 
	 loss: 388.6921, MinusLogProbMetric: 388.6921, val_loss: 391.7930, val_MinusLogProbMetric: 391.7930

Epoch 510: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.6921 - MinusLogProbMetric: 388.6921 - val_loss: 391.7930 - val_MinusLogProbMetric: 391.7930 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 511/1000
2023-10-03 08:20:12.988 
Epoch 511/1000 
	 loss: 388.6530, MinusLogProbMetric: 388.6530, val_loss: 391.9914, val_MinusLogProbMetric: 391.9914

Epoch 511: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.6530 - MinusLogProbMetric: 388.6530 - val_loss: 391.9914 - val_MinusLogProbMetric: 391.9914 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 512/1000
2023-10-03 08:20:22.910 
Epoch 512/1000 
	 loss: 388.5948, MinusLogProbMetric: 388.5948, val_loss: 391.8729, val_MinusLogProbMetric: 391.8729

Epoch 512: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.5948 - MinusLogProbMetric: 388.5948 - val_loss: 391.8729 - val_MinusLogProbMetric: 391.8729 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 513/1000
2023-10-03 08:20:32.893 
Epoch 513/1000 
	 loss: 388.5286, MinusLogProbMetric: 388.5286, val_loss: 391.7935, val_MinusLogProbMetric: 391.7935

Epoch 513: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.5286 - MinusLogProbMetric: 388.5286 - val_loss: 391.7935 - val_MinusLogProbMetric: 391.7935 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 514/1000
2023-10-03 08:20:42.929 
Epoch 514/1000 
	 loss: 388.6917, MinusLogProbMetric: 388.6917, val_loss: 391.8584, val_MinusLogProbMetric: 391.8584

Epoch 514: val_loss did not improve from 391.52524
196/196 - 10s - loss: 388.6917 - MinusLogProbMetric: 388.6917 - val_loss: 391.8584 - val_MinusLogProbMetric: 391.8584 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 515/1000
2023-10-03 08:20:52.863 
Epoch 515/1000 
	 loss: 388.5215, MinusLogProbMetric: 388.5215, val_loss: 391.4028, val_MinusLogProbMetric: 391.4028

Epoch 515: val_loss improved from 391.52524 to 391.40283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_347/weights/best_weights.h5
196/196 - 10s - loss: 388.5215 - MinusLogProbMetric: 388.5215 - val_loss: 391.4028 - val_MinusLogProbMetric: 391.4028 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 516/1000
2023-10-03 08:21:03.212 
Epoch 516/1000 
	 loss: 388.6771, MinusLogProbMetric: 388.6771, val_loss: 391.5996, val_MinusLogProbMetric: 391.5996

Epoch 516: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.6771 - MinusLogProbMetric: 388.6771 - val_loss: 391.5996 - val_MinusLogProbMetric: 391.5996 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 517/1000
2023-10-03 08:21:13.251 
Epoch 517/1000 
	 loss: 388.5669, MinusLogProbMetric: 388.5669, val_loss: 391.5279, val_MinusLogProbMetric: 391.5279

Epoch 517: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.5669 - MinusLogProbMetric: 388.5669 - val_loss: 391.5279 - val_MinusLogProbMetric: 391.5279 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 518/1000
2023-10-03 08:21:23.140 
Epoch 518/1000 
	 loss: 388.6890, MinusLogProbMetric: 388.6890, val_loss: 391.8789, val_MinusLogProbMetric: 391.8789

Epoch 518: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.6890 - MinusLogProbMetric: 388.6890 - val_loss: 391.8789 - val_MinusLogProbMetric: 391.8789 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 519/1000
2023-10-03 08:21:33.210 
Epoch 519/1000 
	 loss: 388.7603, MinusLogProbMetric: 388.7603, val_loss: 392.6837, val_MinusLogProbMetric: 392.6837

Epoch 519: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7603 - MinusLogProbMetric: 388.7603 - val_loss: 392.6837 - val_MinusLogProbMetric: 392.6837 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 520/1000
2023-10-03 08:21:43.320 
Epoch 520/1000 
	 loss: 388.6992, MinusLogProbMetric: 388.6992, val_loss: 391.7219, val_MinusLogProbMetric: 391.7219

Epoch 520: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.6992 - MinusLogProbMetric: 388.6992 - val_loss: 391.7219 - val_MinusLogProbMetric: 391.7219 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 521/1000
2023-10-03 08:21:53.361 
Epoch 521/1000 
	 loss: 388.5580, MinusLogProbMetric: 388.5580, val_loss: 392.5055, val_MinusLogProbMetric: 392.5055

Epoch 521: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.5580 - MinusLogProbMetric: 388.5580 - val_loss: 392.5055 - val_MinusLogProbMetric: 392.5055 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 522/1000
2023-10-03 08:22:03.383 
Epoch 522/1000 
	 loss: 388.7769, MinusLogProbMetric: 388.7769, val_loss: 392.2879, val_MinusLogProbMetric: 392.2879

Epoch 522: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7769 - MinusLogProbMetric: 388.7769 - val_loss: 392.2879 - val_MinusLogProbMetric: 392.2879 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 523/1000
2023-10-03 08:22:13.664 
Epoch 523/1000 
	 loss: 388.7805, MinusLogProbMetric: 388.7805, val_loss: 392.8583, val_MinusLogProbMetric: 392.8583

Epoch 523: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7805 - MinusLogProbMetric: 388.7805 - val_loss: 392.8583 - val_MinusLogProbMetric: 392.8583 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 524/1000
2023-10-03 08:22:23.819 
Epoch 524/1000 
	 loss: 388.7852, MinusLogProbMetric: 388.7852, val_loss: 391.6664, val_MinusLogProbMetric: 391.6664

Epoch 524: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7852 - MinusLogProbMetric: 388.7852 - val_loss: 391.6664 - val_MinusLogProbMetric: 391.6664 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 525/1000
2023-10-03 08:22:33.706 
Epoch 525/1000 
	 loss: 388.8636, MinusLogProbMetric: 388.8636, val_loss: 391.6672, val_MinusLogProbMetric: 391.6672

Epoch 525: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.8636 - MinusLogProbMetric: 388.8636 - val_loss: 391.6672 - val_MinusLogProbMetric: 391.6672 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 526/1000
2023-10-03 08:22:43.412 
Epoch 526/1000 
	 loss: 388.7581, MinusLogProbMetric: 388.7581, val_loss: 391.5837, val_MinusLogProbMetric: 391.5837

Epoch 526: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7581 - MinusLogProbMetric: 388.7581 - val_loss: 391.5837 - val_MinusLogProbMetric: 391.5837 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 527/1000
2023-10-03 08:22:53.579 
Epoch 527/1000 
	 loss: 388.4910, MinusLogProbMetric: 388.4910, val_loss: 392.0032, val_MinusLogProbMetric: 392.0032

Epoch 527: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.4910 - MinusLogProbMetric: 388.4910 - val_loss: 392.0032 - val_MinusLogProbMetric: 392.0032 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 528/1000
2023-10-03 08:23:03.623 
Epoch 528/1000 
	 loss: 388.7978, MinusLogProbMetric: 388.7978, val_loss: 391.9905, val_MinusLogProbMetric: 391.9905

Epoch 528: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7978 - MinusLogProbMetric: 388.7978 - val_loss: 391.9905 - val_MinusLogProbMetric: 391.9905 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 529/1000
2023-10-03 08:23:13.691 
Epoch 529/1000 
	 loss: 388.6333, MinusLogProbMetric: 388.6333, val_loss: 391.5775, val_MinusLogProbMetric: 391.5775

Epoch 529: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.6333 - MinusLogProbMetric: 388.6333 - val_loss: 391.5775 - val_MinusLogProbMetric: 391.5775 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 530/1000
2023-10-03 08:23:23.553 
Epoch 530/1000 
	 loss: 388.7243, MinusLogProbMetric: 388.7243, val_loss: 393.1476, val_MinusLogProbMetric: 393.1476

Epoch 530: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7243 - MinusLogProbMetric: 388.7243 - val_loss: 393.1476 - val_MinusLogProbMetric: 393.1476 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 531/1000
2023-10-03 08:23:33.500 
Epoch 531/1000 
	 loss: 388.9696, MinusLogProbMetric: 388.9696, val_loss: 391.7060, val_MinusLogProbMetric: 391.7060

Epoch 531: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.9696 - MinusLogProbMetric: 388.9696 - val_loss: 391.7060 - val_MinusLogProbMetric: 391.7060 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 532/1000
2023-10-03 08:23:43.556 
Epoch 532/1000 
	 loss: 388.5284, MinusLogProbMetric: 388.5284, val_loss: 391.4136, val_MinusLogProbMetric: 391.4136

Epoch 532: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.5284 - MinusLogProbMetric: 388.5284 - val_loss: 391.4136 - val_MinusLogProbMetric: 391.4136 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 533/1000
2023-10-03 08:23:53.606 
Epoch 533/1000 
	 loss: 388.4660, MinusLogProbMetric: 388.4660, val_loss: 391.7709, val_MinusLogProbMetric: 391.7709

Epoch 533: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.4660 - MinusLogProbMetric: 388.4660 - val_loss: 391.7709 - val_MinusLogProbMetric: 391.7709 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 534/1000
2023-10-03 08:24:03.453 
Epoch 534/1000 
	 loss: 388.5520, MinusLogProbMetric: 388.5520, val_loss: 391.7262, val_MinusLogProbMetric: 391.7262

Epoch 534: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.5520 - MinusLogProbMetric: 388.5520 - val_loss: 391.7262 - val_MinusLogProbMetric: 391.7262 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 535/1000
2023-10-03 08:24:13.343 
Epoch 535/1000 
	 loss: 388.6014, MinusLogProbMetric: 388.6014, val_loss: 392.1356, val_MinusLogProbMetric: 392.1356

Epoch 535: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.6014 - MinusLogProbMetric: 388.6014 - val_loss: 392.1356 - val_MinusLogProbMetric: 392.1356 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 536/1000
2023-10-03 08:24:23.375 
Epoch 536/1000 
	 loss: 388.6653, MinusLogProbMetric: 388.6653, val_loss: 392.0827, val_MinusLogProbMetric: 392.0827

Epoch 536: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.6653 - MinusLogProbMetric: 388.6653 - val_loss: 392.0827 - val_MinusLogProbMetric: 392.0827 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 537/1000
2023-10-03 08:24:33.325 
Epoch 537/1000 
	 loss: 388.9186, MinusLogProbMetric: 388.9186, val_loss: 392.2671, val_MinusLogProbMetric: 392.2671

Epoch 537: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.9186 - MinusLogProbMetric: 388.9186 - val_loss: 392.2671 - val_MinusLogProbMetric: 392.2671 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 538/1000
2023-10-03 08:24:43.438 
Epoch 538/1000 
	 loss: 388.5257, MinusLogProbMetric: 388.5257, val_loss: 392.0397, val_MinusLogProbMetric: 392.0397

Epoch 538: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.5257 - MinusLogProbMetric: 388.5257 - val_loss: 392.0397 - val_MinusLogProbMetric: 392.0397 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 539/1000
2023-10-03 08:24:53.538 
Epoch 539/1000 
	 loss: 388.7896, MinusLogProbMetric: 388.7896, val_loss: 391.9992, val_MinusLogProbMetric: 391.9992

Epoch 539: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7896 - MinusLogProbMetric: 388.7896 - val_loss: 391.9992 - val_MinusLogProbMetric: 391.9992 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 540/1000
2023-10-03 08:25:03.641 
Epoch 540/1000 
	 loss: 388.7065, MinusLogProbMetric: 388.7065, val_loss: 391.8088, val_MinusLogProbMetric: 391.8088

Epoch 540: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7065 - MinusLogProbMetric: 388.7065 - val_loss: 391.8088 - val_MinusLogProbMetric: 391.8088 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 541/1000
2023-10-03 08:25:13.280 
Epoch 541/1000 
	 loss: 388.5405, MinusLogProbMetric: 388.5405, val_loss: 391.4793, val_MinusLogProbMetric: 391.4793

Epoch 541: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.5405 - MinusLogProbMetric: 388.5405 - val_loss: 391.4793 - val_MinusLogProbMetric: 391.4793 - lr: 8.3333e-05 - 10s/epoch - 49ms/step
Epoch 542/1000
2023-10-03 08:25:23.360 
Epoch 542/1000 
	 loss: 388.7687, MinusLogProbMetric: 388.7687, val_loss: 392.6211, val_MinusLogProbMetric: 392.6211

Epoch 542: val_loss did not improve from 391.40283
196/196 - 10s - loss: 388.7687 - MinusLogProbMetric: 388.7687 - val_loss: 392.6211 - val_MinusLogProbMetric: 392.6211 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 543/1000
2023-10-03 08:25:33.433 
Epoch 543/1000 
	 loss: 680.7737, MinusLogProbMetric: 680.7737, val_loss: 592.8519, val_MinusLogProbMetric: 592.8519

Epoch 543: val_loss did not improve from 391.40283
196/196 - 10s - loss: 680.7737 - MinusLogProbMetric: 680.7737 - val_loss: 592.8519 - val_MinusLogProbMetric: 592.8519 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 544/1000
2023-10-03 08:25:43.547 
Epoch 544/1000 
	 loss: 529.4238, MinusLogProbMetric: 529.4238, val_loss: 495.0284, val_MinusLogProbMetric: 495.0284

Epoch 544: val_loss did not improve from 391.40283
196/196 - 10s - loss: 529.4238 - MinusLogProbMetric: 529.4238 - val_loss: 495.0284 - val_MinusLogProbMetric: 495.0284 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 545/1000
2023-10-03 08:25:53.662 
Epoch 545/1000 
	 loss: 478.0265, MinusLogProbMetric: 478.0265, val_loss: 467.3626, val_MinusLogProbMetric: 467.3626

Epoch 545: val_loss did not improve from 391.40283
196/196 - 10s - loss: 478.0265 - MinusLogProbMetric: 478.0265 - val_loss: 467.3626 - val_MinusLogProbMetric: 467.3626 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 546/1000
2023-10-03 08:26:03.699 
Epoch 546/1000 
	 loss: 457.9233, MinusLogProbMetric: 457.9233, val_loss: 452.9329, val_MinusLogProbMetric: 452.9329

Epoch 546: val_loss did not improve from 391.40283
196/196 - 10s - loss: 457.9233 - MinusLogProbMetric: 457.9233 - val_loss: 452.9329 - val_MinusLogProbMetric: 452.9329 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 547/1000
2023-10-03 08:26:13.846 
Epoch 547/1000 
	 loss: 446.2191, MinusLogProbMetric: 446.2191, val_loss: 443.5087, val_MinusLogProbMetric: 443.5087

Epoch 547: val_loss did not improve from 391.40283
196/196 - 10s - loss: 446.2191 - MinusLogProbMetric: 446.2191 - val_loss: 443.5087 - val_MinusLogProbMetric: 443.5087 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 548/1000
2023-10-03 08:26:23.899 
Epoch 548/1000 
	 loss: 438.2180, MinusLogProbMetric: 438.2180, val_loss: 436.9927, val_MinusLogProbMetric: 436.9927

Epoch 548: val_loss did not improve from 391.40283
196/196 - 10s - loss: 438.2180 - MinusLogProbMetric: 438.2180 - val_loss: 436.9927 - val_MinusLogProbMetric: 436.9927 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 549/1000
2023-10-03 08:26:34.177 
Epoch 549/1000 
	 loss: 432.4948, MinusLogProbMetric: 432.4948, val_loss: 432.0849, val_MinusLogProbMetric: 432.0849

Epoch 549: val_loss did not improve from 391.40283
196/196 - 10s - loss: 432.4948 - MinusLogProbMetric: 432.4948 - val_loss: 432.0849 - val_MinusLogProbMetric: 432.0849 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 550/1000
2023-10-03 08:26:44.232 
Epoch 550/1000 
	 loss: 428.1822, MinusLogProbMetric: 428.1822, val_loss: 428.3568, val_MinusLogProbMetric: 428.3568

Epoch 550: val_loss did not improve from 391.40283
196/196 - 10s - loss: 428.1822 - MinusLogProbMetric: 428.1822 - val_loss: 428.3568 - val_MinusLogProbMetric: 428.3568 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 551/1000
2023-10-03 08:26:54.567 
Epoch 551/1000 
	 loss: 424.6995, MinusLogProbMetric: 424.6995, val_loss: 425.2478, val_MinusLogProbMetric: 425.2478

Epoch 551: val_loss did not improve from 391.40283
196/196 - 10s - loss: 424.6995 - MinusLogProbMetric: 424.6995 - val_loss: 425.2478 - val_MinusLogProbMetric: 425.2478 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 552/1000
2023-10-03 08:27:04.908 
Epoch 552/1000 
	 loss: 421.8811, MinusLogProbMetric: 421.8811, val_loss: 422.9041, val_MinusLogProbMetric: 422.9041

Epoch 552: val_loss did not improve from 391.40283
196/196 - 10s - loss: 421.8811 - MinusLogProbMetric: 421.8811 - val_loss: 422.9041 - val_MinusLogProbMetric: 422.9041 - lr: 8.3333e-05 - 10s/epoch - 53ms/step
Epoch 553/1000
2023-10-03 08:27:14.936 
Epoch 553/1000 
	 loss: 419.5850, MinusLogProbMetric: 419.5850, val_loss: 420.6716, val_MinusLogProbMetric: 420.6716

Epoch 553: val_loss did not improve from 391.40283
196/196 - 10s - loss: 419.5850 - MinusLogProbMetric: 419.5850 - val_loss: 420.6716 - val_MinusLogProbMetric: 420.6716 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 554/1000
2023-10-03 08:27:24.900 
Epoch 554/1000 
	 loss: 417.6168, MinusLogProbMetric: 417.6168, val_loss: 418.9674, val_MinusLogProbMetric: 418.9674

Epoch 554: val_loss did not improve from 391.40283
196/196 - 10s - loss: 417.6168 - MinusLogProbMetric: 417.6168 - val_loss: 418.9674 - val_MinusLogProbMetric: 418.9674 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 555/1000
2023-10-03 08:27:35.036 
Epoch 555/1000 
	 loss: 415.9516, MinusLogProbMetric: 415.9516, val_loss: 417.5639, val_MinusLogProbMetric: 417.5639

Epoch 555: val_loss did not improve from 391.40283
196/196 - 10s - loss: 415.9516 - MinusLogProbMetric: 415.9516 - val_loss: 417.5639 - val_MinusLogProbMetric: 417.5639 - lr: 8.3333e-05 - 10s/epoch - 52ms/step
Epoch 556/1000
2023-10-03 08:27:45.035 
Epoch 556/1000 
	 loss: 414.4955, MinusLogProbMetric: 414.4955, val_loss: 416.1371, val_MinusLogProbMetric: 416.1371

Epoch 556: val_loss did not improve from 391.40283
196/196 - 10s - loss: 414.4955 - MinusLogProbMetric: 414.4955 - val_loss: 416.1371 - val_MinusLogProbMetric: 416.1371 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 557/1000
2023-10-03 08:27:54.912 
Epoch 557/1000 
	 loss: 413.1585, MinusLogProbMetric: 413.1585, val_loss: 414.8884, val_MinusLogProbMetric: 414.8884

Epoch 557: val_loss did not improve from 391.40283
196/196 - 10s - loss: 413.1585 - MinusLogProbMetric: 413.1585 - val_loss: 414.8884 - val_MinusLogProbMetric: 414.8884 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 558/1000
2023-10-03 08:28:04.830 
Epoch 558/1000 
	 loss: 411.9883, MinusLogProbMetric: 411.9883, val_loss: 413.9299, val_MinusLogProbMetric: 413.9299

Epoch 558: val_loss did not improve from 391.40283
196/196 - 10s - loss: 411.9883 - MinusLogProbMetric: 411.9883 - val_loss: 413.9299 - val_MinusLogProbMetric: 413.9299 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 559/1000
2023-10-03 08:28:14.904 
Epoch 559/1000 
	 loss: 410.9520, MinusLogProbMetric: 410.9520, val_loss: 413.1072, val_MinusLogProbMetric: 413.1072

Epoch 559: val_loss did not improve from 391.40283
196/196 - 10s - loss: 410.9520 - MinusLogProbMetric: 410.9520 - val_loss: 413.1072 - val_MinusLogProbMetric: 413.1072 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 560/1000
2023-10-03 08:28:24.746 
Epoch 560/1000 
	 loss: 410.0374, MinusLogProbMetric: 410.0374, val_loss: 412.2006, val_MinusLogProbMetric: 412.2006

Epoch 560: val_loss did not improve from 391.40283
196/196 - 10s - loss: 410.0374 - MinusLogProbMetric: 410.0374 - val_loss: 412.2006 - val_MinusLogProbMetric: 412.2006 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 561/1000
2023-10-03 08:28:34.487 
Epoch 561/1000 
	 loss: 409.1531, MinusLogProbMetric: 409.1531, val_loss: 411.1378, val_MinusLogProbMetric: 411.1378

Epoch 561: val_loss did not improve from 391.40283
196/196 - 10s - loss: 409.1531 - MinusLogProbMetric: 409.1531 - val_loss: 411.1378 - val_MinusLogProbMetric: 411.1378 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 562/1000
2023-10-03 08:28:44.430 
Epoch 562/1000 
	 loss: 408.4024, MinusLogProbMetric: 408.4024, val_loss: 410.6478, val_MinusLogProbMetric: 410.6478

Epoch 562: val_loss did not improve from 391.40283
196/196 - 10s - loss: 408.4024 - MinusLogProbMetric: 408.4024 - val_loss: 410.6478 - val_MinusLogProbMetric: 410.6478 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 563/1000
2023-10-03 08:28:54.512 
Epoch 563/1000 
	 loss: 407.7244, MinusLogProbMetric: 407.7244, val_loss: 409.9577, val_MinusLogProbMetric: 409.9577

Epoch 563: val_loss did not improve from 391.40283
196/196 - 10s - loss: 407.7244 - MinusLogProbMetric: 407.7244 - val_loss: 409.9577 - val_MinusLogProbMetric: 409.9577 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 564/1000
2023-10-03 08:29:04.457 
Epoch 564/1000 
	 loss: 407.1017, MinusLogProbMetric: 407.1017, val_loss: 409.2873, val_MinusLogProbMetric: 409.2873

Epoch 564: val_loss did not improve from 391.40283
196/196 - 10s - loss: 407.1017 - MinusLogProbMetric: 407.1017 - val_loss: 409.2873 - val_MinusLogProbMetric: 409.2873 - lr: 8.3333e-05 - 10s/epoch - 51ms/step
Epoch 565/1000
2023-10-03 08:29:14.256 
Epoch 565/1000 
	 loss: 406.5904, MinusLogProbMetric: 406.5904, val_loss: 408.8119, val_MinusLogProbMetric: 408.8119

Epoch 565: val_loss did not improve from 391.40283
196/196 - 10s - loss: 406.5904 - MinusLogProbMetric: 406.5904 - val_loss: 408.8119 - val_MinusLogProbMetric: 408.8119 - lr: 8.3333e-05 - 10s/epoch - 50ms/step
Epoch 566/1000
2023-10-03 08:29:24.326 
Epoch 566/1000 
	 loss: 405.9240, MinusLogProbMetric: 405.9240, val_loss: 408.3492, val_MinusLogProbMetric: 408.3492

Epoch 566: val_loss did not improve from 391.40283
196/196 - 10s - loss: 405.9240 - MinusLogProbMetric: 405.9240 - val_loss: 408.3492 - val_MinusLogProbMetric: 408.3492 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 567/1000
2023-10-03 08:29:34.174 
Epoch 567/1000 
	 loss: 405.6514, MinusLogProbMetric: 405.6514, val_loss: 407.9810, val_MinusLogProbMetric: 407.9810

Epoch 567: val_loss did not improve from 391.40283
196/196 - 10s - loss: 405.6514 - MinusLogProbMetric: 405.6514 - val_loss: 407.9810 - val_MinusLogProbMetric: 407.9810 - lr: 4.1667e-05 - 10s/epoch - 50ms/step
Epoch 568/1000
2023-10-03 08:29:44.187 
Epoch 568/1000 
	 loss: 405.3285, MinusLogProbMetric: 405.3285, val_loss: 407.8325, val_MinusLogProbMetric: 407.8325

Epoch 568: val_loss did not improve from 391.40283
196/196 - 10s - loss: 405.3285 - MinusLogProbMetric: 405.3285 - val_loss: 407.8325 - val_MinusLogProbMetric: 407.8325 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 569/1000
2023-10-03 08:29:54.205 
Epoch 569/1000 
	 loss: 405.0499, MinusLogProbMetric: 405.0499, val_loss: 407.5158, val_MinusLogProbMetric: 407.5158

Epoch 569: val_loss did not improve from 391.40283
196/196 - 10s - loss: 405.0499 - MinusLogProbMetric: 405.0499 - val_loss: 407.5158 - val_MinusLogProbMetric: 407.5158 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 570/1000
2023-10-03 08:30:04.172 
Epoch 570/1000 
	 loss: 404.7953, MinusLogProbMetric: 404.7953, val_loss: 407.2946, val_MinusLogProbMetric: 407.2946

Epoch 570: val_loss did not improve from 391.40283
196/196 - 10s - loss: 404.7953 - MinusLogProbMetric: 404.7953 - val_loss: 407.2946 - val_MinusLogProbMetric: 407.2946 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 571/1000
2023-10-03 08:30:14.089 
Epoch 571/1000 
	 loss: 404.5526, MinusLogProbMetric: 404.5526, val_loss: 407.0647, val_MinusLogProbMetric: 407.0647

Epoch 571: val_loss did not improve from 391.40283
196/196 - 10s - loss: 404.5526 - MinusLogProbMetric: 404.5526 - val_loss: 407.0647 - val_MinusLogProbMetric: 407.0647 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 572/1000
2023-10-03 08:30:24.037 
Epoch 572/1000 
	 loss: 404.3191, MinusLogProbMetric: 404.3191, val_loss: 406.7216, val_MinusLogProbMetric: 406.7216

Epoch 572: val_loss did not improve from 391.40283
196/196 - 10s - loss: 404.3191 - MinusLogProbMetric: 404.3191 - val_loss: 406.7216 - val_MinusLogProbMetric: 406.7216 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 573/1000
2023-10-03 08:30:34.014 
Epoch 573/1000 
	 loss: 404.0874, MinusLogProbMetric: 404.0874, val_loss: 406.6156, val_MinusLogProbMetric: 406.6156

Epoch 573: val_loss did not improve from 391.40283
196/196 - 10s - loss: 404.0874 - MinusLogProbMetric: 404.0874 - val_loss: 406.6156 - val_MinusLogProbMetric: 406.6156 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 574/1000
2023-10-03 08:30:43.884 
Epoch 574/1000 
	 loss: 403.8759, MinusLogProbMetric: 403.8759, val_loss: 406.3577, val_MinusLogProbMetric: 406.3577

Epoch 574: val_loss did not improve from 391.40283
196/196 - 10s - loss: 403.8759 - MinusLogProbMetric: 403.8759 - val_loss: 406.3577 - val_MinusLogProbMetric: 406.3577 - lr: 4.1667e-05 - 10s/epoch - 50ms/step
Epoch 575/1000
2023-10-03 08:30:53.879 
Epoch 575/1000 
	 loss: 403.6756, MinusLogProbMetric: 403.6756, val_loss: 406.1842, val_MinusLogProbMetric: 406.1842

Epoch 575: val_loss did not improve from 391.40283
196/196 - 10s - loss: 403.6756 - MinusLogProbMetric: 403.6756 - val_loss: 406.1842 - val_MinusLogProbMetric: 406.1842 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 576/1000
2023-10-03 08:31:03.882 
Epoch 576/1000 
	 loss: 403.4771, MinusLogProbMetric: 403.4771, val_loss: 406.0444, val_MinusLogProbMetric: 406.0444

Epoch 576: val_loss did not improve from 391.40283
196/196 - 10s - loss: 403.4771 - MinusLogProbMetric: 403.4771 - val_loss: 406.0444 - val_MinusLogProbMetric: 406.0444 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 577/1000
2023-10-03 08:31:13.553 
Epoch 577/1000 
	 loss: 403.2787, MinusLogProbMetric: 403.2787, val_loss: 405.8200, val_MinusLogProbMetric: 405.8200

Epoch 577: val_loss did not improve from 391.40283
196/196 - 10s - loss: 403.2787 - MinusLogProbMetric: 403.2787 - val_loss: 405.8200 - val_MinusLogProbMetric: 405.8200 - lr: 4.1667e-05 - 10s/epoch - 49ms/step
Epoch 578/1000
2023-10-03 08:31:23.501 
Epoch 578/1000 
	 loss: 403.1044, MinusLogProbMetric: 403.1044, val_loss: 405.5723, val_MinusLogProbMetric: 405.5723

Epoch 578: val_loss did not improve from 391.40283
196/196 - 10s - loss: 403.1044 - MinusLogProbMetric: 403.1044 - val_loss: 405.5723 - val_MinusLogProbMetric: 405.5723 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 579/1000
2023-10-03 08:31:33.355 
Epoch 579/1000 
	 loss: 402.9141, MinusLogProbMetric: 402.9141, val_loss: 405.4320, val_MinusLogProbMetric: 405.4320

Epoch 579: val_loss did not improve from 391.40283
196/196 - 10s - loss: 402.9141 - MinusLogProbMetric: 402.9141 - val_loss: 405.4320 - val_MinusLogProbMetric: 405.4320 - lr: 4.1667e-05 - 10s/epoch - 50ms/step
Epoch 580/1000
2023-10-03 08:31:43.325 
Epoch 580/1000 
	 loss: 402.7534, MinusLogProbMetric: 402.7534, val_loss: 405.2202, val_MinusLogProbMetric: 405.2202

Epoch 580: val_loss did not improve from 391.40283
196/196 - 10s - loss: 402.7534 - MinusLogProbMetric: 402.7534 - val_loss: 405.2202 - val_MinusLogProbMetric: 405.2202 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 581/1000
2023-10-03 08:31:53.550 
Epoch 581/1000 
	 loss: 402.5793, MinusLogProbMetric: 402.5793, val_loss: 405.0655, val_MinusLogProbMetric: 405.0655

Epoch 581: val_loss did not improve from 391.40283
196/196 - 10s - loss: 402.5793 - MinusLogProbMetric: 402.5793 - val_loss: 405.0655 - val_MinusLogProbMetric: 405.0655 - lr: 4.1667e-05 - 10s/epoch - 52ms/step
Epoch 582/1000
2023-10-03 08:32:03.444 
Epoch 582/1000 
	 loss: 402.4242, MinusLogProbMetric: 402.4242, val_loss: 404.9788, val_MinusLogProbMetric: 404.9788

Epoch 582: val_loss did not improve from 391.40283
196/196 - 10s - loss: 402.4242 - MinusLogProbMetric: 402.4242 - val_loss: 404.9788 - val_MinusLogProbMetric: 404.9788 - lr: 4.1667e-05 - 10s/epoch - 50ms/step
Epoch 583/1000
2023-10-03 08:32:13.444 
Epoch 583/1000 
	 loss: 402.2667, MinusLogProbMetric: 402.2667, val_loss: 404.8113, val_MinusLogProbMetric: 404.8113

Epoch 583: val_loss did not improve from 391.40283
196/196 - 10s - loss: 402.2667 - MinusLogProbMetric: 402.2667 - val_loss: 404.8113 - val_MinusLogProbMetric: 404.8113 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 584/1000
2023-10-03 08:32:23.390 
Epoch 584/1000 
	 loss: 402.1129, MinusLogProbMetric: 402.1129, val_loss: 404.6693, val_MinusLogProbMetric: 404.6693

Epoch 584: val_loss did not improve from 391.40283
196/196 - 10s - loss: 402.1129 - MinusLogProbMetric: 402.1129 - val_loss: 404.6693 - val_MinusLogProbMetric: 404.6693 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 585/1000
2023-10-03 08:32:33.299 
Epoch 585/1000 
	 loss: 401.9675, MinusLogProbMetric: 401.9675, val_loss: 404.4487, val_MinusLogProbMetric: 404.4487

Epoch 585: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.9675 - MinusLogProbMetric: 401.9675 - val_loss: 404.4487 - val_MinusLogProbMetric: 404.4487 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 586/1000
2023-10-03 08:32:43.332 
Epoch 586/1000 
	 loss: 401.8230, MinusLogProbMetric: 401.8230, val_loss: 404.3446, val_MinusLogProbMetric: 404.3446

Epoch 586: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.8230 - MinusLogProbMetric: 401.8230 - val_loss: 404.3446 - val_MinusLogProbMetric: 404.3446 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 587/1000
2023-10-03 08:32:53.324 
Epoch 587/1000 
	 loss: 401.7371, MinusLogProbMetric: 401.7371, val_loss: 404.3834, val_MinusLogProbMetric: 404.3834

Epoch 587: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.7371 - MinusLogProbMetric: 401.7371 - val_loss: 404.3834 - val_MinusLogProbMetric: 404.3834 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 588/1000
2023-10-03 08:33:03.384 
Epoch 588/1000 
	 loss: 401.5824, MinusLogProbMetric: 401.5824, val_loss: 404.0578, val_MinusLogProbMetric: 404.0578

Epoch 588: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.5824 - MinusLogProbMetric: 401.5824 - val_loss: 404.0578 - val_MinusLogProbMetric: 404.0578 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 589/1000
2023-10-03 08:33:13.453 
Epoch 589/1000 
	 loss: 401.4182, MinusLogProbMetric: 401.4182, val_loss: 403.9792, val_MinusLogProbMetric: 403.9792

Epoch 589: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.4182 - MinusLogProbMetric: 401.4182 - val_loss: 403.9792 - val_MinusLogProbMetric: 403.9792 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 590/1000
2023-10-03 08:33:23.507 
Epoch 590/1000 
	 loss: 401.3397, MinusLogProbMetric: 401.3397, val_loss: 403.8165, val_MinusLogProbMetric: 403.8165

Epoch 590: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.3397 - MinusLogProbMetric: 401.3397 - val_loss: 403.8165 - val_MinusLogProbMetric: 403.8165 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 591/1000
2023-10-03 08:33:33.437 
Epoch 591/1000 
	 loss: 401.1651, MinusLogProbMetric: 401.1651, val_loss: 403.7162, val_MinusLogProbMetric: 403.7162

Epoch 591: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.1651 - MinusLogProbMetric: 401.1651 - val_loss: 403.7162 - val_MinusLogProbMetric: 403.7162 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 592/1000
2023-10-03 08:33:43.505 
Epoch 592/1000 
	 loss: 401.0253, MinusLogProbMetric: 401.0253, val_loss: 403.5348, val_MinusLogProbMetric: 403.5348

Epoch 592: val_loss did not improve from 391.40283
196/196 - 10s - loss: 401.0253 - MinusLogProbMetric: 401.0253 - val_loss: 403.5348 - val_MinusLogProbMetric: 403.5348 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 593/1000
2023-10-03 08:33:53.581 
Epoch 593/1000 
	 loss: 400.9134, MinusLogProbMetric: 400.9134, val_loss: 403.3985, val_MinusLogProbMetric: 403.3985

Epoch 593: val_loss did not improve from 391.40283
196/196 - 10s - loss: 400.9134 - MinusLogProbMetric: 400.9134 - val_loss: 403.3985 - val_MinusLogProbMetric: 403.3985 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 594/1000
2023-10-03 08:34:03.624 
Epoch 594/1000 
	 loss: 400.7805, MinusLogProbMetric: 400.7805, val_loss: 403.2614, val_MinusLogProbMetric: 403.2614

Epoch 594: val_loss did not improve from 391.40283
196/196 - 10s - loss: 400.7805 - MinusLogProbMetric: 400.7805 - val_loss: 403.2614 - val_MinusLogProbMetric: 403.2614 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 595/1000
2023-10-03 08:34:13.561 
Epoch 595/1000 
	 loss: 400.6490, MinusLogProbMetric: 400.6490, val_loss: 403.1405, val_MinusLogProbMetric: 403.1405

Epoch 595: val_loss did not improve from 391.40283
196/196 - 10s - loss: 400.6490 - MinusLogProbMetric: 400.6490 - val_loss: 403.1405 - val_MinusLogProbMetric: 403.1405 - lr: 4.1667e-05 - 10s/epoch - 51ms/step
Epoch 596/1000
2023-10-03 08:34:23.111 
Epoch 596/1000 
	 loss: 400.5342, MinusLogProbMetric: 400.5342, val_loss: 403.0447, val_MinusLogProbMetric: 403.0447

Epoch 596: val_loss did not improve from 391.40283
196/196 - 10s - loss: 400.5342 - MinusLogProbMetric: 400.5342 - val_loss: 403.0447 - val_MinusLogProbMetric: 403.0447 - lr: 4.1667e-05 - 10s/epoch - 49ms/step
Epoch 597/1000
2023-10-03 08:34:32.091 
Epoch 597/1000 
	 loss: 400.4002, MinusLogProbMetric: 400.4002, val_loss: 402.9302, val_MinusLogProbMetric: 402.9302

Epoch 597: val_loss did not improve from 391.40283
196/196 - 9s - loss: 400.4002 - MinusLogProbMetric: 400.4002 - val_loss: 402.9302 - val_MinusLogProbMetric: 402.9302 - lr: 4.1667e-05 - 9s/epoch - 46ms/step
Epoch 598/1000
2023-10-03 08:34:41.099 
Epoch 598/1000 
	 loss: 400.2899, MinusLogProbMetric: 400.2899, val_loss: 402.8831, val_MinusLogProbMetric: 402.8831

Epoch 598: val_loss did not improve from 391.40283
196/196 - 9s - loss: 400.2899 - MinusLogProbMetric: 400.2899 - val_loss: 402.8831 - val_MinusLogProbMetric: 402.8831 - lr: 4.1667e-05 - 9s/epoch - 46ms/step
Epoch 599/1000
2023-10-03 08:34:50.130 
Epoch 599/1000 
	 loss: 400.1850, MinusLogProbMetric: 400.1850, val_loss: 402.7206, val_MinusLogProbMetric: 402.7206

Epoch 599: val_loss did not improve from 391.40283
196/196 - 9s - loss: 400.1850 - MinusLogProbMetric: 400.1850 - val_loss: 402.7206 - val_MinusLogProbMetric: 402.7206 - lr: 4.1667e-05 - 9s/epoch - 46ms/step
Epoch 600/1000
2023-10-03 08:34:59.468 
Epoch 600/1000 
	 loss: 400.0784, MinusLogProbMetric: 400.0784, val_loss: 402.6398, val_MinusLogProbMetric: 402.6398

Epoch 600: val_loss did not improve from 391.40283
196/196 - 9s - loss: 400.0784 - MinusLogProbMetric: 400.0784 - val_loss: 402.6398 - val_MinusLogProbMetric: 402.6398 - lr: 4.1667e-05 - 9s/epoch - 48ms/step
Epoch 601/1000
2023-10-03 08:35:08.683 
Epoch 601/1000 
	 loss: 399.9716, MinusLogProbMetric: 399.9716, val_loss: 402.4390, val_MinusLogProbMetric: 402.4390

Epoch 601: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.9716 - MinusLogProbMetric: 399.9716 - val_loss: 402.4390 - val_MinusLogProbMetric: 402.4390 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 602/1000
2023-10-03 08:35:18.182 
Epoch 602/1000 
	 loss: 399.8669, MinusLogProbMetric: 399.8669, val_loss: 402.3821, val_MinusLogProbMetric: 402.3821

Epoch 602: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.8669 - MinusLogProbMetric: 399.8669 - val_loss: 402.3821 - val_MinusLogProbMetric: 402.3821 - lr: 4.1667e-05 - 9s/epoch - 48ms/step
Epoch 603/1000
2023-10-03 08:35:27.331 
Epoch 603/1000 
	 loss: 399.7726, MinusLogProbMetric: 399.7726, val_loss: 402.2679, val_MinusLogProbMetric: 402.2679

Epoch 603: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.7726 - MinusLogProbMetric: 399.7726 - val_loss: 402.2679 - val_MinusLogProbMetric: 402.2679 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 604/1000
2023-10-03 08:35:36.519 
Epoch 604/1000 
	 loss: 399.6483, MinusLogProbMetric: 399.6483, val_loss: 402.1357, val_MinusLogProbMetric: 402.1357

Epoch 604: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.6483 - MinusLogProbMetric: 399.6483 - val_loss: 402.1357 - val_MinusLogProbMetric: 402.1357 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 605/1000
2023-10-03 08:35:45.735 
Epoch 605/1000 
	 loss: 399.5482, MinusLogProbMetric: 399.5482, val_loss: 402.1367, val_MinusLogProbMetric: 402.1367

Epoch 605: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.5482 - MinusLogProbMetric: 399.5482 - val_loss: 402.1367 - val_MinusLogProbMetric: 402.1367 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 606/1000
2023-10-03 08:35:54.956 
Epoch 606/1000 
	 loss: 399.4429, MinusLogProbMetric: 399.4429, val_loss: 401.9592, val_MinusLogProbMetric: 401.9592

Epoch 606: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.4429 - MinusLogProbMetric: 399.4429 - val_loss: 401.9592 - val_MinusLogProbMetric: 401.9592 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 607/1000
2023-10-03 08:36:04.243 
Epoch 607/1000 
	 loss: 399.3602, MinusLogProbMetric: 399.3602, val_loss: 401.8400, val_MinusLogProbMetric: 401.8400

Epoch 607: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.3602 - MinusLogProbMetric: 399.3602 - val_loss: 401.8400 - val_MinusLogProbMetric: 401.8400 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 608/1000
2023-10-03 08:36:13.390 
Epoch 608/1000 
	 loss: 399.2650, MinusLogProbMetric: 399.2650, val_loss: 401.7740, val_MinusLogProbMetric: 401.7740

Epoch 608: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.2650 - MinusLogProbMetric: 399.2650 - val_loss: 401.7740 - val_MinusLogProbMetric: 401.7740 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 609/1000
2023-10-03 08:36:22.442 
Epoch 609/1000 
	 loss: 399.1786, MinusLogProbMetric: 399.1786, val_loss: 401.6938, val_MinusLogProbMetric: 401.6938

Epoch 609: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.1786 - MinusLogProbMetric: 399.1786 - val_loss: 401.6938 - val_MinusLogProbMetric: 401.6938 - lr: 4.1667e-05 - 9s/epoch - 46ms/step
Epoch 610/1000
2023-10-03 08:36:31.488 
Epoch 610/1000 
	 loss: 399.1003, MinusLogProbMetric: 399.1003, val_loss: 401.6780, val_MinusLogProbMetric: 401.6780

Epoch 610: val_loss did not improve from 391.40283
196/196 - 9s - loss: 399.1003 - MinusLogProbMetric: 399.1003 - val_loss: 401.6780 - val_MinusLogProbMetric: 401.6780 - lr: 4.1667e-05 - 9s/epoch - 46ms/step
Epoch 611/1000
2023-10-03 08:36:40.681 
Epoch 611/1000 
	 loss: 398.9826, MinusLogProbMetric: 398.9826, val_loss: 401.4674, val_MinusLogProbMetric: 401.4674

Epoch 611: val_loss did not improve from 391.40283
196/196 - 9s - loss: 398.9826 - MinusLogProbMetric: 398.9826 - val_loss: 401.4674 - val_MinusLogProbMetric: 401.4674 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 612/1000
2023-10-03 08:36:49.825 
Epoch 612/1000 
	 loss: 398.8886, MinusLogProbMetric: 398.8886, val_loss: 401.3179, val_MinusLogProbMetric: 401.3179

Epoch 612: val_loss did not improve from 391.40283
196/196 - 9s - loss: 398.8886 - MinusLogProbMetric: 398.8886 - val_loss: 401.3179 - val_MinusLogProbMetric: 401.3179 - lr: 4.1667e-05 - 9s/epoch - 47ms/step
Epoch 613/1000
2023-10-03 08:36:58.809 
Epoch 613/1000 
	 loss: 398.7808, MinusLogProbMetric: 398.7808, val_loss: 401.2465, val_MinusLogProbMetric: 401.2465

Epoch 613: val_loss did not improve from 391.40283
196/196 - 9s - loss: 398.7808 - MinusLogProbMetric: 398.7808 - val_loss: 401.2465 - val_MinusLogProbMetric: 401.2465 - lr: 4.1667e-05 - 9s/epoch - 46ms/step
Epoch 614/1000
2023-10-03 08:37:07.641 
Epoch 614/1000 
	 loss: 398.6807, MinusLogProbMetric: 398.6807, val_loss: 401.2416, val_MinusLogProbMetric: 401.2416

Epoch 614: val_loss did not improve from 391.40283
196/196 - 9s - loss: 398.6807 - MinusLogProbMetric: 398.6807 - val_loss: 401.2416 - val_MinusLogProbMetric: 401.2416 - lr: 4.1667e-05 - 9s/epoch - 45ms/step
Epoch 615/1000
2023-10-03 08:37:16.509 
Epoch 615/1000 
	 loss: 398.5991, MinusLogProbMetric: 398.5991, val_loss: 401.0903, val_MinusLogProbMetric: 401.0903

Epoch 615: val_loss did not improve from 391.40283
Restoring model weights from the end of the best epoch: 515.
196/196 - 9s - loss: 398.5991 - MinusLogProbMetric: 398.5991 - val_loss: 401.0903 - val_MinusLogProbMetric: 401.0903 - lr: 4.1667e-05 - 9s/epoch - 46ms/step
Epoch 615: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 2384.137157131918 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 2453.7968594019767 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 2399.2727680690587 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 2474.502268156968 seconds.
Training succeeded with seed 721.
Model trained in 6107.82 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 9761.12 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 9761.40 s.
===========
Run 347/360 done in 16025.55 s.
===========

Directory ../../results/MAFN_new/run_348/ already exists.
Skipping it.
===========
Run 348/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_349/ already exists.
Skipping it.
===========
Run 349/360 already exists. Skipping it.
===========

===========
Generating train data for run 350.
===========
Train data generated in 0.81 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_350
self.data_kwargs: {'seed': 869}
self.x_data: [[ 7.9973636   4.640891    5.32509    ...  3.500269    8.638443
   6.719802  ]
 [ 8.238864    4.485278    5.284702   ...  2.8864994   7.5674953
   7.1954236 ]
 [ 8.268872    4.7704377   5.167857   ...  4.5533733   8.026902
   7.8079686 ]
 ...
 [ 5.836932   -0.15277141  4.6562195  ...  4.799403    6.4058156
   6.0447054 ]
 [ 8.8304      4.5455837   5.226686   ...  3.9410732   7.6945333
   7.224351  ]
 [ 8.222233    4.7859597   5.1849046  ...  2.020081    8.037197
   7.44482   ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_89 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  4509200   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,509,200
Trainable params: 4,509,200
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7fbb349a3580>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbad745b970>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbad745b970>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbad74d8250>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbad745f2b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbad74d8af0>, <keras.callbacks.ModelCheckpoint object at 0x7fbad74d8be0>, <keras.callbacks.EarlyStopping object at 0x7fbad74d8bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbad74d9a50>, <keras.callbacks.TerminateOnNaN object at 0x7fbad74d8850>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_350/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 350/360 with hyperparameters:
timestamp = 2023-10-03 11:20:00.161716
ndims = 1000
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 5
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 4509200
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.99736357e+00  4.64089108e+00  5.32508993e+00  2.54230428e+00
  6.17678785e+00  3.28207493e+00  5.66227102e+00  1.74029863e+00
  1.76519728e+00  3.78703856e+00  4.21053505e+00  3.09312010e+00
  2.03467131e+00  8.71515274e-03  3.30997020e-01  3.03643203e+00
  3.58006954e+00  7.91204977e+00  4.60797453e+00  9.32520199e+00
 -1.47658885e-01  1.08268631e+00  3.49395204e+00  5.92749643e+00
  8.83363056e+00  3.30430484e+00  2.72358418e+00  5.84306300e-01
  7.95439053e+00  4.29114294e+00  5.70392561e+00  9.62326145e+00
  1.91625190e+00  2.05868888e+00  7.86738205e+00  8.31465626e+00
  7.71446276e+00  6.16568375e+00  9.86226177e+00  4.56724548e+00
  5.94829226e+00  5.95013499e-01  4.22035885e+00  7.23472834e+00
  2.47805715e+00  8.42218876e+00  6.81143332e+00  3.05234265e+00
  7.74489641e+00 -1.49918377e-01  9.73857021e+00  8.26392531e-01
  1.01337919e+01  3.15674591e+00  7.51659107e+00 -4.07333463e-01
  2.89285135e+00  6.51461697e+00  6.24277782e+00 -2.83749938e-01
  3.82940078e+00  4.11356688e+00  6.11557388e+00  6.73437071e+00
  8.87053776e+00  4.77861261e+00  3.64606881e+00  1.49473929e+00
  8.43961716e+00  7.42175150e+00  4.57817841e+00  5.29543495e+00
  6.56515741e+00  1.46717739e+00 -1.17198992e+00  3.13418698e+00
  6.17689991e+00  4.57003880e+00  7.09853601e+00  8.16721344e+00
  4.87046480e+00  2.11677957e+00  8.49123096e+00  6.10315180e+00
  7.85925865e+00  5.53120947e+00  9.60370159e+00  2.18746519e+00
  6.79681110e+00  4.55581379e+00  4.16971874e+00  1.73112345e+00
  3.38649940e+00  9.48220348e+00  3.10462046e+00  2.14097667e+00
  5.63125849e+00  3.38328648e+00  5.20010900e+00  8.49408150e+00
  3.82229614e+00  4.20812511e+00  5.16726160e+00  5.89465952e+00
  9.87067223e+00  6.80660152e+00  7.34994841e+00  9.82929134e+00
  3.98204517e+00  3.51628542e+00 -3.33558351e-01  2.19543719e+00
  1.56361794e+00  1.56820440e+00  8.16210842e+00  6.19128990e+00
  6.25914240e+00  8.37748623e+00  2.31200051e+00  3.18747854e+00
  7.11904478e+00  4.19043159e+00 -1.50076985e-01  3.92370987e+00
  9.12942028e+00  4.13348961e+00  2.73112822e+00  9.84571934e+00
  5.88873577e+00  4.89145327e+00  1.00473905e+00  7.55370808e+00
  6.88664293e+00  9.29471016e+00  1.59780979e+00  1.04221833e+00
  7.24209642e+00  2.19776750e+00  3.42702579e+00  3.34507298e+00
  8.18159485e+00  5.71737003e+00  4.56389010e-01  4.59833431e+00
  7.35877991e+00  5.35200214e+00  5.63458967e+00  2.36878228e+00
  8.52304554e+00  5.58948660e+00  7.47698450e+00  9.61558151e+00
  4.02933073e+00  8.54585361e+00 -2.04157352e-01  3.18135262e-01
  5.99070644e+00  3.07406282e+00  2.67563176e+00  2.55177832e+00
  3.00443268e+00  7.63680124e+00  6.20905495e+00  7.80322456e+00
  7.74732971e+00  8.43296623e+00  5.25363827e+00  3.99893999e+00
  1.01927109e+01  7.05619717e+00  7.41717219e-01  5.25023270e+00
  3.77008796e+00  9.90027332e+00 -9.78221714e-01  6.02291298e+00
  6.87749577e+00  3.01261449e+00  9.65150070e+00  9.36239624e+00
  8.82814312e+00  7.89327025e-02  5.79393578e+00  2.59769440e+00
  7.66548109e+00  6.07534552e+00  5.36576509e+00  1.34668684e+00
  4.34030962e+00  6.31810141e+00  9.62684059e+00  4.88660431e+00
  9.60818291e+00  9.36019993e+00  6.71263504e+00  5.22296143e+00
  4.42309809e+00  7.77644825e+00 -1.76710457e-01  1.15464127e+00
  1.00100291e+00  8.11944199e+00  3.27240157e+00  6.37353086e+00
  5.53368807e+00  4.70193911e+00  5.07406664e+00  7.97970533e+00
  9.54048157e+00  4.72122574e+00  6.13606310e+00  3.54982883e-01
  1.08992491e+01  5.27934456e+00  7.85659790e+00  7.82055616e-01
  4.37489986e+00  2.60340428e+00  2.48891091e+00  9.05652618e+00
  6.87852859e+00  4.07554960e+00  1.03486490e+00  9.66742897e+00
  9.06607342e+00  3.15981674e+00  3.57244402e-01  5.63626289e+00
  9.92044353e+00  8.58868217e+00  6.57505751e+00  3.29356098e+00
  3.24422455e+00  9.00940800e+00  5.09291124e+00  2.85960793e-01
  2.23807621e+00  2.78734875e+00  1.29828584e+00  3.93540382e+00
  8.03243542e+00  6.78995275e+00  5.24697185e-01  1.17165899e+00
  3.27114582e+00  3.83798814e+00  8.53496361e+00  8.92454338e+00
  4.21771574e+00  6.39960194e+00  9.93754196e+00  7.64902067e+00
  6.44687891e+00  7.72444129e-01  2.29160905e+00  8.52109241e+00
  1.77464163e+00  8.37704659e+00  7.40880537e+00  2.87169456e+00
  5.39322519e+00  6.86864281e+00  7.26533747e+00  4.94568443e+00
  1.19446182e+00  6.65035486e+00  3.40026999e+00  9.23955250e+00
  2.17107248e+00  2.47258735e+00  2.43059564e+00  6.30585194e+00
  8.65539742e+00  1.45929515e+00  3.26658463e+00  5.87293768e+00
  4.54141855e+00  4.57416773e+00  8.90260696e+00  8.79572392e+00
  4.92429781e+00  6.65130472e+00  8.80965042e+00  4.23099279e+00
  2.90440130e+00  6.38359404e+00  5.79758072e+00  8.99278259e+00
  5.63759208e-01  9.11637592e+00  5.69089317e+00  3.27839077e-01
  2.98394132e+00  7.03010273e+00  3.88054943e+00  9.76950550e+00
  5.22733927e+00  2.11281967e+00  8.35765839e+00  5.03978920e+00
  7.56217527e+00  8.23209476e+00  6.36810303e+00  4.07139969e+00
  1.73890018e+00  7.25630856e+00  9.09160614e+00  3.28961945e+00
  9.31139088e+00  5.12792778e+00  6.50060415e+00  8.50181961e+00
  4.91192722e+00  8.58134687e-01  1.00498667e+01  2.70127869e+00
  6.64366674e+00  4.16276121e+00  4.07800245e+00  2.51310420e+00
  6.86618900e+00  1.04104257e+00  8.65216160e+00  8.34197235e+00
  5.48617649e+00  3.98194122e+00  2.67905617e+00  5.19228172e+00
  7.82842445e+00  2.60275602e+00  4.39831114e+00  2.98556614e+00
  2.60362029e+00  1.57165313e+00  9.64633369e+00  6.88340855e+00
  7.78283501e+00  1.10886335e+00  4.86602402e+00  8.91554070e+00
 -1.24701262e-02  1.84407997e+00  6.24502945e+00  6.37515545e+00
  7.90489101e+00  9.60888958e+00  1.45247686e+00  2.45699430e+00
  4.28265285e+00  6.50822818e-02  1.12332261e+00  2.24159002e+00
  4.71833563e+00  2.72241974e+00  8.28100014e+00  3.28441453e+00
  9.11346054e+00  1.05609627e+01  3.85242462e+00  3.12269568e+00
  2.30633688e+00  7.71115065e+00  2.85296941e+00  3.28263164e+00
  4.81248856e+00  6.73265219e+00  1.02898769e+01  3.37639761e+00
  3.19120789e+00  2.37913132e+00  3.12198162e+00  9.28771555e-01
  5.18783569e+00  1.00527916e+01  1.98218155e+00  3.54625010e+00
  9.82042217e+00  7.37978649e+00  4.98093176e+00  7.74178505e+00
  4.38426554e-01  5.04215908e+00  1.37913942e+00  2.33643341e+00
  1.43258142e+00  3.60961819e+00  4.80302668e+00  9.63227654e+00
  2.33755469e+00  4.40754080e+00  8.69386292e+00  8.12515831e+00
  5.90748072e+00  2.80811763e+00  5.03586197e+00  1.81783056e+00
  1.78456891e+00  4.50792408e+00  7.38818550e+00  5.70210075e+00
  7.67465544e+00  7.92264366e+00 -1.91134661e-01  5.84350491e+00
  6.51570177e+00  9.74592590e+00 -2.64375985e-01  4.72814274e+00
  1.51264036e+00  5.40283203e-01  7.05697918e+00  7.51599932e+00
  7.27936220e+00 -2.28865266e-01  3.87274265e+00  2.85972893e-01
  2.46495175e+00  1.80521321e+00  2.14937496e+00  3.77834606e+00
  3.18217349e+00  1.02763710e+01  3.37995410e+00  8.93162251e+00
  1.96273518e+00  9.14139557e+00 -3.55325580e-01  4.60834980e+00
 -5.89213148e-03  8.24484348e+00  1.03387394e+01  5.88764489e-01
  9.74320602e+00  8.21028519e+00  3.25056553e+00  9.08056259e+00
  2.59105110e+00  3.95399547e+00  9.59685862e-01  7.63103676e+00
 -4.04038966e-01  5.37858295e+00  6.89733744e+00  9.44393730e+00
  1.41217971e+00  7.53022814e+00  7.95942068e+00  5.70683289e+00
  5.70691347e+00 -2.19492435e-01  5.11235189e+00  3.95949173e+00
  9.22287846e+00  2.71982121e+00  5.80306816e+00  4.00738478e+00
  9.05252075e+00  2.45202136e+00  5.51476240e-01  6.91309261e+00
  3.62627077e+00  5.96777725e+00  3.05229813e-01  9.78659534e+00
  9.17897224e+00  5.76029110e+00  9.53746438e-01  3.57887888e+00
  9.68739605e+00  6.38989639e+00  1.90702987e+00  4.66415691e+00
  9.59853172e+00  5.05139923e+00  1.05777577e-01  6.60696030e+00
  2.90498590e+00  9.84483898e-01  4.15751266e+00  6.08889675e+00
 -7.97456264e-01  4.97374535e+00  5.03539562e+00  8.87364101e+00
  4.56227398e+00  6.91264105e+00  6.87507677e+00  8.22698975e+00
  3.93433619e+00  6.01061106e+00  5.63419819e+00  3.52927715e-01
  8.35077000e+00  3.63118911e+00  6.62209034e+00  5.41198444e+00
  6.93401527e+00  5.77218914e+00  1.43877172e+00  6.73276329e+00
  7.93194485e+00  7.98630357e-01  6.14384651e+00  6.38910675e+00
  9.41168022e+00  2.80262136e+00  1.86243892e+00  3.21781826e+00
  2.72202253e+00  7.37366581e+00  7.33060360e+00  6.61256790e+00
  2.12070441e+00  8.59328461e+00  1.00850325e+01  5.48360109e+00
  7.31572151e+00  5.40123463e+00  2.76470375e+00  2.38635635e+00
  2.60676074e+00  1.89509916e+00  8.99138355e+00  8.75982761e+00
  4.44820881e+00  3.67402339e+00  9.41257381e+00  2.81948304e+00
  5.31368160e+00  2.30343890e+00  1.82023335e+00  6.24686146e+00
  8.02147388e+00  6.23021078e+00  7.50943947e+00  9.98944378e+00
  9.65496063e+00  3.44390488e+00  9.90036297e+00  1.90570307e+00
  6.37369108e+00  6.97235632e+00  1.02818668e+00  1.01110497e+01
  3.08032537e+00  6.21633768e+00  8.70998573e+00  5.46199799e+00
  1.22746360e+00  8.02329159e+00  4.09109497e+00  1.01594963e+01
  8.81569862e+00  8.09971428e+00  3.88408160e+00  7.92610073e+00
  9.26926994e+00  6.73566246e+00  5.76671696e+00 -6.34171844e-01
  8.21392536e+00  8.59858513e+00  3.17632437e+00  2.96401882e+00
  4.87884188e+00  1.46060300e+00  7.75016904e-01  5.95933533e+00
  2.85177946e+00  4.70948601e+00  4.13632631e+00  9.87929535e+00
  1.08665876e+01  9.14421260e-01  8.82590580e+00  6.48437381e-01
  5.33739424e+00  2.29306793e+00  4.13651562e+00  6.47616768e+00
  4.24598312e+00  8.10760880e+00  7.45779276e+00  9.63120937e+00
  4.89558411e+00  8.02083302e+00  3.24438405e+00  3.59759498e+00
  4.09306526e+00  4.22914410e+00  9.30973232e-01  5.26565123e+00
  1.01665020e-01  9.00792408e+00  1.68276280e-01  4.04053593e+00
  2.87474799e+00  2.48327708e+00  7.51841593e+00  6.77074766e+00
  1.45175838e+00  6.70365572e+00  8.29291821e+00  7.64892721e+00
  1.97199440e+00  6.84292603e+00  7.54025269e+00  8.23593044e+00
  5.50070477e+00  3.45322466e+00  9.49723530e+00  8.45789242e+00
  5.81870270e+00  3.73827100e+00  8.91483879e+00  5.10812187e+00
  8.88178647e-01  4.46295691e+00  8.31669044e+00  4.49498892e+00
  5.43265700e-01  6.11975908e+00  9.95184231e+00  9.05875778e+00
  4.80623627e+00  9.23525047e+00  7.71717167e+00  5.08452940e+00
  4.99330044e+00  7.49071932e+00  7.34964132e+00  1.30181521e-01
  8.31134415e+00  4.43235731e+00  6.71253824e+00  7.91646671e+00
  4.65235710e+00  1.59666359e+00  4.05373240e+00  7.26302922e-01
  6.89302731e+00  9.45408058e+00  9.24543953e+00  4.22942209e+00
  5.97023821e+00  2.74910712e+00  5.68657923e+00  2.15732288e+00
  6.35400105e+00  2.63340044e+00  9.43256760e+00  6.54860783e+00
  9.90323901e-01  1.34424239e-01  9.92843723e+00  3.24450016e+00
  1.74554217e+00  3.04431581e+00  8.90259075e+00  7.87959576e+00
  5.06868839e+00  4.79971409e+00  2.77283359e+00  5.49839926e+00
  4.38119030e+00  2.14065003e+00  6.93851089e+00  4.98988056e+00
  7.73501110e+00  4.64208126e+00  4.79577160e+00  7.85067606e+00
  4.75812960e+00  9.79898643e+00  7.09330177e+00  8.39217281e+00
  9.55065346e+00  3.69468021e+00  5.54081106e+00  6.72979546e+00
  6.70603752e+00  9.38065147e+00  2.90241790e+00  2.52509665e+00
  8.94938564e+00  8.49423122e+00  2.13432574e+00  5.68594837e+00
  6.90361071e+00  4.37006855e+00  5.16254711e+00  5.72680283e+00
  5.19291306e+00  7.52211380e+00 -2.97093898e-01  2.55037498e+00
  2.54706359e+00  6.59417248e+00  7.53437948e+00  7.30045140e-01
  1.39935553e+00  6.93793297e+00  6.43762541e+00  5.88978815e+00
  9.83007526e+00  6.00890970e+00  6.66946948e-01  6.86854553e+00
  8.03701878e+00  6.11670637e+00  5.35453987e+00  9.57577324e+00
  1.49704802e+00  4.92001712e-01  1.53974497e+00  2.13410378e+00
  5.92641163e+00  6.05311298e+00  2.26492047e+00  4.60379982e+00
  4.60677052e+00  3.77398515e+00  7.27591801e+00  4.27604628e+00
  7.54144859e+00  1.66404676e+00  5.16442060e+00  8.22671890e+00
  1.71123445e+00  4.62352562e+00  6.08017778e+00  2.88557911e+00
  7.73235178e+00  6.43578827e-01  9.65846920e+00  4.54546928e+00
  3.51417971e+00  1.19948006e+00  5.29146385e+00  6.01765347e+00
  2.32322073e+00  2.89909482e+00  9.31757736e+00  6.46213627e+00
  2.06418347e+00  9.80539680e-01  8.66675568e+00  3.23060274e+00
  2.10251164e+00  9.88873005e+00  3.76415300e+00  8.50292981e-01
  5.07699108e+00  3.88809896e+00  9.39960575e+00  5.59372187e-01
 -5.31555176e-01  8.84760439e-01  9.86167717e+00  4.76715946e+00
  5.13683701e+00  5.85030556e+00  6.22967052e+00  1.53158998e+00
  6.54711390e+00  3.91384292e+00  2.45552421e+00  5.99128914e+00
  3.54900932e+00  2.41717911e+00  8.30238152e+00  1.64661229e+00
  2.59730220e+00  3.85882235e+00  2.47771192e+00  2.10063434e+00
  9.37286758e+00  3.17066669e+00  2.51538455e-01  5.38400126e+00
  1.23733842e+00  3.78326035e+00  9.20017338e+00  2.47332525e+00
  5.67668962e+00  5.17319727e+00  2.06113005e+00  6.44534540e+00
  8.66131496e+00  5.21378100e-01  6.50092697e+00  6.78853512e+00
  8.54306507e+00  1.80599499e+00  2.05283189e+00  6.85634184e+00
  1.15905434e-01  6.99897051e+00  8.32928753e+00  4.55407476e+00
  4.38788176e+00  3.54872870e+00 -4.62173462e-01  1.36794913e+00
  9.69468594e+00  8.38702488e+00  1.90114570e+00  7.94150639e+00
  3.36321092e+00  2.99465132e+00  1.99559021e+00  9.02903366e+00
  9.85151100e+00  3.23628950e+00  7.55823898e+00  3.70709014e+00
  1.74990630e+00  3.48457527e+00  5.45959711e+00  1.58513451e+00
  8.12769318e+00  4.73363543e+00  2.32966566e+00  7.44186544e+00
  7.94482851e+00  3.54043078e+00  7.04913664e+00  4.02170229e+00
  1.01847572e+01  8.72599220e+00  3.30791759e+00 -1.08366966e-01
  6.66013432e+00  1.91774535e+00  1.22588658e+00  1.02105055e+01
  9.86835957e+00  2.95132232e+00  3.01797199e+00  1.64833105e+00
  4.49072456e+00  8.49544430e+00  6.82054377e+00  9.07019711e+00
  1.68629599e+00  7.90870810e+00  9.09079742e+00  2.92365527e+00
  6.90887928e+00  4.17319202e+00  3.43049693e+00  9.02845764e+00
  4.29767895e+00  1.09342468e+00  3.74337578e+00  4.81818581e+00
  7.20623732e+00  1.07804976e+01  8.88462162e+00  9.30848122e+00
  4.87059927e+00  7.08490753e+00  2.95342398e+00  1.67907417e+00
  3.60964656e+00  7.03290796e+00  3.57637072e+00  8.74868298e+00
  8.52540398e+00  2.05524230e+00  3.42940140e+00  8.83274364e+00
  1.58101547e+00  3.33796239e+00  3.42331791e+00  6.96280146e+00
  3.25254261e-01 -1.25156724e+00  5.89129639e+00  4.46135187e+00
  6.51815891e+00  2.47705388e+00  9.10450840e+00  8.03964901e+00
  6.83334887e-01  8.24278164e+00  9.73476696e+00  1.00054388e+01
  7.11009216e+00  1.24967217e+00  5.68010473e+00  2.01691651e+00
  9.84855938e+00  7.23796368e+00  6.43789339e+00  6.97472811e+00
  4.51600838e+00  2.94856024e+00  9.13858128e+00  3.29138970e+00
  3.09904784e-01  4.01227283e+00  9.32983780e+00  5.93039370e+00
  1.10625067e+01  1.13827813e+00  1.01018476e+01  5.29424810e+00
  5.45073414e+00  5.82157803e+00  1.17043698e+00  3.22519493e+00
  7.34947968e+00  6.69572353e-01  1.62995577e+00  1.85301498e-01
  8.83085346e+00  5.87161255e+00  1.81599593e+00  7.31665230e+00
  9.78809166e+00  9.34503746e+00  9.14690876e+00  3.59607875e-01
  5.99414158e+00  2.59369469e+00  5.78580284e+00  3.18324995e+00
  1.74265718e+00  4.42781639e+00  4.24171114e+00  9.89190197e+00
  1.12621486e-02  4.74760723e+00  8.55551147e+00  7.21490955e+00
  8.37855244e+00  5.55442858e+00  7.92241478e+00  5.13917971e+00
  3.15440607e+00  8.63028622e+00  5.67239189e+00  1.22252262e+00
  7.74803591e+00  9.87097359e+00  4.27992439e+00  3.46016455e+00
  5.19145608e-01  6.58222771e+00  1.11442089e+01 -5.05199075e-01
  4.88900900e+00  1.03596199e+00  3.49793643e-01  6.70542479e+00
  8.65525723e+00  3.48391366e+00  8.85622883e+00  1.32473469e+00
  1.43105400e+00  9.27033043e+00  5.13401794e+00  6.83637190e+00
  3.17043185e-01  1.26743388e+00  5.01762807e-01  9.27802563e+00
  1.43500125e+00  8.44762039e+00  3.54408193e+00  7.80078506e+00
  3.90129399e+00  3.20132899e+00  1.04541802e+00  2.72584295e+00
  4.51778460e+00  7.11837149e+00  4.60630083e+00  9.86540890e+00
  6.96063471e+00 -3.97378445e-01  3.62382388e+00  6.64995527e+00
  1.94566798e+00  7.08949041e+00  5.43198347e+00  5.80618441e-01
  7.76860094e+00  6.34063721e+00  7.98214495e-01  4.38892889e+00
  2.10588074e+00  8.85570145e+00  2.82261276e+00  9.23659515e+00
  5.26395798e+00  3.50026894e+00  8.63844299e+00  6.71980190e+00]
Epoch 1/1000
2023-10-03 11:20:20.448 
Epoch 1/1000 
	 loss: 1685.8408, MinusLogProbMetric: 1685.8408, val_loss: 604.7186, val_MinusLogProbMetric: 604.7186

Epoch 1: val_loss improved from inf to 604.71863, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 20s - loss: 1685.8408 - MinusLogProbMetric: 1685.8408 - val_loss: 604.7186 - val_MinusLogProbMetric: 604.7186 - lr: 0.0010 - 20s/epoch - 105ms/step
Epoch 2/1000
2023-10-03 11:20:26.872 
Epoch 2/1000 
	 loss: 579.5620, MinusLogProbMetric: 579.5620, val_loss: 575.1782, val_MinusLogProbMetric: 575.1782

Epoch 2: val_loss improved from 604.71863 to 575.17822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 579.5620 - MinusLogProbMetric: 579.5620 - val_loss: 575.1782 - val_MinusLogProbMetric: 575.1782 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 3/1000
2023-10-03 11:20:33.277 
Epoch 3/1000 
	 loss: 750.5228, MinusLogProbMetric: 750.5228, val_loss: 514.4287, val_MinusLogProbMetric: 514.4287

Epoch 3: val_loss improved from 575.17822 to 514.42871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 750.5228 - MinusLogProbMetric: 750.5228 - val_loss: 514.4287 - val_MinusLogProbMetric: 514.4287 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 4/1000
2023-10-03 11:20:39.620 
Epoch 4/1000 
	 loss: 501.8942, MinusLogProbMetric: 501.8942, val_loss: 487.5024, val_MinusLogProbMetric: 487.5024

Epoch 4: val_loss improved from 514.42871 to 487.50241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 501.8942 - MinusLogProbMetric: 501.8942 - val_loss: 487.5024 - val_MinusLogProbMetric: 487.5024 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 5/1000
2023-10-03 11:20:45.967 
Epoch 5/1000 
	 loss: 488.5226, MinusLogProbMetric: 488.5226, val_loss: 466.7170, val_MinusLogProbMetric: 466.7170

Epoch 5: val_loss improved from 487.50241 to 466.71701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 488.5226 - MinusLogProbMetric: 488.5226 - val_loss: 466.7170 - val_MinusLogProbMetric: 466.7170 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 6/1000
2023-10-03 11:20:52.336 
Epoch 6/1000 
	 loss: 477.4804, MinusLogProbMetric: 477.4804, val_loss: 465.4385, val_MinusLogProbMetric: 465.4385

Epoch 6: val_loss improved from 466.71701 to 465.43848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 477.4804 - MinusLogProbMetric: 477.4804 - val_loss: 465.4385 - val_MinusLogProbMetric: 465.4385 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 7/1000
2023-10-03 11:20:58.798 
Epoch 7/1000 
	 loss: 468.8159, MinusLogProbMetric: 468.8159, val_loss: 457.2295, val_MinusLogProbMetric: 457.2295

Epoch 7: val_loss improved from 465.43848 to 457.22949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 468.8159 - MinusLogProbMetric: 468.8159 - val_loss: 457.2295 - val_MinusLogProbMetric: 457.2295 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 8/1000
2023-10-03 11:21:05.336 
Epoch 8/1000 
	 loss: 470.8293, MinusLogProbMetric: 470.8293, val_loss: 450.9491, val_MinusLogProbMetric: 450.9491

Epoch 8: val_loss improved from 457.22949 to 450.94913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 470.8293 - MinusLogProbMetric: 470.8293 - val_loss: 450.9491 - val_MinusLogProbMetric: 450.9491 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 9/1000
2023-10-03 11:21:11.713 
Epoch 9/1000 
	 loss: 454.1265, MinusLogProbMetric: 454.1265, val_loss: 462.9688, val_MinusLogProbMetric: 462.9688

Epoch 9: val_loss did not improve from 450.94913
196/196 - 6s - loss: 454.1265 - MinusLogProbMetric: 454.1265 - val_loss: 462.9688 - val_MinusLogProbMetric: 462.9688 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 10/1000
2023-10-03 11:21:17.850 
Epoch 10/1000 
	 loss: 453.9076, MinusLogProbMetric: 453.9076, val_loss: 440.5541, val_MinusLogProbMetric: 440.5541

Epoch 10: val_loss improved from 450.94913 to 440.55414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 453.9076 - MinusLogProbMetric: 453.9076 - val_loss: 440.5541 - val_MinusLogProbMetric: 440.5541 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 11/1000
2023-10-03 11:21:24.265 
Epoch 11/1000 
	 loss: 449.1294, MinusLogProbMetric: 449.1294, val_loss: 445.4779, val_MinusLogProbMetric: 445.4779

Epoch 11: val_loss did not improve from 440.55414
196/196 - 6s - loss: 449.1294 - MinusLogProbMetric: 449.1294 - val_loss: 445.4779 - val_MinusLogProbMetric: 445.4779 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 12/1000
2023-10-03 11:21:30.274 
Epoch 12/1000 
	 loss: 446.1796, MinusLogProbMetric: 446.1796, val_loss: 455.7892, val_MinusLogProbMetric: 455.7892

Epoch 12: val_loss did not improve from 440.55414
196/196 - 6s - loss: 446.1796 - MinusLogProbMetric: 446.1796 - val_loss: 455.7892 - val_MinusLogProbMetric: 455.7892 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 13/1000
2023-10-03 11:21:36.341 
Epoch 13/1000 
	 loss: 447.0032, MinusLogProbMetric: 447.0032, val_loss: 438.7332, val_MinusLogProbMetric: 438.7332

Epoch 13: val_loss improved from 440.55414 to 438.73318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 447.0032 - MinusLogProbMetric: 447.0032 - val_loss: 438.7332 - val_MinusLogProbMetric: 438.7332 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 14/1000
2023-10-03 11:21:42.778 
Epoch 14/1000 
	 loss: 1305.5895, MinusLogProbMetric: 1305.5895, val_loss: 555.2275, val_MinusLogProbMetric: 555.2275

Epoch 14: val_loss did not improve from 438.73318
196/196 - 6s - loss: 1305.5895 - MinusLogProbMetric: 1305.5895 - val_loss: 555.2275 - val_MinusLogProbMetric: 555.2275 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 15/1000
2023-10-03 11:21:48.828 
Epoch 15/1000 
	 loss: 500.9633, MinusLogProbMetric: 500.9633, val_loss: 471.3738, val_MinusLogProbMetric: 471.3738

Epoch 15: val_loss did not improve from 438.73318
196/196 - 6s - loss: 500.9633 - MinusLogProbMetric: 500.9633 - val_loss: 471.3738 - val_MinusLogProbMetric: 471.3738 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 16/1000
2023-10-03 11:21:54.982 
Epoch 16/1000 
	 loss: 465.0201, MinusLogProbMetric: 465.0201, val_loss: 453.7810, val_MinusLogProbMetric: 453.7810

Epoch 16: val_loss did not improve from 438.73318
196/196 - 6s - loss: 465.0201 - MinusLogProbMetric: 465.0201 - val_loss: 453.7810 - val_MinusLogProbMetric: 453.7810 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 17/1000
2023-10-03 11:22:01.117 
Epoch 17/1000 
	 loss: 453.2168, MinusLogProbMetric: 453.2168, val_loss: 448.6420, val_MinusLogProbMetric: 448.6420

Epoch 17: val_loss did not improve from 438.73318
196/196 - 6s - loss: 453.2168 - MinusLogProbMetric: 453.2168 - val_loss: 448.6420 - val_MinusLogProbMetric: 448.6420 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 18/1000
2023-10-03 11:22:07.263 
Epoch 18/1000 
	 loss: 446.9883, MinusLogProbMetric: 446.9883, val_loss: 441.9283, val_MinusLogProbMetric: 441.9283

Epoch 18: val_loss did not improve from 438.73318
196/196 - 6s - loss: 446.9883 - MinusLogProbMetric: 446.9883 - val_loss: 441.9283 - val_MinusLogProbMetric: 441.9283 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 19/1000
2023-10-03 11:22:13.342 
Epoch 19/1000 
	 loss: 439.9343, MinusLogProbMetric: 439.9343, val_loss: 438.5079, val_MinusLogProbMetric: 438.5079

Epoch 19: val_loss improved from 438.73318 to 438.50793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 439.9343 - MinusLogProbMetric: 439.9343 - val_loss: 438.5079 - val_MinusLogProbMetric: 438.5079 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 20/1000
2023-10-03 11:22:19.919 
Epoch 20/1000 
	 loss: 438.5697, MinusLogProbMetric: 438.5697, val_loss: 434.6110, val_MinusLogProbMetric: 434.6110

Epoch 20: val_loss improved from 438.50793 to 434.61099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 438.5697 - MinusLogProbMetric: 438.5697 - val_loss: 434.6110 - val_MinusLogProbMetric: 434.6110 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 21/1000
2023-10-03 11:22:26.906 
Epoch 21/1000 
	 loss: 435.0325, MinusLogProbMetric: 435.0325, val_loss: 444.0538, val_MinusLogProbMetric: 444.0538

Epoch 21: val_loss did not improve from 434.61099
196/196 - 6s - loss: 435.0325 - MinusLogProbMetric: 435.0325 - val_loss: 444.0538 - val_MinusLogProbMetric: 444.0538 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 22/1000
2023-10-03 11:22:33.117 
Epoch 22/1000 
	 loss: 433.1763, MinusLogProbMetric: 433.1763, val_loss: 429.8773, val_MinusLogProbMetric: 429.8773

Epoch 22: val_loss improved from 434.61099 to 429.87729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 433.1763 - MinusLogProbMetric: 433.1763 - val_loss: 429.8773 - val_MinusLogProbMetric: 429.8773 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 23/1000
2023-10-03 11:22:39.463 
Epoch 23/1000 
	 loss: 429.4696, MinusLogProbMetric: 429.4696, val_loss: 427.7965, val_MinusLogProbMetric: 427.7965

Epoch 23: val_loss improved from 429.87729 to 427.79651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 429.4696 - MinusLogProbMetric: 429.4696 - val_loss: 427.7965 - val_MinusLogProbMetric: 427.7965 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 24/1000
2023-10-03 11:22:45.727 
Epoch 24/1000 
	 loss: 428.3073, MinusLogProbMetric: 428.3073, val_loss: 425.8863, val_MinusLogProbMetric: 425.8863

Epoch 24: val_loss improved from 427.79651 to 425.88629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 428.3073 - MinusLogProbMetric: 428.3073 - val_loss: 425.8863 - val_MinusLogProbMetric: 425.8863 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 25/1000
2023-10-03 11:22:52.462 
Epoch 25/1000 
	 loss: 426.6654, MinusLogProbMetric: 426.6654, val_loss: 423.7526, val_MinusLogProbMetric: 423.7526

Epoch 25: val_loss improved from 425.88629 to 423.75256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 426.6654 - MinusLogProbMetric: 426.6654 - val_loss: 423.7526 - val_MinusLogProbMetric: 423.7526 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 26/1000
2023-10-03 11:22:59.048 
Epoch 26/1000 
	 loss: 426.1103, MinusLogProbMetric: 426.1103, val_loss: 425.9002, val_MinusLogProbMetric: 425.9002

Epoch 26: val_loss did not improve from 423.75256
196/196 - 6s - loss: 426.1103 - MinusLogProbMetric: 426.1103 - val_loss: 425.9002 - val_MinusLogProbMetric: 425.9002 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 27/1000
2023-10-03 11:23:05.235 
Epoch 27/1000 
	 loss: 424.7380, MinusLogProbMetric: 424.7380, val_loss: 444.3454, val_MinusLogProbMetric: 444.3454

Epoch 27: val_loss did not improve from 423.75256
196/196 - 6s - loss: 424.7380 - MinusLogProbMetric: 424.7380 - val_loss: 444.3454 - val_MinusLogProbMetric: 444.3454 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 28/1000
2023-10-03 11:23:11.412 
Epoch 28/1000 
	 loss: 423.3149, MinusLogProbMetric: 423.3149, val_loss: 419.9894, val_MinusLogProbMetric: 419.9894

Epoch 28: val_loss improved from 423.75256 to 419.98938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 423.3149 - MinusLogProbMetric: 423.3149 - val_loss: 419.9894 - val_MinusLogProbMetric: 419.9894 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 29/1000
2023-10-03 11:23:18.384 
Epoch 29/1000 
	 loss: 424.0389, MinusLogProbMetric: 424.0389, val_loss: 419.5234, val_MinusLogProbMetric: 419.5234

Epoch 29: val_loss improved from 419.98938 to 419.52344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 424.0389 - MinusLogProbMetric: 424.0389 - val_loss: 419.5234 - val_MinusLogProbMetric: 419.5234 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 30/1000
2023-10-03 11:23:24.652 
Epoch 30/1000 
	 loss: 423.7282, MinusLogProbMetric: 423.7282, val_loss: 419.7314, val_MinusLogProbMetric: 419.7314

Epoch 30: val_loss did not improve from 419.52344
196/196 - 6s - loss: 423.7282 - MinusLogProbMetric: 423.7282 - val_loss: 419.7314 - val_MinusLogProbMetric: 419.7314 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 31/1000
2023-10-03 11:23:30.775 
Epoch 31/1000 
	 loss: 422.1875, MinusLogProbMetric: 422.1875, val_loss: 431.6718, val_MinusLogProbMetric: 431.6718

Epoch 31: val_loss did not improve from 419.52344
196/196 - 6s - loss: 422.1875 - MinusLogProbMetric: 422.1875 - val_loss: 431.6718 - val_MinusLogProbMetric: 431.6718 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 32/1000
2023-10-03 11:23:36.882 
Epoch 32/1000 
	 loss: 422.7018, MinusLogProbMetric: 422.7018, val_loss: 422.7084, val_MinusLogProbMetric: 422.7084

Epoch 32: val_loss did not improve from 419.52344
196/196 - 6s - loss: 422.7018 - MinusLogProbMetric: 422.7018 - val_loss: 422.7084 - val_MinusLogProbMetric: 422.7084 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 33/1000
2023-10-03 11:23:43.061 
Epoch 33/1000 
	 loss: 419.3059, MinusLogProbMetric: 419.3059, val_loss: 417.3084, val_MinusLogProbMetric: 417.3084

Epoch 33: val_loss improved from 419.52344 to 417.30838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 419.3059 - MinusLogProbMetric: 419.3059 - val_loss: 417.3084 - val_MinusLogProbMetric: 417.3084 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 34/1000
2023-10-03 11:23:49.493 
Epoch 34/1000 
	 loss: 422.8501, MinusLogProbMetric: 422.8501, val_loss: 424.2915, val_MinusLogProbMetric: 424.2915

Epoch 34: val_loss did not improve from 417.30838
196/196 - 6s - loss: 422.8501 - MinusLogProbMetric: 422.8501 - val_loss: 424.2915 - val_MinusLogProbMetric: 424.2915 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 35/1000
2023-10-03 11:23:55.638 
Epoch 35/1000 
	 loss: 419.6649, MinusLogProbMetric: 419.6649, val_loss: 419.7511, val_MinusLogProbMetric: 419.7511

Epoch 35: val_loss did not improve from 417.30838
196/196 - 6s - loss: 419.6649 - MinusLogProbMetric: 419.6649 - val_loss: 419.7511 - val_MinusLogProbMetric: 419.7511 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 36/1000
2023-10-03 11:24:01.733 
Epoch 36/1000 
	 loss: 417.5049, MinusLogProbMetric: 417.5049, val_loss: 418.9553, val_MinusLogProbMetric: 418.9553

Epoch 36: val_loss did not improve from 417.30838
196/196 - 6s - loss: 417.5049 - MinusLogProbMetric: 417.5049 - val_loss: 418.9553 - val_MinusLogProbMetric: 418.9553 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 37/1000
2023-10-03 11:24:07.887 
Epoch 37/1000 
	 loss: 416.1342, MinusLogProbMetric: 416.1342, val_loss: 414.8489, val_MinusLogProbMetric: 414.8489

Epoch 37: val_loss improved from 417.30838 to 414.84888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 416.1342 - MinusLogProbMetric: 416.1342 - val_loss: 414.8489 - val_MinusLogProbMetric: 414.8489 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 38/1000
2023-10-03 11:24:14.407 
Epoch 38/1000 
	 loss: 415.8647, MinusLogProbMetric: 415.8647, val_loss: 417.1006, val_MinusLogProbMetric: 417.1006

Epoch 38: val_loss did not improve from 414.84888
196/196 - 6s - loss: 415.8647 - MinusLogProbMetric: 415.8647 - val_loss: 417.1006 - val_MinusLogProbMetric: 417.1006 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 39/1000
2023-10-03 11:24:20.425 
Epoch 39/1000 
	 loss: 416.6656, MinusLogProbMetric: 416.6656, val_loss: 417.9756, val_MinusLogProbMetric: 417.9756

Epoch 39: val_loss did not improve from 414.84888
196/196 - 6s - loss: 416.6656 - MinusLogProbMetric: 416.6656 - val_loss: 417.9756 - val_MinusLogProbMetric: 417.9756 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 40/1000
2023-10-03 11:24:26.588 
Epoch 40/1000 
	 loss: 415.3272, MinusLogProbMetric: 415.3272, val_loss: 414.1898, val_MinusLogProbMetric: 414.1898

Epoch 40: val_loss improved from 414.84888 to 414.18979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 415.3272 - MinusLogProbMetric: 415.3272 - val_loss: 414.1898 - val_MinusLogProbMetric: 414.1898 - lr: 0.0010 - 7s/epoch - 38ms/step
Epoch 41/1000
2023-10-03 11:24:33.849 
Epoch 41/1000 
	 loss: 413.9964, MinusLogProbMetric: 413.9964, val_loss: 426.4487, val_MinusLogProbMetric: 426.4487

Epoch 41: val_loss did not improve from 414.18979
196/196 - 6s - loss: 413.9964 - MinusLogProbMetric: 413.9964 - val_loss: 426.4487 - val_MinusLogProbMetric: 426.4487 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 42/1000
2023-10-03 11:24:39.922 
Epoch 42/1000 
	 loss: 417.0556, MinusLogProbMetric: 417.0556, val_loss: 413.1345, val_MinusLogProbMetric: 413.1345

Epoch 42: val_loss improved from 414.18979 to 413.13449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 417.0556 - MinusLogProbMetric: 417.0556 - val_loss: 413.1345 - val_MinusLogProbMetric: 413.1345 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 43/1000
2023-10-03 11:24:46.645 
Epoch 43/1000 
	 loss: 415.5576, MinusLogProbMetric: 415.5576, val_loss: 412.2369, val_MinusLogProbMetric: 412.2369

Epoch 43: val_loss improved from 413.13449 to 412.23694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 415.5576 - MinusLogProbMetric: 415.5576 - val_loss: 412.2369 - val_MinusLogProbMetric: 412.2369 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 44/1000
2023-10-03 11:24:53.247 
Epoch 44/1000 
	 loss: 414.9592, MinusLogProbMetric: 414.9592, val_loss: 412.2548, val_MinusLogProbMetric: 412.2548

Epoch 44: val_loss did not improve from 412.23694
196/196 - 6s - loss: 414.9592 - MinusLogProbMetric: 414.9592 - val_loss: 412.2548 - val_MinusLogProbMetric: 412.2548 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 45/1000
2023-10-03 11:24:59.328 
Epoch 45/1000 
	 loss: 412.6625, MinusLogProbMetric: 412.6625, val_loss: 417.7613, val_MinusLogProbMetric: 417.7613

Epoch 45: val_loss did not improve from 412.23694
196/196 - 6s - loss: 412.6625 - MinusLogProbMetric: 412.6625 - val_loss: 417.7613 - val_MinusLogProbMetric: 417.7613 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 46/1000
2023-10-03 11:25:05.483 
Epoch 46/1000 
	 loss: 413.2936, MinusLogProbMetric: 413.2936, val_loss: 412.2543, val_MinusLogProbMetric: 412.2543

Epoch 46: val_loss did not improve from 412.23694
196/196 - 6s - loss: 413.2936 - MinusLogProbMetric: 413.2936 - val_loss: 412.2543 - val_MinusLogProbMetric: 412.2543 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 47/1000
2023-10-03 11:25:11.484 
Epoch 47/1000 
	 loss: 413.2498, MinusLogProbMetric: 413.2498, val_loss: 412.7342, val_MinusLogProbMetric: 412.7342

Epoch 47: val_loss did not improve from 412.23694
196/196 - 6s - loss: 413.2498 - MinusLogProbMetric: 413.2498 - val_loss: 412.7342 - val_MinusLogProbMetric: 412.7342 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 48/1000
2023-10-03 11:25:17.603 
Epoch 48/1000 
	 loss: 414.3088, MinusLogProbMetric: 414.3088, val_loss: 410.8138, val_MinusLogProbMetric: 410.8138

Epoch 48: val_loss improved from 412.23694 to 410.81378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 414.3088 - MinusLogProbMetric: 414.3088 - val_loss: 410.8138 - val_MinusLogProbMetric: 410.8138 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 49/1000
2023-10-03 11:25:24.592 
Epoch 49/1000 
	 loss: 411.8764, MinusLogProbMetric: 411.8764, val_loss: 432.5066, val_MinusLogProbMetric: 432.5066

Epoch 49: val_loss did not improve from 410.81378
196/196 - 6s - loss: 411.8764 - MinusLogProbMetric: 411.8764 - val_loss: 432.5066 - val_MinusLogProbMetric: 432.5066 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 50/1000
2023-10-03 11:25:30.688 
Epoch 50/1000 
	 loss: 413.1718, MinusLogProbMetric: 413.1718, val_loss: 411.2996, val_MinusLogProbMetric: 411.2996

Epoch 50: val_loss did not improve from 410.81378
196/196 - 6s - loss: 413.1718 - MinusLogProbMetric: 413.1718 - val_loss: 411.2996 - val_MinusLogProbMetric: 411.2996 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 51/1000
2023-10-03 11:25:36.770 
Epoch 51/1000 
	 loss: 414.1607, MinusLogProbMetric: 414.1607, val_loss: 414.2478, val_MinusLogProbMetric: 414.2478

Epoch 51: val_loss did not improve from 410.81378
196/196 - 6s - loss: 414.1607 - MinusLogProbMetric: 414.1607 - val_loss: 414.2478 - val_MinusLogProbMetric: 414.2478 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 52/1000
2023-10-03 11:25:42.864 
Epoch 52/1000 
	 loss: 410.8293, MinusLogProbMetric: 410.8293, val_loss: 410.7409, val_MinusLogProbMetric: 410.7409

Epoch 52: val_loss improved from 410.81378 to 410.74091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 410.8293 - MinusLogProbMetric: 410.8293 - val_loss: 410.7409 - val_MinusLogProbMetric: 410.7409 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 53/1000
2023-10-03 11:25:49.590 
Epoch 53/1000 
	 loss: 413.6354, MinusLogProbMetric: 413.6354, val_loss: 410.1875, val_MinusLogProbMetric: 410.1875

Epoch 53: val_loss improved from 410.74091 to 410.18753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 413.6354 - MinusLogProbMetric: 413.6354 - val_loss: 410.1875 - val_MinusLogProbMetric: 410.1875 - lr: 0.0010 - 7s/epoch - 37ms/step
Epoch 54/1000
2023-10-03 11:25:56.744 
Epoch 54/1000 
	 loss: 410.5748, MinusLogProbMetric: 410.5748, val_loss: 410.2979, val_MinusLogProbMetric: 410.2979

Epoch 54: val_loss did not improve from 410.18753
196/196 - 6s - loss: 410.5748 - MinusLogProbMetric: 410.5748 - val_loss: 410.2979 - val_MinusLogProbMetric: 410.2979 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 55/1000
2023-10-03 11:26:02.848 
Epoch 55/1000 
	 loss: 415.4695, MinusLogProbMetric: 415.4695, val_loss: 418.8087, val_MinusLogProbMetric: 418.8087

Epoch 55: val_loss did not improve from 410.18753
196/196 - 6s - loss: 415.4695 - MinusLogProbMetric: 415.4695 - val_loss: 418.8087 - val_MinusLogProbMetric: 418.8087 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 56/1000
2023-10-03 11:26:08.977 
Epoch 56/1000 
	 loss: 410.7236, MinusLogProbMetric: 410.7236, val_loss: 414.0622, val_MinusLogProbMetric: 414.0622

Epoch 56: val_loss did not improve from 410.18753
196/196 - 6s - loss: 410.7236 - MinusLogProbMetric: 410.7236 - val_loss: 414.0622 - val_MinusLogProbMetric: 414.0622 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 57/1000
2023-10-03 11:26:15.097 
Epoch 57/1000 
	 loss: 410.2990, MinusLogProbMetric: 410.2990, val_loss: 407.9072, val_MinusLogProbMetric: 407.9072

Epoch 57: val_loss improved from 410.18753 to 407.90720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 410.2990 - MinusLogProbMetric: 410.2990 - val_loss: 407.9072 - val_MinusLogProbMetric: 407.9072 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 58/1000
2023-10-03 11:26:21.788 
Epoch 58/1000 
	 loss: 410.4076, MinusLogProbMetric: 410.4076, val_loss: 413.1990, val_MinusLogProbMetric: 413.1990

Epoch 58: val_loss did not improve from 407.90720
196/196 - 6s - loss: 410.4076 - MinusLogProbMetric: 410.4076 - val_loss: 413.1990 - val_MinusLogProbMetric: 413.1990 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 59/1000
2023-10-03 11:26:27.874 
Epoch 59/1000 
	 loss: 410.6053, MinusLogProbMetric: 410.6053, val_loss: 408.2775, val_MinusLogProbMetric: 408.2775

Epoch 59: val_loss did not improve from 407.90720
196/196 - 6s - loss: 410.6053 - MinusLogProbMetric: 410.6053 - val_loss: 408.2775 - val_MinusLogProbMetric: 408.2775 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 60/1000
2023-10-03 11:26:33.931 
Epoch 60/1000 
	 loss: 409.8468, MinusLogProbMetric: 409.8468, val_loss: 417.8539, val_MinusLogProbMetric: 417.8539

Epoch 60: val_loss did not improve from 407.90720
196/196 - 6s - loss: 409.8468 - MinusLogProbMetric: 409.8468 - val_loss: 417.8539 - val_MinusLogProbMetric: 417.8539 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 61/1000
2023-10-03 11:26:40.034 
Epoch 61/1000 
	 loss: 409.4532, MinusLogProbMetric: 409.4532, val_loss: 410.3233, val_MinusLogProbMetric: 410.3233

Epoch 61: val_loss did not improve from 407.90720
196/196 - 6s - loss: 409.4532 - MinusLogProbMetric: 409.4532 - val_loss: 410.3233 - val_MinusLogProbMetric: 410.3233 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 62/1000
2023-10-03 11:26:46.102 
Epoch 62/1000 
	 loss: 410.2909, MinusLogProbMetric: 410.2909, val_loss: 411.7642, val_MinusLogProbMetric: 411.7642

Epoch 62: val_loss did not improve from 407.90720
196/196 - 6s - loss: 410.2909 - MinusLogProbMetric: 410.2909 - val_loss: 411.7642 - val_MinusLogProbMetric: 411.7642 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 63/1000
2023-10-03 11:26:52.184 
Epoch 63/1000 
	 loss: 408.3253, MinusLogProbMetric: 408.3253, val_loss: 423.4253, val_MinusLogProbMetric: 423.4253

Epoch 63: val_loss did not improve from 407.90720
196/196 - 6s - loss: 408.3253 - MinusLogProbMetric: 408.3253 - val_loss: 423.4253 - val_MinusLogProbMetric: 423.4253 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 64/1000
2023-10-03 11:26:58.268 
Epoch 64/1000 
	 loss: 410.7355, MinusLogProbMetric: 410.7355, val_loss: 416.6294, val_MinusLogProbMetric: 416.6294

Epoch 64: val_loss did not improve from 407.90720
196/196 - 6s - loss: 410.7355 - MinusLogProbMetric: 410.7355 - val_loss: 416.6294 - val_MinusLogProbMetric: 416.6294 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 65/1000
2023-10-03 11:27:04.369 
Epoch 65/1000 
	 loss: 410.1341, MinusLogProbMetric: 410.1341, val_loss: 407.5541, val_MinusLogProbMetric: 407.5541

Epoch 65: val_loss improved from 407.90720 to 407.55408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 410.1341 - MinusLogProbMetric: 410.1341 - val_loss: 407.5541 - val_MinusLogProbMetric: 407.5541 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 66/1000
2023-10-03 11:27:11.268 
Epoch 66/1000 
	 loss: 409.3748, MinusLogProbMetric: 409.3748, val_loss: 408.0823, val_MinusLogProbMetric: 408.0823

Epoch 66: val_loss did not improve from 407.55408
196/196 - 6s - loss: 409.3748 - MinusLogProbMetric: 409.3748 - val_loss: 408.0823 - val_MinusLogProbMetric: 408.0823 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 67/1000
2023-10-03 11:27:17.390 
Epoch 67/1000 
	 loss: 408.3743, MinusLogProbMetric: 408.3743, val_loss: 407.1893, val_MinusLogProbMetric: 407.1893

Epoch 67: val_loss improved from 407.55408 to 407.18933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 408.3743 - MinusLogProbMetric: 408.3743 - val_loss: 407.1893 - val_MinusLogProbMetric: 407.1893 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 68/1000
2023-10-03 11:27:24.146 
Epoch 68/1000 
	 loss: 408.7663, MinusLogProbMetric: 408.7663, val_loss: 410.8118, val_MinusLogProbMetric: 410.8118

Epoch 68: val_loss did not improve from 407.18933
196/196 - 6s - loss: 408.7663 - MinusLogProbMetric: 408.7663 - val_loss: 410.8118 - val_MinusLogProbMetric: 410.8118 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 69/1000
2023-10-03 11:27:30.327 
Epoch 69/1000 
	 loss: 408.5933, MinusLogProbMetric: 408.5933, val_loss: 412.7216, val_MinusLogProbMetric: 412.7216

Epoch 69: val_loss did not improve from 407.18933
196/196 - 6s - loss: 408.5933 - MinusLogProbMetric: 408.5933 - val_loss: 412.7216 - val_MinusLogProbMetric: 412.7216 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 70/1000
2023-10-03 11:27:36.434 
Epoch 70/1000 
	 loss: 408.4448, MinusLogProbMetric: 408.4448, val_loss: 411.5596, val_MinusLogProbMetric: 411.5596

Epoch 70: val_loss did not improve from 407.18933
196/196 - 6s - loss: 408.4448 - MinusLogProbMetric: 408.4448 - val_loss: 411.5596 - val_MinusLogProbMetric: 411.5596 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 71/1000
2023-10-03 11:27:42.474 
Epoch 71/1000 
	 loss: 408.3302, MinusLogProbMetric: 408.3302, val_loss: 408.3072, val_MinusLogProbMetric: 408.3072

Epoch 71: val_loss did not improve from 407.18933
196/196 - 6s - loss: 408.3302 - MinusLogProbMetric: 408.3302 - val_loss: 408.3072 - val_MinusLogProbMetric: 408.3072 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 72/1000
2023-10-03 11:27:48.565 
Epoch 72/1000 
	 loss: 407.7313, MinusLogProbMetric: 407.7313, val_loss: 407.1197, val_MinusLogProbMetric: 407.1197

Epoch 72: val_loss improved from 407.18933 to 407.11966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 407.7313 - MinusLogProbMetric: 407.7313 - val_loss: 407.1197 - val_MinusLogProbMetric: 407.1197 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 73/1000
2023-10-03 11:27:55.371 
Epoch 73/1000 
	 loss: 408.0835, MinusLogProbMetric: 408.0835, val_loss: 405.8041, val_MinusLogProbMetric: 405.8041

Epoch 73: val_loss improved from 407.11966 to 405.80408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 408.0835 - MinusLogProbMetric: 408.0835 - val_loss: 405.8041 - val_MinusLogProbMetric: 405.8041 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 74/1000
2023-10-03 11:28:02.432 
Epoch 74/1000 
	 loss: 407.2505, MinusLogProbMetric: 407.2505, val_loss: 404.5727, val_MinusLogProbMetric: 404.5727

Epoch 74: val_loss improved from 405.80408 to 404.57269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 407.2505 - MinusLogProbMetric: 407.2505 - val_loss: 404.5727 - val_MinusLogProbMetric: 404.5727 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 75/1000
2023-10-03 11:28:08.933 
Epoch 75/1000 
	 loss: 408.3250, MinusLogProbMetric: 408.3250, val_loss: 406.1192, val_MinusLogProbMetric: 406.1192

Epoch 75: val_loss did not improve from 404.57269
196/196 - 6s - loss: 408.3250 - MinusLogProbMetric: 408.3250 - val_loss: 406.1192 - val_MinusLogProbMetric: 406.1192 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 76/1000
2023-10-03 11:28:15.073 
Epoch 76/1000 
	 loss: 406.4207, MinusLogProbMetric: 406.4207, val_loss: 420.7806, val_MinusLogProbMetric: 420.7806

Epoch 76: val_loss did not improve from 404.57269
196/196 - 6s - loss: 406.4207 - MinusLogProbMetric: 406.4207 - val_loss: 420.7806 - val_MinusLogProbMetric: 420.7806 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 77/1000
2023-10-03 11:28:21.225 
Epoch 77/1000 
	 loss: 408.4594, MinusLogProbMetric: 408.4594, val_loss: 406.7941, val_MinusLogProbMetric: 406.7941

Epoch 77: val_loss did not improve from 404.57269
196/196 - 6s - loss: 408.4594 - MinusLogProbMetric: 408.4594 - val_loss: 406.7941 - val_MinusLogProbMetric: 406.7941 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 78/1000
2023-10-03 11:28:27.322 
Epoch 78/1000 
	 loss: 407.2168, MinusLogProbMetric: 407.2168, val_loss: 409.0377, val_MinusLogProbMetric: 409.0377

Epoch 78: val_loss did not improve from 404.57269
196/196 - 6s - loss: 407.2168 - MinusLogProbMetric: 407.2168 - val_loss: 409.0377 - val_MinusLogProbMetric: 409.0377 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 79/1000
2023-10-03 11:28:33.489 
Epoch 79/1000 
	 loss: 405.9616, MinusLogProbMetric: 405.9616, val_loss: 410.3742, val_MinusLogProbMetric: 410.3742

Epoch 79: val_loss did not improve from 404.57269
196/196 - 6s - loss: 405.9616 - MinusLogProbMetric: 405.9616 - val_loss: 410.3742 - val_MinusLogProbMetric: 410.3742 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 80/1000
2023-10-03 11:28:39.592 
Epoch 80/1000 
	 loss: 407.4257, MinusLogProbMetric: 407.4257, val_loss: 408.6006, val_MinusLogProbMetric: 408.6006

Epoch 80: val_loss did not improve from 404.57269
196/196 - 6s - loss: 407.4257 - MinusLogProbMetric: 407.4257 - val_loss: 408.6006 - val_MinusLogProbMetric: 408.6006 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 81/1000
2023-10-03 11:28:45.724 
Epoch 81/1000 
	 loss: 406.5257, MinusLogProbMetric: 406.5257, val_loss: 405.9876, val_MinusLogProbMetric: 405.9876

Epoch 81: val_loss did not improve from 404.57269
196/196 - 6s - loss: 406.5257 - MinusLogProbMetric: 406.5257 - val_loss: 405.9876 - val_MinusLogProbMetric: 405.9876 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 82/1000
2023-10-03 11:28:51.838 
Epoch 82/1000 
	 loss: 408.8127, MinusLogProbMetric: 408.8127, val_loss: 408.9617, val_MinusLogProbMetric: 408.9617

Epoch 82: val_loss did not improve from 404.57269
196/196 - 6s - loss: 408.8127 - MinusLogProbMetric: 408.8127 - val_loss: 408.9617 - val_MinusLogProbMetric: 408.9617 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 83/1000
2023-10-03 11:28:57.683 
Epoch 83/1000 
	 loss: 405.9873, MinusLogProbMetric: 405.9873, val_loss: 414.7186, val_MinusLogProbMetric: 414.7186

Epoch 83: val_loss did not improve from 404.57269
196/196 - 6s - loss: 405.9873 - MinusLogProbMetric: 405.9873 - val_loss: 414.7186 - val_MinusLogProbMetric: 414.7186 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 84/1000
2023-10-03 11:29:03.833 
Epoch 84/1000 
	 loss: 406.9991, MinusLogProbMetric: 406.9991, val_loss: 406.1946, val_MinusLogProbMetric: 406.1946

Epoch 84: val_loss did not improve from 404.57269
196/196 - 6s - loss: 406.9991 - MinusLogProbMetric: 406.9991 - val_loss: 406.1946 - val_MinusLogProbMetric: 406.1946 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 85/1000
2023-10-03 11:29:09.970 
Epoch 85/1000 
	 loss: 405.9305, MinusLogProbMetric: 405.9305, val_loss: 406.1191, val_MinusLogProbMetric: 406.1191

Epoch 85: val_loss did not improve from 404.57269
196/196 - 6s - loss: 405.9305 - MinusLogProbMetric: 405.9305 - val_loss: 406.1191 - val_MinusLogProbMetric: 406.1191 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 86/1000
2023-10-03 11:29:16.024 
Epoch 86/1000 
	 loss: 405.9835, MinusLogProbMetric: 405.9835, val_loss: 407.0540, val_MinusLogProbMetric: 407.0540

Epoch 86: val_loss did not improve from 404.57269
196/196 - 6s - loss: 405.9835 - MinusLogProbMetric: 405.9835 - val_loss: 407.0540 - val_MinusLogProbMetric: 407.0540 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 87/1000
2023-10-03 11:29:22.030 
Epoch 87/1000 
	 loss: 406.1184, MinusLogProbMetric: 406.1184, val_loss: 404.7587, val_MinusLogProbMetric: 404.7587

Epoch 87: val_loss did not improve from 404.57269
196/196 - 6s - loss: 406.1184 - MinusLogProbMetric: 406.1184 - val_loss: 404.7587 - val_MinusLogProbMetric: 404.7587 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 88/1000
2023-10-03 11:29:28.134 
Epoch 88/1000 
	 loss: 405.4261, MinusLogProbMetric: 405.4261, val_loss: 405.6707, val_MinusLogProbMetric: 405.6707

Epoch 88: val_loss did not improve from 404.57269
196/196 - 6s - loss: 405.4261 - MinusLogProbMetric: 405.4261 - val_loss: 405.6707 - val_MinusLogProbMetric: 405.6707 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 89/1000
2023-10-03 11:29:34.366 
Epoch 89/1000 
	 loss: 405.8190, MinusLogProbMetric: 405.8190, val_loss: 406.0053, val_MinusLogProbMetric: 406.0053

Epoch 89: val_loss did not improve from 404.57269
196/196 - 6s - loss: 405.8190 - MinusLogProbMetric: 405.8190 - val_loss: 406.0053 - val_MinusLogProbMetric: 406.0053 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 90/1000
2023-10-03 11:29:40.488 
Epoch 90/1000 
	 loss: 405.3390, MinusLogProbMetric: 405.3390, val_loss: 405.6692, val_MinusLogProbMetric: 405.6692

Epoch 90: val_loss did not improve from 404.57269
196/196 - 6s - loss: 405.3390 - MinusLogProbMetric: 405.3390 - val_loss: 405.6692 - val_MinusLogProbMetric: 405.6692 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 91/1000
2023-10-03 11:29:46.621 
Epoch 91/1000 
	 loss: 405.7992, MinusLogProbMetric: 405.7992, val_loss: 404.4014, val_MinusLogProbMetric: 404.4014

Epoch 91: val_loss improved from 404.57269 to 404.40137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 405.7992 - MinusLogProbMetric: 405.7992 - val_loss: 404.4014 - val_MinusLogProbMetric: 404.4014 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 92/1000
2023-10-03 11:29:53.411 
Epoch 92/1000 
	 loss: 407.2370, MinusLogProbMetric: 407.2370, val_loss: 408.5057, val_MinusLogProbMetric: 408.5057

Epoch 92: val_loss did not improve from 404.40137
196/196 - 6s - loss: 407.2370 - MinusLogProbMetric: 407.2370 - val_loss: 408.5057 - val_MinusLogProbMetric: 408.5057 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 93/1000
2023-10-03 11:29:59.555 
Epoch 93/1000 
	 loss: 406.9171, MinusLogProbMetric: 406.9171, val_loss: 405.7287, val_MinusLogProbMetric: 405.7287

Epoch 93: val_loss did not improve from 404.40137
196/196 - 6s - loss: 406.9171 - MinusLogProbMetric: 406.9171 - val_loss: 405.7287 - val_MinusLogProbMetric: 405.7287 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 94/1000
2023-10-03 11:30:05.636 
Epoch 94/1000 
	 loss: 404.9033, MinusLogProbMetric: 404.9033, val_loss: 403.9115, val_MinusLogProbMetric: 403.9115

Epoch 94: val_loss improved from 404.40137 to 403.91150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 404.9033 - MinusLogProbMetric: 404.9033 - val_loss: 403.9115 - val_MinusLogProbMetric: 403.9115 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 95/1000
2023-10-03 11:30:12.122 
Epoch 95/1000 
	 loss: 405.0712, MinusLogProbMetric: 405.0712, val_loss: 404.3763, val_MinusLogProbMetric: 404.3763

Epoch 95: val_loss did not improve from 403.91150
196/196 - 6s - loss: 405.0712 - MinusLogProbMetric: 405.0712 - val_loss: 404.3763 - val_MinusLogProbMetric: 404.3763 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 96/1000
2023-10-03 11:30:18.227 
Epoch 96/1000 
	 loss: 404.6512, MinusLogProbMetric: 404.6512, val_loss: 403.3886, val_MinusLogProbMetric: 403.3886

Epoch 96: val_loss improved from 403.91150 to 403.38861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 404.6512 - MinusLogProbMetric: 404.6512 - val_loss: 403.3886 - val_MinusLogProbMetric: 403.3886 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 97/1000
2023-10-03 11:30:24.632 
Epoch 97/1000 
	 loss: 405.3127, MinusLogProbMetric: 405.3127, val_loss: 403.2865, val_MinusLogProbMetric: 403.2865

Epoch 97: val_loss improved from 403.38861 to 403.28647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 405.3127 - MinusLogProbMetric: 405.3127 - val_loss: 403.2865 - val_MinusLogProbMetric: 403.2865 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 98/1000
2023-10-03 11:30:31.022 
Epoch 98/1000 
	 loss: 405.1962, MinusLogProbMetric: 405.1962, val_loss: 405.2101, val_MinusLogProbMetric: 405.2101

Epoch 98: val_loss did not improve from 403.28647
196/196 - 6s - loss: 405.1962 - MinusLogProbMetric: 405.1962 - val_loss: 405.2101 - val_MinusLogProbMetric: 405.2101 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 99/1000
2023-10-03 11:30:37.118 
Epoch 99/1000 
	 loss: 405.8091, MinusLogProbMetric: 405.8091, val_loss: 408.1594, val_MinusLogProbMetric: 408.1594

Epoch 99: val_loss did not improve from 403.28647
196/196 - 6s - loss: 405.8091 - MinusLogProbMetric: 405.8091 - val_loss: 408.1594 - val_MinusLogProbMetric: 408.1594 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 100/1000
2023-10-03 11:30:43.206 
Epoch 100/1000 
	 loss: 405.9394, MinusLogProbMetric: 405.9394, val_loss: 405.9435, val_MinusLogProbMetric: 405.9435

Epoch 100: val_loss did not improve from 403.28647
196/196 - 6s - loss: 405.9394 - MinusLogProbMetric: 405.9394 - val_loss: 405.9435 - val_MinusLogProbMetric: 405.9435 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 101/1000
2023-10-03 11:30:49.274 
Epoch 101/1000 
	 loss: 404.5536, MinusLogProbMetric: 404.5536, val_loss: 403.2183, val_MinusLogProbMetric: 403.2183

Epoch 101: val_loss improved from 403.28647 to 403.21829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 404.5536 - MinusLogProbMetric: 404.5536 - val_loss: 403.2183 - val_MinusLogProbMetric: 403.2183 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 102/1000
2023-10-03 11:30:56.269 
Epoch 102/1000 
	 loss: 404.0772, MinusLogProbMetric: 404.0772, val_loss: 414.6891, val_MinusLogProbMetric: 414.6891

Epoch 102: val_loss did not improve from 403.21829
196/196 - 6s - loss: 404.0772 - MinusLogProbMetric: 404.0772 - val_loss: 414.6891 - val_MinusLogProbMetric: 414.6891 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 103/1000
2023-10-03 11:31:02.348 
Epoch 103/1000 
	 loss: 404.7837, MinusLogProbMetric: 404.7837, val_loss: 403.6915, val_MinusLogProbMetric: 403.6915

Epoch 103: val_loss did not improve from 403.21829
196/196 - 6s - loss: 404.7837 - MinusLogProbMetric: 404.7837 - val_loss: 403.6915 - val_MinusLogProbMetric: 403.6915 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 104/1000
2023-10-03 11:31:08.546 
Epoch 104/1000 
	 loss: 404.1902, MinusLogProbMetric: 404.1902, val_loss: 404.9377, val_MinusLogProbMetric: 404.9377

Epoch 104: val_loss did not improve from 403.21829
196/196 - 6s - loss: 404.1902 - MinusLogProbMetric: 404.1902 - val_loss: 404.9377 - val_MinusLogProbMetric: 404.9377 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 105/1000
2023-10-03 11:31:14.621 
Epoch 105/1000 
	 loss: 403.9355, MinusLogProbMetric: 403.9355, val_loss: 405.2949, val_MinusLogProbMetric: 405.2949

Epoch 105: val_loss did not improve from 403.21829
196/196 - 6s - loss: 403.9355 - MinusLogProbMetric: 403.9355 - val_loss: 405.2949 - val_MinusLogProbMetric: 405.2949 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 106/1000
2023-10-03 11:31:20.702 
Epoch 106/1000 
	 loss: 404.3385, MinusLogProbMetric: 404.3385, val_loss: 402.4033, val_MinusLogProbMetric: 402.4033

Epoch 106: val_loss improved from 403.21829 to 402.40329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 404.3385 - MinusLogProbMetric: 404.3385 - val_loss: 402.4033 - val_MinusLogProbMetric: 402.4033 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 107/1000
2023-10-03 11:31:27.459 
Epoch 107/1000 
	 loss: 403.5568, MinusLogProbMetric: 403.5568, val_loss: 403.9892, val_MinusLogProbMetric: 403.9892

Epoch 107: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.5568 - MinusLogProbMetric: 403.5568 - val_loss: 403.9892 - val_MinusLogProbMetric: 403.9892 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 108/1000
2023-10-03 11:31:33.550 
Epoch 108/1000 
	 loss: 404.0370, MinusLogProbMetric: 404.0370, val_loss: 405.2011, val_MinusLogProbMetric: 405.2011

Epoch 108: val_loss did not improve from 402.40329
196/196 - 6s - loss: 404.0370 - MinusLogProbMetric: 404.0370 - val_loss: 405.2011 - val_MinusLogProbMetric: 405.2011 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 109/1000
2023-10-03 11:31:39.686 
Epoch 109/1000 
	 loss: 403.8254, MinusLogProbMetric: 403.8254, val_loss: 403.7308, val_MinusLogProbMetric: 403.7308

Epoch 109: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.8254 - MinusLogProbMetric: 403.8254 - val_loss: 403.7308 - val_MinusLogProbMetric: 403.7308 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 110/1000
2023-10-03 11:31:45.869 
Epoch 110/1000 
	 loss: 403.7531, MinusLogProbMetric: 403.7531, val_loss: 405.0139, val_MinusLogProbMetric: 405.0139

Epoch 110: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.7531 - MinusLogProbMetric: 403.7531 - val_loss: 405.0139 - val_MinusLogProbMetric: 405.0139 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 111/1000
2023-10-03 11:31:51.962 
Epoch 111/1000 
	 loss: 403.2951, MinusLogProbMetric: 403.2951, val_loss: 408.2080, val_MinusLogProbMetric: 408.2080

Epoch 111: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.2951 - MinusLogProbMetric: 403.2951 - val_loss: 408.2080 - val_MinusLogProbMetric: 408.2080 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 112/1000
2023-10-03 11:31:58.031 
Epoch 112/1000 
	 loss: 402.9686, MinusLogProbMetric: 402.9686, val_loss: 408.0604, val_MinusLogProbMetric: 408.0604

Epoch 112: val_loss did not improve from 402.40329
196/196 - 6s - loss: 402.9686 - MinusLogProbMetric: 402.9686 - val_loss: 408.0604 - val_MinusLogProbMetric: 408.0604 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 113/1000
2023-10-03 11:32:04.152 
Epoch 113/1000 
	 loss: 404.3781, MinusLogProbMetric: 404.3781, val_loss: 411.7404, val_MinusLogProbMetric: 411.7404

Epoch 113: val_loss did not improve from 402.40329
196/196 - 6s - loss: 404.3781 - MinusLogProbMetric: 404.3781 - val_loss: 411.7404 - val_MinusLogProbMetric: 411.7404 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 114/1000
2023-10-03 11:32:10.269 
Epoch 114/1000 
	 loss: 403.3748, MinusLogProbMetric: 403.3748, val_loss: 403.0722, val_MinusLogProbMetric: 403.0722

Epoch 114: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.3748 - MinusLogProbMetric: 403.3748 - val_loss: 403.0722 - val_MinusLogProbMetric: 403.0722 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 115/1000
2023-10-03 11:32:16.288 
Epoch 115/1000 
	 loss: 404.3566, MinusLogProbMetric: 404.3566, val_loss: 402.8141, val_MinusLogProbMetric: 402.8141

Epoch 115: val_loss did not improve from 402.40329
196/196 - 6s - loss: 404.3566 - MinusLogProbMetric: 404.3566 - val_loss: 402.8141 - val_MinusLogProbMetric: 402.8141 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 116/1000
2023-10-03 11:32:22.370 
Epoch 116/1000 
	 loss: 402.8697, MinusLogProbMetric: 402.8697, val_loss: 403.3184, val_MinusLogProbMetric: 403.3184

Epoch 116: val_loss did not improve from 402.40329
196/196 - 6s - loss: 402.8697 - MinusLogProbMetric: 402.8697 - val_loss: 403.3184 - val_MinusLogProbMetric: 403.3184 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 117/1000
2023-10-03 11:32:28.464 
Epoch 117/1000 
	 loss: 402.8806, MinusLogProbMetric: 402.8806, val_loss: 404.6907, val_MinusLogProbMetric: 404.6907

Epoch 117: val_loss did not improve from 402.40329
196/196 - 6s - loss: 402.8806 - MinusLogProbMetric: 402.8806 - val_loss: 404.6907 - val_MinusLogProbMetric: 404.6907 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 118/1000
2023-10-03 11:32:34.621 
Epoch 118/1000 
	 loss: 403.0833, MinusLogProbMetric: 403.0833, val_loss: 402.9620, val_MinusLogProbMetric: 402.9620

Epoch 118: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.0833 - MinusLogProbMetric: 403.0833 - val_loss: 402.9620 - val_MinusLogProbMetric: 402.9620 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 119/1000
2023-10-03 11:32:40.704 
Epoch 119/1000 
	 loss: 403.4178, MinusLogProbMetric: 403.4178, val_loss: 406.3216, val_MinusLogProbMetric: 406.3216

Epoch 119: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.4178 - MinusLogProbMetric: 403.4178 - val_loss: 406.3216 - val_MinusLogProbMetric: 406.3216 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 120/1000
2023-10-03 11:32:46.764 
Epoch 120/1000 
	 loss: 403.5620, MinusLogProbMetric: 403.5620, val_loss: 403.9808, val_MinusLogProbMetric: 403.9808

Epoch 120: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.5620 - MinusLogProbMetric: 403.5620 - val_loss: 403.9808 - val_MinusLogProbMetric: 403.9808 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 121/1000
2023-10-03 11:32:52.832 
Epoch 121/1000 
	 loss: 402.6672, MinusLogProbMetric: 402.6672, val_loss: 406.1844, val_MinusLogProbMetric: 406.1844

Epoch 121: val_loss did not improve from 402.40329
196/196 - 6s - loss: 402.6672 - MinusLogProbMetric: 402.6672 - val_loss: 406.1844 - val_MinusLogProbMetric: 406.1844 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 122/1000
2023-10-03 11:32:58.835 
Epoch 122/1000 
	 loss: 403.0000, MinusLogProbMetric: 403.0000, val_loss: 403.3504, val_MinusLogProbMetric: 403.3504

Epoch 122: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.0000 - MinusLogProbMetric: 403.0000 - val_loss: 403.3504 - val_MinusLogProbMetric: 403.3504 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 123/1000
2023-10-03 11:33:04.896 
Epoch 123/1000 
	 loss: 403.1544, MinusLogProbMetric: 403.1544, val_loss: 403.5999, val_MinusLogProbMetric: 403.5999

Epoch 123: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.1544 - MinusLogProbMetric: 403.1544 - val_loss: 403.5999 - val_MinusLogProbMetric: 403.5999 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 124/1000
2023-10-03 11:33:10.994 
Epoch 124/1000 
	 loss: 402.7123, MinusLogProbMetric: 402.7123, val_loss: 402.6573, val_MinusLogProbMetric: 402.6573

Epoch 124: val_loss did not improve from 402.40329
196/196 - 6s - loss: 402.7123 - MinusLogProbMetric: 402.7123 - val_loss: 402.6573 - val_MinusLogProbMetric: 402.6573 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 125/1000
2023-10-03 11:33:17.074 
Epoch 125/1000 
	 loss: 403.0446, MinusLogProbMetric: 403.0446, val_loss: 404.8386, val_MinusLogProbMetric: 404.8386

Epoch 125: val_loss did not improve from 402.40329
196/196 - 6s - loss: 403.0446 - MinusLogProbMetric: 403.0446 - val_loss: 404.8386 - val_MinusLogProbMetric: 404.8386 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 126/1000
2023-10-03 11:33:23.196 
Epoch 126/1000 
	 loss: 402.0864, MinusLogProbMetric: 402.0864, val_loss: 403.8869, val_MinusLogProbMetric: 403.8869

Epoch 126: val_loss did not improve from 402.40329
196/196 - 6s - loss: 402.0864 - MinusLogProbMetric: 402.0864 - val_loss: 403.8869 - val_MinusLogProbMetric: 403.8869 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 127/1000
2023-10-03 11:33:29.295 
Epoch 127/1000 
	 loss: 401.9183, MinusLogProbMetric: 401.9183, val_loss: 402.1900, val_MinusLogProbMetric: 402.1900

Epoch 127: val_loss improved from 402.40329 to 402.19000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 401.9183 - MinusLogProbMetric: 401.9183 - val_loss: 402.1900 - val_MinusLogProbMetric: 402.1900 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 128/1000
2023-10-03 11:33:36.123 
Epoch 128/1000 
	 loss: 402.2353, MinusLogProbMetric: 402.2353, val_loss: 404.3011, val_MinusLogProbMetric: 404.3011

Epoch 128: val_loss did not improve from 402.19000
196/196 - 6s - loss: 402.2353 - MinusLogProbMetric: 402.2353 - val_loss: 404.3011 - val_MinusLogProbMetric: 404.3011 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 129/1000
2023-10-03 11:33:42.291 
Epoch 129/1000 
	 loss: 402.2485, MinusLogProbMetric: 402.2485, val_loss: 403.6469, val_MinusLogProbMetric: 403.6469

Epoch 129: val_loss did not improve from 402.19000
196/196 - 6s - loss: 402.2485 - MinusLogProbMetric: 402.2485 - val_loss: 403.6469 - val_MinusLogProbMetric: 403.6469 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 130/1000
2023-10-03 11:33:48.318 
Epoch 130/1000 
	 loss: 402.4953, MinusLogProbMetric: 402.4953, val_loss: 403.8642, val_MinusLogProbMetric: 403.8642

Epoch 130: val_loss did not improve from 402.19000
196/196 - 6s - loss: 402.4953 - MinusLogProbMetric: 402.4953 - val_loss: 403.8642 - val_MinusLogProbMetric: 403.8642 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 131/1000
2023-10-03 11:33:54.471 
Epoch 131/1000 
	 loss: 402.0443, MinusLogProbMetric: 402.0443, val_loss: 409.7664, val_MinusLogProbMetric: 409.7664

Epoch 131: val_loss did not improve from 402.19000
196/196 - 6s - loss: 402.0443 - MinusLogProbMetric: 402.0443 - val_loss: 409.7664 - val_MinusLogProbMetric: 409.7664 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 132/1000
2023-10-03 11:34:00.565 
Epoch 132/1000 
	 loss: 401.9988, MinusLogProbMetric: 401.9988, val_loss: 402.8649, val_MinusLogProbMetric: 402.8649

Epoch 132: val_loss did not improve from 402.19000
196/196 - 6s - loss: 401.9988 - MinusLogProbMetric: 401.9988 - val_loss: 402.8649 - val_MinusLogProbMetric: 402.8649 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 133/1000
2023-10-03 11:34:06.645 
Epoch 133/1000 
	 loss: 401.6428, MinusLogProbMetric: 401.6428, val_loss: 404.1765, val_MinusLogProbMetric: 404.1765

Epoch 133: val_loss did not improve from 402.19000
196/196 - 6s - loss: 401.6428 - MinusLogProbMetric: 401.6428 - val_loss: 404.1765 - val_MinusLogProbMetric: 404.1765 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 134/1000
2023-10-03 11:34:12.744 
Epoch 134/1000 
	 loss: 402.1554, MinusLogProbMetric: 402.1554, val_loss: 403.9312, val_MinusLogProbMetric: 403.9312

Epoch 134: val_loss did not improve from 402.19000
196/196 - 6s - loss: 402.1554 - MinusLogProbMetric: 402.1554 - val_loss: 403.9312 - val_MinusLogProbMetric: 403.9312 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 135/1000
2023-10-03 11:34:18.801 
Epoch 135/1000 
	 loss: 401.7196, MinusLogProbMetric: 401.7196, val_loss: 402.2614, val_MinusLogProbMetric: 402.2614

Epoch 135: val_loss did not improve from 402.19000
196/196 - 6s - loss: 401.7196 - MinusLogProbMetric: 401.7196 - val_loss: 402.2614 - val_MinusLogProbMetric: 402.2614 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 136/1000
2023-10-03 11:34:24.845 
Epoch 136/1000 
	 loss: 401.5733, MinusLogProbMetric: 401.5733, val_loss: 402.4887, val_MinusLogProbMetric: 402.4887

Epoch 136: val_loss did not improve from 402.19000
196/196 - 6s - loss: 401.5733 - MinusLogProbMetric: 401.5733 - val_loss: 402.4887 - val_MinusLogProbMetric: 402.4887 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 137/1000
2023-10-03 11:34:30.953 
Epoch 137/1000 
	 loss: 401.7264, MinusLogProbMetric: 401.7264, val_loss: 401.5310, val_MinusLogProbMetric: 401.5310

Epoch 137: val_loss improved from 402.19000 to 401.53101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 401.7264 - MinusLogProbMetric: 401.7264 - val_loss: 401.5310 - val_MinusLogProbMetric: 401.5310 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 138/1000
2023-10-03 11:34:37.323 
Epoch 138/1000 
	 loss: 401.8153, MinusLogProbMetric: 401.8153, val_loss: 402.6717, val_MinusLogProbMetric: 402.6717

Epoch 138: val_loss did not improve from 401.53101
196/196 - 6s - loss: 401.8153 - MinusLogProbMetric: 401.8153 - val_loss: 402.6717 - val_MinusLogProbMetric: 402.6717 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 139/1000
2023-10-03 11:34:43.457 
Epoch 139/1000 
	 loss: 402.1729, MinusLogProbMetric: 402.1729, val_loss: 402.9104, val_MinusLogProbMetric: 402.9104

Epoch 139: val_loss did not improve from 401.53101
196/196 - 6s - loss: 402.1729 - MinusLogProbMetric: 402.1729 - val_loss: 402.9104 - val_MinusLogProbMetric: 402.9104 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 140/1000
2023-10-03 11:34:49.523 
Epoch 140/1000 
	 loss: 401.1110, MinusLogProbMetric: 401.1110, val_loss: 404.8484, val_MinusLogProbMetric: 404.8484

Epoch 140: val_loss did not improve from 401.53101
196/196 - 6s - loss: 401.1110 - MinusLogProbMetric: 401.1110 - val_loss: 404.8484 - val_MinusLogProbMetric: 404.8484 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 141/1000
2023-10-03 11:34:55.520 
Epoch 141/1000 
	 loss: 401.5397, MinusLogProbMetric: 401.5397, val_loss: 401.3942, val_MinusLogProbMetric: 401.3942

Epoch 141: val_loss improved from 401.53101 to 401.39420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 401.5397 - MinusLogProbMetric: 401.5397 - val_loss: 401.3942 - val_MinusLogProbMetric: 401.3942 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 142/1000
2023-10-03 11:35:01.937 
Epoch 142/1000 
	 loss: 401.0905, MinusLogProbMetric: 401.0905, val_loss: 401.9341, val_MinusLogProbMetric: 401.9341

Epoch 142: val_loss did not improve from 401.39420
196/196 - 6s - loss: 401.0905 - MinusLogProbMetric: 401.0905 - val_loss: 401.9341 - val_MinusLogProbMetric: 401.9341 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 143/1000
2023-10-03 11:35:07.977 
Epoch 143/1000 
	 loss: 401.0608, MinusLogProbMetric: 401.0608, val_loss: 401.6016, val_MinusLogProbMetric: 401.6016

Epoch 143: val_loss did not improve from 401.39420
196/196 - 6s - loss: 401.0608 - MinusLogProbMetric: 401.0608 - val_loss: 401.6016 - val_MinusLogProbMetric: 401.6016 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 144/1000
2023-10-03 11:35:14.058 
Epoch 144/1000 
	 loss: 402.0839, MinusLogProbMetric: 402.0839, val_loss: 402.2457, val_MinusLogProbMetric: 402.2457

Epoch 144: val_loss did not improve from 401.39420
196/196 - 6s - loss: 402.0839 - MinusLogProbMetric: 402.0839 - val_loss: 402.2457 - val_MinusLogProbMetric: 402.2457 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 145/1000
2023-10-03 11:35:20.165 
Epoch 145/1000 
	 loss: 401.5637, MinusLogProbMetric: 401.5637, val_loss: 404.9385, val_MinusLogProbMetric: 404.9385

Epoch 145: val_loss did not improve from 401.39420
196/196 - 6s - loss: 401.5637 - MinusLogProbMetric: 401.5637 - val_loss: 404.9385 - val_MinusLogProbMetric: 404.9385 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 146/1000
2023-10-03 11:35:26.305 
Epoch 146/1000 
	 loss: 402.0638, MinusLogProbMetric: 402.0638, val_loss: 401.7631, val_MinusLogProbMetric: 401.7631

Epoch 146: val_loss did not improve from 401.39420
196/196 - 6s - loss: 402.0638 - MinusLogProbMetric: 402.0638 - val_loss: 401.7631 - val_MinusLogProbMetric: 401.7631 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 147/1000
2023-10-03 11:35:32.405 
Epoch 147/1000 
	 loss: 400.7995, MinusLogProbMetric: 400.7995, val_loss: 399.6304, val_MinusLogProbMetric: 399.6304

Epoch 147: val_loss improved from 401.39420 to 399.63043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 400.7995 - MinusLogProbMetric: 400.7995 - val_loss: 399.6304 - val_MinusLogProbMetric: 399.6304 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 148/1000
2023-10-03 11:35:38.922 
Epoch 148/1000 
	 loss: 401.1060, MinusLogProbMetric: 401.1060, val_loss: 402.5598, val_MinusLogProbMetric: 402.5598

Epoch 148: val_loss did not improve from 399.63043
196/196 - 6s - loss: 401.1060 - MinusLogProbMetric: 401.1060 - val_loss: 402.5598 - val_MinusLogProbMetric: 402.5598 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 149/1000
2023-10-03 11:35:45.017 
Epoch 149/1000 
	 loss: 401.2011, MinusLogProbMetric: 401.2011, val_loss: 402.4264, val_MinusLogProbMetric: 402.4264

Epoch 149: val_loss did not improve from 399.63043
196/196 - 6s - loss: 401.2011 - MinusLogProbMetric: 401.2011 - val_loss: 402.4264 - val_MinusLogProbMetric: 402.4264 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 150/1000
2023-10-03 11:35:51.102 
Epoch 150/1000 
	 loss: 400.9928, MinusLogProbMetric: 400.9928, val_loss: 403.4571, val_MinusLogProbMetric: 403.4571

Epoch 150: val_loss did not improve from 399.63043
196/196 - 6s - loss: 400.9928 - MinusLogProbMetric: 400.9928 - val_loss: 403.4571 - val_MinusLogProbMetric: 403.4571 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 151/1000
2023-10-03 11:35:57.182 
Epoch 151/1000 
	 loss: 400.7186, MinusLogProbMetric: 400.7186, val_loss: 403.8544, val_MinusLogProbMetric: 403.8544

Epoch 151: val_loss did not improve from 399.63043
196/196 - 6s - loss: 400.7186 - MinusLogProbMetric: 400.7186 - val_loss: 403.8544 - val_MinusLogProbMetric: 403.8544 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 152/1000
2023-10-03 11:36:03.339 
Epoch 152/1000 
	 loss: 400.4217, MinusLogProbMetric: 400.4217, val_loss: 400.6001, val_MinusLogProbMetric: 400.6001

Epoch 152: val_loss did not improve from 399.63043
196/196 - 6s - loss: 400.4217 - MinusLogProbMetric: 400.4217 - val_loss: 400.6001 - val_MinusLogProbMetric: 400.6001 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 153/1000
2023-10-03 11:36:09.376 
Epoch 153/1000 
	 loss: 401.1049, MinusLogProbMetric: 401.1049, val_loss: 401.2481, val_MinusLogProbMetric: 401.2481

Epoch 153: val_loss did not improve from 399.63043
196/196 - 6s - loss: 401.1049 - MinusLogProbMetric: 401.1049 - val_loss: 401.2481 - val_MinusLogProbMetric: 401.2481 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 154/1000
2023-10-03 11:36:15.499 
Epoch 154/1000 
	 loss: 401.2931, MinusLogProbMetric: 401.2931, val_loss: 406.1687, val_MinusLogProbMetric: 406.1687

Epoch 154: val_loss did not improve from 399.63043
196/196 - 6s - loss: 401.2931 - MinusLogProbMetric: 401.2931 - val_loss: 406.1687 - val_MinusLogProbMetric: 406.1687 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 155/1000
2023-10-03 11:36:21.588 
Epoch 155/1000 
	 loss: 400.4520, MinusLogProbMetric: 400.4520, val_loss: 405.2300, val_MinusLogProbMetric: 405.2300

Epoch 155: val_loss did not improve from 399.63043
196/196 - 6s - loss: 400.4520 - MinusLogProbMetric: 400.4520 - val_loss: 405.2300 - val_MinusLogProbMetric: 405.2300 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 156/1000
2023-10-03 11:36:27.618 
Epoch 156/1000 
	 loss: 400.9472, MinusLogProbMetric: 400.9472, val_loss: 400.4463, val_MinusLogProbMetric: 400.4463

Epoch 156: val_loss did not improve from 399.63043
196/196 - 6s - loss: 400.9472 - MinusLogProbMetric: 400.9472 - val_loss: 400.4463 - val_MinusLogProbMetric: 400.4463 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 157/1000
2023-10-03 11:36:33.708 
Epoch 157/1000 
	 loss: 401.2129, MinusLogProbMetric: 401.2129, val_loss: 402.6312, val_MinusLogProbMetric: 402.6312

Epoch 157: val_loss did not improve from 399.63043
196/196 - 6s - loss: 401.2129 - MinusLogProbMetric: 401.2129 - val_loss: 402.6312 - val_MinusLogProbMetric: 402.6312 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 158/1000
2023-10-03 11:36:39.893 
Epoch 158/1000 
	 loss: 400.0739, MinusLogProbMetric: 400.0739, val_loss: 405.5936, val_MinusLogProbMetric: 405.5936

Epoch 158: val_loss did not improve from 399.63043
196/196 - 6s - loss: 400.0739 - MinusLogProbMetric: 400.0739 - val_loss: 405.5936 - val_MinusLogProbMetric: 405.5936 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 159/1000
2023-10-03 11:36:45.898 
Epoch 159/1000 
	 loss: 400.5640, MinusLogProbMetric: 400.5640, val_loss: 399.4622, val_MinusLogProbMetric: 399.4622

Epoch 159: val_loss improved from 399.63043 to 399.46216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 400.5640 - MinusLogProbMetric: 400.5640 - val_loss: 399.4622 - val_MinusLogProbMetric: 399.4622 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 160/1000
2023-10-03 11:36:52.457 
Epoch 160/1000 
	 loss: 400.4299, MinusLogProbMetric: 400.4299, val_loss: 401.8920, val_MinusLogProbMetric: 401.8920

Epoch 160: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.4299 - MinusLogProbMetric: 400.4299 - val_loss: 401.8920 - val_MinusLogProbMetric: 401.8920 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 161/1000
2023-10-03 11:36:58.514 
Epoch 161/1000 
	 loss: 400.6165, MinusLogProbMetric: 400.6165, val_loss: 400.3215, val_MinusLogProbMetric: 400.3215

Epoch 161: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.6165 - MinusLogProbMetric: 400.6165 - val_loss: 400.3215 - val_MinusLogProbMetric: 400.3215 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 162/1000
2023-10-03 11:37:04.599 
Epoch 162/1000 
	 loss: 402.4551, MinusLogProbMetric: 402.4551, val_loss: 403.4295, val_MinusLogProbMetric: 403.4295

Epoch 162: val_loss did not improve from 399.46216
196/196 - 6s - loss: 402.4551 - MinusLogProbMetric: 402.4551 - val_loss: 403.4295 - val_MinusLogProbMetric: 403.4295 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 163/1000
2023-10-03 11:37:10.686 
Epoch 163/1000 
	 loss: 399.6653, MinusLogProbMetric: 399.6653, val_loss: 409.2251, val_MinusLogProbMetric: 409.2251

Epoch 163: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.6653 - MinusLogProbMetric: 399.6653 - val_loss: 409.2251 - val_MinusLogProbMetric: 409.2251 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 164/1000
2023-10-03 11:37:16.787 
Epoch 164/1000 
	 loss: 400.1819, MinusLogProbMetric: 400.1819, val_loss: 401.3839, val_MinusLogProbMetric: 401.3839

Epoch 164: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.1819 - MinusLogProbMetric: 400.1819 - val_loss: 401.3839 - val_MinusLogProbMetric: 401.3839 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 165/1000
2023-10-03 11:37:22.861 
Epoch 165/1000 
	 loss: 399.8942, MinusLogProbMetric: 399.8942, val_loss: 400.2619, val_MinusLogProbMetric: 400.2619

Epoch 165: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.8942 - MinusLogProbMetric: 399.8942 - val_loss: 400.2619 - val_MinusLogProbMetric: 400.2619 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 166/1000
2023-10-03 11:37:28.999 
Epoch 166/1000 
	 loss: 400.4794, MinusLogProbMetric: 400.4794, val_loss: 400.7693, val_MinusLogProbMetric: 400.7693

Epoch 166: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.4794 - MinusLogProbMetric: 400.4794 - val_loss: 400.7693 - val_MinusLogProbMetric: 400.7693 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 167/1000
2023-10-03 11:37:35.050 
Epoch 167/1000 
	 loss: 400.2454, MinusLogProbMetric: 400.2454, val_loss: 402.5934, val_MinusLogProbMetric: 402.5934

Epoch 167: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.2454 - MinusLogProbMetric: 400.2454 - val_loss: 402.5934 - val_MinusLogProbMetric: 402.5934 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 168/1000
2023-10-03 11:37:41.074 
Epoch 168/1000 
	 loss: 399.8499, MinusLogProbMetric: 399.8499, val_loss: 400.5647, val_MinusLogProbMetric: 400.5647

Epoch 168: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.8499 - MinusLogProbMetric: 399.8499 - val_loss: 400.5647 - val_MinusLogProbMetric: 400.5647 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 169/1000
2023-10-03 11:37:47.226 
Epoch 169/1000 
	 loss: 403.9156, MinusLogProbMetric: 403.9156, val_loss: 404.3671, val_MinusLogProbMetric: 404.3671

Epoch 169: val_loss did not improve from 399.46216
196/196 - 6s - loss: 403.9156 - MinusLogProbMetric: 403.9156 - val_loss: 404.3671 - val_MinusLogProbMetric: 404.3671 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 170/1000
2023-10-03 11:37:53.274 
Epoch 170/1000 
	 loss: 399.1814, MinusLogProbMetric: 399.1814, val_loss: 401.9226, val_MinusLogProbMetric: 401.9226

Epoch 170: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.1814 - MinusLogProbMetric: 399.1814 - val_loss: 401.9226 - val_MinusLogProbMetric: 401.9226 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 171/1000
2023-10-03 11:37:59.409 
Epoch 171/1000 
	 loss: 400.0093, MinusLogProbMetric: 400.0093, val_loss: 403.6338, val_MinusLogProbMetric: 403.6338

Epoch 171: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.0093 - MinusLogProbMetric: 400.0093 - val_loss: 403.6338 - val_MinusLogProbMetric: 403.6338 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 172/1000
2023-10-03 11:38:05.453 
Epoch 172/1000 
	 loss: 399.7193, MinusLogProbMetric: 399.7193, val_loss: 403.0565, val_MinusLogProbMetric: 403.0565

Epoch 172: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.7193 - MinusLogProbMetric: 399.7193 - val_loss: 403.0565 - val_MinusLogProbMetric: 403.0565 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 173/1000
2023-10-03 11:38:11.490 
Epoch 173/1000 
	 loss: 400.8886, MinusLogProbMetric: 400.8886, val_loss: 400.0550, val_MinusLogProbMetric: 400.0550

Epoch 173: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.8886 - MinusLogProbMetric: 400.8886 - val_loss: 400.0550 - val_MinusLogProbMetric: 400.0550 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 174/1000
2023-10-03 11:38:17.613 
Epoch 174/1000 
	 loss: 399.3539, MinusLogProbMetric: 399.3539, val_loss: 401.0143, val_MinusLogProbMetric: 401.0143

Epoch 174: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.3539 - MinusLogProbMetric: 399.3539 - val_loss: 401.0143 - val_MinusLogProbMetric: 401.0143 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 175/1000
2023-10-03 11:38:23.786 
Epoch 175/1000 
	 loss: 399.5687, MinusLogProbMetric: 399.5687, val_loss: 402.0101, val_MinusLogProbMetric: 402.0101

Epoch 175: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.5687 - MinusLogProbMetric: 399.5687 - val_loss: 402.0101 - val_MinusLogProbMetric: 402.0101 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 176/1000
2023-10-03 11:38:29.771 
Epoch 176/1000 
	 loss: 400.5195, MinusLogProbMetric: 400.5195, val_loss: 404.5297, val_MinusLogProbMetric: 404.5297

Epoch 176: val_loss did not improve from 399.46216
196/196 - 6s - loss: 400.5195 - MinusLogProbMetric: 400.5195 - val_loss: 404.5297 - val_MinusLogProbMetric: 404.5297 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 177/1000
2023-10-03 11:38:35.824 
Epoch 177/1000 
	 loss: 399.7688, MinusLogProbMetric: 399.7688, val_loss: 402.1002, val_MinusLogProbMetric: 402.1002

Epoch 177: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.7688 - MinusLogProbMetric: 399.7688 - val_loss: 402.1002 - val_MinusLogProbMetric: 402.1002 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 178/1000
2023-10-03 11:38:41.895 
Epoch 178/1000 
	 loss: 399.4834, MinusLogProbMetric: 399.4834, val_loss: 403.2985, val_MinusLogProbMetric: 403.2985

Epoch 178: val_loss did not improve from 399.46216
196/196 - 6s - loss: 399.4834 - MinusLogProbMetric: 399.4834 - val_loss: 403.2985 - val_MinusLogProbMetric: 403.2985 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 179/1000
2023-10-03 11:38:47.959 
Epoch 179/1000 
	 loss: 399.4247, MinusLogProbMetric: 399.4247, val_loss: 399.4495, val_MinusLogProbMetric: 399.4495

Epoch 179: val_loss improved from 399.46216 to 399.44952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 399.4247 - MinusLogProbMetric: 399.4247 - val_loss: 399.4495 - val_MinusLogProbMetric: 399.4495 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 180/1000
2023-10-03 11:38:54.480 
Epoch 180/1000 
	 loss: 399.5887, MinusLogProbMetric: 399.5887, val_loss: 399.1197, val_MinusLogProbMetric: 399.1197

Epoch 180: val_loss improved from 399.44952 to 399.11972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 399.5887 - MinusLogProbMetric: 399.5887 - val_loss: 399.1197 - val_MinusLogProbMetric: 399.1197 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 181/1000
2023-10-03 11:39:00.762 
Epoch 181/1000 
	 loss: 399.4094, MinusLogProbMetric: 399.4094, val_loss: 405.2106, val_MinusLogProbMetric: 405.2106

Epoch 181: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.4094 - MinusLogProbMetric: 399.4094 - val_loss: 405.2106 - val_MinusLogProbMetric: 405.2106 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 182/1000
2023-10-03 11:39:06.906 
Epoch 182/1000 
	 loss: 399.2902, MinusLogProbMetric: 399.2902, val_loss: 401.2247, val_MinusLogProbMetric: 401.2247

Epoch 182: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.2902 - MinusLogProbMetric: 399.2902 - val_loss: 401.2247 - val_MinusLogProbMetric: 401.2247 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 183/1000
2023-10-03 11:39:13.002 
Epoch 183/1000 
	 loss: 399.2599, MinusLogProbMetric: 399.2599, val_loss: 401.2697, val_MinusLogProbMetric: 401.2697

Epoch 183: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.2599 - MinusLogProbMetric: 399.2599 - val_loss: 401.2697 - val_MinusLogProbMetric: 401.2697 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 184/1000
2023-10-03 11:39:19.132 
Epoch 184/1000 
	 loss: 399.4854, MinusLogProbMetric: 399.4854, val_loss: 399.8040, val_MinusLogProbMetric: 399.8040

Epoch 184: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.4854 - MinusLogProbMetric: 399.4854 - val_loss: 399.8040 - val_MinusLogProbMetric: 399.8040 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 185/1000
2023-10-03 11:39:25.144 
Epoch 185/1000 
	 loss: 399.7084, MinusLogProbMetric: 399.7084, val_loss: 399.8999, val_MinusLogProbMetric: 399.8999

Epoch 185: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.7084 - MinusLogProbMetric: 399.7084 - val_loss: 399.8999 - val_MinusLogProbMetric: 399.8999 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 186/1000
2023-10-03 11:39:31.235 
Epoch 186/1000 
	 loss: 400.0429, MinusLogProbMetric: 400.0429, val_loss: 400.3720, val_MinusLogProbMetric: 400.3720

Epoch 186: val_loss did not improve from 399.11972
196/196 - 6s - loss: 400.0429 - MinusLogProbMetric: 400.0429 - val_loss: 400.3720 - val_MinusLogProbMetric: 400.3720 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 187/1000
2023-10-03 11:39:37.348 
Epoch 187/1000 
	 loss: 398.8770, MinusLogProbMetric: 398.8770, val_loss: 405.3514, val_MinusLogProbMetric: 405.3514

Epoch 187: val_loss did not improve from 399.11972
196/196 - 6s - loss: 398.8770 - MinusLogProbMetric: 398.8770 - val_loss: 405.3514 - val_MinusLogProbMetric: 405.3514 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 188/1000
2023-10-03 11:39:43.438 
Epoch 188/1000 
	 loss: 400.2454, MinusLogProbMetric: 400.2454, val_loss: 400.2482, val_MinusLogProbMetric: 400.2482

Epoch 188: val_loss did not improve from 399.11972
196/196 - 6s - loss: 400.2454 - MinusLogProbMetric: 400.2454 - val_loss: 400.2482 - val_MinusLogProbMetric: 400.2482 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 189/1000
2023-10-03 11:39:49.512 
Epoch 189/1000 
	 loss: 399.1206, MinusLogProbMetric: 399.1206, val_loss: 400.4588, val_MinusLogProbMetric: 400.4588

Epoch 189: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.1206 - MinusLogProbMetric: 399.1206 - val_loss: 400.4588 - val_MinusLogProbMetric: 400.4588 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 190/1000
2023-10-03 11:39:55.690 
Epoch 190/1000 
	 loss: 400.0677, MinusLogProbMetric: 400.0677, val_loss: 405.7599, val_MinusLogProbMetric: 405.7599

Epoch 190: val_loss did not improve from 399.11972
196/196 - 6s - loss: 400.0677 - MinusLogProbMetric: 400.0677 - val_loss: 405.7599 - val_MinusLogProbMetric: 405.7599 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 191/1000
2023-10-03 11:40:01.917 
Epoch 191/1000 
	 loss: 399.8074, MinusLogProbMetric: 399.8074, val_loss: 403.4041, val_MinusLogProbMetric: 403.4041

Epoch 191: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.8074 - MinusLogProbMetric: 399.8074 - val_loss: 403.4041 - val_MinusLogProbMetric: 403.4041 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 192/1000
2023-10-03 11:40:07.922 
Epoch 192/1000 
	 loss: 398.4665, MinusLogProbMetric: 398.4665, val_loss: 399.4984, val_MinusLogProbMetric: 399.4984

Epoch 192: val_loss did not improve from 399.11972
196/196 - 6s - loss: 398.4665 - MinusLogProbMetric: 398.4665 - val_loss: 399.4984 - val_MinusLogProbMetric: 399.4984 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 193/1000
2023-10-03 11:40:13.900 
Epoch 193/1000 
	 loss: 400.0511, MinusLogProbMetric: 400.0511, val_loss: 401.8526, val_MinusLogProbMetric: 401.8526

Epoch 193: val_loss did not improve from 399.11972
196/196 - 6s - loss: 400.0511 - MinusLogProbMetric: 400.0511 - val_loss: 401.8526 - val_MinusLogProbMetric: 401.8526 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 194/1000
2023-10-03 11:40:20.038 
Epoch 194/1000 
	 loss: 398.9629, MinusLogProbMetric: 398.9629, val_loss: 400.7691, val_MinusLogProbMetric: 400.7691

Epoch 194: val_loss did not improve from 399.11972
196/196 - 6s - loss: 398.9629 - MinusLogProbMetric: 398.9629 - val_loss: 400.7691 - val_MinusLogProbMetric: 400.7691 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 195/1000
2023-10-03 11:40:26.091 
Epoch 195/1000 
	 loss: 398.8860, MinusLogProbMetric: 398.8860, val_loss: 402.0267, val_MinusLogProbMetric: 402.0267

Epoch 195: val_loss did not improve from 399.11972
196/196 - 6s - loss: 398.8860 - MinusLogProbMetric: 398.8860 - val_loss: 402.0267 - val_MinusLogProbMetric: 402.0267 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 196/1000
2023-10-03 11:40:32.205 
Epoch 196/1000 
	 loss: 399.1633, MinusLogProbMetric: 399.1633, val_loss: 399.8180, val_MinusLogProbMetric: 399.8180

Epoch 196: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.1633 - MinusLogProbMetric: 399.1633 - val_loss: 399.8180 - val_MinusLogProbMetric: 399.8180 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 197/1000
2023-10-03 11:40:38.233 
Epoch 197/1000 
	 loss: 398.2316, MinusLogProbMetric: 398.2316, val_loss: 399.3744, val_MinusLogProbMetric: 399.3744

Epoch 197: val_loss did not improve from 399.11972
196/196 - 6s - loss: 398.2316 - MinusLogProbMetric: 398.2316 - val_loss: 399.3744 - val_MinusLogProbMetric: 399.3744 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 198/1000
2023-10-03 11:40:44.372 
Epoch 198/1000 
	 loss: 399.7885, MinusLogProbMetric: 399.7885, val_loss: 400.8331, val_MinusLogProbMetric: 400.8331

Epoch 198: val_loss did not improve from 399.11972
196/196 - 6s - loss: 399.7885 - MinusLogProbMetric: 399.7885 - val_loss: 400.8331 - val_MinusLogProbMetric: 400.8331 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 199/1000
2023-10-03 11:40:50.512 
Epoch 199/1000 
	 loss: 398.3535, MinusLogProbMetric: 398.3535, val_loss: 400.3833, val_MinusLogProbMetric: 400.3833

Epoch 199: val_loss did not improve from 399.11972
196/196 - 6s - loss: 398.3535 - MinusLogProbMetric: 398.3535 - val_loss: 400.3833 - val_MinusLogProbMetric: 400.3833 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 200/1000
2023-10-03 11:40:56.625 
Epoch 200/1000 
	 loss: 398.7293, MinusLogProbMetric: 398.7293, val_loss: 399.3924, val_MinusLogProbMetric: 399.3924

Epoch 200: val_loss did not improve from 399.11972
196/196 - 6s - loss: 398.7293 - MinusLogProbMetric: 398.7293 - val_loss: 399.3924 - val_MinusLogProbMetric: 399.3924 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 201/1000
2023-10-03 11:41:02.733 
Epoch 201/1000 
	 loss: 398.9294, MinusLogProbMetric: 398.9294, val_loss: 398.2954, val_MinusLogProbMetric: 398.2954

Epoch 201: val_loss improved from 399.11972 to 398.29544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 398.9294 - MinusLogProbMetric: 398.9294 - val_loss: 398.2954 - val_MinusLogProbMetric: 398.2954 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 202/1000
2023-10-03 11:41:09.029 
Epoch 202/1000 
	 loss: 398.6927, MinusLogProbMetric: 398.6927, val_loss: 400.8798, val_MinusLogProbMetric: 400.8798

Epoch 202: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.6927 - MinusLogProbMetric: 398.6927 - val_loss: 400.8798 - val_MinusLogProbMetric: 400.8798 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 203/1000
2023-10-03 11:41:15.170 
Epoch 203/1000 
	 loss: 398.9838, MinusLogProbMetric: 398.9838, val_loss: 400.5952, val_MinusLogProbMetric: 400.5952

Epoch 203: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.9838 - MinusLogProbMetric: 398.9838 - val_loss: 400.5952 - val_MinusLogProbMetric: 400.5952 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 204/1000
2023-10-03 11:41:21.210 
Epoch 204/1000 
	 loss: 399.4555, MinusLogProbMetric: 399.4555, val_loss: 416.7777, val_MinusLogProbMetric: 416.7777

Epoch 204: val_loss did not improve from 398.29544
196/196 - 6s - loss: 399.4555 - MinusLogProbMetric: 399.4555 - val_loss: 416.7777 - val_MinusLogProbMetric: 416.7777 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 205/1000
2023-10-03 11:41:27.329 
Epoch 205/1000 
	 loss: 400.0951, MinusLogProbMetric: 400.0951, val_loss: 399.9653, val_MinusLogProbMetric: 399.9653

Epoch 205: val_loss did not improve from 398.29544
196/196 - 6s - loss: 400.0951 - MinusLogProbMetric: 400.0951 - val_loss: 399.9653 - val_MinusLogProbMetric: 399.9653 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 206/1000
2023-10-03 11:41:33.465 
Epoch 206/1000 
	 loss: 398.3298, MinusLogProbMetric: 398.3298, val_loss: 399.0093, val_MinusLogProbMetric: 399.0093

Epoch 206: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.3298 - MinusLogProbMetric: 398.3298 - val_loss: 399.0093 - val_MinusLogProbMetric: 399.0093 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 207/1000
2023-10-03 11:41:39.569 
Epoch 207/1000 
	 loss: 399.0431, MinusLogProbMetric: 399.0431, val_loss: 399.8643, val_MinusLogProbMetric: 399.8643

Epoch 207: val_loss did not improve from 398.29544
196/196 - 6s - loss: 399.0431 - MinusLogProbMetric: 399.0431 - val_loss: 399.8643 - val_MinusLogProbMetric: 399.8643 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 208/1000
2023-10-03 11:41:45.668 
Epoch 208/1000 
	 loss: 398.3326, MinusLogProbMetric: 398.3326, val_loss: 402.0764, val_MinusLogProbMetric: 402.0764

Epoch 208: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.3326 - MinusLogProbMetric: 398.3326 - val_loss: 402.0764 - val_MinusLogProbMetric: 402.0764 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 209/1000
2023-10-03 11:41:51.809 
Epoch 209/1000 
	 loss: 400.7054, MinusLogProbMetric: 400.7054, val_loss: 400.0210, val_MinusLogProbMetric: 400.0210

Epoch 209: val_loss did not improve from 398.29544
196/196 - 6s - loss: 400.7054 - MinusLogProbMetric: 400.7054 - val_loss: 400.0210 - val_MinusLogProbMetric: 400.0210 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 210/1000
2023-10-03 11:41:57.851 
Epoch 210/1000 
	 loss: 398.2057, MinusLogProbMetric: 398.2057, val_loss: 398.6907, val_MinusLogProbMetric: 398.6907

Epoch 210: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.2057 - MinusLogProbMetric: 398.2057 - val_loss: 398.6907 - val_MinusLogProbMetric: 398.6907 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 211/1000
2023-10-03 11:42:04.028 
Epoch 211/1000 
	 loss: 398.2010, MinusLogProbMetric: 398.2010, val_loss: 399.0239, val_MinusLogProbMetric: 399.0239

Epoch 211: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.2010 - MinusLogProbMetric: 398.2010 - val_loss: 399.0239 - val_MinusLogProbMetric: 399.0239 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 212/1000
2023-10-03 11:42:10.144 
Epoch 212/1000 
	 loss: 397.9660, MinusLogProbMetric: 397.9660, val_loss: 400.2852, val_MinusLogProbMetric: 400.2852

Epoch 212: val_loss did not improve from 398.29544
196/196 - 6s - loss: 397.9660 - MinusLogProbMetric: 397.9660 - val_loss: 400.2852 - val_MinusLogProbMetric: 400.2852 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 213/1000
2023-10-03 11:42:16.237 
Epoch 213/1000 
	 loss: 398.2780, MinusLogProbMetric: 398.2780, val_loss: 398.4515, val_MinusLogProbMetric: 398.4515

Epoch 213: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.2780 - MinusLogProbMetric: 398.2780 - val_loss: 398.4515 - val_MinusLogProbMetric: 398.4515 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 214/1000
2023-10-03 11:42:22.316 
Epoch 214/1000 
	 loss: 398.1885, MinusLogProbMetric: 398.1885, val_loss: 400.7122, val_MinusLogProbMetric: 400.7122

Epoch 214: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.1885 - MinusLogProbMetric: 398.1885 - val_loss: 400.7122 - val_MinusLogProbMetric: 400.7122 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 215/1000
2023-10-03 11:42:28.418 
Epoch 215/1000 
	 loss: 398.4430, MinusLogProbMetric: 398.4430, val_loss: 403.3096, val_MinusLogProbMetric: 403.3096

Epoch 215: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.4430 - MinusLogProbMetric: 398.4430 - val_loss: 403.3096 - val_MinusLogProbMetric: 403.3096 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 216/1000
2023-10-03 11:42:34.513 
Epoch 216/1000 
	 loss: 398.3690, MinusLogProbMetric: 398.3690, val_loss: 400.7700, val_MinusLogProbMetric: 400.7700

Epoch 216: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.3690 - MinusLogProbMetric: 398.3690 - val_loss: 400.7700 - val_MinusLogProbMetric: 400.7700 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 217/1000
2023-10-03 11:42:40.640 
Epoch 217/1000 
	 loss: 398.4423, MinusLogProbMetric: 398.4423, val_loss: 399.0204, val_MinusLogProbMetric: 399.0204

Epoch 217: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.4423 - MinusLogProbMetric: 398.4423 - val_loss: 399.0204 - val_MinusLogProbMetric: 399.0204 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 218/1000
2023-10-03 11:42:46.747 
Epoch 218/1000 
	 loss: 398.0421, MinusLogProbMetric: 398.0421, val_loss: 399.5372, val_MinusLogProbMetric: 399.5372

Epoch 218: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.0421 - MinusLogProbMetric: 398.0421 - val_loss: 399.5372 - val_MinusLogProbMetric: 399.5372 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 219/1000
2023-10-03 11:42:52.879 
Epoch 219/1000 
	 loss: 398.3590, MinusLogProbMetric: 398.3590, val_loss: 407.5748, val_MinusLogProbMetric: 407.5748

Epoch 219: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.3590 - MinusLogProbMetric: 398.3590 - val_loss: 407.5748 - val_MinusLogProbMetric: 407.5748 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 220/1000
2023-10-03 11:42:58.989 
Epoch 220/1000 
	 loss: 397.7147, MinusLogProbMetric: 397.7147, val_loss: 399.2387, val_MinusLogProbMetric: 399.2387

Epoch 220: val_loss did not improve from 398.29544
196/196 - 6s - loss: 397.7147 - MinusLogProbMetric: 397.7147 - val_loss: 399.2387 - val_MinusLogProbMetric: 399.2387 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 221/1000
2023-10-03 11:43:05.173 
Epoch 221/1000 
	 loss: 398.2460, MinusLogProbMetric: 398.2460, val_loss: 399.3568, val_MinusLogProbMetric: 399.3568

Epoch 221: val_loss did not improve from 398.29544
196/196 - 6s - loss: 398.2460 - MinusLogProbMetric: 398.2460 - val_loss: 399.3568 - val_MinusLogProbMetric: 399.3568 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 222/1000
2023-10-03 11:43:11.274 
Epoch 222/1000 
	 loss: 400.1076, MinusLogProbMetric: 400.1076, val_loss: 401.1056, val_MinusLogProbMetric: 401.1056

Epoch 222: val_loss did not improve from 398.29544
196/196 - 6s - loss: 400.1076 - MinusLogProbMetric: 400.1076 - val_loss: 401.1056 - val_MinusLogProbMetric: 401.1056 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 223/1000
2023-10-03 11:43:17.351 
Epoch 223/1000 
	 loss: 397.7362, MinusLogProbMetric: 397.7362, val_loss: 399.6249, val_MinusLogProbMetric: 399.6249

Epoch 223: val_loss did not improve from 398.29544
196/196 - 6s - loss: 397.7362 - MinusLogProbMetric: 397.7362 - val_loss: 399.6249 - val_MinusLogProbMetric: 399.6249 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 224/1000
2023-10-03 11:43:23.462 
Epoch 224/1000 
	 loss: 398.9140, MinusLogProbMetric: 398.9140, val_loss: 398.2108, val_MinusLogProbMetric: 398.2108

Epoch 224: val_loss improved from 398.29544 to 398.21078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 398.9140 - MinusLogProbMetric: 398.9140 - val_loss: 398.2108 - val_MinusLogProbMetric: 398.2108 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 225/1000
2023-10-03 11:43:30.312 
Epoch 225/1000 
	 loss: 397.4998, MinusLogProbMetric: 397.4998, val_loss: 401.3494, val_MinusLogProbMetric: 401.3494

Epoch 225: val_loss did not improve from 398.21078
196/196 - 6s - loss: 397.4998 - MinusLogProbMetric: 397.4998 - val_loss: 401.3494 - val_MinusLogProbMetric: 401.3494 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 226/1000
2023-10-03 11:43:36.421 
Epoch 226/1000 
	 loss: 398.1074, MinusLogProbMetric: 398.1074, val_loss: 398.5082, val_MinusLogProbMetric: 398.5082

Epoch 226: val_loss did not improve from 398.21078
196/196 - 6s - loss: 398.1074 - MinusLogProbMetric: 398.1074 - val_loss: 398.5082 - val_MinusLogProbMetric: 398.5082 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 227/1000
2023-10-03 11:43:42.470 
Epoch 227/1000 
	 loss: 398.1141, MinusLogProbMetric: 398.1141, val_loss: 403.9215, val_MinusLogProbMetric: 403.9215

Epoch 227: val_loss did not improve from 398.21078
196/196 - 6s - loss: 398.1141 - MinusLogProbMetric: 398.1141 - val_loss: 403.9215 - val_MinusLogProbMetric: 403.9215 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 228/1000
2023-10-03 11:43:48.541 
Epoch 228/1000 
	 loss: 398.1679, MinusLogProbMetric: 398.1679, val_loss: 399.2603, val_MinusLogProbMetric: 399.2603

Epoch 228: val_loss did not improve from 398.21078
196/196 - 6s - loss: 398.1679 - MinusLogProbMetric: 398.1679 - val_loss: 399.2603 - val_MinusLogProbMetric: 399.2603 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 229/1000
2023-10-03 11:43:54.604 
Epoch 229/1000 
	 loss: 397.2896, MinusLogProbMetric: 397.2896, val_loss: 397.4482, val_MinusLogProbMetric: 397.4482

Epoch 229: val_loss improved from 398.21078 to 397.44824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 397.2896 - MinusLogProbMetric: 397.2896 - val_loss: 397.4482 - val_MinusLogProbMetric: 397.4482 - lr: 0.0010 - 6s/epoch - 33ms/step
Epoch 230/1000
2023-10-03 11:44:01.090 
Epoch 230/1000 
	 loss: 401.9307, MinusLogProbMetric: 401.9307, val_loss: 399.0761, val_MinusLogProbMetric: 399.0761

Epoch 230: val_loss did not improve from 397.44824
196/196 - 6s - loss: 401.9307 - MinusLogProbMetric: 401.9307 - val_loss: 399.0761 - val_MinusLogProbMetric: 399.0761 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 231/1000
2023-10-03 11:44:07.177 
Epoch 231/1000 
	 loss: 397.0609, MinusLogProbMetric: 397.0609, val_loss: 398.7790, val_MinusLogProbMetric: 398.7790

Epoch 231: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.0609 - MinusLogProbMetric: 397.0609 - val_loss: 398.7790 - val_MinusLogProbMetric: 398.7790 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 232/1000
2023-10-03 11:44:13.174 
Epoch 232/1000 
	 loss: 397.6015, MinusLogProbMetric: 397.6015, val_loss: 399.6560, val_MinusLogProbMetric: 399.6560

Epoch 232: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.6015 - MinusLogProbMetric: 397.6015 - val_loss: 399.6560 - val_MinusLogProbMetric: 399.6560 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 233/1000
2023-10-03 11:44:19.298 
Epoch 233/1000 
	 loss: 397.5064, MinusLogProbMetric: 397.5064, val_loss: 398.3809, val_MinusLogProbMetric: 398.3809

Epoch 233: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.5064 - MinusLogProbMetric: 397.5064 - val_loss: 398.3809 - val_MinusLogProbMetric: 398.3809 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 234/1000
2023-10-03 11:44:25.424 
Epoch 234/1000 
	 loss: 397.1262, MinusLogProbMetric: 397.1262, val_loss: 399.8059, val_MinusLogProbMetric: 399.8059

Epoch 234: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.1262 - MinusLogProbMetric: 397.1262 - val_loss: 399.8059 - val_MinusLogProbMetric: 399.8059 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 235/1000
2023-10-03 11:44:31.532 
Epoch 235/1000 
	 loss: 397.5821, MinusLogProbMetric: 397.5821, val_loss: 401.7114, val_MinusLogProbMetric: 401.7114

Epoch 235: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.5821 - MinusLogProbMetric: 397.5821 - val_loss: 401.7114 - val_MinusLogProbMetric: 401.7114 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 236/1000
2023-10-03 11:44:37.637 
Epoch 236/1000 
	 loss: 399.1987, MinusLogProbMetric: 399.1987, val_loss: 399.1074, val_MinusLogProbMetric: 399.1074

Epoch 236: val_loss did not improve from 397.44824
196/196 - 6s - loss: 399.1987 - MinusLogProbMetric: 399.1987 - val_loss: 399.1074 - val_MinusLogProbMetric: 399.1074 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 237/1000
2023-10-03 11:44:43.728 
Epoch 237/1000 
	 loss: 397.5294, MinusLogProbMetric: 397.5294, val_loss: 399.3349, val_MinusLogProbMetric: 399.3349

Epoch 237: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.5294 - MinusLogProbMetric: 397.5294 - val_loss: 399.3349 - val_MinusLogProbMetric: 399.3349 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 238/1000
2023-10-03 11:44:49.747 
Epoch 238/1000 
	 loss: 397.6346, MinusLogProbMetric: 397.6346, val_loss: 402.1772, val_MinusLogProbMetric: 402.1772

Epoch 238: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.6346 - MinusLogProbMetric: 397.6346 - val_loss: 402.1772 - val_MinusLogProbMetric: 402.1772 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 239/1000
2023-10-03 11:44:55.821 
Epoch 239/1000 
	 loss: 398.7891, MinusLogProbMetric: 398.7891, val_loss: 398.2943, val_MinusLogProbMetric: 398.2943

Epoch 239: val_loss did not improve from 397.44824
196/196 - 6s - loss: 398.7891 - MinusLogProbMetric: 398.7891 - val_loss: 398.2943 - val_MinusLogProbMetric: 398.2943 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 240/1000
2023-10-03 11:45:01.897 
Epoch 240/1000 
	 loss: 397.4001, MinusLogProbMetric: 397.4001, val_loss: 401.0906, val_MinusLogProbMetric: 401.0906

Epoch 240: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.4001 - MinusLogProbMetric: 397.4001 - val_loss: 401.0906 - val_MinusLogProbMetric: 401.0906 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 241/1000
2023-10-03 11:45:08.051 
Epoch 241/1000 
	 loss: 397.2924, MinusLogProbMetric: 397.2924, val_loss: 398.2647, val_MinusLogProbMetric: 398.2647

Epoch 241: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.2924 - MinusLogProbMetric: 397.2924 - val_loss: 398.2647 - val_MinusLogProbMetric: 398.2647 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 242/1000
2023-10-03 11:45:14.197 
Epoch 242/1000 
	 loss: 397.1831, MinusLogProbMetric: 397.1831, val_loss: 401.5871, val_MinusLogProbMetric: 401.5871

Epoch 242: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.1831 - MinusLogProbMetric: 397.1831 - val_loss: 401.5871 - val_MinusLogProbMetric: 401.5871 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 243/1000
2023-10-03 11:45:20.317 
Epoch 243/1000 
	 loss: 397.7266, MinusLogProbMetric: 397.7266, val_loss: 404.2526, val_MinusLogProbMetric: 404.2526

Epoch 243: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.7266 - MinusLogProbMetric: 397.7266 - val_loss: 404.2526 - val_MinusLogProbMetric: 404.2526 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 244/1000
2023-10-03 11:45:26.459 
Epoch 244/1000 
	 loss: 396.9396, MinusLogProbMetric: 396.9396, val_loss: 401.0923, val_MinusLogProbMetric: 401.0923

Epoch 244: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.9396 - MinusLogProbMetric: 396.9396 - val_loss: 401.0923 - val_MinusLogProbMetric: 401.0923 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 245/1000
2023-10-03 11:45:32.553 
Epoch 245/1000 
	 loss: 399.0823, MinusLogProbMetric: 399.0823, val_loss: 399.0242, val_MinusLogProbMetric: 399.0242

Epoch 245: val_loss did not improve from 397.44824
196/196 - 6s - loss: 399.0823 - MinusLogProbMetric: 399.0823 - val_loss: 399.0242 - val_MinusLogProbMetric: 399.0242 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 246/1000
2023-10-03 11:45:38.530 
Epoch 246/1000 
	 loss: 397.6071, MinusLogProbMetric: 397.6071, val_loss: 401.1589, val_MinusLogProbMetric: 401.1589

Epoch 246: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.6071 - MinusLogProbMetric: 397.6071 - val_loss: 401.1589 - val_MinusLogProbMetric: 401.1589 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 247/1000
2023-10-03 11:45:44.499 
Epoch 247/1000 
	 loss: 397.7409, MinusLogProbMetric: 397.7409, val_loss: 398.3530, val_MinusLogProbMetric: 398.3530

Epoch 247: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.7409 - MinusLogProbMetric: 397.7409 - val_loss: 398.3530 - val_MinusLogProbMetric: 398.3530 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 248/1000
2023-10-03 11:45:50.511 
Epoch 248/1000 
	 loss: 398.0128, MinusLogProbMetric: 398.0128, val_loss: 398.3332, val_MinusLogProbMetric: 398.3332

Epoch 248: val_loss did not improve from 397.44824
196/196 - 6s - loss: 398.0128 - MinusLogProbMetric: 398.0128 - val_loss: 398.3332 - val_MinusLogProbMetric: 398.3332 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 249/1000
2023-10-03 11:45:56.535 
Epoch 249/1000 
	 loss: 396.6920, MinusLogProbMetric: 396.6920, val_loss: 398.0964, val_MinusLogProbMetric: 398.0964

Epoch 249: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.6920 - MinusLogProbMetric: 396.6920 - val_loss: 398.0964 - val_MinusLogProbMetric: 398.0964 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 250/1000
2023-10-03 11:46:02.615 
Epoch 250/1000 
	 loss: 397.2260, MinusLogProbMetric: 397.2260, val_loss: 399.3218, val_MinusLogProbMetric: 399.3218

Epoch 250: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.2260 - MinusLogProbMetric: 397.2260 - val_loss: 399.3218 - val_MinusLogProbMetric: 399.3218 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 251/1000
2023-10-03 11:46:08.617 
Epoch 251/1000 
	 loss: 397.2706, MinusLogProbMetric: 397.2706, val_loss: 400.5002, val_MinusLogProbMetric: 400.5002

Epoch 251: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.2706 - MinusLogProbMetric: 397.2706 - val_loss: 400.5002 - val_MinusLogProbMetric: 400.5002 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 252/1000
2023-10-03 11:46:14.671 
Epoch 252/1000 
	 loss: 397.5407, MinusLogProbMetric: 397.5407, val_loss: 397.9562, val_MinusLogProbMetric: 397.9562

Epoch 252: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.5407 - MinusLogProbMetric: 397.5407 - val_loss: 397.9562 - val_MinusLogProbMetric: 397.9562 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 253/1000
2023-10-03 11:46:20.634 
Epoch 253/1000 
	 loss: 396.9337, MinusLogProbMetric: 396.9337, val_loss: 403.4004, val_MinusLogProbMetric: 403.4004

Epoch 253: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.9337 - MinusLogProbMetric: 396.9337 - val_loss: 403.4004 - val_MinusLogProbMetric: 403.4004 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 254/1000
2023-10-03 11:46:26.664 
Epoch 254/1000 
	 loss: 396.7225, MinusLogProbMetric: 396.7225, val_loss: 398.7457, val_MinusLogProbMetric: 398.7457

Epoch 254: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.7225 - MinusLogProbMetric: 396.7225 - val_loss: 398.7457 - val_MinusLogProbMetric: 398.7457 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 255/1000
2023-10-03 11:46:32.733 
Epoch 255/1000 
	 loss: 397.2012, MinusLogProbMetric: 397.2012, val_loss: 398.5205, val_MinusLogProbMetric: 398.5205

Epoch 255: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.2012 - MinusLogProbMetric: 397.2012 - val_loss: 398.5205 - val_MinusLogProbMetric: 398.5205 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 256/1000
2023-10-03 11:46:38.854 
Epoch 256/1000 
	 loss: 396.8319, MinusLogProbMetric: 396.8319, val_loss: 398.7732, val_MinusLogProbMetric: 398.7732

Epoch 256: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.8319 - MinusLogProbMetric: 396.8319 - val_loss: 398.7732 - val_MinusLogProbMetric: 398.7732 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 257/1000
2023-10-03 11:46:44.941 
Epoch 257/1000 
	 loss: 397.2209, MinusLogProbMetric: 397.2209, val_loss: 398.1575, val_MinusLogProbMetric: 398.1575

Epoch 257: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.2209 - MinusLogProbMetric: 397.2209 - val_loss: 398.1575 - val_MinusLogProbMetric: 398.1575 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 258/1000
2023-10-03 11:46:51.018 
Epoch 258/1000 
	 loss: 397.3363, MinusLogProbMetric: 397.3363, val_loss: 399.9282, val_MinusLogProbMetric: 399.9282

Epoch 258: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.3363 - MinusLogProbMetric: 397.3363 - val_loss: 399.9282 - val_MinusLogProbMetric: 399.9282 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 259/1000
2023-10-03 11:46:57.023 
Epoch 259/1000 
	 loss: 397.8732, MinusLogProbMetric: 397.8732, val_loss: 401.2408, val_MinusLogProbMetric: 401.2408

Epoch 259: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.8732 - MinusLogProbMetric: 397.8732 - val_loss: 401.2408 - val_MinusLogProbMetric: 401.2408 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 260/1000
2023-10-03 11:47:03.101 
Epoch 260/1000 
	 loss: 396.5172, MinusLogProbMetric: 396.5172, val_loss: 399.9328, val_MinusLogProbMetric: 399.9328

Epoch 260: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.5172 - MinusLogProbMetric: 396.5172 - val_loss: 399.9328 - val_MinusLogProbMetric: 399.9328 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 261/1000
2023-10-03 11:47:09.204 
Epoch 261/1000 
	 loss: 396.9601, MinusLogProbMetric: 396.9601, val_loss: 406.1006, val_MinusLogProbMetric: 406.1006

Epoch 261: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.9601 - MinusLogProbMetric: 396.9601 - val_loss: 406.1006 - val_MinusLogProbMetric: 406.1006 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 262/1000
2023-10-03 11:47:15.296 
Epoch 262/1000 
	 loss: 397.6358, MinusLogProbMetric: 397.6358, val_loss: 398.8645, val_MinusLogProbMetric: 398.8645

Epoch 262: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.6358 - MinusLogProbMetric: 397.6358 - val_loss: 398.8645 - val_MinusLogProbMetric: 398.8645 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 263/1000
2023-10-03 11:47:21.438 
Epoch 263/1000 
	 loss: 396.4888, MinusLogProbMetric: 396.4888, val_loss: 398.5048, val_MinusLogProbMetric: 398.5048

Epoch 263: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.4888 - MinusLogProbMetric: 396.4888 - val_loss: 398.5048 - val_MinusLogProbMetric: 398.5048 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 264/1000
2023-10-03 11:47:27.452 
Epoch 264/1000 
	 loss: 397.4195, MinusLogProbMetric: 397.4195, val_loss: 398.3078, val_MinusLogProbMetric: 398.3078

Epoch 264: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.4195 - MinusLogProbMetric: 397.4195 - val_loss: 398.3078 - val_MinusLogProbMetric: 398.3078 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 265/1000
2023-10-03 11:47:33.410 
Epoch 265/1000 
	 loss: 397.0000, MinusLogProbMetric: 397.0000, val_loss: 399.2370, val_MinusLogProbMetric: 399.2370

Epoch 265: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.0000 - MinusLogProbMetric: 397.0000 - val_loss: 399.2370 - val_MinusLogProbMetric: 399.2370 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 266/1000
2023-10-03 11:47:39.388 
Epoch 266/1000 
	 loss: 396.7468, MinusLogProbMetric: 396.7468, val_loss: 402.3418, val_MinusLogProbMetric: 402.3418

Epoch 266: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.7468 - MinusLogProbMetric: 396.7468 - val_loss: 402.3418 - val_MinusLogProbMetric: 402.3418 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 267/1000
2023-10-03 11:47:45.436 
Epoch 267/1000 
	 loss: 396.8990, MinusLogProbMetric: 396.8990, val_loss: 402.4257, val_MinusLogProbMetric: 402.4257

Epoch 267: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.8990 - MinusLogProbMetric: 396.8990 - val_loss: 402.4257 - val_MinusLogProbMetric: 402.4257 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 268/1000
2023-10-03 11:47:51.570 
Epoch 268/1000 
	 loss: 396.3808, MinusLogProbMetric: 396.3808, val_loss: 398.2533, val_MinusLogProbMetric: 398.2533

Epoch 268: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.3808 - MinusLogProbMetric: 396.3808 - val_loss: 398.2533 - val_MinusLogProbMetric: 398.2533 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 269/1000
2023-10-03 11:47:57.683 
Epoch 269/1000 
	 loss: 396.8793, MinusLogProbMetric: 396.8793, val_loss: 399.5404, val_MinusLogProbMetric: 399.5404

Epoch 269: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.8793 - MinusLogProbMetric: 396.8793 - val_loss: 399.5404 - val_MinusLogProbMetric: 399.5404 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 270/1000
2023-10-03 11:48:03.773 
Epoch 270/1000 
	 loss: 398.4218, MinusLogProbMetric: 398.4218, val_loss: 420.6831, val_MinusLogProbMetric: 420.6831

Epoch 270: val_loss did not improve from 397.44824
196/196 - 6s - loss: 398.4218 - MinusLogProbMetric: 398.4218 - val_loss: 420.6831 - val_MinusLogProbMetric: 420.6831 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 271/1000
2023-10-03 11:48:09.880 
Epoch 271/1000 
	 loss: 397.1611, MinusLogProbMetric: 397.1611, val_loss: 403.1494, val_MinusLogProbMetric: 403.1494

Epoch 271: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.1611 - MinusLogProbMetric: 397.1611 - val_loss: 403.1494 - val_MinusLogProbMetric: 403.1494 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 272/1000
2023-10-03 11:48:15.914 
Epoch 272/1000 
	 loss: 396.3437, MinusLogProbMetric: 396.3437, val_loss: 399.9682, val_MinusLogProbMetric: 399.9682

Epoch 272: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.3437 - MinusLogProbMetric: 396.3437 - val_loss: 399.9682 - val_MinusLogProbMetric: 399.9682 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 273/1000
2023-10-03 11:48:22.018 
Epoch 273/1000 
	 loss: 397.0418, MinusLogProbMetric: 397.0418, val_loss: 400.1234, val_MinusLogProbMetric: 400.1234

Epoch 273: val_loss did not improve from 397.44824
196/196 - 6s - loss: 397.0418 - MinusLogProbMetric: 397.0418 - val_loss: 400.1234 - val_MinusLogProbMetric: 400.1234 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 274/1000
2023-10-03 11:48:28.145 
Epoch 274/1000 
	 loss: 396.4998, MinusLogProbMetric: 396.4998, val_loss: 398.2947, val_MinusLogProbMetric: 398.2947

Epoch 274: val_loss did not improve from 397.44824
196/196 - 6s - loss: 396.4998 - MinusLogProbMetric: 396.4998 - val_loss: 398.2947 - val_MinusLogProbMetric: 398.2947 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 275/1000
2023-10-03 11:48:34.208 
Epoch 275/1000 
	 loss: 398.7981, MinusLogProbMetric: 398.7981, val_loss: 402.4172, val_MinusLogProbMetric: 402.4172

Epoch 275: val_loss did not improve from 397.44824
196/196 - 6s - loss: 398.7981 - MinusLogProbMetric: 398.7981 - val_loss: 402.4172 - val_MinusLogProbMetric: 402.4172 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 276/1000
2023-10-03 11:48:40.338 
Epoch 276/1000 
	 loss: 395.8638, MinusLogProbMetric: 395.8638, val_loss: 399.9499, val_MinusLogProbMetric: 399.9499

Epoch 276: val_loss did not improve from 397.44824
196/196 - 6s - loss: 395.8638 - MinusLogProbMetric: 395.8638 - val_loss: 399.9499 - val_MinusLogProbMetric: 399.9499 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 277/1000
2023-10-03 11:48:46.468 
Epoch 277/1000 
	 loss: 396.4702, MinusLogProbMetric: 396.4702, val_loss: 397.2783, val_MinusLogProbMetric: 397.2783

Epoch 277: val_loss improved from 397.44824 to 397.27829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 396.4702 - MinusLogProbMetric: 396.4702 - val_loss: 397.2783 - val_MinusLogProbMetric: 397.2783 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 278/1000
2023-10-03 11:48:53.041 
Epoch 278/1000 
	 loss: 396.5294, MinusLogProbMetric: 396.5294, val_loss: 397.8343, val_MinusLogProbMetric: 397.8343

Epoch 278: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.5294 - MinusLogProbMetric: 396.5294 - val_loss: 397.8343 - val_MinusLogProbMetric: 397.8343 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 279/1000
2023-10-03 11:48:59.197 
Epoch 279/1000 
	 loss: 396.2417, MinusLogProbMetric: 396.2417, val_loss: 400.5183, val_MinusLogProbMetric: 400.5183

Epoch 279: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.2417 - MinusLogProbMetric: 396.2417 - val_loss: 400.5183 - val_MinusLogProbMetric: 400.5183 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 280/1000
2023-10-03 11:49:05.291 
Epoch 280/1000 
	 loss: 396.2543, MinusLogProbMetric: 396.2543, val_loss: 399.9216, val_MinusLogProbMetric: 399.9216

Epoch 280: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.2543 - MinusLogProbMetric: 396.2543 - val_loss: 399.9216 - val_MinusLogProbMetric: 399.9216 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 281/1000
2023-10-03 11:49:11.343 
Epoch 281/1000 
	 loss: 398.1635, MinusLogProbMetric: 398.1635, val_loss: 399.2999, val_MinusLogProbMetric: 399.2999

Epoch 281: val_loss did not improve from 397.27829
196/196 - 6s - loss: 398.1635 - MinusLogProbMetric: 398.1635 - val_loss: 399.2999 - val_MinusLogProbMetric: 399.2999 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 282/1000
2023-10-03 11:49:17.428 
Epoch 282/1000 
	 loss: 398.4008, MinusLogProbMetric: 398.4008, val_loss: 398.0805, val_MinusLogProbMetric: 398.0805

Epoch 282: val_loss did not improve from 397.27829
196/196 - 6s - loss: 398.4008 - MinusLogProbMetric: 398.4008 - val_loss: 398.0805 - val_MinusLogProbMetric: 398.0805 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 283/1000
2023-10-03 11:49:23.603 
Epoch 283/1000 
	 loss: 396.0145, MinusLogProbMetric: 396.0145, val_loss: 398.7790, val_MinusLogProbMetric: 398.7790

Epoch 283: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.0145 - MinusLogProbMetric: 396.0145 - val_loss: 398.7790 - val_MinusLogProbMetric: 398.7790 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 284/1000
2023-10-03 11:49:29.687 
Epoch 284/1000 
	 loss: 396.2307, MinusLogProbMetric: 396.2307, val_loss: 401.4984, val_MinusLogProbMetric: 401.4984

Epoch 284: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.2307 - MinusLogProbMetric: 396.2307 - val_loss: 401.4984 - val_MinusLogProbMetric: 401.4984 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 285/1000
2023-10-03 11:49:35.835 
Epoch 285/1000 
	 loss: 397.7338, MinusLogProbMetric: 397.7338, val_loss: 398.0526, val_MinusLogProbMetric: 398.0526

Epoch 285: val_loss did not improve from 397.27829
196/196 - 6s - loss: 397.7338 - MinusLogProbMetric: 397.7338 - val_loss: 398.0526 - val_MinusLogProbMetric: 398.0526 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 286/1000
2023-10-03 11:49:41.919 
Epoch 286/1000 
	 loss: 395.6716, MinusLogProbMetric: 395.6716, val_loss: 397.7722, val_MinusLogProbMetric: 397.7722

Epoch 286: val_loss did not improve from 397.27829
196/196 - 6s - loss: 395.6716 - MinusLogProbMetric: 395.6716 - val_loss: 397.7722 - val_MinusLogProbMetric: 397.7722 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 287/1000
2023-10-03 11:49:48.089 
Epoch 287/1000 
	 loss: 396.6370, MinusLogProbMetric: 396.6370, val_loss: 400.4437, val_MinusLogProbMetric: 400.4437

Epoch 287: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.6370 - MinusLogProbMetric: 396.6370 - val_loss: 400.4437 - val_MinusLogProbMetric: 400.4437 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 288/1000
2023-10-03 11:49:54.182 
Epoch 288/1000 
	 loss: 397.0640, MinusLogProbMetric: 397.0640, val_loss: 398.4653, val_MinusLogProbMetric: 398.4653

Epoch 288: val_loss did not improve from 397.27829
196/196 - 6s - loss: 397.0640 - MinusLogProbMetric: 397.0640 - val_loss: 398.4653 - val_MinusLogProbMetric: 398.4653 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 289/1000
2023-10-03 11:50:00.279 
Epoch 289/1000 
	 loss: 395.7701, MinusLogProbMetric: 395.7701, val_loss: 398.7820, val_MinusLogProbMetric: 398.7820

Epoch 289: val_loss did not improve from 397.27829
196/196 - 6s - loss: 395.7701 - MinusLogProbMetric: 395.7701 - val_loss: 398.7820 - val_MinusLogProbMetric: 398.7820 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 290/1000
2023-10-03 11:50:06.490 
Epoch 290/1000 
	 loss: 396.2914, MinusLogProbMetric: 396.2914, val_loss: 398.5217, val_MinusLogProbMetric: 398.5217

Epoch 290: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.2914 - MinusLogProbMetric: 396.2914 - val_loss: 398.5217 - val_MinusLogProbMetric: 398.5217 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 291/1000
2023-10-03 11:50:12.672 
Epoch 291/1000 
	 loss: 396.2989, MinusLogProbMetric: 396.2989, val_loss: 399.1296, val_MinusLogProbMetric: 399.1296

Epoch 291: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.2989 - MinusLogProbMetric: 396.2989 - val_loss: 399.1296 - val_MinusLogProbMetric: 399.1296 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 292/1000
2023-10-03 11:50:18.697 
Epoch 292/1000 
	 loss: 400.9826, MinusLogProbMetric: 400.9826, val_loss: 398.5090, val_MinusLogProbMetric: 398.5090

Epoch 292: val_loss did not improve from 397.27829
196/196 - 6s - loss: 400.9826 - MinusLogProbMetric: 400.9826 - val_loss: 398.5090 - val_MinusLogProbMetric: 398.5090 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 293/1000
2023-10-03 11:50:24.761 
Epoch 293/1000 
	 loss: 396.5044, MinusLogProbMetric: 396.5044, val_loss: 397.4203, val_MinusLogProbMetric: 397.4203

Epoch 293: val_loss did not improve from 397.27829
196/196 - 6s - loss: 396.5044 - MinusLogProbMetric: 396.5044 - val_loss: 397.4203 - val_MinusLogProbMetric: 397.4203 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 294/1000
2023-10-03 11:50:30.895 
Epoch 294/1000 
	 loss: 395.6763, MinusLogProbMetric: 395.6763, val_loss: 396.5842, val_MinusLogProbMetric: 396.5842

Epoch 294: val_loss improved from 397.27829 to 396.58420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 395.6763 - MinusLogProbMetric: 395.6763 - val_loss: 396.5842 - val_MinusLogProbMetric: 396.5842 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 295/1000
2023-10-03 11:50:37.535 
Epoch 295/1000 
	 loss: 397.3602, MinusLogProbMetric: 397.3602, val_loss: 397.5735, val_MinusLogProbMetric: 397.5735

Epoch 295: val_loss did not improve from 396.58420
196/196 - 6s - loss: 397.3602 - MinusLogProbMetric: 397.3602 - val_loss: 397.5735 - val_MinusLogProbMetric: 397.5735 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 296/1000
2023-10-03 11:50:43.662 
Epoch 296/1000 
	 loss: 395.6703, MinusLogProbMetric: 395.6703, val_loss: 400.2678, val_MinusLogProbMetric: 400.2678

Epoch 296: val_loss did not improve from 396.58420
196/196 - 6s - loss: 395.6703 - MinusLogProbMetric: 395.6703 - val_loss: 400.2678 - val_MinusLogProbMetric: 400.2678 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 297/1000
2023-10-03 11:50:49.888 
Epoch 297/1000 
	 loss: 395.7879, MinusLogProbMetric: 395.7879, val_loss: 401.9253, val_MinusLogProbMetric: 401.9253

Epoch 297: val_loss did not improve from 396.58420
196/196 - 6s - loss: 395.7879 - MinusLogProbMetric: 395.7879 - val_loss: 401.9253 - val_MinusLogProbMetric: 401.9253 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 298/1000
2023-10-03 11:50:56.065 
Epoch 298/1000 
	 loss: 397.9145, MinusLogProbMetric: 397.9145, val_loss: 396.9660, val_MinusLogProbMetric: 396.9660

Epoch 298: val_loss did not improve from 396.58420
196/196 - 6s - loss: 397.9145 - MinusLogProbMetric: 397.9145 - val_loss: 396.9660 - val_MinusLogProbMetric: 396.9660 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 299/1000
2023-10-03 11:51:02.225 
Epoch 299/1000 
	 loss: 395.7169, MinusLogProbMetric: 395.7169, val_loss: 397.6077, val_MinusLogProbMetric: 397.6077

Epoch 299: val_loss did not improve from 396.58420
196/196 - 6s - loss: 395.7169 - MinusLogProbMetric: 395.7169 - val_loss: 397.6077 - val_MinusLogProbMetric: 397.6077 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 300/1000
2023-10-03 11:51:08.297 
Epoch 300/1000 
	 loss: 396.0041, MinusLogProbMetric: 396.0041, val_loss: 403.8855, val_MinusLogProbMetric: 403.8855

Epoch 300: val_loss did not improve from 396.58420
196/196 - 6s - loss: 396.0041 - MinusLogProbMetric: 396.0041 - val_loss: 403.8855 - val_MinusLogProbMetric: 403.8855 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 301/1000
2023-10-03 11:51:14.433 
Epoch 301/1000 
	 loss: 396.1042, MinusLogProbMetric: 396.1042, val_loss: 401.0677, val_MinusLogProbMetric: 401.0677

Epoch 301: val_loss did not improve from 396.58420
196/196 - 6s - loss: 396.1042 - MinusLogProbMetric: 396.1042 - val_loss: 401.0677 - val_MinusLogProbMetric: 401.0677 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 302/1000
2023-10-03 11:51:20.495 
Epoch 302/1000 
	 loss: 396.9536, MinusLogProbMetric: 396.9536, val_loss: 398.1548, val_MinusLogProbMetric: 398.1548

Epoch 302: val_loss did not improve from 396.58420
196/196 - 6s - loss: 396.9536 - MinusLogProbMetric: 396.9536 - val_loss: 398.1548 - val_MinusLogProbMetric: 398.1548 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 303/1000
2023-10-03 11:51:26.540 
Epoch 303/1000 
	 loss: 395.4886, MinusLogProbMetric: 395.4886, val_loss: 399.2895, val_MinusLogProbMetric: 399.2895

Epoch 303: val_loss did not improve from 396.58420
196/196 - 6s - loss: 395.4886 - MinusLogProbMetric: 395.4886 - val_loss: 399.2895 - val_MinusLogProbMetric: 399.2895 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 304/1000
2023-10-03 11:51:32.551 
Epoch 304/1000 
	 loss: 396.1685, MinusLogProbMetric: 396.1685, val_loss: 397.1261, val_MinusLogProbMetric: 397.1261

Epoch 304: val_loss did not improve from 396.58420
196/196 - 6s - loss: 396.1685 - MinusLogProbMetric: 396.1685 - val_loss: 397.1261 - val_MinusLogProbMetric: 397.1261 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 305/1000
2023-10-03 11:51:38.622 
Epoch 305/1000 
	 loss: 395.4753, MinusLogProbMetric: 395.4753, val_loss: 399.0520, val_MinusLogProbMetric: 399.0520

Epoch 305: val_loss did not improve from 396.58420
196/196 - 6s - loss: 395.4753 - MinusLogProbMetric: 395.4753 - val_loss: 399.0520 - val_MinusLogProbMetric: 399.0520 - lr: 0.0010 - 6s/epoch - 31ms/step
Epoch 306/1000
2023-10-03 11:51:44.510 
Epoch 306/1000 
	 loss: 395.7936, MinusLogProbMetric: 395.7936, val_loss: 399.4081, val_MinusLogProbMetric: 399.4081

Epoch 306: val_loss did not improve from 396.58420
196/196 - 6s - loss: 395.7936 - MinusLogProbMetric: 395.7936 - val_loss: 399.4081 - val_MinusLogProbMetric: 399.4081 - lr: 0.0010 - 6s/epoch - 30ms/step
Epoch 307/1000
2023-10-03 11:51:51.104 
Epoch 307/1000 
	 loss: 396.0492, MinusLogProbMetric: 396.0492, val_loss: 397.7020, val_MinusLogProbMetric: 397.7020

Epoch 307: val_loss did not improve from 396.58420
196/196 - 7s - loss: 396.0492 - MinusLogProbMetric: 396.0492 - val_loss: 397.7020 - val_MinusLogProbMetric: 397.7020 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 308/1000
2023-10-03 11:51:57.735 
Epoch 308/1000 
	 loss: 395.5325, MinusLogProbMetric: 395.5325, val_loss: 401.8190, val_MinusLogProbMetric: 401.8190

Epoch 308: val_loss did not improve from 396.58420
196/196 - 7s - loss: 395.5325 - MinusLogProbMetric: 395.5325 - val_loss: 401.8190 - val_MinusLogProbMetric: 401.8190 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 309/1000
2023-10-03 11:52:03.936 
Epoch 309/1000 
	 loss: 396.1919, MinusLogProbMetric: 396.1919, val_loss: 398.2331, val_MinusLogProbMetric: 398.2331

Epoch 309: val_loss did not improve from 396.58420
196/196 - 6s - loss: 396.1919 - MinusLogProbMetric: 396.1919 - val_loss: 398.2331 - val_MinusLogProbMetric: 398.2331 - lr: 0.0010 - 6s/epoch - 32ms/step
Epoch 310/1000
2023-10-03 11:52:09.679 
Epoch 310/1000 
	 loss: 396.4828, MinusLogProbMetric: 396.4828, val_loss: 396.5543, val_MinusLogProbMetric: 396.5543

Epoch 310: val_loss improved from 396.58420 to 396.55426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 396.4828 - MinusLogProbMetric: 396.4828 - val_loss: 396.5543 - val_MinusLogProbMetric: 396.5543 - lr: 0.0010 - 7s/epoch - 33ms/step
Epoch 311/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 151: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-03 11:52:14.987 
Epoch 311/1000 
	 loss: inf, MinusLogProbMetric: 143372963776272337489681643995136.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 311: val_loss did not improve from 396.55426
196/196 - 5s - loss: inf - MinusLogProbMetric: 143372963776272337489681643995136.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 5s/epoch - 23ms/step
The loss history contains Inf values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 350.
===========
Train data generated in 0.42 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_350/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_350/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_350
self.data_kwargs: {'seed': 869}
self.x_data: [[ 7.9973636   4.640891    5.32509    ...  3.500269    8.638443
   6.719802  ]
 [ 8.238864    4.485278    5.284702   ...  2.8864994   7.5674953
   7.1954236 ]
 [ 8.268872    4.7704377   5.167857   ...  4.5533733   8.026902
   7.8079686 ]
 ...
 [ 5.836932   -0.15277141  4.6562195  ...  4.799403    6.4058156
   6.0447054 ]
 [ 8.8304      4.5455837   5.226686   ...  3.9410732   7.6945333
   7.224351  ]
 [ 8.222233    4.7859597   5.1849046  ...  2.020081    8.037197
   7.44482   ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_95 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  4509200   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,509,200
Trainable params: 4,509,200
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7fbaf7f03520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbaf7e33670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbaf7e33670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbaf7e332b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb3506abf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbaf7f01a50>, <keras.callbacks.ModelCheckpoint object at 0x7fbaf7e32320>, <keras.callbacks.EarlyStopping object at 0x7fbaf7e7a800>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbaf7e32d40>, <keras.callbacks.TerminateOnNaN object at 0x7fbaf7e33880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 350/360 with hyperparameters:
timestamp = 2023-10-03 11:52:16.768537
ndims = 1000
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 5
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 4509200
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.99736357e+00  4.64089108e+00  5.32508993e+00  2.54230428e+00
  6.17678785e+00  3.28207493e+00  5.66227102e+00  1.74029863e+00
  1.76519728e+00  3.78703856e+00  4.21053505e+00  3.09312010e+00
  2.03467131e+00  8.71515274e-03  3.30997020e-01  3.03643203e+00
  3.58006954e+00  7.91204977e+00  4.60797453e+00  9.32520199e+00
 -1.47658885e-01  1.08268631e+00  3.49395204e+00  5.92749643e+00
  8.83363056e+00  3.30430484e+00  2.72358418e+00  5.84306300e-01
  7.95439053e+00  4.29114294e+00  5.70392561e+00  9.62326145e+00
  1.91625190e+00  2.05868888e+00  7.86738205e+00  8.31465626e+00
  7.71446276e+00  6.16568375e+00  9.86226177e+00  4.56724548e+00
  5.94829226e+00  5.95013499e-01  4.22035885e+00  7.23472834e+00
  2.47805715e+00  8.42218876e+00  6.81143332e+00  3.05234265e+00
  7.74489641e+00 -1.49918377e-01  9.73857021e+00  8.26392531e-01
  1.01337919e+01  3.15674591e+00  7.51659107e+00 -4.07333463e-01
  2.89285135e+00  6.51461697e+00  6.24277782e+00 -2.83749938e-01
  3.82940078e+00  4.11356688e+00  6.11557388e+00  6.73437071e+00
  8.87053776e+00  4.77861261e+00  3.64606881e+00  1.49473929e+00
  8.43961716e+00  7.42175150e+00  4.57817841e+00  5.29543495e+00
  6.56515741e+00  1.46717739e+00 -1.17198992e+00  3.13418698e+00
  6.17689991e+00  4.57003880e+00  7.09853601e+00  8.16721344e+00
  4.87046480e+00  2.11677957e+00  8.49123096e+00  6.10315180e+00
  7.85925865e+00  5.53120947e+00  9.60370159e+00  2.18746519e+00
  6.79681110e+00  4.55581379e+00  4.16971874e+00  1.73112345e+00
  3.38649940e+00  9.48220348e+00  3.10462046e+00  2.14097667e+00
  5.63125849e+00  3.38328648e+00  5.20010900e+00  8.49408150e+00
  3.82229614e+00  4.20812511e+00  5.16726160e+00  5.89465952e+00
  9.87067223e+00  6.80660152e+00  7.34994841e+00  9.82929134e+00
  3.98204517e+00  3.51628542e+00 -3.33558351e-01  2.19543719e+00
  1.56361794e+00  1.56820440e+00  8.16210842e+00  6.19128990e+00
  6.25914240e+00  8.37748623e+00  2.31200051e+00  3.18747854e+00
  7.11904478e+00  4.19043159e+00 -1.50076985e-01  3.92370987e+00
  9.12942028e+00  4.13348961e+00  2.73112822e+00  9.84571934e+00
  5.88873577e+00  4.89145327e+00  1.00473905e+00  7.55370808e+00
  6.88664293e+00  9.29471016e+00  1.59780979e+00  1.04221833e+00
  7.24209642e+00  2.19776750e+00  3.42702579e+00  3.34507298e+00
  8.18159485e+00  5.71737003e+00  4.56389010e-01  4.59833431e+00
  7.35877991e+00  5.35200214e+00  5.63458967e+00  2.36878228e+00
  8.52304554e+00  5.58948660e+00  7.47698450e+00  9.61558151e+00
  4.02933073e+00  8.54585361e+00 -2.04157352e-01  3.18135262e-01
  5.99070644e+00  3.07406282e+00  2.67563176e+00  2.55177832e+00
  3.00443268e+00  7.63680124e+00  6.20905495e+00  7.80322456e+00
  7.74732971e+00  8.43296623e+00  5.25363827e+00  3.99893999e+00
  1.01927109e+01  7.05619717e+00  7.41717219e-01  5.25023270e+00
  3.77008796e+00  9.90027332e+00 -9.78221714e-01  6.02291298e+00
  6.87749577e+00  3.01261449e+00  9.65150070e+00  9.36239624e+00
  8.82814312e+00  7.89327025e-02  5.79393578e+00  2.59769440e+00
  7.66548109e+00  6.07534552e+00  5.36576509e+00  1.34668684e+00
  4.34030962e+00  6.31810141e+00  9.62684059e+00  4.88660431e+00
  9.60818291e+00  9.36019993e+00  6.71263504e+00  5.22296143e+00
  4.42309809e+00  7.77644825e+00 -1.76710457e-01  1.15464127e+00
  1.00100291e+00  8.11944199e+00  3.27240157e+00  6.37353086e+00
  5.53368807e+00  4.70193911e+00  5.07406664e+00  7.97970533e+00
  9.54048157e+00  4.72122574e+00  6.13606310e+00  3.54982883e-01
  1.08992491e+01  5.27934456e+00  7.85659790e+00  7.82055616e-01
  4.37489986e+00  2.60340428e+00  2.48891091e+00  9.05652618e+00
  6.87852859e+00  4.07554960e+00  1.03486490e+00  9.66742897e+00
  9.06607342e+00  3.15981674e+00  3.57244402e-01  5.63626289e+00
  9.92044353e+00  8.58868217e+00  6.57505751e+00  3.29356098e+00
  3.24422455e+00  9.00940800e+00  5.09291124e+00  2.85960793e-01
  2.23807621e+00  2.78734875e+00  1.29828584e+00  3.93540382e+00
  8.03243542e+00  6.78995275e+00  5.24697185e-01  1.17165899e+00
  3.27114582e+00  3.83798814e+00  8.53496361e+00  8.92454338e+00
  4.21771574e+00  6.39960194e+00  9.93754196e+00  7.64902067e+00
  6.44687891e+00  7.72444129e-01  2.29160905e+00  8.52109241e+00
  1.77464163e+00  8.37704659e+00  7.40880537e+00  2.87169456e+00
  5.39322519e+00  6.86864281e+00  7.26533747e+00  4.94568443e+00
  1.19446182e+00  6.65035486e+00  3.40026999e+00  9.23955250e+00
  2.17107248e+00  2.47258735e+00  2.43059564e+00  6.30585194e+00
  8.65539742e+00  1.45929515e+00  3.26658463e+00  5.87293768e+00
  4.54141855e+00  4.57416773e+00  8.90260696e+00  8.79572392e+00
  4.92429781e+00  6.65130472e+00  8.80965042e+00  4.23099279e+00
  2.90440130e+00  6.38359404e+00  5.79758072e+00  8.99278259e+00
  5.63759208e-01  9.11637592e+00  5.69089317e+00  3.27839077e-01
  2.98394132e+00  7.03010273e+00  3.88054943e+00  9.76950550e+00
  5.22733927e+00  2.11281967e+00  8.35765839e+00  5.03978920e+00
  7.56217527e+00  8.23209476e+00  6.36810303e+00  4.07139969e+00
  1.73890018e+00  7.25630856e+00  9.09160614e+00  3.28961945e+00
  9.31139088e+00  5.12792778e+00  6.50060415e+00  8.50181961e+00
  4.91192722e+00  8.58134687e-01  1.00498667e+01  2.70127869e+00
  6.64366674e+00  4.16276121e+00  4.07800245e+00  2.51310420e+00
  6.86618900e+00  1.04104257e+00  8.65216160e+00  8.34197235e+00
  5.48617649e+00  3.98194122e+00  2.67905617e+00  5.19228172e+00
  7.82842445e+00  2.60275602e+00  4.39831114e+00  2.98556614e+00
  2.60362029e+00  1.57165313e+00  9.64633369e+00  6.88340855e+00
  7.78283501e+00  1.10886335e+00  4.86602402e+00  8.91554070e+00
 -1.24701262e-02  1.84407997e+00  6.24502945e+00  6.37515545e+00
  7.90489101e+00  9.60888958e+00  1.45247686e+00  2.45699430e+00
  4.28265285e+00  6.50822818e-02  1.12332261e+00  2.24159002e+00
  4.71833563e+00  2.72241974e+00  8.28100014e+00  3.28441453e+00
  9.11346054e+00  1.05609627e+01  3.85242462e+00  3.12269568e+00
  2.30633688e+00  7.71115065e+00  2.85296941e+00  3.28263164e+00
  4.81248856e+00  6.73265219e+00  1.02898769e+01  3.37639761e+00
  3.19120789e+00  2.37913132e+00  3.12198162e+00  9.28771555e-01
  5.18783569e+00  1.00527916e+01  1.98218155e+00  3.54625010e+00
  9.82042217e+00  7.37978649e+00  4.98093176e+00  7.74178505e+00
  4.38426554e-01  5.04215908e+00  1.37913942e+00  2.33643341e+00
  1.43258142e+00  3.60961819e+00  4.80302668e+00  9.63227654e+00
  2.33755469e+00  4.40754080e+00  8.69386292e+00  8.12515831e+00
  5.90748072e+00  2.80811763e+00  5.03586197e+00  1.81783056e+00
  1.78456891e+00  4.50792408e+00  7.38818550e+00  5.70210075e+00
  7.67465544e+00  7.92264366e+00 -1.91134661e-01  5.84350491e+00
  6.51570177e+00  9.74592590e+00 -2.64375985e-01  4.72814274e+00
  1.51264036e+00  5.40283203e-01  7.05697918e+00  7.51599932e+00
  7.27936220e+00 -2.28865266e-01  3.87274265e+00  2.85972893e-01
  2.46495175e+00  1.80521321e+00  2.14937496e+00  3.77834606e+00
  3.18217349e+00  1.02763710e+01  3.37995410e+00  8.93162251e+00
  1.96273518e+00  9.14139557e+00 -3.55325580e-01  4.60834980e+00
 -5.89213148e-03  8.24484348e+00  1.03387394e+01  5.88764489e-01
  9.74320602e+00  8.21028519e+00  3.25056553e+00  9.08056259e+00
  2.59105110e+00  3.95399547e+00  9.59685862e-01  7.63103676e+00
 -4.04038966e-01  5.37858295e+00  6.89733744e+00  9.44393730e+00
  1.41217971e+00  7.53022814e+00  7.95942068e+00  5.70683289e+00
  5.70691347e+00 -2.19492435e-01  5.11235189e+00  3.95949173e+00
  9.22287846e+00  2.71982121e+00  5.80306816e+00  4.00738478e+00
  9.05252075e+00  2.45202136e+00  5.51476240e-01  6.91309261e+00
  3.62627077e+00  5.96777725e+00  3.05229813e-01  9.78659534e+00
  9.17897224e+00  5.76029110e+00  9.53746438e-01  3.57887888e+00
  9.68739605e+00  6.38989639e+00  1.90702987e+00  4.66415691e+00
  9.59853172e+00  5.05139923e+00  1.05777577e-01  6.60696030e+00
  2.90498590e+00  9.84483898e-01  4.15751266e+00  6.08889675e+00
 -7.97456264e-01  4.97374535e+00  5.03539562e+00  8.87364101e+00
  4.56227398e+00  6.91264105e+00  6.87507677e+00  8.22698975e+00
  3.93433619e+00  6.01061106e+00  5.63419819e+00  3.52927715e-01
  8.35077000e+00  3.63118911e+00  6.62209034e+00  5.41198444e+00
  6.93401527e+00  5.77218914e+00  1.43877172e+00  6.73276329e+00
  7.93194485e+00  7.98630357e-01  6.14384651e+00  6.38910675e+00
  9.41168022e+00  2.80262136e+00  1.86243892e+00  3.21781826e+00
  2.72202253e+00  7.37366581e+00  7.33060360e+00  6.61256790e+00
  2.12070441e+00  8.59328461e+00  1.00850325e+01  5.48360109e+00
  7.31572151e+00  5.40123463e+00  2.76470375e+00  2.38635635e+00
  2.60676074e+00  1.89509916e+00  8.99138355e+00  8.75982761e+00
  4.44820881e+00  3.67402339e+00  9.41257381e+00  2.81948304e+00
  5.31368160e+00  2.30343890e+00  1.82023335e+00  6.24686146e+00
  8.02147388e+00  6.23021078e+00  7.50943947e+00  9.98944378e+00
  9.65496063e+00  3.44390488e+00  9.90036297e+00  1.90570307e+00
  6.37369108e+00  6.97235632e+00  1.02818668e+00  1.01110497e+01
  3.08032537e+00  6.21633768e+00  8.70998573e+00  5.46199799e+00
  1.22746360e+00  8.02329159e+00  4.09109497e+00  1.01594963e+01
  8.81569862e+00  8.09971428e+00  3.88408160e+00  7.92610073e+00
  9.26926994e+00  6.73566246e+00  5.76671696e+00 -6.34171844e-01
  8.21392536e+00  8.59858513e+00  3.17632437e+00  2.96401882e+00
  4.87884188e+00  1.46060300e+00  7.75016904e-01  5.95933533e+00
  2.85177946e+00  4.70948601e+00  4.13632631e+00  9.87929535e+00
  1.08665876e+01  9.14421260e-01  8.82590580e+00  6.48437381e-01
  5.33739424e+00  2.29306793e+00  4.13651562e+00  6.47616768e+00
  4.24598312e+00  8.10760880e+00  7.45779276e+00  9.63120937e+00
  4.89558411e+00  8.02083302e+00  3.24438405e+00  3.59759498e+00
  4.09306526e+00  4.22914410e+00  9.30973232e-01  5.26565123e+00
  1.01665020e-01  9.00792408e+00  1.68276280e-01  4.04053593e+00
  2.87474799e+00  2.48327708e+00  7.51841593e+00  6.77074766e+00
  1.45175838e+00  6.70365572e+00  8.29291821e+00  7.64892721e+00
  1.97199440e+00  6.84292603e+00  7.54025269e+00  8.23593044e+00
  5.50070477e+00  3.45322466e+00  9.49723530e+00  8.45789242e+00
  5.81870270e+00  3.73827100e+00  8.91483879e+00  5.10812187e+00
  8.88178647e-01  4.46295691e+00  8.31669044e+00  4.49498892e+00
  5.43265700e-01  6.11975908e+00  9.95184231e+00  9.05875778e+00
  4.80623627e+00  9.23525047e+00  7.71717167e+00  5.08452940e+00
  4.99330044e+00  7.49071932e+00  7.34964132e+00  1.30181521e-01
  8.31134415e+00  4.43235731e+00  6.71253824e+00  7.91646671e+00
  4.65235710e+00  1.59666359e+00  4.05373240e+00  7.26302922e-01
  6.89302731e+00  9.45408058e+00  9.24543953e+00  4.22942209e+00
  5.97023821e+00  2.74910712e+00  5.68657923e+00  2.15732288e+00
  6.35400105e+00  2.63340044e+00  9.43256760e+00  6.54860783e+00
  9.90323901e-01  1.34424239e-01  9.92843723e+00  3.24450016e+00
  1.74554217e+00  3.04431581e+00  8.90259075e+00  7.87959576e+00
  5.06868839e+00  4.79971409e+00  2.77283359e+00  5.49839926e+00
  4.38119030e+00  2.14065003e+00  6.93851089e+00  4.98988056e+00
  7.73501110e+00  4.64208126e+00  4.79577160e+00  7.85067606e+00
  4.75812960e+00  9.79898643e+00  7.09330177e+00  8.39217281e+00
  9.55065346e+00  3.69468021e+00  5.54081106e+00  6.72979546e+00
  6.70603752e+00  9.38065147e+00  2.90241790e+00  2.52509665e+00
  8.94938564e+00  8.49423122e+00  2.13432574e+00  5.68594837e+00
  6.90361071e+00  4.37006855e+00  5.16254711e+00  5.72680283e+00
  5.19291306e+00  7.52211380e+00 -2.97093898e-01  2.55037498e+00
  2.54706359e+00  6.59417248e+00  7.53437948e+00  7.30045140e-01
  1.39935553e+00  6.93793297e+00  6.43762541e+00  5.88978815e+00
  9.83007526e+00  6.00890970e+00  6.66946948e-01  6.86854553e+00
  8.03701878e+00  6.11670637e+00  5.35453987e+00  9.57577324e+00
  1.49704802e+00  4.92001712e-01  1.53974497e+00  2.13410378e+00
  5.92641163e+00  6.05311298e+00  2.26492047e+00  4.60379982e+00
  4.60677052e+00  3.77398515e+00  7.27591801e+00  4.27604628e+00
  7.54144859e+00  1.66404676e+00  5.16442060e+00  8.22671890e+00
  1.71123445e+00  4.62352562e+00  6.08017778e+00  2.88557911e+00
  7.73235178e+00  6.43578827e-01  9.65846920e+00  4.54546928e+00
  3.51417971e+00  1.19948006e+00  5.29146385e+00  6.01765347e+00
  2.32322073e+00  2.89909482e+00  9.31757736e+00  6.46213627e+00
  2.06418347e+00  9.80539680e-01  8.66675568e+00  3.23060274e+00
  2.10251164e+00  9.88873005e+00  3.76415300e+00  8.50292981e-01
  5.07699108e+00  3.88809896e+00  9.39960575e+00  5.59372187e-01
 -5.31555176e-01  8.84760439e-01  9.86167717e+00  4.76715946e+00
  5.13683701e+00  5.85030556e+00  6.22967052e+00  1.53158998e+00
  6.54711390e+00  3.91384292e+00  2.45552421e+00  5.99128914e+00
  3.54900932e+00  2.41717911e+00  8.30238152e+00  1.64661229e+00
  2.59730220e+00  3.85882235e+00  2.47771192e+00  2.10063434e+00
  9.37286758e+00  3.17066669e+00  2.51538455e-01  5.38400126e+00
  1.23733842e+00  3.78326035e+00  9.20017338e+00  2.47332525e+00
  5.67668962e+00  5.17319727e+00  2.06113005e+00  6.44534540e+00
  8.66131496e+00  5.21378100e-01  6.50092697e+00  6.78853512e+00
  8.54306507e+00  1.80599499e+00  2.05283189e+00  6.85634184e+00
  1.15905434e-01  6.99897051e+00  8.32928753e+00  4.55407476e+00
  4.38788176e+00  3.54872870e+00 -4.62173462e-01  1.36794913e+00
  9.69468594e+00  8.38702488e+00  1.90114570e+00  7.94150639e+00
  3.36321092e+00  2.99465132e+00  1.99559021e+00  9.02903366e+00
  9.85151100e+00  3.23628950e+00  7.55823898e+00  3.70709014e+00
  1.74990630e+00  3.48457527e+00  5.45959711e+00  1.58513451e+00
  8.12769318e+00  4.73363543e+00  2.32966566e+00  7.44186544e+00
  7.94482851e+00  3.54043078e+00  7.04913664e+00  4.02170229e+00
  1.01847572e+01  8.72599220e+00  3.30791759e+00 -1.08366966e-01
  6.66013432e+00  1.91774535e+00  1.22588658e+00  1.02105055e+01
  9.86835957e+00  2.95132232e+00  3.01797199e+00  1.64833105e+00
  4.49072456e+00  8.49544430e+00  6.82054377e+00  9.07019711e+00
  1.68629599e+00  7.90870810e+00  9.09079742e+00  2.92365527e+00
  6.90887928e+00  4.17319202e+00  3.43049693e+00  9.02845764e+00
  4.29767895e+00  1.09342468e+00  3.74337578e+00  4.81818581e+00
  7.20623732e+00  1.07804976e+01  8.88462162e+00  9.30848122e+00
  4.87059927e+00  7.08490753e+00  2.95342398e+00  1.67907417e+00
  3.60964656e+00  7.03290796e+00  3.57637072e+00  8.74868298e+00
  8.52540398e+00  2.05524230e+00  3.42940140e+00  8.83274364e+00
  1.58101547e+00  3.33796239e+00  3.42331791e+00  6.96280146e+00
  3.25254261e-01 -1.25156724e+00  5.89129639e+00  4.46135187e+00
  6.51815891e+00  2.47705388e+00  9.10450840e+00  8.03964901e+00
  6.83334887e-01  8.24278164e+00  9.73476696e+00  1.00054388e+01
  7.11009216e+00  1.24967217e+00  5.68010473e+00  2.01691651e+00
  9.84855938e+00  7.23796368e+00  6.43789339e+00  6.97472811e+00
  4.51600838e+00  2.94856024e+00  9.13858128e+00  3.29138970e+00
  3.09904784e-01  4.01227283e+00  9.32983780e+00  5.93039370e+00
  1.10625067e+01  1.13827813e+00  1.01018476e+01  5.29424810e+00
  5.45073414e+00  5.82157803e+00  1.17043698e+00  3.22519493e+00
  7.34947968e+00  6.69572353e-01  1.62995577e+00  1.85301498e-01
  8.83085346e+00  5.87161255e+00  1.81599593e+00  7.31665230e+00
  9.78809166e+00  9.34503746e+00  9.14690876e+00  3.59607875e-01
  5.99414158e+00  2.59369469e+00  5.78580284e+00  3.18324995e+00
  1.74265718e+00  4.42781639e+00  4.24171114e+00  9.89190197e+00
  1.12621486e-02  4.74760723e+00  8.55551147e+00  7.21490955e+00
  8.37855244e+00  5.55442858e+00  7.92241478e+00  5.13917971e+00
  3.15440607e+00  8.63028622e+00  5.67239189e+00  1.22252262e+00
  7.74803591e+00  9.87097359e+00  4.27992439e+00  3.46016455e+00
  5.19145608e-01  6.58222771e+00  1.11442089e+01 -5.05199075e-01
  4.88900900e+00  1.03596199e+00  3.49793643e-01  6.70542479e+00
  8.65525723e+00  3.48391366e+00  8.85622883e+00  1.32473469e+00
  1.43105400e+00  9.27033043e+00  5.13401794e+00  6.83637190e+00
  3.17043185e-01  1.26743388e+00  5.01762807e-01  9.27802563e+00
  1.43500125e+00  8.44762039e+00  3.54408193e+00  7.80078506e+00
  3.90129399e+00  3.20132899e+00  1.04541802e+00  2.72584295e+00
  4.51778460e+00  7.11837149e+00  4.60630083e+00  9.86540890e+00
  6.96063471e+00 -3.97378445e-01  3.62382388e+00  6.64995527e+00
  1.94566798e+00  7.08949041e+00  5.43198347e+00  5.80618441e-01
  7.76860094e+00  6.34063721e+00  7.98214495e-01  4.38892889e+00
  2.10588074e+00  8.85570145e+00  2.82261276e+00  9.23659515e+00
  5.26395798e+00  3.50026894e+00  8.63844299e+00  6.71980190e+00]
Epoch 1/1000
2023-10-03 11:52:38.584 
Epoch 1/1000 
	 loss: 439.0144, MinusLogProbMetric: 439.0144, val_loss: 397.5385, val_MinusLogProbMetric: 397.5385

Epoch 1: val_loss improved from inf to 397.53851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 22s - loss: 439.0144 - MinusLogProbMetric: 439.0144 - val_loss: 397.5385 - val_MinusLogProbMetric: 397.5385 - lr: 3.3333e-04 - 22s/epoch - 114ms/step
Epoch 2/1000
2023-10-03 11:52:45.317 
Epoch 2/1000 
	 loss: 393.6956, MinusLogProbMetric: 393.6956, val_loss: 395.6113, val_MinusLogProbMetric: 395.6113

Epoch 2: val_loss improved from 397.53851 to 395.61130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 7s - loss: 393.6956 - MinusLogProbMetric: 393.6956 - val_loss: 395.6113 - val_MinusLogProbMetric: 395.6113 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 3/1000
2023-10-03 11:52:51.730 
Epoch 3/1000 
	 loss: 392.6122, MinusLogProbMetric: 392.6122, val_loss: 395.0370, val_MinusLogProbMetric: 395.0370

Epoch 3: val_loss improved from 395.61130 to 395.03696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 6s - loss: 392.6122 - MinusLogProbMetric: 392.6122 - val_loss: 395.0370 - val_MinusLogProbMetric: 395.0370 - lr: 3.3333e-04 - 6s/epoch - 32ms/step
Epoch 4/1000
2023-10-03 11:52:59.190 
Epoch 4/1000 
	 loss: 392.2666, MinusLogProbMetric: 392.2666, val_loss: 395.4261, val_MinusLogProbMetric: 395.4261

Epoch 4: val_loss did not improve from 395.03696
196/196 - 7s - loss: 392.2666 - MinusLogProbMetric: 392.2666 - val_loss: 395.4261 - val_MinusLogProbMetric: 395.4261 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 5/1000
2023-10-03 11:53:06.413 
Epoch 5/1000 
	 loss: 392.5569, MinusLogProbMetric: 392.5569, val_loss: 394.8178, val_MinusLogProbMetric: 394.8178

Epoch 5: val_loss improved from 395.03696 to 394.81781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 8s - loss: 392.5569 - MinusLogProbMetric: 392.5569 - val_loss: 394.8178 - val_MinusLogProbMetric: 394.8178 - lr: 3.3333e-04 - 8s/epoch - 39ms/step
Epoch 6/1000
2023-10-03 11:53:14.106 
Epoch 6/1000 
	 loss: 392.1519, MinusLogProbMetric: 392.1519, val_loss: 396.6600, val_MinusLogProbMetric: 396.6600

Epoch 6: val_loss did not improve from 394.81781
196/196 - 7s - loss: 392.1519 - MinusLogProbMetric: 392.1519 - val_loss: 396.6600 - val_MinusLogProbMetric: 396.6600 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 7/1000
2023-10-03 11:53:21.457 
Epoch 7/1000 
	 loss: 392.5133, MinusLogProbMetric: 392.5133, val_loss: 394.6167, val_MinusLogProbMetric: 394.6167

Epoch 7: val_loss improved from 394.81781 to 394.61670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 8s - loss: 392.5133 - MinusLogProbMetric: 392.5133 - val_loss: 394.6167 - val_MinusLogProbMetric: 394.6167 - lr: 3.3333e-04 - 8s/epoch - 39ms/step
Epoch 8/1000
2023-10-03 11:53:29.046 
Epoch 8/1000 
	 loss: 392.4728, MinusLogProbMetric: 392.4728, val_loss: 395.1188, val_MinusLogProbMetric: 395.1188

Epoch 8: val_loss did not improve from 394.61670
196/196 - 7s - loss: 392.4728 - MinusLogProbMetric: 392.4728 - val_loss: 395.1188 - val_MinusLogProbMetric: 395.1188 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 9/1000
2023-10-03 11:53:36.279 
Epoch 9/1000 
	 loss: 392.0896, MinusLogProbMetric: 392.0896, val_loss: 395.5106, val_MinusLogProbMetric: 395.5106

Epoch 9: val_loss did not improve from 394.61670
196/196 - 7s - loss: 392.0896 - MinusLogProbMetric: 392.0896 - val_loss: 395.5106 - val_MinusLogProbMetric: 395.5106 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 10/1000
2023-10-03 11:53:43.406 
Epoch 10/1000 
	 loss: 392.2742, MinusLogProbMetric: 392.2742, val_loss: 395.1392, val_MinusLogProbMetric: 395.1392

Epoch 10: val_loss did not improve from 394.61670
196/196 - 7s - loss: 392.2742 - MinusLogProbMetric: 392.2742 - val_loss: 395.1392 - val_MinusLogProbMetric: 395.1392 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 11/1000
2023-10-03 11:53:50.527 
Epoch 11/1000 
	 loss: 392.1629, MinusLogProbMetric: 392.1629, val_loss: 394.4739, val_MinusLogProbMetric: 394.4739

Epoch 11: val_loss improved from 394.61670 to 394.47388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 8s - loss: 392.1629 - MinusLogProbMetric: 392.1629 - val_loss: 394.4739 - val_MinusLogProbMetric: 394.4739 - lr: 3.3333e-04 - 8s/epoch - 40ms/step
Epoch 12/1000
2023-10-03 11:53:58.594 
Epoch 12/1000 
	 loss: 392.2958, MinusLogProbMetric: 392.2958, val_loss: 397.4847, val_MinusLogProbMetric: 397.4847

Epoch 12: val_loss did not improve from 394.47388
196/196 - 7s - loss: 392.2958 - MinusLogProbMetric: 392.2958 - val_loss: 397.4847 - val_MinusLogProbMetric: 397.4847 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 13/1000
2023-10-03 11:54:05.807 
Epoch 13/1000 
	 loss: 392.2979, MinusLogProbMetric: 392.2979, val_loss: 396.7611, val_MinusLogProbMetric: 396.7611

Epoch 13: val_loss did not improve from 394.47388
196/196 - 7s - loss: 392.2979 - MinusLogProbMetric: 392.2979 - val_loss: 396.7611 - val_MinusLogProbMetric: 396.7611 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 14/1000
2023-10-03 11:54:13.141 
Epoch 14/1000 
	 loss: 391.9657, MinusLogProbMetric: 391.9657, val_loss: 395.0297, val_MinusLogProbMetric: 395.0297

Epoch 14: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.9657 - MinusLogProbMetric: 391.9657 - val_loss: 395.0297 - val_MinusLogProbMetric: 395.0297 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 15/1000
2023-10-03 11:54:20.622 
Epoch 15/1000 
	 loss: 391.7693, MinusLogProbMetric: 391.7693, val_loss: 396.5827, val_MinusLogProbMetric: 396.5827

Epoch 15: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.7693 - MinusLogProbMetric: 391.7693 - val_loss: 396.5827 - val_MinusLogProbMetric: 396.5827 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 16/1000
2023-10-03 11:54:28.304 
Epoch 16/1000 
	 loss: 392.9240, MinusLogProbMetric: 392.9240, val_loss: 395.3593, val_MinusLogProbMetric: 395.3593

Epoch 16: val_loss did not improve from 394.47388
196/196 - 8s - loss: 392.9240 - MinusLogProbMetric: 392.9240 - val_loss: 395.3593 - val_MinusLogProbMetric: 395.3593 - lr: 3.3333e-04 - 8s/epoch - 39ms/step
Epoch 17/1000
2023-10-03 11:54:35.717 
Epoch 17/1000 
	 loss: 391.7011, MinusLogProbMetric: 391.7011, val_loss: 396.2559, val_MinusLogProbMetric: 396.2559

Epoch 17: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.7011 - MinusLogProbMetric: 391.7011 - val_loss: 396.2559 - val_MinusLogProbMetric: 396.2559 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 18/1000
2023-10-03 11:54:43.103 
Epoch 18/1000 
	 loss: 392.2354, MinusLogProbMetric: 392.2354, val_loss: 395.3274, val_MinusLogProbMetric: 395.3274

Epoch 18: val_loss did not improve from 394.47388
196/196 - 7s - loss: 392.2354 - MinusLogProbMetric: 392.2354 - val_loss: 395.3274 - val_MinusLogProbMetric: 395.3274 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 19/1000
2023-10-03 11:54:50.382 
Epoch 19/1000 
	 loss: 391.7116, MinusLogProbMetric: 391.7116, val_loss: 395.7872, val_MinusLogProbMetric: 395.7872

Epoch 19: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.7116 - MinusLogProbMetric: 391.7116 - val_loss: 395.7872 - val_MinusLogProbMetric: 395.7872 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 20/1000
2023-10-03 11:54:57.759 
Epoch 20/1000 
	 loss: 392.4168, MinusLogProbMetric: 392.4168, val_loss: 396.6619, val_MinusLogProbMetric: 396.6619

Epoch 20: val_loss did not improve from 394.47388
196/196 - 7s - loss: 392.4168 - MinusLogProbMetric: 392.4168 - val_loss: 396.6619 - val_MinusLogProbMetric: 396.6619 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 21/1000
2023-10-03 11:55:05.111 
Epoch 21/1000 
	 loss: 391.6289, MinusLogProbMetric: 391.6289, val_loss: 395.3962, val_MinusLogProbMetric: 395.3962

Epoch 21: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.6289 - MinusLogProbMetric: 391.6289 - val_loss: 395.3962 - val_MinusLogProbMetric: 395.3962 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 22/1000
2023-10-03 11:55:12.481 
Epoch 22/1000 
	 loss: 392.1707, MinusLogProbMetric: 392.1707, val_loss: 396.3440, val_MinusLogProbMetric: 396.3440

Epoch 22: val_loss did not improve from 394.47388
196/196 - 7s - loss: 392.1707 - MinusLogProbMetric: 392.1707 - val_loss: 396.3440 - val_MinusLogProbMetric: 396.3440 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 23/1000
2023-10-03 11:55:19.797 
Epoch 23/1000 
	 loss: 393.3353, MinusLogProbMetric: 393.3353, val_loss: 395.2196, val_MinusLogProbMetric: 395.2196

Epoch 23: val_loss did not improve from 394.47388
196/196 - 7s - loss: 393.3353 - MinusLogProbMetric: 393.3353 - val_loss: 395.2196 - val_MinusLogProbMetric: 395.2196 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 24/1000
2023-10-03 11:55:27.196 
Epoch 24/1000 
	 loss: 391.2150, MinusLogProbMetric: 391.2150, val_loss: 394.4797, val_MinusLogProbMetric: 394.4797

Epoch 24: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.2150 - MinusLogProbMetric: 391.2150 - val_loss: 394.4797 - val_MinusLogProbMetric: 394.4797 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 25/1000
2023-10-03 11:55:34.516 
Epoch 25/1000 
	 loss: 391.7772, MinusLogProbMetric: 391.7772, val_loss: 395.2297, val_MinusLogProbMetric: 395.2297

Epoch 25: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.7772 - MinusLogProbMetric: 391.7772 - val_loss: 395.2297 - val_MinusLogProbMetric: 395.2297 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 26/1000
2023-10-03 11:55:41.802 
Epoch 26/1000 
	 loss: 391.0390, MinusLogProbMetric: 391.0390, val_loss: 395.2583, val_MinusLogProbMetric: 395.2583

Epoch 26: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.0390 - MinusLogProbMetric: 391.0390 - val_loss: 395.2583 - val_MinusLogProbMetric: 395.2583 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 27/1000
2023-10-03 11:55:49.150 
Epoch 27/1000 
	 loss: 391.7518, MinusLogProbMetric: 391.7518, val_loss: 396.7185, val_MinusLogProbMetric: 396.7185

Epoch 27: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.7518 - MinusLogProbMetric: 391.7518 - val_loss: 396.7185 - val_MinusLogProbMetric: 396.7185 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 28/1000
2023-10-03 11:55:56.443 
Epoch 28/1000 
	 loss: 391.0751, MinusLogProbMetric: 391.0751, val_loss: 395.1251, val_MinusLogProbMetric: 395.1251

Epoch 28: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.0751 - MinusLogProbMetric: 391.0751 - val_loss: 395.1251 - val_MinusLogProbMetric: 395.1251 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 29/1000
2023-10-03 11:56:03.696 
Epoch 29/1000 
	 loss: 391.2021, MinusLogProbMetric: 391.2021, val_loss: 394.7227, val_MinusLogProbMetric: 394.7227

Epoch 29: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.2021 - MinusLogProbMetric: 391.2021 - val_loss: 394.7227 - val_MinusLogProbMetric: 394.7227 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 30/1000
2023-10-03 11:56:11.021 
Epoch 30/1000 
	 loss: 394.8929, MinusLogProbMetric: 394.8929, val_loss: 395.0859, val_MinusLogProbMetric: 395.0859

Epoch 30: val_loss did not improve from 394.47388
196/196 - 7s - loss: 394.8929 - MinusLogProbMetric: 394.8929 - val_loss: 395.0859 - val_MinusLogProbMetric: 395.0859 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 31/1000
2023-10-03 11:56:18.433 
Epoch 31/1000 
	 loss: 390.9378, MinusLogProbMetric: 390.9378, val_loss: 397.4423, val_MinusLogProbMetric: 397.4423

Epoch 31: val_loss did not improve from 394.47388
196/196 - 7s - loss: 390.9378 - MinusLogProbMetric: 390.9378 - val_loss: 397.4423 - val_MinusLogProbMetric: 397.4423 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 32/1000
2023-10-03 11:56:25.810 
Epoch 32/1000 
	 loss: 390.8593, MinusLogProbMetric: 390.8593, val_loss: 399.7681, val_MinusLogProbMetric: 399.7681

Epoch 32: val_loss did not improve from 394.47388
196/196 - 7s - loss: 390.8593 - MinusLogProbMetric: 390.8593 - val_loss: 399.7681 - val_MinusLogProbMetric: 399.7681 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 33/1000
2023-10-03 11:56:33.110 
Epoch 33/1000 
	 loss: 391.4973, MinusLogProbMetric: 391.4973, val_loss: 395.9861, val_MinusLogProbMetric: 395.9861

Epoch 33: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.4973 - MinusLogProbMetric: 391.4973 - val_loss: 395.9861 - val_MinusLogProbMetric: 395.9861 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 34/1000
2023-10-03 11:56:40.519 
Epoch 34/1000 
	 loss: 391.3807, MinusLogProbMetric: 391.3807, val_loss: 394.8305, val_MinusLogProbMetric: 394.8305

Epoch 34: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.3807 - MinusLogProbMetric: 391.3807 - val_loss: 394.8305 - val_MinusLogProbMetric: 394.8305 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 35/1000
2023-10-03 11:56:47.998 
Epoch 35/1000 
	 loss: 391.0316, MinusLogProbMetric: 391.0316, val_loss: 396.8523, val_MinusLogProbMetric: 396.8523

Epoch 35: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.0316 - MinusLogProbMetric: 391.0316 - val_loss: 396.8523 - val_MinusLogProbMetric: 396.8523 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 36/1000
2023-10-03 11:56:55.463 
Epoch 36/1000 
	 loss: 390.7710, MinusLogProbMetric: 390.7710, val_loss: 397.4721, val_MinusLogProbMetric: 397.4721

Epoch 36: val_loss did not improve from 394.47388
196/196 - 7s - loss: 390.7710 - MinusLogProbMetric: 390.7710 - val_loss: 397.4721 - val_MinusLogProbMetric: 397.4721 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 37/1000
2023-10-03 11:57:02.855 
Epoch 37/1000 
	 loss: 390.7721, MinusLogProbMetric: 390.7721, val_loss: 395.5208, val_MinusLogProbMetric: 395.5208

Epoch 37: val_loss did not improve from 394.47388
196/196 - 7s - loss: 390.7721 - MinusLogProbMetric: 390.7721 - val_loss: 395.5208 - val_MinusLogProbMetric: 395.5208 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 38/1000
2023-10-03 11:57:10.150 
Epoch 38/1000 
	 loss: 391.0631, MinusLogProbMetric: 391.0631, val_loss: 395.7269, val_MinusLogProbMetric: 395.7269

Epoch 38: val_loss did not improve from 394.47388
196/196 - 7s - loss: 391.0631 - MinusLogProbMetric: 391.0631 - val_loss: 395.7269 - val_MinusLogProbMetric: 395.7269 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 39/1000
2023-10-03 11:57:17.533 
Epoch 39/1000 
	 loss: 390.7839, MinusLogProbMetric: 390.7839, val_loss: 396.3233, val_MinusLogProbMetric: 396.3233

Epoch 39: val_loss did not improve from 394.47388
196/196 - 7s - loss: 390.7839 - MinusLogProbMetric: 390.7839 - val_loss: 396.3233 - val_MinusLogProbMetric: 396.3233 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 40/1000
2023-10-03 11:57:24.856 
Epoch 40/1000 
	 loss: 390.7563, MinusLogProbMetric: 390.7563, val_loss: 394.4379, val_MinusLogProbMetric: 394.4379

Epoch 40: val_loss improved from 394.47388 to 394.43790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_350/weights/best_weights.h5
196/196 - 8s - loss: 390.7563 - MinusLogProbMetric: 390.7563 - val_loss: 394.4379 - val_MinusLogProbMetric: 394.4379 - lr: 3.3333e-04 - 8s/epoch - 40ms/step
Epoch 41/1000
2023-10-03 11:57:32.651 
Epoch 41/1000 
	 loss: 390.3841, MinusLogProbMetric: 390.3841, val_loss: 395.0693, val_MinusLogProbMetric: 395.0693

Epoch 41: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.3841 - MinusLogProbMetric: 390.3841 - val_loss: 395.0693 - val_MinusLogProbMetric: 395.0693 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 42/1000
2023-10-03 11:57:39.978 
Epoch 42/1000 
	 loss: 390.6518, MinusLogProbMetric: 390.6518, val_loss: 395.0065, val_MinusLogProbMetric: 395.0065

Epoch 42: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.6518 - MinusLogProbMetric: 390.6518 - val_loss: 395.0065 - val_MinusLogProbMetric: 395.0065 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 43/1000
2023-10-03 11:57:47.252 
Epoch 43/1000 
	 loss: 390.6476, MinusLogProbMetric: 390.6476, val_loss: 396.1793, val_MinusLogProbMetric: 396.1793

Epoch 43: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.6476 - MinusLogProbMetric: 390.6476 - val_loss: 396.1793 - val_MinusLogProbMetric: 396.1793 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 44/1000
2023-10-03 11:57:54.648 
Epoch 44/1000 
	 loss: 392.0001, MinusLogProbMetric: 392.0001, val_loss: 395.0083, val_MinusLogProbMetric: 395.0083

Epoch 44: val_loss did not improve from 394.43790
196/196 - 7s - loss: 392.0001 - MinusLogProbMetric: 392.0001 - val_loss: 395.0083 - val_MinusLogProbMetric: 395.0083 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 45/1000
2023-10-03 11:58:01.905 
Epoch 45/1000 
	 loss: 390.5739, MinusLogProbMetric: 390.5739, val_loss: 395.0970, val_MinusLogProbMetric: 395.0970

Epoch 45: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.5739 - MinusLogProbMetric: 390.5739 - val_loss: 395.0970 - val_MinusLogProbMetric: 395.0970 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 46/1000
2023-10-03 11:58:09.195 
Epoch 46/1000 
	 loss: 390.3111, MinusLogProbMetric: 390.3111, val_loss: 397.3746, val_MinusLogProbMetric: 397.3746

Epoch 46: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.3111 - MinusLogProbMetric: 390.3111 - val_loss: 397.3746 - val_MinusLogProbMetric: 397.3746 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 47/1000
2023-10-03 11:58:16.526 
Epoch 47/1000 
	 loss: 390.8492, MinusLogProbMetric: 390.8492, val_loss: 395.9559, val_MinusLogProbMetric: 395.9559

Epoch 47: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.8492 - MinusLogProbMetric: 390.8492 - val_loss: 395.9559 - val_MinusLogProbMetric: 395.9559 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 48/1000
2023-10-03 11:58:23.939 
Epoch 48/1000 
	 loss: 390.2261, MinusLogProbMetric: 390.2261, val_loss: 397.0494, val_MinusLogProbMetric: 397.0494

Epoch 48: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.2261 - MinusLogProbMetric: 390.2261 - val_loss: 397.0494 - val_MinusLogProbMetric: 397.0494 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 49/1000
2023-10-03 11:58:31.310 
Epoch 49/1000 
	 loss: 390.4773, MinusLogProbMetric: 390.4773, val_loss: 397.3155, val_MinusLogProbMetric: 397.3155

Epoch 49: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.4773 - MinusLogProbMetric: 390.4773 - val_loss: 397.3155 - val_MinusLogProbMetric: 397.3155 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 50/1000
2023-10-03 11:58:38.646 
Epoch 50/1000 
	 loss: 390.4331, MinusLogProbMetric: 390.4331, val_loss: 396.4808, val_MinusLogProbMetric: 396.4808

Epoch 50: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.4331 - MinusLogProbMetric: 390.4331 - val_loss: 396.4808 - val_MinusLogProbMetric: 396.4808 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 51/1000
2023-10-03 11:58:46.002 
Epoch 51/1000 
	 loss: 390.6682, MinusLogProbMetric: 390.6682, val_loss: 395.0105, val_MinusLogProbMetric: 395.0105

Epoch 51: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.6682 - MinusLogProbMetric: 390.6682 - val_loss: 395.0105 - val_MinusLogProbMetric: 395.0105 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 52/1000
2023-10-03 11:58:53.364 
Epoch 52/1000 
	 loss: 390.0556, MinusLogProbMetric: 390.0556, val_loss: 398.0798, val_MinusLogProbMetric: 398.0798

Epoch 52: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.0556 - MinusLogProbMetric: 390.0556 - val_loss: 398.0798 - val_MinusLogProbMetric: 398.0798 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 53/1000
2023-10-03 11:59:00.624 
Epoch 53/1000 
	 loss: 390.4126, MinusLogProbMetric: 390.4126, val_loss: 397.9536, val_MinusLogProbMetric: 397.9536

Epoch 53: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.4126 - MinusLogProbMetric: 390.4126 - val_loss: 397.9536 - val_MinusLogProbMetric: 397.9536 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 54/1000
2023-10-03 11:59:07.968 
Epoch 54/1000 
	 loss: 390.1048, MinusLogProbMetric: 390.1048, val_loss: 396.4018, val_MinusLogProbMetric: 396.4018

Epoch 54: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.1048 - MinusLogProbMetric: 390.1048 - val_loss: 396.4018 - val_MinusLogProbMetric: 396.4018 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 55/1000
2023-10-03 11:59:15.405 
Epoch 55/1000 
	 loss: 390.1857, MinusLogProbMetric: 390.1857, val_loss: 396.5194, val_MinusLogProbMetric: 396.5194

Epoch 55: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.1857 - MinusLogProbMetric: 390.1857 - val_loss: 396.5194 - val_MinusLogProbMetric: 396.5194 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 56/1000
2023-10-03 11:59:22.757 
Epoch 56/1000 
	 loss: 390.0733, MinusLogProbMetric: 390.0733, val_loss: 395.5189, val_MinusLogProbMetric: 395.5189

Epoch 56: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.0733 - MinusLogProbMetric: 390.0733 - val_loss: 395.5189 - val_MinusLogProbMetric: 395.5189 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 57/1000
2023-10-03 11:59:30.096 
Epoch 57/1000 
	 loss: 389.9655, MinusLogProbMetric: 389.9655, val_loss: 395.3961, val_MinusLogProbMetric: 395.3961

Epoch 57: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.9655 - MinusLogProbMetric: 389.9655 - val_loss: 395.3961 - val_MinusLogProbMetric: 395.3961 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 58/1000
2023-10-03 11:59:37.426 
Epoch 58/1000 
	 loss: 390.0045, MinusLogProbMetric: 390.0045, val_loss: 399.4954, val_MinusLogProbMetric: 399.4954

Epoch 58: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.0045 - MinusLogProbMetric: 390.0045 - val_loss: 399.4954 - val_MinusLogProbMetric: 399.4954 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 59/1000
2023-10-03 11:59:44.780 
Epoch 59/1000 
	 loss: 390.4979, MinusLogProbMetric: 390.4979, val_loss: 395.4027, val_MinusLogProbMetric: 395.4027

Epoch 59: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.4979 - MinusLogProbMetric: 390.4979 - val_loss: 395.4027 - val_MinusLogProbMetric: 395.4027 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 60/1000
2023-10-03 11:59:52.211 
Epoch 60/1000 
	 loss: 390.5729, MinusLogProbMetric: 390.5729, val_loss: 396.5681, val_MinusLogProbMetric: 396.5681

Epoch 60: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.5729 - MinusLogProbMetric: 390.5729 - val_loss: 396.5681 - val_MinusLogProbMetric: 396.5681 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 61/1000
2023-10-03 11:59:59.514 
Epoch 61/1000 
	 loss: 389.6061, MinusLogProbMetric: 389.6061, val_loss: 397.5387, val_MinusLogProbMetric: 397.5387

Epoch 61: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.6061 - MinusLogProbMetric: 389.6061 - val_loss: 397.5387 - val_MinusLogProbMetric: 397.5387 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 62/1000
2023-10-03 12:00:06.858 
Epoch 62/1000 
	 loss: 389.9536, MinusLogProbMetric: 389.9536, val_loss: 397.2438, val_MinusLogProbMetric: 397.2438

Epoch 62: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.9536 - MinusLogProbMetric: 389.9536 - val_loss: 397.2438 - val_MinusLogProbMetric: 397.2438 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 63/1000
2023-10-03 12:00:14.175 
Epoch 63/1000 
	 loss: 390.8029, MinusLogProbMetric: 390.8029, val_loss: 396.0157, val_MinusLogProbMetric: 396.0157

Epoch 63: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.8029 - MinusLogProbMetric: 390.8029 - val_loss: 396.0157 - val_MinusLogProbMetric: 396.0157 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 64/1000
2023-10-03 12:00:21.519 
Epoch 64/1000 
	 loss: 389.4677, MinusLogProbMetric: 389.4677, val_loss: 396.0718, val_MinusLogProbMetric: 396.0718

Epoch 64: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.4677 - MinusLogProbMetric: 389.4677 - val_loss: 396.0718 - val_MinusLogProbMetric: 396.0718 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 65/1000
2023-10-03 12:00:28.678 
Epoch 65/1000 
	 loss: 389.8073, MinusLogProbMetric: 389.8073, val_loss: 396.5902, val_MinusLogProbMetric: 396.5902

Epoch 65: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.8073 - MinusLogProbMetric: 389.8073 - val_loss: 396.5902 - val_MinusLogProbMetric: 396.5902 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 66/1000
2023-10-03 12:00:35.930 
Epoch 66/1000 
	 loss: 389.6644, MinusLogProbMetric: 389.6644, val_loss: 396.4833, val_MinusLogProbMetric: 396.4833

Epoch 66: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.6644 - MinusLogProbMetric: 389.6644 - val_loss: 396.4833 - val_MinusLogProbMetric: 396.4833 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 67/1000
2023-10-03 12:00:43.233 
Epoch 67/1000 
	 loss: 392.9364, MinusLogProbMetric: 392.9364, val_loss: 395.9785, val_MinusLogProbMetric: 395.9785

Epoch 67: val_loss did not improve from 394.43790
196/196 - 7s - loss: 392.9364 - MinusLogProbMetric: 392.9364 - val_loss: 395.9785 - val_MinusLogProbMetric: 395.9785 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 68/1000
2023-10-03 12:00:50.588 
Epoch 68/1000 
	 loss: 389.4001, MinusLogProbMetric: 389.4001, val_loss: 395.7817, val_MinusLogProbMetric: 395.7817

Epoch 68: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.4001 - MinusLogProbMetric: 389.4001 - val_loss: 395.7817 - val_MinusLogProbMetric: 395.7817 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 69/1000
2023-10-03 12:00:57.999 
Epoch 69/1000 
	 loss: 389.5155, MinusLogProbMetric: 389.5155, val_loss: 396.8864, val_MinusLogProbMetric: 396.8864

Epoch 69: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.5155 - MinusLogProbMetric: 389.5155 - val_loss: 396.8864 - val_MinusLogProbMetric: 396.8864 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 70/1000
2023-10-03 12:01:05.254 
Epoch 70/1000 
	 loss: 389.6648, MinusLogProbMetric: 389.6648, val_loss: 395.9414, val_MinusLogProbMetric: 395.9414

Epoch 70: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.6648 - MinusLogProbMetric: 389.6648 - val_loss: 395.9414 - val_MinusLogProbMetric: 395.9414 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 71/1000
2023-10-03 12:01:12.709 
Epoch 71/1000 
	 loss: 389.5697, MinusLogProbMetric: 389.5697, val_loss: 395.8223, val_MinusLogProbMetric: 395.8223

Epoch 71: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.5697 - MinusLogProbMetric: 389.5697 - val_loss: 395.8223 - val_MinusLogProbMetric: 395.8223 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 72/1000
2023-10-03 12:01:20.030 
Epoch 72/1000 
	 loss: 389.7833, MinusLogProbMetric: 389.7833, val_loss: 405.3447, val_MinusLogProbMetric: 405.3447

Epoch 72: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.7833 - MinusLogProbMetric: 389.7833 - val_loss: 405.3447 - val_MinusLogProbMetric: 405.3447 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 73/1000
2023-10-03 12:01:27.375 
Epoch 73/1000 
	 loss: 389.4263, MinusLogProbMetric: 389.4263, val_loss: 396.1389, val_MinusLogProbMetric: 396.1389

Epoch 73: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.4263 - MinusLogProbMetric: 389.4263 - val_loss: 396.1389 - val_MinusLogProbMetric: 396.1389 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 74/1000
2023-10-03 12:01:34.864 
Epoch 74/1000 
	 loss: 389.7844, MinusLogProbMetric: 389.7844, val_loss: 395.8866, val_MinusLogProbMetric: 395.8866

Epoch 74: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.7844 - MinusLogProbMetric: 389.7844 - val_loss: 395.8866 - val_MinusLogProbMetric: 395.8866 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 75/1000
2023-10-03 12:01:42.253 
Epoch 75/1000 
	 loss: 389.3853, MinusLogProbMetric: 389.3853, val_loss: 397.3651, val_MinusLogProbMetric: 397.3651

Epoch 75: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.3853 - MinusLogProbMetric: 389.3853 - val_loss: 397.3651 - val_MinusLogProbMetric: 397.3651 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 76/1000
2023-10-03 12:01:49.633 
Epoch 76/1000 
	 loss: 389.4176, MinusLogProbMetric: 389.4176, val_loss: 395.6909, val_MinusLogProbMetric: 395.6909

Epoch 76: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.4176 - MinusLogProbMetric: 389.4176 - val_loss: 395.6909 - val_MinusLogProbMetric: 395.6909 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 77/1000
2023-10-03 12:01:56.801 
Epoch 77/1000 
	 loss: 389.3006, MinusLogProbMetric: 389.3006, val_loss: 396.9063, val_MinusLogProbMetric: 396.9063

Epoch 77: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.3006 - MinusLogProbMetric: 389.3006 - val_loss: 396.9063 - val_MinusLogProbMetric: 396.9063 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 78/1000
2023-10-03 12:02:04.096 
Epoch 78/1000 
	 loss: 389.3919, MinusLogProbMetric: 389.3919, val_loss: 396.5851, val_MinusLogProbMetric: 396.5851

Epoch 78: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.3919 - MinusLogProbMetric: 389.3919 - val_loss: 396.5851 - val_MinusLogProbMetric: 396.5851 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 79/1000
2023-10-03 12:02:11.405 
Epoch 79/1000 
	 loss: 391.0010, MinusLogProbMetric: 391.0010, val_loss: 396.4407, val_MinusLogProbMetric: 396.4407

Epoch 79: val_loss did not improve from 394.43790
196/196 - 7s - loss: 391.0010 - MinusLogProbMetric: 391.0010 - val_loss: 396.4407 - val_MinusLogProbMetric: 396.4407 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 80/1000
2023-10-03 12:02:18.712 
Epoch 80/1000 
	 loss: 389.5612, MinusLogProbMetric: 389.5612, val_loss: 396.2164, val_MinusLogProbMetric: 396.2164

Epoch 80: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.5612 - MinusLogProbMetric: 389.5612 - val_loss: 396.2164 - val_MinusLogProbMetric: 396.2164 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 81/1000
2023-10-03 12:02:25.805 
Epoch 81/1000 
	 loss: 389.0944, MinusLogProbMetric: 389.0944, val_loss: 397.3545, val_MinusLogProbMetric: 397.3545

Epoch 81: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.0944 - MinusLogProbMetric: 389.0944 - val_loss: 397.3545 - val_MinusLogProbMetric: 397.3545 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 82/1000
2023-10-03 12:02:33.536 
Epoch 82/1000 
	 loss: 389.1272, MinusLogProbMetric: 389.1272, val_loss: 396.5141, val_MinusLogProbMetric: 396.5141

Epoch 82: val_loss did not improve from 394.43790
196/196 - 8s - loss: 389.1272 - MinusLogProbMetric: 389.1272 - val_loss: 396.5141 - val_MinusLogProbMetric: 396.5141 - lr: 3.3333e-04 - 8s/epoch - 39ms/step
Epoch 83/1000
2023-10-03 12:02:40.824 
Epoch 83/1000 
	 loss: 390.4973, MinusLogProbMetric: 390.4973, val_loss: 396.0185, val_MinusLogProbMetric: 396.0185

Epoch 83: val_loss did not improve from 394.43790
196/196 - 7s - loss: 390.4973 - MinusLogProbMetric: 390.4973 - val_loss: 396.0185 - val_MinusLogProbMetric: 396.0185 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 84/1000
2023-10-03 12:02:48.028 
Epoch 84/1000 
	 loss: 388.7366, MinusLogProbMetric: 388.7366, val_loss: 398.2212, val_MinusLogProbMetric: 398.2212

Epoch 84: val_loss did not improve from 394.43790
196/196 - 7s - loss: 388.7366 - MinusLogProbMetric: 388.7366 - val_loss: 398.2212 - val_MinusLogProbMetric: 398.2212 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 85/1000
2023-10-03 12:02:55.018 
Epoch 85/1000 
	 loss: 389.2137, MinusLogProbMetric: 389.2137, val_loss: 397.7844, val_MinusLogProbMetric: 397.7844

Epoch 85: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.2137 - MinusLogProbMetric: 389.2137 - val_loss: 397.7844 - val_MinusLogProbMetric: 397.7844 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 86/1000
2023-10-03 12:03:02.246 
Epoch 86/1000 
	 loss: 388.7537, MinusLogProbMetric: 388.7537, val_loss: 400.7164, val_MinusLogProbMetric: 400.7164

Epoch 86: val_loss did not improve from 394.43790
196/196 - 7s - loss: 388.7537 - MinusLogProbMetric: 388.7537 - val_loss: 400.7164 - val_MinusLogProbMetric: 400.7164 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 87/1000
2023-10-03 12:03:09.553 
Epoch 87/1000 
	 loss: 389.1406, MinusLogProbMetric: 389.1406, val_loss: 396.7492, val_MinusLogProbMetric: 396.7492

Epoch 87: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.1406 - MinusLogProbMetric: 389.1406 - val_loss: 396.7492 - val_MinusLogProbMetric: 396.7492 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 88/1000
2023-10-03 12:03:16.819 
Epoch 88/1000 
	 loss: 388.8985, MinusLogProbMetric: 388.8985, val_loss: 396.0027, val_MinusLogProbMetric: 396.0027

Epoch 88: val_loss did not improve from 394.43790
196/196 - 7s - loss: 388.8985 - MinusLogProbMetric: 388.8985 - val_loss: 396.0027 - val_MinusLogProbMetric: 396.0027 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 89/1000
2023-10-03 12:03:24.224 
Epoch 89/1000 
	 loss: 388.9494, MinusLogProbMetric: 388.9494, val_loss: 397.3114, val_MinusLogProbMetric: 397.3114

Epoch 89: val_loss did not improve from 394.43790
196/196 - 7s - loss: 388.9494 - MinusLogProbMetric: 388.9494 - val_loss: 397.3114 - val_MinusLogProbMetric: 397.3114 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 90/1000
2023-10-03 12:03:31.563 
Epoch 90/1000 
	 loss: 389.2152, MinusLogProbMetric: 389.2152, val_loss: 396.5295, val_MinusLogProbMetric: 396.5295

Epoch 90: val_loss did not improve from 394.43790
196/196 - 7s - loss: 389.2152 - MinusLogProbMetric: 389.2152 - val_loss: 396.5295 - val_MinusLogProbMetric: 396.5295 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 91/1000
2023-10-03 12:03:38.997 
Epoch 91/1000 
	 loss: 386.2697, MinusLogProbMetric: 386.2697, val_loss: 394.8066, val_MinusLogProbMetric: 394.8066

Epoch 91: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.2697 - MinusLogProbMetric: 386.2697 - val_loss: 394.8066 - val_MinusLogProbMetric: 394.8066 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 92/1000
2023-10-03 12:03:46.363 
Epoch 92/1000 
	 loss: 386.1996, MinusLogProbMetric: 386.1996, val_loss: 396.3367, val_MinusLogProbMetric: 396.3367

Epoch 92: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1996 - MinusLogProbMetric: 386.1996 - val_loss: 396.3367 - val_MinusLogProbMetric: 396.3367 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 93/1000
2023-10-03 12:03:53.766 
Epoch 93/1000 
	 loss: 386.2157, MinusLogProbMetric: 386.2157, val_loss: 395.2350, val_MinusLogProbMetric: 395.2350

Epoch 93: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.2157 - MinusLogProbMetric: 386.2157 - val_loss: 395.2350 - val_MinusLogProbMetric: 395.2350 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 94/1000
2023-10-03 12:04:01.096 
Epoch 94/1000 
	 loss: 386.3050, MinusLogProbMetric: 386.3050, val_loss: 395.3204, val_MinusLogProbMetric: 395.3204

Epoch 94: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.3050 - MinusLogProbMetric: 386.3050 - val_loss: 395.3204 - val_MinusLogProbMetric: 395.3204 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 95/1000
2023-10-03 12:04:08.562 
Epoch 95/1000 
	 loss: 386.1840, MinusLogProbMetric: 386.1840, val_loss: 395.7585, val_MinusLogProbMetric: 395.7585

Epoch 95: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1840 - MinusLogProbMetric: 386.1840 - val_loss: 395.7585 - val_MinusLogProbMetric: 395.7585 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 96/1000
2023-10-03 12:04:15.936 
Epoch 96/1000 
	 loss: 386.0904, MinusLogProbMetric: 386.0904, val_loss: 395.6999, val_MinusLogProbMetric: 395.6999

Epoch 96: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0904 - MinusLogProbMetric: 386.0904 - val_loss: 395.6999 - val_MinusLogProbMetric: 395.6999 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 97/1000
2023-10-03 12:04:23.288 
Epoch 97/1000 
	 loss: 386.1809, MinusLogProbMetric: 386.1809, val_loss: 395.4898, val_MinusLogProbMetric: 395.4898

Epoch 97: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1809 - MinusLogProbMetric: 386.1809 - val_loss: 395.4898 - val_MinusLogProbMetric: 395.4898 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 98/1000
2023-10-03 12:04:30.614 
Epoch 98/1000 
	 loss: 386.1105, MinusLogProbMetric: 386.1105, val_loss: 395.8930, val_MinusLogProbMetric: 395.8930

Epoch 98: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1105 - MinusLogProbMetric: 386.1105 - val_loss: 395.8930 - val_MinusLogProbMetric: 395.8930 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 99/1000
2023-10-03 12:04:37.841 
Epoch 99/1000 
	 loss: 386.3748, MinusLogProbMetric: 386.3748, val_loss: 395.7043, val_MinusLogProbMetric: 395.7043

Epoch 99: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.3748 - MinusLogProbMetric: 386.3748 - val_loss: 395.7043 - val_MinusLogProbMetric: 395.7043 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 100/1000
2023-10-03 12:04:45.080 
Epoch 100/1000 
	 loss: 386.1108, MinusLogProbMetric: 386.1108, val_loss: 395.5269, val_MinusLogProbMetric: 395.5269

Epoch 100: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1108 - MinusLogProbMetric: 386.1108 - val_loss: 395.5269 - val_MinusLogProbMetric: 395.5269 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 101/1000
2023-10-03 12:04:52.379 
Epoch 101/1000 
	 loss: 386.5125, MinusLogProbMetric: 386.5125, val_loss: 395.8223, val_MinusLogProbMetric: 395.8223

Epoch 101: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.5125 - MinusLogProbMetric: 386.5125 - val_loss: 395.8223 - val_MinusLogProbMetric: 395.8223 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 102/1000
2023-10-03 12:04:59.720 
Epoch 102/1000 
	 loss: 386.0675, MinusLogProbMetric: 386.0675, val_loss: 396.2711, val_MinusLogProbMetric: 396.2711

Epoch 102: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0675 - MinusLogProbMetric: 386.0675 - val_loss: 396.2711 - val_MinusLogProbMetric: 396.2711 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 103/1000
2023-10-03 12:05:06.982 
Epoch 103/1000 
	 loss: 385.9732, MinusLogProbMetric: 385.9732, val_loss: 395.4074, val_MinusLogProbMetric: 395.4074

Epoch 103: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9732 - MinusLogProbMetric: 385.9732 - val_loss: 395.4074 - val_MinusLogProbMetric: 395.4074 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 104/1000
2023-10-03 12:05:14.165 
Epoch 104/1000 
	 loss: 386.1567, MinusLogProbMetric: 386.1567, val_loss: 395.5177, val_MinusLogProbMetric: 395.5177

Epoch 104: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1567 - MinusLogProbMetric: 386.1567 - val_loss: 395.5177 - val_MinusLogProbMetric: 395.5177 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 105/1000
2023-10-03 12:05:21.457 
Epoch 105/1000 
	 loss: 386.0232, MinusLogProbMetric: 386.0232, val_loss: 395.9336, val_MinusLogProbMetric: 395.9336

Epoch 105: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0232 - MinusLogProbMetric: 386.0232 - val_loss: 395.9336 - val_MinusLogProbMetric: 395.9336 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 106/1000
2023-10-03 12:05:28.826 
Epoch 106/1000 
	 loss: 385.9971, MinusLogProbMetric: 385.9971, val_loss: 395.8149, val_MinusLogProbMetric: 395.8149

Epoch 106: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9971 - MinusLogProbMetric: 385.9971 - val_loss: 395.8149 - val_MinusLogProbMetric: 395.8149 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 107/1000
2023-10-03 12:05:36.163 
Epoch 107/1000 
	 loss: 386.1273, MinusLogProbMetric: 386.1273, val_loss: 396.5389, val_MinusLogProbMetric: 396.5389

Epoch 107: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1273 - MinusLogProbMetric: 386.1273 - val_loss: 396.5389 - val_MinusLogProbMetric: 396.5389 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 108/1000
2023-10-03 12:05:43.491 
Epoch 108/1000 
	 loss: 386.0364, MinusLogProbMetric: 386.0364, val_loss: 396.2102, val_MinusLogProbMetric: 396.2102

Epoch 108: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0364 - MinusLogProbMetric: 386.0364 - val_loss: 396.2102 - val_MinusLogProbMetric: 396.2102 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 109/1000
2023-10-03 12:05:50.822 
Epoch 109/1000 
	 loss: 386.0294, MinusLogProbMetric: 386.0294, val_loss: 395.7923, val_MinusLogProbMetric: 395.7923

Epoch 109: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0294 - MinusLogProbMetric: 386.0294 - val_loss: 395.7923 - val_MinusLogProbMetric: 395.7923 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 110/1000
2023-10-03 12:05:58.110 
Epoch 110/1000 
	 loss: 385.9391, MinusLogProbMetric: 385.9391, val_loss: 396.5429, val_MinusLogProbMetric: 396.5429

Epoch 110: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9391 - MinusLogProbMetric: 385.9391 - val_loss: 396.5429 - val_MinusLogProbMetric: 396.5429 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 111/1000
2023-10-03 12:06:05.425 
Epoch 111/1000 
	 loss: 385.9376, MinusLogProbMetric: 385.9376, val_loss: 396.1578, val_MinusLogProbMetric: 396.1578

Epoch 111: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9376 - MinusLogProbMetric: 385.9376 - val_loss: 396.1578 - val_MinusLogProbMetric: 396.1578 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 112/1000
2023-10-03 12:06:12.447 
Epoch 112/1000 
	 loss: 386.0079, MinusLogProbMetric: 386.0079, val_loss: 396.3574, val_MinusLogProbMetric: 396.3574

Epoch 112: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0079 - MinusLogProbMetric: 386.0079 - val_loss: 396.3574 - val_MinusLogProbMetric: 396.3574 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 113/1000
2023-10-03 12:06:19.404 
Epoch 113/1000 
	 loss: 386.1726, MinusLogProbMetric: 386.1726, val_loss: 396.5382, val_MinusLogProbMetric: 396.5382

Epoch 113: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.1726 - MinusLogProbMetric: 386.1726 - val_loss: 396.5382 - val_MinusLogProbMetric: 396.5382 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 114/1000
2023-10-03 12:06:26.792 
Epoch 114/1000 
	 loss: 386.0009, MinusLogProbMetric: 386.0009, val_loss: 396.3149, val_MinusLogProbMetric: 396.3149

Epoch 114: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0009 - MinusLogProbMetric: 386.0009 - val_loss: 396.3149 - val_MinusLogProbMetric: 396.3149 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 115/1000
2023-10-03 12:06:34.230 
Epoch 115/1000 
	 loss: 386.2713, MinusLogProbMetric: 386.2713, val_loss: 396.4950, val_MinusLogProbMetric: 396.4950

Epoch 115: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.2713 - MinusLogProbMetric: 386.2713 - val_loss: 396.4950 - val_MinusLogProbMetric: 396.4950 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 116/1000
2023-10-03 12:06:41.666 
Epoch 116/1000 
	 loss: 385.9224, MinusLogProbMetric: 385.9224, val_loss: 398.7215, val_MinusLogProbMetric: 398.7215

Epoch 116: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9224 - MinusLogProbMetric: 385.9224 - val_loss: 398.7215 - val_MinusLogProbMetric: 398.7215 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 117/1000
2023-10-03 12:06:49.311 
Epoch 117/1000 
	 loss: 386.1624, MinusLogProbMetric: 386.1624, val_loss: 397.1241, val_MinusLogProbMetric: 397.1241

Epoch 117: val_loss did not improve from 394.43790
196/196 - 8s - loss: 386.1624 - MinusLogProbMetric: 386.1624 - val_loss: 397.1241 - val_MinusLogProbMetric: 397.1241 - lr: 1.6667e-04 - 8s/epoch - 39ms/step
Epoch 118/1000
2023-10-03 12:06:56.274 
Epoch 118/1000 
	 loss: 385.9372, MinusLogProbMetric: 385.9372, val_loss: 397.0650, val_MinusLogProbMetric: 397.0650

Epoch 118: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9372 - MinusLogProbMetric: 385.9372 - val_loss: 397.0650 - val_MinusLogProbMetric: 397.0650 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 119/1000
2023-10-03 12:07:03.046 
Epoch 119/1000 
	 loss: 385.9640, MinusLogProbMetric: 385.9640, val_loss: 396.7648, val_MinusLogProbMetric: 396.7648

Epoch 119: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9640 - MinusLogProbMetric: 385.9640 - val_loss: 396.7648 - val_MinusLogProbMetric: 396.7648 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 120/1000
2023-10-03 12:07:09.746 
Epoch 120/1000 
	 loss: 385.9572, MinusLogProbMetric: 385.9572, val_loss: 397.2867, val_MinusLogProbMetric: 397.2867

Epoch 120: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9572 - MinusLogProbMetric: 385.9572 - val_loss: 397.2867 - val_MinusLogProbMetric: 397.2867 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 121/1000
2023-10-03 12:07:16.486 
Epoch 121/1000 
	 loss: 385.8948, MinusLogProbMetric: 385.8948, val_loss: 396.8734, val_MinusLogProbMetric: 396.8734

Epoch 121: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.8948 - MinusLogProbMetric: 385.8948 - val_loss: 396.8734 - val_MinusLogProbMetric: 396.8734 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 122/1000
2023-10-03 12:07:23.216 
Epoch 122/1000 
	 loss: 385.8362, MinusLogProbMetric: 385.8362, val_loss: 397.6873, val_MinusLogProbMetric: 397.6873

Epoch 122: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.8362 - MinusLogProbMetric: 385.8362 - val_loss: 397.6873 - val_MinusLogProbMetric: 397.6873 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 123/1000
2023-10-03 12:07:29.939 
Epoch 123/1000 
	 loss: 385.7431, MinusLogProbMetric: 385.7431, val_loss: 396.6530, val_MinusLogProbMetric: 396.6530

Epoch 123: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.7431 - MinusLogProbMetric: 385.7431 - val_loss: 396.6530 - val_MinusLogProbMetric: 396.6530 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 124/1000
2023-10-03 12:07:36.632 
Epoch 124/1000 
	 loss: 385.9122, MinusLogProbMetric: 385.9122, val_loss: 397.0254, val_MinusLogProbMetric: 397.0254

Epoch 124: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9122 - MinusLogProbMetric: 385.9122 - val_loss: 397.0254 - val_MinusLogProbMetric: 397.0254 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 125/1000
2023-10-03 12:07:43.355 
Epoch 125/1000 
	 loss: 385.7309, MinusLogProbMetric: 385.7309, val_loss: 397.3774, val_MinusLogProbMetric: 397.3774

Epoch 125: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.7309 - MinusLogProbMetric: 385.7309 - val_loss: 397.3774 - val_MinusLogProbMetric: 397.3774 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 126/1000
2023-10-03 12:07:50.021 
Epoch 126/1000 
	 loss: 385.9341, MinusLogProbMetric: 385.9341, val_loss: 397.1429, val_MinusLogProbMetric: 397.1429

Epoch 126: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9341 - MinusLogProbMetric: 385.9341 - val_loss: 397.1429 - val_MinusLogProbMetric: 397.1429 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 127/1000
2023-10-03 12:07:56.746 
Epoch 127/1000 
	 loss: 385.9722, MinusLogProbMetric: 385.9722, val_loss: 398.6650, val_MinusLogProbMetric: 398.6650

Epoch 127: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.9722 - MinusLogProbMetric: 385.9722 - val_loss: 398.6650 - val_MinusLogProbMetric: 398.6650 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 128/1000
2023-10-03 12:08:03.479 
Epoch 128/1000 
	 loss: 386.0435, MinusLogProbMetric: 386.0435, val_loss: 397.4513, val_MinusLogProbMetric: 397.4513

Epoch 128: val_loss did not improve from 394.43790
196/196 - 7s - loss: 386.0435 - MinusLogProbMetric: 386.0435 - val_loss: 397.4513 - val_MinusLogProbMetric: 397.4513 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 129/1000
2023-10-03 12:08:10.210 
Epoch 129/1000 
	 loss: 385.6169, MinusLogProbMetric: 385.6169, val_loss: 397.2671, val_MinusLogProbMetric: 397.2671

Epoch 129: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.6169 - MinusLogProbMetric: 385.6169 - val_loss: 397.2671 - val_MinusLogProbMetric: 397.2671 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 130/1000
2023-10-03 12:08:16.890 
Epoch 130/1000 
	 loss: 385.8474, MinusLogProbMetric: 385.8474, val_loss: 397.7690, val_MinusLogProbMetric: 397.7690

Epoch 130: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.8474 - MinusLogProbMetric: 385.8474 - val_loss: 397.7690 - val_MinusLogProbMetric: 397.7690 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 131/1000
2023-10-03 12:08:23.631 
Epoch 131/1000 
	 loss: 385.6749, MinusLogProbMetric: 385.6749, val_loss: 397.8806, val_MinusLogProbMetric: 397.8806

Epoch 131: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.6749 - MinusLogProbMetric: 385.6749 - val_loss: 397.8806 - val_MinusLogProbMetric: 397.8806 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 132/1000
2023-10-03 12:08:30.299 
Epoch 132/1000 
	 loss: 385.6061, MinusLogProbMetric: 385.6061, val_loss: 398.4885, val_MinusLogProbMetric: 398.4885

Epoch 132: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.6061 - MinusLogProbMetric: 385.6061 - val_loss: 398.4885 - val_MinusLogProbMetric: 398.4885 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 133/1000
2023-10-03 12:08:36.994 
Epoch 133/1000 
	 loss: 385.7212, MinusLogProbMetric: 385.7212, val_loss: 397.5776, val_MinusLogProbMetric: 397.5776

Epoch 133: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.7212 - MinusLogProbMetric: 385.7212 - val_loss: 397.5776 - val_MinusLogProbMetric: 397.5776 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 134/1000
2023-10-03 12:08:43.661 
Epoch 134/1000 
	 loss: 385.6183, MinusLogProbMetric: 385.6183, val_loss: 397.5090, val_MinusLogProbMetric: 397.5090

Epoch 134: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.6183 - MinusLogProbMetric: 385.6183 - val_loss: 397.5090 - val_MinusLogProbMetric: 397.5090 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 135/1000
2023-10-03 12:08:50.345 
Epoch 135/1000 
	 loss: 385.7576, MinusLogProbMetric: 385.7576, val_loss: 399.2073, val_MinusLogProbMetric: 399.2073

Epoch 135: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.7576 - MinusLogProbMetric: 385.7576 - val_loss: 399.2073 - val_MinusLogProbMetric: 399.2073 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 136/1000
2023-10-03 12:08:57.040 
Epoch 136/1000 
	 loss: 385.5334, MinusLogProbMetric: 385.5334, val_loss: 398.1774, val_MinusLogProbMetric: 398.1774

Epoch 136: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.5334 - MinusLogProbMetric: 385.5334 - val_loss: 398.1774 - val_MinusLogProbMetric: 398.1774 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 137/1000
2023-10-03 12:09:03.717 
Epoch 137/1000 
	 loss: 385.4895, MinusLogProbMetric: 385.4895, val_loss: 397.6347, val_MinusLogProbMetric: 397.6347

Epoch 137: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.4895 - MinusLogProbMetric: 385.4895 - val_loss: 397.6347 - val_MinusLogProbMetric: 397.6347 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 138/1000
2023-10-03 12:09:10.354 
Epoch 138/1000 
	 loss: 385.8000, MinusLogProbMetric: 385.8000, val_loss: 397.6298, val_MinusLogProbMetric: 397.6298

Epoch 138: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.8000 - MinusLogProbMetric: 385.8000 - val_loss: 397.6298 - val_MinusLogProbMetric: 397.6298 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 139/1000
2023-10-03 12:09:17.045 
Epoch 139/1000 
	 loss: 385.5981, MinusLogProbMetric: 385.5981, val_loss: 399.2985, val_MinusLogProbMetric: 399.2985

Epoch 139: val_loss did not improve from 394.43790
196/196 - 7s - loss: 385.5981 - MinusLogProbMetric: 385.5981 - val_loss: 399.2985 - val_MinusLogProbMetric: 399.2985 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 140/1000
2023-10-03 12:09:23.794 
Epoch 140/1000 
	 loss: 385.6809, MinusLogProbMetric: 385.6809, val_loss: 397.9512, val_MinusLogProbMetric: 397.9512

Epoch 140: val_loss did not improve from 394.43790
Restoring model weights from the end of the best epoch: 40.
196/196 - 7s - loss: 385.6809 - MinusLogProbMetric: 385.6809 - val_loss: 397.9512 - val_MinusLogProbMetric: 397.9512 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 140: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7fbb2341e440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 2778.6332907179603 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:5 out of the last 5 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7fbb2341f760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 2806.5590681990143 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7fbb083541f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 2702.035600491101 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:5 out of the last 5 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7fb798297c70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 2640.7841686250176 seconds.
Training succeeded with seed 869.
Model trained in 1027.12 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 10956.59 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 10956.94 s.
===========
Run 350/360 done in 13922.72 s.
===========

Directory ../../results/MAFN_new/run_351/ already exists.
Skipping it.
===========
Run 351/360 already exists. Skipping it.
===========

===========
Generating train data for run 352.
===========
Train data generated in 0.96 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_352
self.data_kwargs: {'seed': 869}
self.x_data: [[ 7.9973636   4.640891    5.32509    ...  3.500269    8.638443
   6.719802  ]
 [ 8.238864    4.485278    5.284702   ...  2.8864994   7.5674953
   7.1954236 ]
 [ 8.268872    4.7704377   5.167857   ...  4.5533733   8.026902
   7.8079686 ]
 ...
 [ 5.836932   -0.15277141  4.6562195  ...  4.799403    6.4058156
   6.0447054 ]
 [ 8.8304      4.5455837   5.226686   ...  3.9410732   7.6945333
   7.224351  ]
 [ 8.222233    4.7859597   5.1849046  ...  2.020081    8.037197
   7.44482   ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_101 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  9018400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7fbb2c641a50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb7504c0880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb7504c0880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb232d8d30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb72c6d6c50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb232da110>, <keras.callbacks.ModelCheckpoint object at 0x7fbb232daf20>, <keras.callbacks.EarlyStopping object at 0x7fbb232da620>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbb232d8850>, <keras.callbacks.TerminateOnNaN object at 0x7fbb232d8220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/360 with hyperparameters:
timestamp = 2023-10-03 15:12:03.992754
ndims = 1000
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.99736357e+00  4.64089108e+00  5.32508993e+00  2.54230428e+00
  6.17678785e+00  3.28207493e+00  5.66227102e+00  1.74029863e+00
  1.76519728e+00  3.78703856e+00  4.21053505e+00  3.09312010e+00
  2.03467131e+00  8.71515274e-03  3.30997020e-01  3.03643203e+00
  3.58006954e+00  7.91204977e+00  4.60797453e+00  9.32520199e+00
 -1.47658885e-01  1.08268631e+00  3.49395204e+00  5.92749643e+00
  8.83363056e+00  3.30430484e+00  2.72358418e+00  5.84306300e-01
  7.95439053e+00  4.29114294e+00  5.70392561e+00  9.62326145e+00
  1.91625190e+00  2.05868888e+00  7.86738205e+00  8.31465626e+00
  7.71446276e+00  6.16568375e+00  9.86226177e+00  4.56724548e+00
  5.94829226e+00  5.95013499e-01  4.22035885e+00  7.23472834e+00
  2.47805715e+00  8.42218876e+00  6.81143332e+00  3.05234265e+00
  7.74489641e+00 -1.49918377e-01  9.73857021e+00  8.26392531e-01
  1.01337919e+01  3.15674591e+00  7.51659107e+00 -4.07333463e-01
  2.89285135e+00  6.51461697e+00  6.24277782e+00 -2.83749938e-01
  3.82940078e+00  4.11356688e+00  6.11557388e+00  6.73437071e+00
  8.87053776e+00  4.77861261e+00  3.64606881e+00  1.49473929e+00
  8.43961716e+00  7.42175150e+00  4.57817841e+00  5.29543495e+00
  6.56515741e+00  1.46717739e+00 -1.17198992e+00  3.13418698e+00
  6.17689991e+00  4.57003880e+00  7.09853601e+00  8.16721344e+00
  4.87046480e+00  2.11677957e+00  8.49123096e+00  6.10315180e+00
  7.85925865e+00  5.53120947e+00  9.60370159e+00  2.18746519e+00
  6.79681110e+00  4.55581379e+00  4.16971874e+00  1.73112345e+00
  3.38649940e+00  9.48220348e+00  3.10462046e+00  2.14097667e+00
  5.63125849e+00  3.38328648e+00  5.20010900e+00  8.49408150e+00
  3.82229614e+00  4.20812511e+00  5.16726160e+00  5.89465952e+00
  9.87067223e+00  6.80660152e+00  7.34994841e+00  9.82929134e+00
  3.98204517e+00  3.51628542e+00 -3.33558351e-01  2.19543719e+00
  1.56361794e+00  1.56820440e+00  8.16210842e+00  6.19128990e+00
  6.25914240e+00  8.37748623e+00  2.31200051e+00  3.18747854e+00
  7.11904478e+00  4.19043159e+00 -1.50076985e-01  3.92370987e+00
  9.12942028e+00  4.13348961e+00  2.73112822e+00  9.84571934e+00
  5.88873577e+00  4.89145327e+00  1.00473905e+00  7.55370808e+00
  6.88664293e+00  9.29471016e+00  1.59780979e+00  1.04221833e+00
  7.24209642e+00  2.19776750e+00  3.42702579e+00  3.34507298e+00
  8.18159485e+00  5.71737003e+00  4.56389010e-01  4.59833431e+00
  7.35877991e+00  5.35200214e+00  5.63458967e+00  2.36878228e+00
  8.52304554e+00  5.58948660e+00  7.47698450e+00  9.61558151e+00
  4.02933073e+00  8.54585361e+00 -2.04157352e-01  3.18135262e-01
  5.99070644e+00  3.07406282e+00  2.67563176e+00  2.55177832e+00
  3.00443268e+00  7.63680124e+00  6.20905495e+00  7.80322456e+00
  7.74732971e+00  8.43296623e+00  5.25363827e+00  3.99893999e+00
  1.01927109e+01  7.05619717e+00  7.41717219e-01  5.25023270e+00
  3.77008796e+00  9.90027332e+00 -9.78221714e-01  6.02291298e+00
  6.87749577e+00  3.01261449e+00  9.65150070e+00  9.36239624e+00
  8.82814312e+00  7.89327025e-02  5.79393578e+00  2.59769440e+00
  7.66548109e+00  6.07534552e+00  5.36576509e+00  1.34668684e+00
  4.34030962e+00  6.31810141e+00  9.62684059e+00  4.88660431e+00
  9.60818291e+00  9.36019993e+00  6.71263504e+00  5.22296143e+00
  4.42309809e+00  7.77644825e+00 -1.76710457e-01  1.15464127e+00
  1.00100291e+00  8.11944199e+00  3.27240157e+00  6.37353086e+00
  5.53368807e+00  4.70193911e+00  5.07406664e+00  7.97970533e+00
  9.54048157e+00  4.72122574e+00  6.13606310e+00  3.54982883e-01
  1.08992491e+01  5.27934456e+00  7.85659790e+00  7.82055616e-01
  4.37489986e+00  2.60340428e+00  2.48891091e+00  9.05652618e+00
  6.87852859e+00  4.07554960e+00  1.03486490e+00  9.66742897e+00
  9.06607342e+00  3.15981674e+00  3.57244402e-01  5.63626289e+00
  9.92044353e+00  8.58868217e+00  6.57505751e+00  3.29356098e+00
  3.24422455e+00  9.00940800e+00  5.09291124e+00  2.85960793e-01
  2.23807621e+00  2.78734875e+00  1.29828584e+00  3.93540382e+00
  8.03243542e+00  6.78995275e+00  5.24697185e-01  1.17165899e+00
  3.27114582e+00  3.83798814e+00  8.53496361e+00  8.92454338e+00
  4.21771574e+00  6.39960194e+00  9.93754196e+00  7.64902067e+00
  6.44687891e+00  7.72444129e-01  2.29160905e+00  8.52109241e+00
  1.77464163e+00  8.37704659e+00  7.40880537e+00  2.87169456e+00
  5.39322519e+00  6.86864281e+00  7.26533747e+00  4.94568443e+00
  1.19446182e+00  6.65035486e+00  3.40026999e+00  9.23955250e+00
  2.17107248e+00  2.47258735e+00  2.43059564e+00  6.30585194e+00
  8.65539742e+00  1.45929515e+00  3.26658463e+00  5.87293768e+00
  4.54141855e+00  4.57416773e+00  8.90260696e+00  8.79572392e+00
  4.92429781e+00  6.65130472e+00  8.80965042e+00  4.23099279e+00
  2.90440130e+00  6.38359404e+00  5.79758072e+00  8.99278259e+00
  5.63759208e-01  9.11637592e+00  5.69089317e+00  3.27839077e-01
  2.98394132e+00  7.03010273e+00  3.88054943e+00  9.76950550e+00
  5.22733927e+00  2.11281967e+00  8.35765839e+00  5.03978920e+00
  7.56217527e+00  8.23209476e+00  6.36810303e+00  4.07139969e+00
  1.73890018e+00  7.25630856e+00  9.09160614e+00  3.28961945e+00
  9.31139088e+00  5.12792778e+00  6.50060415e+00  8.50181961e+00
  4.91192722e+00  8.58134687e-01  1.00498667e+01  2.70127869e+00
  6.64366674e+00  4.16276121e+00  4.07800245e+00  2.51310420e+00
  6.86618900e+00  1.04104257e+00  8.65216160e+00  8.34197235e+00
  5.48617649e+00  3.98194122e+00  2.67905617e+00  5.19228172e+00
  7.82842445e+00  2.60275602e+00  4.39831114e+00  2.98556614e+00
  2.60362029e+00  1.57165313e+00  9.64633369e+00  6.88340855e+00
  7.78283501e+00  1.10886335e+00  4.86602402e+00  8.91554070e+00
 -1.24701262e-02  1.84407997e+00  6.24502945e+00  6.37515545e+00
  7.90489101e+00  9.60888958e+00  1.45247686e+00  2.45699430e+00
  4.28265285e+00  6.50822818e-02  1.12332261e+00  2.24159002e+00
  4.71833563e+00  2.72241974e+00  8.28100014e+00  3.28441453e+00
  9.11346054e+00  1.05609627e+01  3.85242462e+00  3.12269568e+00
  2.30633688e+00  7.71115065e+00  2.85296941e+00  3.28263164e+00
  4.81248856e+00  6.73265219e+00  1.02898769e+01  3.37639761e+00
  3.19120789e+00  2.37913132e+00  3.12198162e+00  9.28771555e-01
  5.18783569e+00  1.00527916e+01  1.98218155e+00  3.54625010e+00
  9.82042217e+00  7.37978649e+00  4.98093176e+00  7.74178505e+00
  4.38426554e-01  5.04215908e+00  1.37913942e+00  2.33643341e+00
  1.43258142e+00  3.60961819e+00  4.80302668e+00  9.63227654e+00
  2.33755469e+00  4.40754080e+00  8.69386292e+00  8.12515831e+00
  5.90748072e+00  2.80811763e+00  5.03586197e+00  1.81783056e+00
  1.78456891e+00  4.50792408e+00  7.38818550e+00  5.70210075e+00
  7.67465544e+00  7.92264366e+00 -1.91134661e-01  5.84350491e+00
  6.51570177e+00  9.74592590e+00 -2.64375985e-01  4.72814274e+00
  1.51264036e+00  5.40283203e-01  7.05697918e+00  7.51599932e+00
  7.27936220e+00 -2.28865266e-01  3.87274265e+00  2.85972893e-01
  2.46495175e+00  1.80521321e+00  2.14937496e+00  3.77834606e+00
  3.18217349e+00  1.02763710e+01  3.37995410e+00  8.93162251e+00
  1.96273518e+00  9.14139557e+00 -3.55325580e-01  4.60834980e+00
 -5.89213148e-03  8.24484348e+00  1.03387394e+01  5.88764489e-01
  9.74320602e+00  8.21028519e+00  3.25056553e+00  9.08056259e+00
  2.59105110e+00  3.95399547e+00  9.59685862e-01  7.63103676e+00
 -4.04038966e-01  5.37858295e+00  6.89733744e+00  9.44393730e+00
  1.41217971e+00  7.53022814e+00  7.95942068e+00  5.70683289e+00
  5.70691347e+00 -2.19492435e-01  5.11235189e+00  3.95949173e+00
  9.22287846e+00  2.71982121e+00  5.80306816e+00  4.00738478e+00
  9.05252075e+00  2.45202136e+00  5.51476240e-01  6.91309261e+00
  3.62627077e+00  5.96777725e+00  3.05229813e-01  9.78659534e+00
  9.17897224e+00  5.76029110e+00  9.53746438e-01  3.57887888e+00
  9.68739605e+00  6.38989639e+00  1.90702987e+00  4.66415691e+00
  9.59853172e+00  5.05139923e+00  1.05777577e-01  6.60696030e+00
  2.90498590e+00  9.84483898e-01  4.15751266e+00  6.08889675e+00
 -7.97456264e-01  4.97374535e+00  5.03539562e+00  8.87364101e+00
  4.56227398e+00  6.91264105e+00  6.87507677e+00  8.22698975e+00
  3.93433619e+00  6.01061106e+00  5.63419819e+00  3.52927715e-01
  8.35077000e+00  3.63118911e+00  6.62209034e+00  5.41198444e+00
  6.93401527e+00  5.77218914e+00  1.43877172e+00  6.73276329e+00
  7.93194485e+00  7.98630357e-01  6.14384651e+00  6.38910675e+00
  9.41168022e+00  2.80262136e+00  1.86243892e+00  3.21781826e+00
  2.72202253e+00  7.37366581e+00  7.33060360e+00  6.61256790e+00
  2.12070441e+00  8.59328461e+00  1.00850325e+01  5.48360109e+00
  7.31572151e+00  5.40123463e+00  2.76470375e+00  2.38635635e+00
  2.60676074e+00  1.89509916e+00  8.99138355e+00  8.75982761e+00
  4.44820881e+00  3.67402339e+00  9.41257381e+00  2.81948304e+00
  5.31368160e+00  2.30343890e+00  1.82023335e+00  6.24686146e+00
  8.02147388e+00  6.23021078e+00  7.50943947e+00  9.98944378e+00
  9.65496063e+00  3.44390488e+00  9.90036297e+00  1.90570307e+00
  6.37369108e+00  6.97235632e+00  1.02818668e+00  1.01110497e+01
  3.08032537e+00  6.21633768e+00  8.70998573e+00  5.46199799e+00
  1.22746360e+00  8.02329159e+00  4.09109497e+00  1.01594963e+01
  8.81569862e+00  8.09971428e+00  3.88408160e+00  7.92610073e+00
  9.26926994e+00  6.73566246e+00  5.76671696e+00 -6.34171844e-01
  8.21392536e+00  8.59858513e+00  3.17632437e+00  2.96401882e+00
  4.87884188e+00  1.46060300e+00  7.75016904e-01  5.95933533e+00
  2.85177946e+00  4.70948601e+00  4.13632631e+00  9.87929535e+00
  1.08665876e+01  9.14421260e-01  8.82590580e+00  6.48437381e-01
  5.33739424e+00  2.29306793e+00  4.13651562e+00  6.47616768e+00
  4.24598312e+00  8.10760880e+00  7.45779276e+00  9.63120937e+00
  4.89558411e+00  8.02083302e+00  3.24438405e+00  3.59759498e+00
  4.09306526e+00  4.22914410e+00  9.30973232e-01  5.26565123e+00
  1.01665020e-01  9.00792408e+00  1.68276280e-01  4.04053593e+00
  2.87474799e+00  2.48327708e+00  7.51841593e+00  6.77074766e+00
  1.45175838e+00  6.70365572e+00  8.29291821e+00  7.64892721e+00
  1.97199440e+00  6.84292603e+00  7.54025269e+00  8.23593044e+00
  5.50070477e+00  3.45322466e+00  9.49723530e+00  8.45789242e+00
  5.81870270e+00  3.73827100e+00  8.91483879e+00  5.10812187e+00
  8.88178647e-01  4.46295691e+00  8.31669044e+00  4.49498892e+00
  5.43265700e-01  6.11975908e+00  9.95184231e+00  9.05875778e+00
  4.80623627e+00  9.23525047e+00  7.71717167e+00  5.08452940e+00
  4.99330044e+00  7.49071932e+00  7.34964132e+00  1.30181521e-01
  8.31134415e+00  4.43235731e+00  6.71253824e+00  7.91646671e+00
  4.65235710e+00  1.59666359e+00  4.05373240e+00  7.26302922e-01
  6.89302731e+00  9.45408058e+00  9.24543953e+00  4.22942209e+00
  5.97023821e+00  2.74910712e+00  5.68657923e+00  2.15732288e+00
  6.35400105e+00  2.63340044e+00  9.43256760e+00  6.54860783e+00
  9.90323901e-01  1.34424239e-01  9.92843723e+00  3.24450016e+00
  1.74554217e+00  3.04431581e+00  8.90259075e+00  7.87959576e+00
  5.06868839e+00  4.79971409e+00  2.77283359e+00  5.49839926e+00
  4.38119030e+00  2.14065003e+00  6.93851089e+00  4.98988056e+00
  7.73501110e+00  4.64208126e+00  4.79577160e+00  7.85067606e+00
  4.75812960e+00  9.79898643e+00  7.09330177e+00  8.39217281e+00
  9.55065346e+00  3.69468021e+00  5.54081106e+00  6.72979546e+00
  6.70603752e+00  9.38065147e+00  2.90241790e+00  2.52509665e+00
  8.94938564e+00  8.49423122e+00  2.13432574e+00  5.68594837e+00
  6.90361071e+00  4.37006855e+00  5.16254711e+00  5.72680283e+00
  5.19291306e+00  7.52211380e+00 -2.97093898e-01  2.55037498e+00
  2.54706359e+00  6.59417248e+00  7.53437948e+00  7.30045140e-01
  1.39935553e+00  6.93793297e+00  6.43762541e+00  5.88978815e+00
  9.83007526e+00  6.00890970e+00  6.66946948e-01  6.86854553e+00
  8.03701878e+00  6.11670637e+00  5.35453987e+00  9.57577324e+00
  1.49704802e+00  4.92001712e-01  1.53974497e+00  2.13410378e+00
  5.92641163e+00  6.05311298e+00  2.26492047e+00  4.60379982e+00
  4.60677052e+00  3.77398515e+00  7.27591801e+00  4.27604628e+00
  7.54144859e+00  1.66404676e+00  5.16442060e+00  8.22671890e+00
  1.71123445e+00  4.62352562e+00  6.08017778e+00  2.88557911e+00
  7.73235178e+00  6.43578827e-01  9.65846920e+00  4.54546928e+00
  3.51417971e+00  1.19948006e+00  5.29146385e+00  6.01765347e+00
  2.32322073e+00  2.89909482e+00  9.31757736e+00  6.46213627e+00
  2.06418347e+00  9.80539680e-01  8.66675568e+00  3.23060274e+00
  2.10251164e+00  9.88873005e+00  3.76415300e+00  8.50292981e-01
  5.07699108e+00  3.88809896e+00  9.39960575e+00  5.59372187e-01
 -5.31555176e-01  8.84760439e-01  9.86167717e+00  4.76715946e+00
  5.13683701e+00  5.85030556e+00  6.22967052e+00  1.53158998e+00
  6.54711390e+00  3.91384292e+00  2.45552421e+00  5.99128914e+00
  3.54900932e+00  2.41717911e+00  8.30238152e+00  1.64661229e+00
  2.59730220e+00  3.85882235e+00  2.47771192e+00  2.10063434e+00
  9.37286758e+00  3.17066669e+00  2.51538455e-01  5.38400126e+00
  1.23733842e+00  3.78326035e+00  9.20017338e+00  2.47332525e+00
  5.67668962e+00  5.17319727e+00  2.06113005e+00  6.44534540e+00
  8.66131496e+00  5.21378100e-01  6.50092697e+00  6.78853512e+00
  8.54306507e+00  1.80599499e+00  2.05283189e+00  6.85634184e+00
  1.15905434e-01  6.99897051e+00  8.32928753e+00  4.55407476e+00
  4.38788176e+00  3.54872870e+00 -4.62173462e-01  1.36794913e+00
  9.69468594e+00  8.38702488e+00  1.90114570e+00  7.94150639e+00
  3.36321092e+00  2.99465132e+00  1.99559021e+00  9.02903366e+00
  9.85151100e+00  3.23628950e+00  7.55823898e+00  3.70709014e+00
  1.74990630e+00  3.48457527e+00  5.45959711e+00  1.58513451e+00
  8.12769318e+00  4.73363543e+00  2.32966566e+00  7.44186544e+00
  7.94482851e+00  3.54043078e+00  7.04913664e+00  4.02170229e+00
  1.01847572e+01  8.72599220e+00  3.30791759e+00 -1.08366966e-01
  6.66013432e+00  1.91774535e+00  1.22588658e+00  1.02105055e+01
  9.86835957e+00  2.95132232e+00  3.01797199e+00  1.64833105e+00
  4.49072456e+00  8.49544430e+00  6.82054377e+00  9.07019711e+00
  1.68629599e+00  7.90870810e+00  9.09079742e+00  2.92365527e+00
  6.90887928e+00  4.17319202e+00  3.43049693e+00  9.02845764e+00
  4.29767895e+00  1.09342468e+00  3.74337578e+00  4.81818581e+00
  7.20623732e+00  1.07804976e+01  8.88462162e+00  9.30848122e+00
  4.87059927e+00  7.08490753e+00  2.95342398e+00  1.67907417e+00
  3.60964656e+00  7.03290796e+00  3.57637072e+00  8.74868298e+00
  8.52540398e+00  2.05524230e+00  3.42940140e+00  8.83274364e+00
  1.58101547e+00  3.33796239e+00  3.42331791e+00  6.96280146e+00
  3.25254261e-01 -1.25156724e+00  5.89129639e+00  4.46135187e+00
  6.51815891e+00  2.47705388e+00  9.10450840e+00  8.03964901e+00
  6.83334887e-01  8.24278164e+00  9.73476696e+00  1.00054388e+01
  7.11009216e+00  1.24967217e+00  5.68010473e+00  2.01691651e+00
  9.84855938e+00  7.23796368e+00  6.43789339e+00  6.97472811e+00
  4.51600838e+00  2.94856024e+00  9.13858128e+00  3.29138970e+00
  3.09904784e-01  4.01227283e+00  9.32983780e+00  5.93039370e+00
  1.10625067e+01  1.13827813e+00  1.01018476e+01  5.29424810e+00
  5.45073414e+00  5.82157803e+00  1.17043698e+00  3.22519493e+00
  7.34947968e+00  6.69572353e-01  1.62995577e+00  1.85301498e-01
  8.83085346e+00  5.87161255e+00  1.81599593e+00  7.31665230e+00
  9.78809166e+00  9.34503746e+00  9.14690876e+00  3.59607875e-01
  5.99414158e+00  2.59369469e+00  5.78580284e+00  3.18324995e+00
  1.74265718e+00  4.42781639e+00  4.24171114e+00  9.89190197e+00
  1.12621486e-02  4.74760723e+00  8.55551147e+00  7.21490955e+00
  8.37855244e+00  5.55442858e+00  7.92241478e+00  5.13917971e+00
  3.15440607e+00  8.63028622e+00  5.67239189e+00  1.22252262e+00
  7.74803591e+00  9.87097359e+00  4.27992439e+00  3.46016455e+00
  5.19145608e-01  6.58222771e+00  1.11442089e+01 -5.05199075e-01
  4.88900900e+00  1.03596199e+00  3.49793643e-01  6.70542479e+00
  8.65525723e+00  3.48391366e+00  8.85622883e+00  1.32473469e+00
  1.43105400e+00  9.27033043e+00  5.13401794e+00  6.83637190e+00
  3.17043185e-01  1.26743388e+00  5.01762807e-01  9.27802563e+00
  1.43500125e+00  8.44762039e+00  3.54408193e+00  7.80078506e+00
  3.90129399e+00  3.20132899e+00  1.04541802e+00  2.72584295e+00
  4.51778460e+00  7.11837149e+00  4.60630083e+00  9.86540890e+00
  6.96063471e+00 -3.97378445e-01  3.62382388e+00  6.64995527e+00
  1.94566798e+00  7.08949041e+00  5.43198347e+00  5.80618441e-01
  7.76860094e+00  6.34063721e+00  7.98214495e-01  4.38892889e+00
  2.10588074e+00  8.85570145e+00  2.82261276e+00  9.23659515e+00
  5.26395798e+00  3.50026894e+00  8.63844299e+00  6.71980190e+00]
Epoch 1/1000
2023-10-03 15:12:40.213 
Epoch 1/1000 
	 loss: 1498.8744, MinusLogProbMetric: 1498.8744, val_loss: 597.3953, val_MinusLogProbMetric: 597.3953

Epoch 1: val_loss improved from inf to 597.39526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 36s - loss: 1498.8744 - MinusLogProbMetric: 1498.8744 - val_loss: 597.3953 - val_MinusLogProbMetric: 597.3953 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 2/1000
2023-10-03 15:12:52.106 
Epoch 2/1000 
	 loss: 556.5603, MinusLogProbMetric: 556.5603, val_loss: 527.3843, val_MinusLogProbMetric: 527.3843

Epoch 2: val_loss improved from 597.39526 to 527.38434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 556.5603 - MinusLogProbMetric: 556.5603 - val_loss: 527.3843 - val_MinusLogProbMetric: 527.3843 - lr: 0.0010 - 12s/epoch - 62ms/step
Epoch 3/1000
2023-10-03 15:13:03.719 
Epoch 3/1000 
	 loss: 525.6636, MinusLogProbMetric: 525.6636, val_loss: 749.8119, val_MinusLogProbMetric: 749.8119

Epoch 3: val_loss did not improve from 527.38434
196/196 - 11s - loss: 525.6636 - MinusLogProbMetric: 525.6636 - val_loss: 749.8119 - val_MinusLogProbMetric: 749.8119 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 4/1000
2023-10-03 15:13:14.606 
Epoch 4/1000 
	 loss: 503.4455, MinusLogProbMetric: 503.4455, val_loss: 482.3637, val_MinusLogProbMetric: 482.3637

Epoch 4: val_loss improved from 527.38434 to 482.36371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 503.4455 - MinusLogProbMetric: 503.4455 - val_loss: 482.3637 - val_MinusLogProbMetric: 482.3637 - lr: 0.0010 - 12s/epoch - 62ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 98: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-03 15:13:22.466 
Epoch 5/1000 
	 loss: inf, MinusLogProbMetric: inf, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 482.36371
196/196 - 7s - loss: inf - MinusLogProbMetric: inf - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 7s/epoch - 33ms/step
The loss history contains Inf values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 352.
===========
Train data generated in 0.90 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_352
self.data_kwargs: {'seed': 869}
self.x_data: [[ 7.9973636   4.640891    5.32509    ...  3.500269    8.638443
   6.719802  ]
 [ 8.238864    4.485278    5.284702   ...  2.8864994   7.5674953
   7.1954236 ]
 [ 8.268872    4.7704377   5.167857   ...  4.5533733   8.026902
   7.8079686 ]
 ...
 [ 5.836932   -0.15277141  4.6562195  ...  4.799403    6.4058156
   6.0447054 ]
 [ 8.8304      4.5455837   5.226686   ...  3.9410732   7.6945333
   7.224351  ]
 [ 8.222233    4.7859597   5.1849046  ...  2.020081    8.037197
   7.44482   ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_112 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  9018400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7fbad7864280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb2c83a320>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb2c83a320>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb45db4bb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb45db4520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb46282c80>, <keras.callbacks.ModelCheckpoint object at 0x7fbb19d8f9a0>, <keras.callbacks.EarlyStopping object at 0x7fbaf7a64fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbb19d8c700>, <keras.callbacks.TerminateOnNaN object at 0x7fbac6aceb90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 352/360 with hyperparameters:
timestamp = 2023-10-03 15:13:26.936180
ndims = 1000
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.99736357e+00  4.64089108e+00  5.32508993e+00  2.54230428e+00
  6.17678785e+00  3.28207493e+00  5.66227102e+00  1.74029863e+00
  1.76519728e+00  3.78703856e+00  4.21053505e+00  3.09312010e+00
  2.03467131e+00  8.71515274e-03  3.30997020e-01  3.03643203e+00
  3.58006954e+00  7.91204977e+00  4.60797453e+00  9.32520199e+00
 -1.47658885e-01  1.08268631e+00  3.49395204e+00  5.92749643e+00
  8.83363056e+00  3.30430484e+00  2.72358418e+00  5.84306300e-01
  7.95439053e+00  4.29114294e+00  5.70392561e+00  9.62326145e+00
  1.91625190e+00  2.05868888e+00  7.86738205e+00  8.31465626e+00
  7.71446276e+00  6.16568375e+00  9.86226177e+00  4.56724548e+00
  5.94829226e+00  5.95013499e-01  4.22035885e+00  7.23472834e+00
  2.47805715e+00  8.42218876e+00  6.81143332e+00  3.05234265e+00
  7.74489641e+00 -1.49918377e-01  9.73857021e+00  8.26392531e-01
  1.01337919e+01  3.15674591e+00  7.51659107e+00 -4.07333463e-01
  2.89285135e+00  6.51461697e+00  6.24277782e+00 -2.83749938e-01
  3.82940078e+00  4.11356688e+00  6.11557388e+00  6.73437071e+00
  8.87053776e+00  4.77861261e+00  3.64606881e+00  1.49473929e+00
  8.43961716e+00  7.42175150e+00  4.57817841e+00  5.29543495e+00
  6.56515741e+00  1.46717739e+00 -1.17198992e+00  3.13418698e+00
  6.17689991e+00  4.57003880e+00  7.09853601e+00  8.16721344e+00
  4.87046480e+00  2.11677957e+00  8.49123096e+00  6.10315180e+00
  7.85925865e+00  5.53120947e+00  9.60370159e+00  2.18746519e+00
  6.79681110e+00  4.55581379e+00  4.16971874e+00  1.73112345e+00
  3.38649940e+00  9.48220348e+00  3.10462046e+00  2.14097667e+00
  5.63125849e+00  3.38328648e+00  5.20010900e+00  8.49408150e+00
  3.82229614e+00  4.20812511e+00  5.16726160e+00  5.89465952e+00
  9.87067223e+00  6.80660152e+00  7.34994841e+00  9.82929134e+00
  3.98204517e+00  3.51628542e+00 -3.33558351e-01  2.19543719e+00
  1.56361794e+00  1.56820440e+00  8.16210842e+00  6.19128990e+00
  6.25914240e+00  8.37748623e+00  2.31200051e+00  3.18747854e+00
  7.11904478e+00  4.19043159e+00 -1.50076985e-01  3.92370987e+00
  9.12942028e+00  4.13348961e+00  2.73112822e+00  9.84571934e+00
  5.88873577e+00  4.89145327e+00  1.00473905e+00  7.55370808e+00
  6.88664293e+00  9.29471016e+00  1.59780979e+00  1.04221833e+00
  7.24209642e+00  2.19776750e+00  3.42702579e+00  3.34507298e+00
  8.18159485e+00  5.71737003e+00  4.56389010e-01  4.59833431e+00
  7.35877991e+00  5.35200214e+00  5.63458967e+00  2.36878228e+00
  8.52304554e+00  5.58948660e+00  7.47698450e+00  9.61558151e+00
  4.02933073e+00  8.54585361e+00 -2.04157352e-01  3.18135262e-01
  5.99070644e+00  3.07406282e+00  2.67563176e+00  2.55177832e+00
  3.00443268e+00  7.63680124e+00  6.20905495e+00  7.80322456e+00
  7.74732971e+00  8.43296623e+00  5.25363827e+00  3.99893999e+00
  1.01927109e+01  7.05619717e+00  7.41717219e-01  5.25023270e+00
  3.77008796e+00  9.90027332e+00 -9.78221714e-01  6.02291298e+00
  6.87749577e+00  3.01261449e+00  9.65150070e+00  9.36239624e+00
  8.82814312e+00  7.89327025e-02  5.79393578e+00  2.59769440e+00
  7.66548109e+00  6.07534552e+00  5.36576509e+00  1.34668684e+00
  4.34030962e+00  6.31810141e+00  9.62684059e+00  4.88660431e+00
  9.60818291e+00  9.36019993e+00  6.71263504e+00  5.22296143e+00
  4.42309809e+00  7.77644825e+00 -1.76710457e-01  1.15464127e+00
  1.00100291e+00  8.11944199e+00  3.27240157e+00  6.37353086e+00
  5.53368807e+00  4.70193911e+00  5.07406664e+00  7.97970533e+00
  9.54048157e+00  4.72122574e+00  6.13606310e+00  3.54982883e-01
  1.08992491e+01  5.27934456e+00  7.85659790e+00  7.82055616e-01
  4.37489986e+00  2.60340428e+00  2.48891091e+00  9.05652618e+00
  6.87852859e+00  4.07554960e+00  1.03486490e+00  9.66742897e+00
  9.06607342e+00  3.15981674e+00  3.57244402e-01  5.63626289e+00
  9.92044353e+00  8.58868217e+00  6.57505751e+00  3.29356098e+00
  3.24422455e+00  9.00940800e+00  5.09291124e+00  2.85960793e-01
  2.23807621e+00  2.78734875e+00  1.29828584e+00  3.93540382e+00
  8.03243542e+00  6.78995275e+00  5.24697185e-01  1.17165899e+00
  3.27114582e+00  3.83798814e+00  8.53496361e+00  8.92454338e+00
  4.21771574e+00  6.39960194e+00  9.93754196e+00  7.64902067e+00
  6.44687891e+00  7.72444129e-01  2.29160905e+00  8.52109241e+00
  1.77464163e+00  8.37704659e+00  7.40880537e+00  2.87169456e+00
  5.39322519e+00  6.86864281e+00  7.26533747e+00  4.94568443e+00
  1.19446182e+00  6.65035486e+00  3.40026999e+00  9.23955250e+00
  2.17107248e+00  2.47258735e+00  2.43059564e+00  6.30585194e+00
  8.65539742e+00  1.45929515e+00  3.26658463e+00  5.87293768e+00
  4.54141855e+00  4.57416773e+00  8.90260696e+00  8.79572392e+00
  4.92429781e+00  6.65130472e+00  8.80965042e+00  4.23099279e+00
  2.90440130e+00  6.38359404e+00  5.79758072e+00  8.99278259e+00
  5.63759208e-01  9.11637592e+00  5.69089317e+00  3.27839077e-01
  2.98394132e+00  7.03010273e+00  3.88054943e+00  9.76950550e+00
  5.22733927e+00  2.11281967e+00  8.35765839e+00  5.03978920e+00
  7.56217527e+00  8.23209476e+00  6.36810303e+00  4.07139969e+00
  1.73890018e+00  7.25630856e+00  9.09160614e+00  3.28961945e+00
  9.31139088e+00  5.12792778e+00  6.50060415e+00  8.50181961e+00
  4.91192722e+00  8.58134687e-01  1.00498667e+01  2.70127869e+00
  6.64366674e+00  4.16276121e+00  4.07800245e+00  2.51310420e+00
  6.86618900e+00  1.04104257e+00  8.65216160e+00  8.34197235e+00
  5.48617649e+00  3.98194122e+00  2.67905617e+00  5.19228172e+00
  7.82842445e+00  2.60275602e+00  4.39831114e+00  2.98556614e+00
  2.60362029e+00  1.57165313e+00  9.64633369e+00  6.88340855e+00
  7.78283501e+00  1.10886335e+00  4.86602402e+00  8.91554070e+00
 -1.24701262e-02  1.84407997e+00  6.24502945e+00  6.37515545e+00
  7.90489101e+00  9.60888958e+00  1.45247686e+00  2.45699430e+00
  4.28265285e+00  6.50822818e-02  1.12332261e+00  2.24159002e+00
  4.71833563e+00  2.72241974e+00  8.28100014e+00  3.28441453e+00
  9.11346054e+00  1.05609627e+01  3.85242462e+00  3.12269568e+00
  2.30633688e+00  7.71115065e+00  2.85296941e+00  3.28263164e+00
  4.81248856e+00  6.73265219e+00  1.02898769e+01  3.37639761e+00
  3.19120789e+00  2.37913132e+00  3.12198162e+00  9.28771555e-01
  5.18783569e+00  1.00527916e+01  1.98218155e+00  3.54625010e+00
  9.82042217e+00  7.37978649e+00  4.98093176e+00  7.74178505e+00
  4.38426554e-01  5.04215908e+00  1.37913942e+00  2.33643341e+00
  1.43258142e+00  3.60961819e+00  4.80302668e+00  9.63227654e+00
  2.33755469e+00  4.40754080e+00  8.69386292e+00  8.12515831e+00
  5.90748072e+00  2.80811763e+00  5.03586197e+00  1.81783056e+00
  1.78456891e+00  4.50792408e+00  7.38818550e+00  5.70210075e+00
  7.67465544e+00  7.92264366e+00 -1.91134661e-01  5.84350491e+00
  6.51570177e+00  9.74592590e+00 -2.64375985e-01  4.72814274e+00
  1.51264036e+00  5.40283203e-01  7.05697918e+00  7.51599932e+00
  7.27936220e+00 -2.28865266e-01  3.87274265e+00  2.85972893e-01
  2.46495175e+00  1.80521321e+00  2.14937496e+00  3.77834606e+00
  3.18217349e+00  1.02763710e+01  3.37995410e+00  8.93162251e+00
  1.96273518e+00  9.14139557e+00 -3.55325580e-01  4.60834980e+00
 -5.89213148e-03  8.24484348e+00  1.03387394e+01  5.88764489e-01
  9.74320602e+00  8.21028519e+00  3.25056553e+00  9.08056259e+00
  2.59105110e+00  3.95399547e+00  9.59685862e-01  7.63103676e+00
 -4.04038966e-01  5.37858295e+00  6.89733744e+00  9.44393730e+00
  1.41217971e+00  7.53022814e+00  7.95942068e+00  5.70683289e+00
  5.70691347e+00 -2.19492435e-01  5.11235189e+00  3.95949173e+00
  9.22287846e+00  2.71982121e+00  5.80306816e+00  4.00738478e+00
  9.05252075e+00  2.45202136e+00  5.51476240e-01  6.91309261e+00
  3.62627077e+00  5.96777725e+00  3.05229813e-01  9.78659534e+00
  9.17897224e+00  5.76029110e+00  9.53746438e-01  3.57887888e+00
  9.68739605e+00  6.38989639e+00  1.90702987e+00  4.66415691e+00
  9.59853172e+00  5.05139923e+00  1.05777577e-01  6.60696030e+00
  2.90498590e+00  9.84483898e-01  4.15751266e+00  6.08889675e+00
 -7.97456264e-01  4.97374535e+00  5.03539562e+00  8.87364101e+00
  4.56227398e+00  6.91264105e+00  6.87507677e+00  8.22698975e+00
  3.93433619e+00  6.01061106e+00  5.63419819e+00  3.52927715e-01
  8.35077000e+00  3.63118911e+00  6.62209034e+00  5.41198444e+00
  6.93401527e+00  5.77218914e+00  1.43877172e+00  6.73276329e+00
  7.93194485e+00  7.98630357e-01  6.14384651e+00  6.38910675e+00
  9.41168022e+00  2.80262136e+00  1.86243892e+00  3.21781826e+00
  2.72202253e+00  7.37366581e+00  7.33060360e+00  6.61256790e+00
  2.12070441e+00  8.59328461e+00  1.00850325e+01  5.48360109e+00
  7.31572151e+00  5.40123463e+00  2.76470375e+00  2.38635635e+00
  2.60676074e+00  1.89509916e+00  8.99138355e+00  8.75982761e+00
  4.44820881e+00  3.67402339e+00  9.41257381e+00  2.81948304e+00
  5.31368160e+00  2.30343890e+00  1.82023335e+00  6.24686146e+00
  8.02147388e+00  6.23021078e+00  7.50943947e+00  9.98944378e+00
  9.65496063e+00  3.44390488e+00  9.90036297e+00  1.90570307e+00
  6.37369108e+00  6.97235632e+00  1.02818668e+00  1.01110497e+01
  3.08032537e+00  6.21633768e+00  8.70998573e+00  5.46199799e+00
  1.22746360e+00  8.02329159e+00  4.09109497e+00  1.01594963e+01
  8.81569862e+00  8.09971428e+00  3.88408160e+00  7.92610073e+00
  9.26926994e+00  6.73566246e+00  5.76671696e+00 -6.34171844e-01
  8.21392536e+00  8.59858513e+00  3.17632437e+00  2.96401882e+00
  4.87884188e+00  1.46060300e+00  7.75016904e-01  5.95933533e+00
  2.85177946e+00  4.70948601e+00  4.13632631e+00  9.87929535e+00
  1.08665876e+01  9.14421260e-01  8.82590580e+00  6.48437381e-01
  5.33739424e+00  2.29306793e+00  4.13651562e+00  6.47616768e+00
  4.24598312e+00  8.10760880e+00  7.45779276e+00  9.63120937e+00
  4.89558411e+00  8.02083302e+00  3.24438405e+00  3.59759498e+00
  4.09306526e+00  4.22914410e+00  9.30973232e-01  5.26565123e+00
  1.01665020e-01  9.00792408e+00  1.68276280e-01  4.04053593e+00
  2.87474799e+00  2.48327708e+00  7.51841593e+00  6.77074766e+00
  1.45175838e+00  6.70365572e+00  8.29291821e+00  7.64892721e+00
  1.97199440e+00  6.84292603e+00  7.54025269e+00  8.23593044e+00
  5.50070477e+00  3.45322466e+00  9.49723530e+00  8.45789242e+00
  5.81870270e+00  3.73827100e+00  8.91483879e+00  5.10812187e+00
  8.88178647e-01  4.46295691e+00  8.31669044e+00  4.49498892e+00
  5.43265700e-01  6.11975908e+00  9.95184231e+00  9.05875778e+00
  4.80623627e+00  9.23525047e+00  7.71717167e+00  5.08452940e+00
  4.99330044e+00  7.49071932e+00  7.34964132e+00  1.30181521e-01
  8.31134415e+00  4.43235731e+00  6.71253824e+00  7.91646671e+00
  4.65235710e+00  1.59666359e+00  4.05373240e+00  7.26302922e-01
  6.89302731e+00  9.45408058e+00  9.24543953e+00  4.22942209e+00
  5.97023821e+00  2.74910712e+00  5.68657923e+00  2.15732288e+00
  6.35400105e+00  2.63340044e+00  9.43256760e+00  6.54860783e+00
  9.90323901e-01  1.34424239e-01  9.92843723e+00  3.24450016e+00
  1.74554217e+00  3.04431581e+00  8.90259075e+00  7.87959576e+00
  5.06868839e+00  4.79971409e+00  2.77283359e+00  5.49839926e+00
  4.38119030e+00  2.14065003e+00  6.93851089e+00  4.98988056e+00
  7.73501110e+00  4.64208126e+00  4.79577160e+00  7.85067606e+00
  4.75812960e+00  9.79898643e+00  7.09330177e+00  8.39217281e+00
  9.55065346e+00  3.69468021e+00  5.54081106e+00  6.72979546e+00
  6.70603752e+00  9.38065147e+00  2.90241790e+00  2.52509665e+00
  8.94938564e+00  8.49423122e+00  2.13432574e+00  5.68594837e+00
  6.90361071e+00  4.37006855e+00  5.16254711e+00  5.72680283e+00
  5.19291306e+00  7.52211380e+00 -2.97093898e-01  2.55037498e+00
  2.54706359e+00  6.59417248e+00  7.53437948e+00  7.30045140e-01
  1.39935553e+00  6.93793297e+00  6.43762541e+00  5.88978815e+00
  9.83007526e+00  6.00890970e+00  6.66946948e-01  6.86854553e+00
  8.03701878e+00  6.11670637e+00  5.35453987e+00  9.57577324e+00
  1.49704802e+00  4.92001712e-01  1.53974497e+00  2.13410378e+00
  5.92641163e+00  6.05311298e+00  2.26492047e+00  4.60379982e+00
  4.60677052e+00  3.77398515e+00  7.27591801e+00  4.27604628e+00
  7.54144859e+00  1.66404676e+00  5.16442060e+00  8.22671890e+00
  1.71123445e+00  4.62352562e+00  6.08017778e+00  2.88557911e+00
  7.73235178e+00  6.43578827e-01  9.65846920e+00  4.54546928e+00
  3.51417971e+00  1.19948006e+00  5.29146385e+00  6.01765347e+00
  2.32322073e+00  2.89909482e+00  9.31757736e+00  6.46213627e+00
  2.06418347e+00  9.80539680e-01  8.66675568e+00  3.23060274e+00
  2.10251164e+00  9.88873005e+00  3.76415300e+00  8.50292981e-01
  5.07699108e+00  3.88809896e+00  9.39960575e+00  5.59372187e-01
 -5.31555176e-01  8.84760439e-01  9.86167717e+00  4.76715946e+00
  5.13683701e+00  5.85030556e+00  6.22967052e+00  1.53158998e+00
  6.54711390e+00  3.91384292e+00  2.45552421e+00  5.99128914e+00
  3.54900932e+00  2.41717911e+00  8.30238152e+00  1.64661229e+00
  2.59730220e+00  3.85882235e+00  2.47771192e+00  2.10063434e+00
  9.37286758e+00  3.17066669e+00  2.51538455e-01  5.38400126e+00
  1.23733842e+00  3.78326035e+00  9.20017338e+00  2.47332525e+00
  5.67668962e+00  5.17319727e+00  2.06113005e+00  6.44534540e+00
  8.66131496e+00  5.21378100e-01  6.50092697e+00  6.78853512e+00
  8.54306507e+00  1.80599499e+00  2.05283189e+00  6.85634184e+00
  1.15905434e-01  6.99897051e+00  8.32928753e+00  4.55407476e+00
  4.38788176e+00  3.54872870e+00 -4.62173462e-01  1.36794913e+00
  9.69468594e+00  8.38702488e+00  1.90114570e+00  7.94150639e+00
  3.36321092e+00  2.99465132e+00  1.99559021e+00  9.02903366e+00
  9.85151100e+00  3.23628950e+00  7.55823898e+00  3.70709014e+00
  1.74990630e+00  3.48457527e+00  5.45959711e+00  1.58513451e+00
  8.12769318e+00  4.73363543e+00  2.32966566e+00  7.44186544e+00
  7.94482851e+00  3.54043078e+00  7.04913664e+00  4.02170229e+00
  1.01847572e+01  8.72599220e+00  3.30791759e+00 -1.08366966e-01
  6.66013432e+00  1.91774535e+00  1.22588658e+00  1.02105055e+01
  9.86835957e+00  2.95132232e+00  3.01797199e+00  1.64833105e+00
  4.49072456e+00  8.49544430e+00  6.82054377e+00  9.07019711e+00
  1.68629599e+00  7.90870810e+00  9.09079742e+00  2.92365527e+00
  6.90887928e+00  4.17319202e+00  3.43049693e+00  9.02845764e+00
  4.29767895e+00  1.09342468e+00  3.74337578e+00  4.81818581e+00
  7.20623732e+00  1.07804976e+01  8.88462162e+00  9.30848122e+00
  4.87059927e+00  7.08490753e+00  2.95342398e+00  1.67907417e+00
  3.60964656e+00  7.03290796e+00  3.57637072e+00  8.74868298e+00
  8.52540398e+00  2.05524230e+00  3.42940140e+00  8.83274364e+00
  1.58101547e+00  3.33796239e+00  3.42331791e+00  6.96280146e+00
  3.25254261e-01 -1.25156724e+00  5.89129639e+00  4.46135187e+00
  6.51815891e+00  2.47705388e+00  9.10450840e+00  8.03964901e+00
  6.83334887e-01  8.24278164e+00  9.73476696e+00  1.00054388e+01
  7.11009216e+00  1.24967217e+00  5.68010473e+00  2.01691651e+00
  9.84855938e+00  7.23796368e+00  6.43789339e+00  6.97472811e+00
  4.51600838e+00  2.94856024e+00  9.13858128e+00  3.29138970e+00
  3.09904784e-01  4.01227283e+00  9.32983780e+00  5.93039370e+00
  1.10625067e+01  1.13827813e+00  1.01018476e+01  5.29424810e+00
  5.45073414e+00  5.82157803e+00  1.17043698e+00  3.22519493e+00
  7.34947968e+00  6.69572353e-01  1.62995577e+00  1.85301498e-01
  8.83085346e+00  5.87161255e+00  1.81599593e+00  7.31665230e+00
  9.78809166e+00  9.34503746e+00  9.14690876e+00  3.59607875e-01
  5.99414158e+00  2.59369469e+00  5.78580284e+00  3.18324995e+00
  1.74265718e+00  4.42781639e+00  4.24171114e+00  9.89190197e+00
  1.12621486e-02  4.74760723e+00  8.55551147e+00  7.21490955e+00
  8.37855244e+00  5.55442858e+00  7.92241478e+00  5.13917971e+00
  3.15440607e+00  8.63028622e+00  5.67239189e+00  1.22252262e+00
  7.74803591e+00  9.87097359e+00  4.27992439e+00  3.46016455e+00
  5.19145608e-01  6.58222771e+00  1.11442089e+01 -5.05199075e-01
  4.88900900e+00  1.03596199e+00  3.49793643e-01  6.70542479e+00
  8.65525723e+00  3.48391366e+00  8.85622883e+00  1.32473469e+00
  1.43105400e+00  9.27033043e+00  5.13401794e+00  6.83637190e+00
  3.17043185e-01  1.26743388e+00  5.01762807e-01  9.27802563e+00
  1.43500125e+00  8.44762039e+00  3.54408193e+00  7.80078506e+00
  3.90129399e+00  3.20132899e+00  1.04541802e+00  2.72584295e+00
  4.51778460e+00  7.11837149e+00  4.60630083e+00  9.86540890e+00
  6.96063471e+00 -3.97378445e-01  3.62382388e+00  6.64995527e+00
  1.94566798e+00  7.08949041e+00  5.43198347e+00  5.80618441e-01
  7.76860094e+00  6.34063721e+00  7.98214495e-01  4.38892889e+00
  2.10588074e+00  8.85570145e+00  2.82261276e+00  9.23659515e+00
  5.26395798e+00  3.50026894e+00  8.63844299e+00  6.71980190e+00]
Epoch 1/1000
2023-10-03 15:14:03.052 
Epoch 1/1000 
	 loss: 481.2391, MinusLogProbMetric: 481.2391, val_loss: 448.9094, val_MinusLogProbMetric: 448.9094

Epoch 1: val_loss improved from inf to 448.90939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 37s - loss: 481.2391 - MinusLogProbMetric: 481.2391 - val_loss: 448.9094 - val_MinusLogProbMetric: 448.9094 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 2/1000
2023-10-03 15:14:15.928 
Epoch 2/1000 
	 loss: 446.9640, MinusLogProbMetric: 446.9640, val_loss: 441.1366, val_MinusLogProbMetric: 441.1366

Epoch 2: val_loss improved from 448.90939 to 441.13660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 446.9640 - MinusLogProbMetric: 446.9640 - val_loss: 441.1366 - val_MinusLogProbMetric: 441.1366 - lr: 3.3333e-04 - 12s/epoch - 63ms/step
Epoch 3/1000
2023-10-03 15:14:27.984 
Epoch 3/1000 
	 loss: 442.4050, MinusLogProbMetric: 442.4050, val_loss: 444.7915, val_MinusLogProbMetric: 444.7915

Epoch 3: val_loss did not improve from 441.13660
196/196 - 12s - loss: 442.4050 - MinusLogProbMetric: 442.4050 - val_loss: 444.7915 - val_MinusLogProbMetric: 444.7915 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 4/1000
2023-10-03 15:14:39.313 
Epoch 4/1000 
	 loss: 438.5620, MinusLogProbMetric: 438.5620, val_loss: 436.4128, val_MinusLogProbMetric: 436.4128

Epoch 4: val_loss improved from 441.13660 to 436.41281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 438.5620 - MinusLogProbMetric: 438.5620 - val_loss: 436.4128 - val_MinusLogProbMetric: 436.4128 - lr: 3.3333e-04 - 12s/epoch - 61ms/step
Epoch 5/1000
2023-10-03 15:14:50.910 
Epoch 5/1000 
	 loss: 436.7314, MinusLogProbMetric: 436.7314, val_loss: 445.0380, val_MinusLogProbMetric: 445.0380

Epoch 5: val_loss did not improve from 436.41281
196/196 - 11s - loss: 436.7314 - MinusLogProbMetric: 436.7314 - val_loss: 445.0380 - val_MinusLogProbMetric: 445.0380 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 6/1000
2023-10-03 15:15:02.372 
Epoch 6/1000 
	 loss: 433.9760, MinusLogProbMetric: 433.9760, val_loss: 430.4155, val_MinusLogProbMetric: 430.4155

Epoch 6: val_loss improved from 436.41281 to 430.41553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 433.9760 - MinusLogProbMetric: 433.9760 - val_loss: 430.4155 - val_MinusLogProbMetric: 430.4155 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 7/1000
2023-10-03 15:15:14.003 
Epoch 7/1000 
	 loss: 430.2046, MinusLogProbMetric: 430.2046, val_loss: 457.5667, val_MinusLogProbMetric: 457.5667

Epoch 7: val_loss did not improve from 430.41553
196/196 - 11s - loss: 430.2046 - MinusLogProbMetric: 430.2046 - val_loss: 457.5667 - val_MinusLogProbMetric: 457.5667 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 8/1000
2023-10-03 15:15:25.261 
Epoch 8/1000 
	 loss: 431.6579, MinusLogProbMetric: 431.6579, val_loss: 427.7092, val_MinusLogProbMetric: 427.7092

Epoch 8: val_loss improved from 430.41553 to 427.70917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 431.6579 - MinusLogProbMetric: 431.6579 - val_loss: 427.7092 - val_MinusLogProbMetric: 427.7092 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 9/1000
2023-10-03 15:15:37.013 
Epoch 9/1000 
	 loss: 427.6920, MinusLogProbMetric: 427.6920, val_loss: 425.8801, val_MinusLogProbMetric: 425.8801

Epoch 9: val_loss improved from 427.70917 to 425.88010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 427.6920 - MinusLogProbMetric: 427.6920 - val_loss: 425.8801 - val_MinusLogProbMetric: 425.8801 - lr: 3.3333e-04 - 12s/epoch - 61ms/step
Epoch 10/1000
2023-10-03 15:15:49.249 
Epoch 10/1000 
	 loss: 430.0659, MinusLogProbMetric: 430.0659, val_loss: 428.1858, val_MinusLogProbMetric: 428.1858

Epoch 10: val_loss did not improve from 425.88010
196/196 - 11s - loss: 430.0659 - MinusLogProbMetric: 430.0659 - val_loss: 428.1858 - val_MinusLogProbMetric: 428.1858 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 11/1000
2023-10-03 15:16:00.420 
Epoch 11/1000 
	 loss: 426.0702, MinusLogProbMetric: 426.0702, val_loss: 428.8868, val_MinusLogProbMetric: 428.8868

Epoch 11: val_loss did not improve from 425.88010
196/196 - 11s - loss: 426.0702 - MinusLogProbMetric: 426.0702 - val_loss: 428.8868 - val_MinusLogProbMetric: 428.8868 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 12/1000
2023-10-03 15:16:11.632 
Epoch 12/1000 
	 loss: 422.4885, MinusLogProbMetric: 422.4885, val_loss: 445.5693, val_MinusLogProbMetric: 445.5693

Epoch 12: val_loss did not improve from 425.88010
196/196 - 11s - loss: 422.4885 - MinusLogProbMetric: 422.4885 - val_loss: 445.5693 - val_MinusLogProbMetric: 445.5693 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 13/1000
2023-10-03 15:16:23.158 
Epoch 13/1000 
	 loss: 422.4363, MinusLogProbMetric: 422.4363, val_loss: 421.4274, val_MinusLogProbMetric: 421.4274

Epoch 13: val_loss improved from 425.88010 to 421.42743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 422.4363 - MinusLogProbMetric: 422.4363 - val_loss: 421.4274 - val_MinusLogProbMetric: 421.4274 - lr: 3.3333e-04 - 12s/epoch - 61ms/step
Epoch 14/1000
2023-10-03 15:16:35.119 
Epoch 14/1000 
	 loss: 420.1189, MinusLogProbMetric: 420.1189, val_loss: 443.9700, val_MinusLogProbMetric: 443.9700

Epoch 14: val_loss did not improve from 421.42743
196/196 - 11s - loss: 420.1189 - MinusLogProbMetric: 420.1189 - val_loss: 443.9700 - val_MinusLogProbMetric: 443.9700 - lr: 3.3333e-04 - 11s/epoch - 59ms/step
Epoch 15/1000
2023-10-03 15:16:46.877 
Epoch 15/1000 
	 loss: 420.2468, MinusLogProbMetric: 420.2468, val_loss: 422.1440, val_MinusLogProbMetric: 422.1440

Epoch 15: val_loss did not improve from 421.42743
196/196 - 12s - loss: 420.2468 - MinusLogProbMetric: 420.2468 - val_loss: 422.1440 - val_MinusLogProbMetric: 422.1440 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 16/1000
2023-10-03 15:16:58.248 
Epoch 16/1000 
	 loss: 418.6137, MinusLogProbMetric: 418.6137, val_loss: 419.7979, val_MinusLogProbMetric: 419.7979

Epoch 16: val_loss improved from 421.42743 to 419.79785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 418.6137 - MinusLogProbMetric: 418.6137 - val_loss: 419.7979 - val_MinusLogProbMetric: 419.7979 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 17/1000
2023-10-03 15:17:09.717 
Epoch 17/1000 
	 loss: 419.8163, MinusLogProbMetric: 419.8163, val_loss: 417.3541, val_MinusLogProbMetric: 417.3541

Epoch 17: val_loss improved from 419.79785 to 417.35413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 419.8163 - MinusLogProbMetric: 419.8163 - val_loss: 417.3541 - val_MinusLogProbMetric: 417.3541 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 18/1000
2023-10-03 15:17:21.364 
Epoch 18/1000 
	 loss: 416.8079, MinusLogProbMetric: 416.8079, val_loss: 420.2553, val_MinusLogProbMetric: 420.2553

Epoch 18: val_loss did not improve from 417.35413
196/196 - 11s - loss: 416.8079 - MinusLogProbMetric: 416.8079 - val_loss: 420.2553 - val_MinusLogProbMetric: 420.2553 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 19/1000
2023-10-03 15:17:32.641 
Epoch 19/1000 
	 loss: 417.3688, MinusLogProbMetric: 417.3688, val_loss: 415.1215, val_MinusLogProbMetric: 415.1215

Epoch 19: val_loss improved from 417.35413 to 415.12152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 417.3688 - MinusLogProbMetric: 417.3688 - val_loss: 415.1215 - val_MinusLogProbMetric: 415.1215 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 20/1000
2023-10-03 15:17:44.145 
Epoch 20/1000 
	 loss: 416.1566, MinusLogProbMetric: 416.1566, val_loss: 414.2841, val_MinusLogProbMetric: 414.2841

Epoch 20: val_loss improved from 415.12152 to 414.28412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 416.1566 - MinusLogProbMetric: 416.1566 - val_loss: 414.2841 - val_MinusLogProbMetric: 414.2841 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 21/1000
2023-10-03 15:17:55.546 
Epoch 21/1000 
	 loss: 416.0831, MinusLogProbMetric: 416.0831, val_loss: 418.0630, val_MinusLogProbMetric: 418.0630

Epoch 21: val_loss did not improve from 414.28412
196/196 - 11s - loss: 416.0831 - MinusLogProbMetric: 416.0831 - val_loss: 418.0630 - val_MinusLogProbMetric: 418.0630 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 22/1000
2023-10-03 15:18:06.611 
Epoch 22/1000 
	 loss: 415.6951, MinusLogProbMetric: 415.6951, val_loss: 431.4943, val_MinusLogProbMetric: 431.4943

Epoch 22: val_loss did not improve from 414.28412
196/196 - 11s - loss: 415.6951 - MinusLogProbMetric: 415.6951 - val_loss: 431.4943 - val_MinusLogProbMetric: 431.4943 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 23/1000
2023-10-03 15:18:17.526 
Epoch 23/1000 
	 loss: 415.3026, MinusLogProbMetric: 415.3026, val_loss: 416.1301, val_MinusLogProbMetric: 416.1301

Epoch 23: val_loss did not improve from 414.28412
196/196 - 11s - loss: 415.3026 - MinusLogProbMetric: 415.3026 - val_loss: 416.1301 - val_MinusLogProbMetric: 416.1301 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 24/1000
2023-10-03 15:18:28.675 
Epoch 24/1000 
	 loss: 413.6552, MinusLogProbMetric: 413.6552, val_loss: 420.9983, val_MinusLogProbMetric: 420.9983

Epoch 24: val_loss did not improve from 414.28412
196/196 - 11s - loss: 413.6552 - MinusLogProbMetric: 413.6552 - val_loss: 420.9983 - val_MinusLogProbMetric: 420.9983 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 25/1000
2023-10-03 15:18:39.581 
Epoch 25/1000 
	 loss: 415.3055, MinusLogProbMetric: 415.3055, val_loss: 468.0572, val_MinusLogProbMetric: 468.0572

Epoch 25: val_loss did not improve from 414.28412
196/196 - 11s - loss: 415.3055 - MinusLogProbMetric: 415.3055 - val_loss: 468.0572 - val_MinusLogProbMetric: 468.0572 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 26/1000
2023-10-03 15:18:50.768 
Epoch 26/1000 
	 loss: 415.2513, MinusLogProbMetric: 415.2513, val_loss: 412.2350, val_MinusLogProbMetric: 412.2350

Epoch 26: val_loss improved from 414.28412 to 412.23499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 415.2513 - MinusLogProbMetric: 415.2513 - val_loss: 412.2350 - val_MinusLogProbMetric: 412.2350 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 27/1000
2023-10-03 15:19:02.331 
Epoch 27/1000 
	 loss: 414.4122, MinusLogProbMetric: 414.4122, val_loss: 413.6560, val_MinusLogProbMetric: 413.6560

Epoch 27: val_loss did not improve from 412.23499
196/196 - 11s - loss: 414.4122 - MinusLogProbMetric: 414.4122 - val_loss: 413.6560 - val_MinusLogProbMetric: 413.6560 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 28/1000
2023-10-03 15:19:13.612 
Epoch 28/1000 
	 loss: 411.4112, MinusLogProbMetric: 411.4112, val_loss: 417.4210, val_MinusLogProbMetric: 417.4210

Epoch 28: val_loss did not improve from 412.23499
196/196 - 11s - loss: 411.4112 - MinusLogProbMetric: 411.4112 - val_loss: 417.4210 - val_MinusLogProbMetric: 417.4210 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 29/1000
2023-10-03 15:19:24.905 
Epoch 29/1000 
	 loss: 411.6331, MinusLogProbMetric: 411.6331, val_loss: 412.6289, val_MinusLogProbMetric: 412.6289

Epoch 29: val_loss did not improve from 412.23499
196/196 - 11s - loss: 411.6331 - MinusLogProbMetric: 411.6331 - val_loss: 412.6289 - val_MinusLogProbMetric: 412.6289 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 30/1000
2023-10-03 15:19:35.974 
Epoch 30/1000 
	 loss: 410.5001, MinusLogProbMetric: 410.5001, val_loss: 411.8862, val_MinusLogProbMetric: 411.8862

Epoch 30: val_loss improved from 412.23499 to 411.88623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 410.5001 - MinusLogProbMetric: 410.5001 - val_loss: 411.8862 - val_MinusLogProbMetric: 411.8862 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 31/1000
2023-10-03 15:19:47.367 
Epoch 31/1000 
	 loss: 411.2793, MinusLogProbMetric: 411.2793, val_loss: 414.1035, val_MinusLogProbMetric: 414.1035

Epoch 31: val_loss did not improve from 411.88623
196/196 - 11s - loss: 411.2793 - MinusLogProbMetric: 411.2793 - val_loss: 414.1035 - val_MinusLogProbMetric: 414.1035 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 32/1000
2023-10-03 15:19:58.716 
Epoch 32/1000 
	 loss: 410.1929, MinusLogProbMetric: 410.1929, val_loss: 414.1591, val_MinusLogProbMetric: 414.1591

Epoch 32: val_loss did not improve from 411.88623
196/196 - 11s - loss: 410.1929 - MinusLogProbMetric: 410.1929 - val_loss: 414.1591 - val_MinusLogProbMetric: 414.1591 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 33/1000
2023-10-03 15:20:09.697 
Epoch 33/1000 
	 loss: 409.5463, MinusLogProbMetric: 409.5463, val_loss: 409.5086, val_MinusLogProbMetric: 409.5086

Epoch 33: val_loss improved from 411.88623 to 409.50864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 409.5463 - MinusLogProbMetric: 409.5463 - val_loss: 409.5086 - val_MinusLogProbMetric: 409.5086 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 34/1000
2023-10-03 15:20:21.188 
Epoch 34/1000 
	 loss: 409.4356, MinusLogProbMetric: 409.4356, val_loss: 409.9781, val_MinusLogProbMetric: 409.9781

Epoch 34: val_loss did not improve from 409.50864
196/196 - 11s - loss: 409.4356 - MinusLogProbMetric: 409.4356 - val_loss: 409.9781 - val_MinusLogProbMetric: 409.9781 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 35/1000
2023-10-03 15:20:32.398 
Epoch 35/1000 
	 loss: 409.1028, MinusLogProbMetric: 409.1028, val_loss: 411.1104, val_MinusLogProbMetric: 411.1104

Epoch 35: val_loss did not improve from 409.50864
196/196 - 11s - loss: 409.1028 - MinusLogProbMetric: 409.1028 - val_loss: 411.1104 - val_MinusLogProbMetric: 411.1104 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 36/1000
2023-10-03 15:20:43.772 
Epoch 36/1000 
	 loss: 409.7030, MinusLogProbMetric: 409.7030, val_loss: 409.5953, val_MinusLogProbMetric: 409.5953

Epoch 36: val_loss did not improve from 409.50864
196/196 - 11s - loss: 409.7030 - MinusLogProbMetric: 409.7030 - val_loss: 409.5953 - val_MinusLogProbMetric: 409.5953 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 37/1000
2023-10-03 15:20:54.931 
Epoch 37/1000 
	 loss: 408.0468, MinusLogProbMetric: 408.0468, val_loss: 423.0872, val_MinusLogProbMetric: 423.0872

Epoch 37: val_loss did not improve from 409.50864
196/196 - 11s - loss: 408.0468 - MinusLogProbMetric: 408.0468 - val_loss: 423.0872 - val_MinusLogProbMetric: 423.0872 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 38/1000
2023-10-03 15:21:06.151 
Epoch 38/1000 
	 loss: 408.6438, MinusLogProbMetric: 408.6438, val_loss: 417.3965, val_MinusLogProbMetric: 417.3965

Epoch 38: val_loss did not improve from 409.50864
196/196 - 11s - loss: 408.6438 - MinusLogProbMetric: 408.6438 - val_loss: 417.3965 - val_MinusLogProbMetric: 417.3965 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 39/1000
2023-10-03 15:21:17.502 
Epoch 39/1000 
	 loss: 408.3965, MinusLogProbMetric: 408.3965, val_loss: 411.6685, val_MinusLogProbMetric: 411.6685

Epoch 39: val_loss did not improve from 409.50864
196/196 - 11s - loss: 408.3965 - MinusLogProbMetric: 408.3965 - val_loss: 411.6685 - val_MinusLogProbMetric: 411.6685 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 40/1000
2023-10-03 15:21:29.086 
Epoch 40/1000 
	 loss: 408.5380, MinusLogProbMetric: 408.5380, val_loss: 408.1982, val_MinusLogProbMetric: 408.1982

Epoch 40: val_loss improved from 409.50864 to 408.19821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 408.5380 - MinusLogProbMetric: 408.5380 - val_loss: 408.1982 - val_MinusLogProbMetric: 408.1982 - lr: 3.3333e-04 - 12s/epoch - 61ms/step
Epoch 41/1000
2023-10-03 15:21:41.137 
Epoch 41/1000 
	 loss: 407.3322, MinusLogProbMetric: 407.3322, val_loss: 409.8115, val_MinusLogProbMetric: 409.8115

Epoch 41: val_loss did not improve from 408.19821
196/196 - 12s - loss: 407.3322 - MinusLogProbMetric: 407.3322 - val_loss: 409.8115 - val_MinusLogProbMetric: 409.8115 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 42/1000
2023-10-03 15:21:52.510 
Epoch 42/1000 
	 loss: 408.8602, MinusLogProbMetric: 408.8602, val_loss: 411.3704, val_MinusLogProbMetric: 411.3704

Epoch 42: val_loss did not improve from 408.19821
196/196 - 11s - loss: 408.8602 - MinusLogProbMetric: 408.8602 - val_loss: 411.3704 - val_MinusLogProbMetric: 411.3704 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 43/1000
2023-10-03 15:22:03.645 
Epoch 43/1000 
	 loss: 408.3862, MinusLogProbMetric: 408.3862, val_loss: 423.6402, val_MinusLogProbMetric: 423.6402

Epoch 43: val_loss did not improve from 408.19821
196/196 - 11s - loss: 408.3862 - MinusLogProbMetric: 408.3862 - val_loss: 423.6402 - val_MinusLogProbMetric: 423.6402 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 44/1000
2023-10-03 15:22:15.202 
Epoch 44/1000 
	 loss: 407.5938, MinusLogProbMetric: 407.5938, val_loss: 419.8992, val_MinusLogProbMetric: 419.8992

Epoch 44: val_loss did not improve from 408.19821
196/196 - 12s - loss: 407.5938 - MinusLogProbMetric: 407.5938 - val_loss: 419.8992 - val_MinusLogProbMetric: 419.8992 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 45/1000
2023-10-03 15:22:26.696 
Epoch 45/1000 
	 loss: 407.8970, MinusLogProbMetric: 407.8970, val_loss: 407.8597, val_MinusLogProbMetric: 407.8597

Epoch 45: val_loss improved from 408.19821 to 407.85974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 407.8970 - MinusLogProbMetric: 407.8970 - val_loss: 407.8597 - val_MinusLogProbMetric: 407.8597 - lr: 3.3333e-04 - 12s/epoch - 61ms/step
Epoch 46/1000
2023-10-03 15:22:38.454 
Epoch 46/1000 
	 loss: 406.8670, MinusLogProbMetric: 406.8670, val_loss: 411.9756, val_MinusLogProbMetric: 411.9756

Epoch 46: val_loss did not improve from 407.85974
196/196 - 11s - loss: 406.8670 - MinusLogProbMetric: 406.8670 - val_loss: 411.9756 - val_MinusLogProbMetric: 411.9756 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 47/1000
2023-10-03 15:22:49.693 
Epoch 47/1000 
	 loss: 406.0945, MinusLogProbMetric: 406.0945, val_loss: 407.5797, val_MinusLogProbMetric: 407.5797

Epoch 47: val_loss improved from 407.85974 to 407.57974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 406.0945 - MinusLogProbMetric: 406.0945 - val_loss: 407.5797 - val_MinusLogProbMetric: 407.5797 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 48/1000
2023-10-03 15:23:01.355 
Epoch 48/1000 
	 loss: 406.2530, MinusLogProbMetric: 406.2530, val_loss: 408.4126, val_MinusLogProbMetric: 408.4126

Epoch 48: val_loss did not improve from 407.57974
196/196 - 11s - loss: 406.2530 - MinusLogProbMetric: 406.2530 - val_loss: 408.4126 - val_MinusLogProbMetric: 408.4126 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 49/1000
2023-10-03 15:23:12.606 
Epoch 49/1000 
	 loss: 406.2188, MinusLogProbMetric: 406.2188, val_loss: 408.3085, val_MinusLogProbMetric: 408.3085

Epoch 49: val_loss did not improve from 407.57974
196/196 - 11s - loss: 406.2188 - MinusLogProbMetric: 406.2188 - val_loss: 408.3085 - val_MinusLogProbMetric: 408.3085 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 50/1000
2023-10-03 15:23:24.010 
Epoch 50/1000 
	 loss: 405.8209, MinusLogProbMetric: 405.8209, val_loss: 409.8906, val_MinusLogProbMetric: 409.8906

Epoch 50: val_loss did not improve from 407.57974
196/196 - 11s - loss: 405.8209 - MinusLogProbMetric: 405.8209 - val_loss: 409.8906 - val_MinusLogProbMetric: 409.8906 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 51/1000
2023-10-03 15:23:35.284 
Epoch 51/1000 
	 loss: 405.6608, MinusLogProbMetric: 405.6608, val_loss: 408.9422, val_MinusLogProbMetric: 408.9422

Epoch 51: val_loss did not improve from 407.57974
196/196 - 11s - loss: 405.6608 - MinusLogProbMetric: 405.6608 - val_loss: 408.9422 - val_MinusLogProbMetric: 408.9422 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 52/1000
2023-10-03 15:23:46.846 
Epoch 52/1000 
	 loss: 408.0901, MinusLogProbMetric: 408.0901, val_loss: 440.7657, val_MinusLogProbMetric: 440.7657

Epoch 52: val_loss did not improve from 407.57974
196/196 - 12s - loss: 408.0901 - MinusLogProbMetric: 408.0901 - val_loss: 440.7657 - val_MinusLogProbMetric: 440.7657 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 53/1000
2023-10-03 15:23:58.850 
Epoch 53/1000 
	 loss: 406.2067, MinusLogProbMetric: 406.2067, val_loss: 406.5921, val_MinusLogProbMetric: 406.5921

Epoch 53: val_loss improved from 407.57974 to 406.59213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 13s - loss: 406.2067 - MinusLogProbMetric: 406.2067 - val_loss: 406.5921 - val_MinusLogProbMetric: 406.5921 - lr: 3.3333e-04 - 13s/epoch - 65ms/step
Epoch 54/1000
2023-10-03 15:24:11.556 
Epoch 54/1000 
	 loss: 405.2213, MinusLogProbMetric: 405.2213, val_loss: 411.2771, val_MinusLogProbMetric: 411.2771

Epoch 54: val_loss did not improve from 406.59213
196/196 - 12s - loss: 405.2213 - MinusLogProbMetric: 405.2213 - val_loss: 411.2771 - val_MinusLogProbMetric: 411.2771 - lr: 3.3333e-04 - 12s/epoch - 61ms/step
Epoch 55/1000
2023-10-03 15:24:23.347 
Epoch 55/1000 
	 loss: 404.8482, MinusLogProbMetric: 404.8482, val_loss: 407.9591, val_MinusLogProbMetric: 407.9591

Epoch 55: val_loss did not improve from 406.59213
196/196 - 12s - loss: 404.8482 - MinusLogProbMetric: 404.8482 - val_loss: 407.9591 - val_MinusLogProbMetric: 407.9591 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 56/1000
2023-10-03 15:24:34.646 
Epoch 56/1000 
	 loss: 405.1545, MinusLogProbMetric: 405.1545, val_loss: 416.0555, val_MinusLogProbMetric: 416.0555

Epoch 56: val_loss did not improve from 406.59213
196/196 - 11s - loss: 405.1545 - MinusLogProbMetric: 405.1545 - val_loss: 416.0555 - val_MinusLogProbMetric: 416.0555 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 57/1000
2023-10-03 15:24:45.923 
Epoch 57/1000 
	 loss: 405.5428, MinusLogProbMetric: 405.5428, val_loss: 405.7984, val_MinusLogProbMetric: 405.7984

Epoch 57: val_loss improved from 406.59213 to 405.79843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 405.5428 - MinusLogProbMetric: 405.5428 - val_loss: 405.7984 - val_MinusLogProbMetric: 405.7984 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 58/1000
2023-10-03 15:24:57.813 
Epoch 58/1000 
	 loss: 404.7123, MinusLogProbMetric: 404.7123, val_loss: 409.4967, val_MinusLogProbMetric: 409.4967

Epoch 58: val_loss did not improve from 405.79843
196/196 - 11s - loss: 404.7123 - MinusLogProbMetric: 404.7123 - val_loss: 409.4967 - val_MinusLogProbMetric: 409.4967 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 59/1000
2023-10-03 15:25:09.192 
Epoch 59/1000 
	 loss: 404.2899, MinusLogProbMetric: 404.2899, val_loss: 416.8485, val_MinusLogProbMetric: 416.8485

Epoch 59: val_loss did not improve from 405.79843
196/196 - 11s - loss: 404.2899 - MinusLogProbMetric: 404.2899 - val_loss: 416.8485 - val_MinusLogProbMetric: 416.8485 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 60/1000
2023-10-03 15:25:20.424 
Epoch 60/1000 
	 loss: 404.8510, MinusLogProbMetric: 404.8510, val_loss: 404.0628, val_MinusLogProbMetric: 404.0628

Epoch 60: val_loss improved from 405.79843 to 404.06277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 404.8510 - MinusLogProbMetric: 404.8510 - val_loss: 404.0628 - val_MinusLogProbMetric: 404.0628 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 61/1000
2023-10-03 15:25:32.284 
Epoch 61/1000 
	 loss: 404.5477, MinusLogProbMetric: 404.5477, val_loss: 406.1298, val_MinusLogProbMetric: 406.1298

Epoch 61: val_loss did not improve from 404.06277
196/196 - 11s - loss: 404.5477 - MinusLogProbMetric: 404.5477 - val_loss: 406.1298 - val_MinusLogProbMetric: 406.1298 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 62/1000
2023-10-03 15:25:43.472 
Epoch 62/1000 
	 loss: 404.1220, MinusLogProbMetric: 404.1220, val_loss: 405.1699, val_MinusLogProbMetric: 405.1699

Epoch 62: val_loss did not improve from 404.06277
196/196 - 11s - loss: 404.1220 - MinusLogProbMetric: 404.1220 - val_loss: 405.1699 - val_MinusLogProbMetric: 405.1699 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 63/1000
2023-10-03 15:25:54.380 
Epoch 63/1000 
	 loss: 403.8557, MinusLogProbMetric: 403.8557, val_loss: 407.2266, val_MinusLogProbMetric: 407.2266

Epoch 63: val_loss did not improve from 404.06277
196/196 - 11s - loss: 403.8557 - MinusLogProbMetric: 403.8557 - val_loss: 407.2266 - val_MinusLogProbMetric: 407.2266 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 64/1000
2023-10-03 15:26:05.598 
Epoch 64/1000 
	 loss: 404.5358, MinusLogProbMetric: 404.5358, val_loss: 407.0206, val_MinusLogProbMetric: 407.0206

Epoch 64: val_loss did not improve from 404.06277
196/196 - 11s - loss: 404.5358 - MinusLogProbMetric: 404.5358 - val_loss: 407.0206 - val_MinusLogProbMetric: 407.0206 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 65/1000
2023-10-03 15:26:16.939 
Epoch 65/1000 
	 loss: 403.4664, MinusLogProbMetric: 403.4664, val_loss: 408.9229, val_MinusLogProbMetric: 408.9229

Epoch 65: val_loss did not improve from 404.06277
196/196 - 11s - loss: 403.4664 - MinusLogProbMetric: 403.4664 - val_loss: 408.9229 - val_MinusLogProbMetric: 408.9229 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 66/1000
2023-10-03 15:26:28.334 
Epoch 66/1000 
	 loss: 403.5160, MinusLogProbMetric: 403.5160, val_loss: 407.3587, val_MinusLogProbMetric: 407.3587

Epoch 66: val_loss did not improve from 404.06277
196/196 - 11s - loss: 403.5160 - MinusLogProbMetric: 403.5160 - val_loss: 407.3587 - val_MinusLogProbMetric: 407.3587 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 67/1000
2023-10-03 15:26:39.741 
Epoch 67/1000 
	 loss: 403.3031, MinusLogProbMetric: 403.3031, val_loss: 407.8646, val_MinusLogProbMetric: 407.8646

Epoch 67: val_loss did not improve from 404.06277
196/196 - 11s - loss: 403.3031 - MinusLogProbMetric: 403.3031 - val_loss: 407.8646 - val_MinusLogProbMetric: 407.8646 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 68/1000
2023-10-03 15:26:51.322 
Epoch 68/1000 
	 loss: 403.1684, MinusLogProbMetric: 403.1684, val_loss: 404.9291, val_MinusLogProbMetric: 404.9291

Epoch 68: val_loss did not improve from 404.06277
196/196 - 12s - loss: 403.1684 - MinusLogProbMetric: 403.1684 - val_loss: 404.9291 - val_MinusLogProbMetric: 404.9291 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 69/1000
2023-10-03 15:27:03.079 
Epoch 69/1000 
	 loss: 403.3796, MinusLogProbMetric: 403.3796, val_loss: 404.5646, val_MinusLogProbMetric: 404.5646

Epoch 69: val_loss did not improve from 404.06277
196/196 - 12s - loss: 403.3796 - MinusLogProbMetric: 403.3796 - val_loss: 404.5646 - val_MinusLogProbMetric: 404.5646 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 70/1000
2023-10-03 15:27:14.578 
Epoch 70/1000 
	 loss: 403.3022, MinusLogProbMetric: 403.3022, val_loss: 404.2745, val_MinusLogProbMetric: 404.2745

Epoch 70: val_loss did not improve from 404.06277
196/196 - 11s - loss: 403.3022 - MinusLogProbMetric: 403.3022 - val_loss: 404.2745 - val_MinusLogProbMetric: 404.2745 - lr: 3.3333e-04 - 11s/epoch - 59ms/step
Epoch 71/1000
2023-10-03 15:27:25.443 
Epoch 71/1000 
	 loss: 403.4877, MinusLogProbMetric: 403.4877, val_loss: 405.2869, val_MinusLogProbMetric: 405.2869

Epoch 71: val_loss did not improve from 404.06277
196/196 - 11s - loss: 403.4877 - MinusLogProbMetric: 403.4877 - val_loss: 405.2869 - val_MinusLogProbMetric: 405.2869 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 72/1000
2023-10-03 15:27:36.752 
Epoch 72/1000 
	 loss: 402.5392, MinusLogProbMetric: 402.5392, val_loss: 405.9112, val_MinusLogProbMetric: 405.9112

Epoch 72: val_loss did not improve from 404.06277
196/196 - 11s - loss: 402.5392 - MinusLogProbMetric: 402.5392 - val_loss: 405.9112 - val_MinusLogProbMetric: 405.9112 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 73/1000
2023-10-03 15:27:47.142 
Epoch 73/1000 
	 loss: 403.8897, MinusLogProbMetric: 403.8897, val_loss: 405.4115, val_MinusLogProbMetric: 405.4115

Epoch 73: val_loss did not improve from 404.06277
196/196 - 10s - loss: 403.8897 - MinusLogProbMetric: 403.8897 - val_loss: 405.4115 - val_MinusLogProbMetric: 405.4115 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 74/1000
2023-10-03 15:27:58.180 
Epoch 74/1000 
	 loss: 402.7401, MinusLogProbMetric: 402.7401, val_loss: 404.8632, val_MinusLogProbMetric: 404.8632

Epoch 74: val_loss did not improve from 404.06277
196/196 - 11s - loss: 402.7401 - MinusLogProbMetric: 402.7401 - val_loss: 404.8632 - val_MinusLogProbMetric: 404.8632 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 75/1000
2023-10-03 15:28:09.325 
Epoch 75/1000 
	 loss: 403.1453, MinusLogProbMetric: 403.1453, val_loss: 404.4086, val_MinusLogProbMetric: 404.4086

Epoch 75: val_loss did not improve from 404.06277
196/196 - 11s - loss: 403.1453 - MinusLogProbMetric: 403.1453 - val_loss: 404.4086 - val_MinusLogProbMetric: 404.4086 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 76/1000
2023-10-03 15:28:20.644 
Epoch 76/1000 
	 loss: 402.2096, MinusLogProbMetric: 402.2096, val_loss: 408.2135, val_MinusLogProbMetric: 408.2135

Epoch 76: val_loss did not improve from 404.06277
196/196 - 11s - loss: 402.2096 - MinusLogProbMetric: 402.2096 - val_loss: 408.2135 - val_MinusLogProbMetric: 408.2135 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 77/1000
2023-10-03 15:28:31.932 
Epoch 77/1000 
	 loss: 402.4870, MinusLogProbMetric: 402.4870, val_loss: 404.9119, val_MinusLogProbMetric: 404.9119

Epoch 77: val_loss did not improve from 404.06277
196/196 - 11s - loss: 402.4870 - MinusLogProbMetric: 402.4870 - val_loss: 404.9119 - val_MinusLogProbMetric: 404.9119 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 78/1000
2023-10-03 15:28:43.574 
Epoch 78/1000 
	 loss: 404.4875, MinusLogProbMetric: 404.4875, val_loss: 406.9145, val_MinusLogProbMetric: 406.9145

Epoch 78: val_loss did not improve from 404.06277
196/196 - 12s - loss: 404.4875 - MinusLogProbMetric: 404.4875 - val_loss: 406.9145 - val_MinusLogProbMetric: 406.9145 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 79/1000
2023-10-03 15:28:55.486 
Epoch 79/1000 
	 loss: 402.1630, MinusLogProbMetric: 402.1630, val_loss: 404.0544, val_MinusLogProbMetric: 404.0544

Epoch 79: val_loss improved from 404.06277 to 404.05441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 402.1630 - MinusLogProbMetric: 402.1630 - val_loss: 404.0544 - val_MinusLogProbMetric: 404.0544 - lr: 3.3333e-04 - 12s/epoch - 63ms/step
Epoch 80/1000
2023-10-03 15:29:07.414 
Epoch 80/1000 
	 loss: 402.1997, MinusLogProbMetric: 402.1997, val_loss: 405.5422, val_MinusLogProbMetric: 405.5422

Epoch 80: val_loss did not improve from 404.05441
196/196 - 11s - loss: 402.1997 - MinusLogProbMetric: 402.1997 - val_loss: 405.5422 - val_MinusLogProbMetric: 405.5422 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 81/1000
2023-10-03 15:29:18.832 
Epoch 81/1000 
	 loss: 401.7689, MinusLogProbMetric: 401.7689, val_loss: 404.6443, val_MinusLogProbMetric: 404.6443

Epoch 81: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.7689 - MinusLogProbMetric: 401.7689 - val_loss: 404.6443 - val_MinusLogProbMetric: 404.6443 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 82/1000
2023-10-03 15:29:30.240 
Epoch 82/1000 
	 loss: 402.0816, MinusLogProbMetric: 402.0816, val_loss: 406.2238, val_MinusLogProbMetric: 406.2238

Epoch 82: val_loss did not improve from 404.05441
196/196 - 11s - loss: 402.0816 - MinusLogProbMetric: 402.0816 - val_loss: 406.2238 - val_MinusLogProbMetric: 406.2238 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 83/1000
2023-10-03 15:29:41.621 
Epoch 83/1000 
	 loss: 401.5972, MinusLogProbMetric: 401.5972, val_loss: 409.6902, val_MinusLogProbMetric: 409.6902

Epoch 83: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.5972 - MinusLogProbMetric: 401.5972 - val_loss: 409.6902 - val_MinusLogProbMetric: 409.6902 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 84/1000
2023-10-03 15:29:52.945 
Epoch 84/1000 
	 loss: 401.6610, MinusLogProbMetric: 401.6610, val_loss: 406.3294, val_MinusLogProbMetric: 406.3294

Epoch 84: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.6610 - MinusLogProbMetric: 401.6610 - val_loss: 406.3294 - val_MinusLogProbMetric: 406.3294 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 85/1000
2023-10-03 15:30:04.101 
Epoch 85/1000 
	 loss: 401.2725, MinusLogProbMetric: 401.2725, val_loss: 404.6655, val_MinusLogProbMetric: 404.6655

Epoch 85: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.2725 - MinusLogProbMetric: 401.2725 - val_loss: 404.6655 - val_MinusLogProbMetric: 404.6655 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 86/1000
2023-10-03 15:30:15.350 
Epoch 86/1000 
	 loss: 401.9388, MinusLogProbMetric: 401.9388, val_loss: 407.1906, val_MinusLogProbMetric: 407.1906

Epoch 86: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.9388 - MinusLogProbMetric: 401.9388 - val_loss: 407.1906 - val_MinusLogProbMetric: 407.1906 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 87/1000
2023-10-03 15:30:26.659 
Epoch 87/1000 
	 loss: 401.6300, MinusLogProbMetric: 401.6300, val_loss: 409.5920, val_MinusLogProbMetric: 409.5920

Epoch 87: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.6300 - MinusLogProbMetric: 401.6300 - val_loss: 409.5920 - val_MinusLogProbMetric: 409.5920 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 88/1000
2023-10-03 15:30:37.857 
Epoch 88/1000 
	 loss: 401.5030, MinusLogProbMetric: 401.5030, val_loss: 404.5202, val_MinusLogProbMetric: 404.5202

Epoch 88: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.5030 - MinusLogProbMetric: 401.5030 - val_loss: 404.5202 - val_MinusLogProbMetric: 404.5202 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 89/1000
2023-10-03 15:30:49.415 
Epoch 89/1000 
	 loss: 401.1514, MinusLogProbMetric: 401.1514, val_loss: 409.7456, val_MinusLogProbMetric: 409.7456

Epoch 89: val_loss did not improve from 404.05441
196/196 - 12s - loss: 401.1514 - MinusLogProbMetric: 401.1514 - val_loss: 409.7456 - val_MinusLogProbMetric: 409.7456 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 90/1000
2023-10-03 15:31:00.940 
Epoch 90/1000 
	 loss: 402.0635, MinusLogProbMetric: 402.0635, val_loss: 406.4029, val_MinusLogProbMetric: 406.4029

Epoch 90: val_loss did not improve from 404.05441
196/196 - 12s - loss: 402.0635 - MinusLogProbMetric: 402.0635 - val_loss: 406.4029 - val_MinusLogProbMetric: 406.4029 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 91/1000
2023-10-03 15:31:12.544 
Epoch 91/1000 
	 loss: 401.0788, MinusLogProbMetric: 401.0788, val_loss: 406.6833, val_MinusLogProbMetric: 406.6833

Epoch 91: val_loss did not improve from 404.05441
196/196 - 12s - loss: 401.0788 - MinusLogProbMetric: 401.0788 - val_loss: 406.6833 - val_MinusLogProbMetric: 406.6833 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 92/1000
2023-10-03 15:31:23.822 
Epoch 92/1000 
	 loss: 401.4684, MinusLogProbMetric: 401.4684, val_loss: 404.8013, val_MinusLogProbMetric: 404.8013

Epoch 92: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.4684 - MinusLogProbMetric: 401.4684 - val_loss: 404.8013 - val_MinusLogProbMetric: 404.8013 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 93/1000
2023-10-03 15:31:35.178 
Epoch 93/1000 
	 loss: 400.6147, MinusLogProbMetric: 400.6147, val_loss: 404.1250, val_MinusLogProbMetric: 404.1250

Epoch 93: val_loss did not improve from 404.05441
196/196 - 11s - loss: 400.6147 - MinusLogProbMetric: 400.6147 - val_loss: 404.1250 - val_MinusLogProbMetric: 404.1250 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 94/1000
2023-10-03 15:31:46.525 
Epoch 94/1000 
	 loss: 400.7149, MinusLogProbMetric: 400.7149, val_loss: 405.7939, val_MinusLogProbMetric: 405.7939

Epoch 94: val_loss did not improve from 404.05441
196/196 - 11s - loss: 400.7149 - MinusLogProbMetric: 400.7149 - val_loss: 405.7939 - val_MinusLogProbMetric: 405.7939 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 95/1000
2023-10-03 15:31:57.798 
Epoch 95/1000 
	 loss: 400.4495, MinusLogProbMetric: 400.4495, val_loss: 406.3547, val_MinusLogProbMetric: 406.3547

Epoch 95: val_loss did not improve from 404.05441
196/196 - 11s - loss: 400.4495 - MinusLogProbMetric: 400.4495 - val_loss: 406.3547 - val_MinusLogProbMetric: 406.3547 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 96/1000
2023-10-03 15:32:09.158 
Epoch 96/1000 
	 loss: 401.4388, MinusLogProbMetric: 401.4388, val_loss: 407.3024, val_MinusLogProbMetric: 407.3024

Epoch 96: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.4388 - MinusLogProbMetric: 401.4388 - val_loss: 407.3024 - val_MinusLogProbMetric: 407.3024 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 97/1000
2023-10-03 15:32:20.518 
Epoch 97/1000 
	 loss: 401.8305, MinusLogProbMetric: 401.8305, val_loss: 405.9185, val_MinusLogProbMetric: 405.9185

Epoch 97: val_loss did not improve from 404.05441
196/196 - 11s - loss: 401.8305 - MinusLogProbMetric: 401.8305 - val_loss: 405.9185 - val_MinusLogProbMetric: 405.9185 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 98/1000
2023-10-03 15:32:31.809 
Epoch 98/1000 
	 loss: 400.0940, MinusLogProbMetric: 400.0940, val_loss: 406.5327, val_MinusLogProbMetric: 406.5327

Epoch 98: val_loss did not improve from 404.05441
196/196 - 11s - loss: 400.0940 - MinusLogProbMetric: 400.0940 - val_loss: 406.5327 - val_MinusLogProbMetric: 406.5327 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 99/1000
2023-10-03 15:32:42.738 
Epoch 99/1000 
	 loss: 399.7831, MinusLogProbMetric: 399.7831, val_loss: 407.3565, val_MinusLogProbMetric: 407.3565

Epoch 99: val_loss did not improve from 404.05441
196/196 - 11s - loss: 399.7831 - MinusLogProbMetric: 399.7831 - val_loss: 407.3565 - val_MinusLogProbMetric: 407.3565 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 100/1000
2023-10-03 15:32:53.962 
Epoch 100/1000 
	 loss: 400.7620, MinusLogProbMetric: 400.7620, val_loss: 405.8451, val_MinusLogProbMetric: 405.8451

Epoch 100: val_loss did not improve from 404.05441
196/196 - 11s - loss: 400.7620 - MinusLogProbMetric: 400.7620 - val_loss: 405.8451 - val_MinusLogProbMetric: 405.8451 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 101/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 57: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-03 15:32:58.456 
Epoch 101/1000 
	 loss: nan, MinusLogProbMetric: 4217303871802684666345603203072.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 101: val_loss did not improve from 404.05441
196/196 - 4s - loss: nan - MinusLogProbMetric: 4217303871802684666345603203072.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 4s/epoch - 23ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 352.
===========
Train data generated in 1.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_352
self.data_kwargs: {'seed': 869}
self.x_data: [[ 7.9973636   4.640891    5.32509    ...  3.500269    8.638443
   6.719802  ]
 [ 8.238864    4.485278    5.284702   ...  2.8864994   7.5674953
   7.1954236 ]
 [ 8.268872    4.7704377   5.167857   ...  4.5533733   8.026902
   7.8079686 ]
 ...
 [ 5.836932   -0.15277141  4.6562195  ...  4.799403    6.4058156
   6.0447054 ]
 [ 8.8304      4.5455837   5.226686   ...  3.9410732   7.6945333
   7.224351  ]
 [ 8.222233    4.7859597   5.1849046  ...  2.020081    8.037197
   7.44482   ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_123 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  9018400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7fb72c37d000>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb45a3c640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb45a3c640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb6ec284940>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb70c7df0d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb70c7deaa0>, <keras.callbacks.ModelCheckpoint object at 0x7fb70c7df700>, <keras.callbacks.EarlyStopping object at 0x7fb70c7de9b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb70c7de800>, <keras.callbacks.TerminateOnNaN object at 0x7fb70c7deb00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 8.539409 ,  4.7004476,  5.253654 , ...,  3.5209675,  7.8213096,
         7.264228 ],
       [ 4.841292 ,  7.7743006,  6.040214 , ..., 10.24335  ,  2.4278429,
         7.039813 ],
       [ 6.1221642,  8.600275 ,  5.6249857, ...,  9.526408 ,  3.8184025,
         6.5956736],
       ...,
       [ 8.154603 ,  4.544891 ,  5.270275 , ...,  3.1086564,  9.448143 ,
         6.9100637],
       [ 5.5309157,  6.5874166,  6.361618 , ...,  9.1334715,  0.7858764,
         7.091966 ],
       [ 7.667079 ,  4.542436 ,  5.327686 , ...,  2.9722602,  7.8458834,
         6.5847554]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 352/360 with hyperparameters:
timestamp = 2023-10-03 15:33:02.261998
ndims = 1000
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.99736357e+00  4.64089108e+00  5.32508993e+00  2.54230428e+00
  6.17678785e+00  3.28207493e+00  5.66227102e+00  1.74029863e+00
  1.76519728e+00  3.78703856e+00  4.21053505e+00  3.09312010e+00
  2.03467131e+00  8.71515274e-03  3.30997020e-01  3.03643203e+00
  3.58006954e+00  7.91204977e+00  4.60797453e+00  9.32520199e+00
 -1.47658885e-01  1.08268631e+00  3.49395204e+00  5.92749643e+00
  8.83363056e+00  3.30430484e+00  2.72358418e+00  5.84306300e-01
  7.95439053e+00  4.29114294e+00  5.70392561e+00  9.62326145e+00
  1.91625190e+00  2.05868888e+00  7.86738205e+00  8.31465626e+00
  7.71446276e+00  6.16568375e+00  9.86226177e+00  4.56724548e+00
  5.94829226e+00  5.95013499e-01  4.22035885e+00  7.23472834e+00
  2.47805715e+00  8.42218876e+00  6.81143332e+00  3.05234265e+00
  7.74489641e+00 -1.49918377e-01  9.73857021e+00  8.26392531e-01
  1.01337919e+01  3.15674591e+00  7.51659107e+00 -4.07333463e-01
  2.89285135e+00  6.51461697e+00  6.24277782e+00 -2.83749938e-01
  3.82940078e+00  4.11356688e+00  6.11557388e+00  6.73437071e+00
  8.87053776e+00  4.77861261e+00  3.64606881e+00  1.49473929e+00
  8.43961716e+00  7.42175150e+00  4.57817841e+00  5.29543495e+00
  6.56515741e+00  1.46717739e+00 -1.17198992e+00  3.13418698e+00
  6.17689991e+00  4.57003880e+00  7.09853601e+00  8.16721344e+00
  4.87046480e+00  2.11677957e+00  8.49123096e+00  6.10315180e+00
  7.85925865e+00  5.53120947e+00  9.60370159e+00  2.18746519e+00
  6.79681110e+00  4.55581379e+00  4.16971874e+00  1.73112345e+00
  3.38649940e+00  9.48220348e+00  3.10462046e+00  2.14097667e+00
  5.63125849e+00  3.38328648e+00  5.20010900e+00  8.49408150e+00
  3.82229614e+00  4.20812511e+00  5.16726160e+00  5.89465952e+00
  9.87067223e+00  6.80660152e+00  7.34994841e+00  9.82929134e+00
  3.98204517e+00  3.51628542e+00 -3.33558351e-01  2.19543719e+00
  1.56361794e+00  1.56820440e+00  8.16210842e+00  6.19128990e+00
  6.25914240e+00  8.37748623e+00  2.31200051e+00  3.18747854e+00
  7.11904478e+00  4.19043159e+00 -1.50076985e-01  3.92370987e+00
  9.12942028e+00  4.13348961e+00  2.73112822e+00  9.84571934e+00
  5.88873577e+00  4.89145327e+00  1.00473905e+00  7.55370808e+00
  6.88664293e+00  9.29471016e+00  1.59780979e+00  1.04221833e+00
  7.24209642e+00  2.19776750e+00  3.42702579e+00  3.34507298e+00
  8.18159485e+00  5.71737003e+00  4.56389010e-01  4.59833431e+00
  7.35877991e+00  5.35200214e+00  5.63458967e+00  2.36878228e+00
  8.52304554e+00  5.58948660e+00  7.47698450e+00  9.61558151e+00
  4.02933073e+00  8.54585361e+00 -2.04157352e-01  3.18135262e-01
  5.99070644e+00  3.07406282e+00  2.67563176e+00  2.55177832e+00
  3.00443268e+00  7.63680124e+00  6.20905495e+00  7.80322456e+00
  7.74732971e+00  8.43296623e+00  5.25363827e+00  3.99893999e+00
  1.01927109e+01  7.05619717e+00  7.41717219e-01  5.25023270e+00
  3.77008796e+00  9.90027332e+00 -9.78221714e-01  6.02291298e+00
  6.87749577e+00  3.01261449e+00  9.65150070e+00  9.36239624e+00
  8.82814312e+00  7.89327025e-02  5.79393578e+00  2.59769440e+00
  7.66548109e+00  6.07534552e+00  5.36576509e+00  1.34668684e+00
  4.34030962e+00  6.31810141e+00  9.62684059e+00  4.88660431e+00
  9.60818291e+00  9.36019993e+00  6.71263504e+00  5.22296143e+00
  4.42309809e+00  7.77644825e+00 -1.76710457e-01  1.15464127e+00
  1.00100291e+00  8.11944199e+00  3.27240157e+00  6.37353086e+00
  5.53368807e+00  4.70193911e+00  5.07406664e+00  7.97970533e+00
  9.54048157e+00  4.72122574e+00  6.13606310e+00  3.54982883e-01
  1.08992491e+01  5.27934456e+00  7.85659790e+00  7.82055616e-01
  4.37489986e+00  2.60340428e+00  2.48891091e+00  9.05652618e+00
  6.87852859e+00  4.07554960e+00  1.03486490e+00  9.66742897e+00
  9.06607342e+00  3.15981674e+00  3.57244402e-01  5.63626289e+00
  9.92044353e+00  8.58868217e+00  6.57505751e+00  3.29356098e+00
  3.24422455e+00  9.00940800e+00  5.09291124e+00  2.85960793e-01
  2.23807621e+00  2.78734875e+00  1.29828584e+00  3.93540382e+00
  8.03243542e+00  6.78995275e+00  5.24697185e-01  1.17165899e+00
  3.27114582e+00  3.83798814e+00  8.53496361e+00  8.92454338e+00
  4.21771574e+00  6.39960194e+00  9.93754196e+00  7.64902067e+00
  6.44687891e+00  7.72444129e-01  2.29160905e+00  8.52109241e+00
  1.77464163e+00  8.37704659e+00  7.40880537e+00  2.87169456e+00
  5.39322519e+00  6.86864281e+00  7.26533747e+00  4.94568443e+00
  1.19446182e+00  6.65035486e+00  3.40026999e+00  9.23955250e+00
  2.17107248e+00  2.47258735e+00  2.43059564e+00  6.30585194e+00
  8.65539742e+00  1.45929515e+00  3.26658463e+00  5.87293768e+00
  4.54141855e+00  4.57416773e+00  8.90260696e+00  8.79572392e+00
  4.92429781e+00  6.65130472e+00  8.80965042e+00  4.23099279e+00
  2.90440130e+00  6.38359404e+00  5.79758072e+00  8.99278259e+00
  5.63759208e-01  9.11637592e+00  5.69089317e+00  3.27839077e-01
  2.98394132e+00  7.03010273e+00  3.88054943e+00  9.76950550e+00
  5.22733927e+00  2.11281967e+00  8.35765839e+00  5.03978920e+00
  7.56217527e+00  8.23209476e+00  6.36810303e+00  4.07139969e+00
  1.73890018e+00  7.25630856e+00  9.09160614e+00  3.28961945e+00
  9.31139088e+00  5.12792778e+00  6.50060415e+00  8.50181961e+00
  4.91192722e+00  8.58134687e-01  1.00498667e+01  2.70127869e+00
  6.64366674e+00  4.16276121e+00  4.07800245e+00  2.51310420e+00
  6.86618900e+00  1.04104257e+00  8.65216160e+00  8.34197235e+00
  5.48617649e+00  3.98194122e+00  2.67905617e+00  5.19228172e+00
  7.82842445e+00  2.60275602e+00  4.39831114e+00  2.98556614e+00
  2.60362029e+00  1.57165313e+00  9.64633369e+00  6.88340855e+00
  7.78283501e+00  1.10886335e+00  4.86602402e+00  8.91554070e+00
 -1.24701262e-02  1.84407997e+00  6.24502945e+00  6.37515545e+00
  7.90489101e+00  9.60888958e+00  1.45247686e+00  2.45699430e+00
  4.28265285e+00  6.50822818e-02  1.12332261e+00  2.24159002e+00
  4.71833563e+00  2.72241974e+00  8.28100014e+00  3.28441453e+00
  9.11346054e+00  1.05609627e+01  3.85242462e+00  3.12269568e+00
  2.30633688e+00  7.71115065e+00  2.85296941e+00  3.28263164e+00
  4.81248856e+00  6.73265219e+00  1.02898769e+01  3.37639761e+00
  3.19120789e+00  2.37913132e+00  3.12198162e+00  9.28771555e-01
  5.18783569e+00  1.00527916e+01  1.98218155e+00  3.54625010e+00
  9.82042217e+00  7.37978649e+00  4.98093176e+00  7.74178505e+00
  4.38426554e-01  5.04215908e+00  1.37913942e+00  2.33643341e+00
  1.43258142e+00  3.60961819e+00  4.80302668e+00  9.63227654e+00
  2.33755469e+00  4.40754080e+00  8.69386292e+00  8.12515831e+00
  5.90748072e+00  2.80811763e+00  5.03586197e+00  1.81783056e+00
  1.78456891e+00  4.50792408e+00  7.38818550e+00  5.70210075e+00
  7.67465544e+00  7.92264366e+00 -1.91134661e-01  5.84350491e+00
  6.51570177e+00  9.74592590e+00 -2.64375985e-01  4.72814274e+00
  1.51264036e+00  5.40283203e-01  7.05697918e+00  7.51599932e+00
  7.27936220e+00 -2.28865266e-01  3.87274265e+00  2.85972893e-01
  2.46495175e+00  1.80521321e+00  2.14937496e+00  3.77834606e+00
  3.18217349e+00  1.02763710e+01  3.37995410e+00  8.93162251e+00
  1.96273518e+00  9.14139557e+00 -3.55325580e-01  4.60834980e+00
 -5.89213148e-03  8.24484348e+00  1.03387394e+01  5.88764489e-01
  9.74320602e+00  8.21028519e+00  3.25056553e+00  9.08056259e+00
  2.59105110e+00  3.95399547e+00  9.59685862e-01  7.63103676e+00
 -4.04038966e-01  5.37858295e+00  6.89733744e+00  9.44393730e+00
  1.41217971e+00  7.53022814e+00  7.95942068e+00  5.70683289e+00
  5.70691347e+00 -2.19492435e-01  5.11235189e+00  3.95949173e+00
  9.22287846e+00  2.71982121e+00  5.80306816e+00  4.00738478e+00
  9.05252075e+00  2.45202136e+00  5.51476240e-01  6.91309261e+00
  3.62627077e+00  5.96777725e+00  3.05229813e-01  9.78659534e+00
  9.17897224e+00  5.76029110e+00  9.53746438e-01  3.57887888e+00
  9.68739605e+00  6.38989639e+00  1.90702987e+00  4.66415691e+00
  9.59853172e+00  5.05139923e+00  1.05777577e-01  6.60696030e+00
  2.90498590e+00  9.84483898e-01  4.15751266e+00  6.08889675e+00
 -7.97456264e-01  4.97374535e+00  5.03539562e+00  8.87364101e+00
  4.56227398e+00  6.91264105e+00  6.87507677e+00  8.22698975e+00
  3.93433619e+00  6.01061106e+00  5.63419819e+00  3.52927715e-01
  8.35077000e+00  3.63118911e+00  6.62209034e+00  5.41198444e+00
  6.93401527e+00  5.77218914e+00  1.43877172e+00  6.73276329e+00
  7.93194485e+00  7.98630357e-01  6.14384651e+00  6.38910675e+00
  9.41168022e+00  2.80262136e+00  1.86243892e+00  3.21781826e+00
  2.72202253e+00  7.37366581e+00  7.33060360e+00  6.61256790e+00
  2.12070441e+00  8.59328461e+00  1.00850325e+01  5.48360109e+00
  7.31572151e+00  5.40123463e+00  2.76470375e+00  2.38635635e+00
  2.60676074e+00  1.89509916e+00  8.99138355e+00  8.75982761e+00
  4.44820881e+00  3.67402339e+00  9.41257381e+00  2.81948304e+00
  5.31368160e+00  2.30343890e+00  1.82023335e+00  6.24686146e+00
  8.02147388e+00  6.23021078e+00  7.50943947e+00  9.98944378e+00
  9.65496063e+00  3.44390488e+00  9.90036297e+00  1.90570307e+00
  6.37369108e+00  6.97235632e+00  1.02818668e+00  1.01110497e+01
  3.08032537e+00  6.21633768e+00  8.70998573e+00  5.46199799e+00
  1.22746360e+00  8.02329159e+00  4.09109497e+00  1.01594963e+01
  8.81569862e+00  8.09971428e+00  3.88408160e+00  7.92610073e+00
  9.26926994e+00  6.73566246e+00  5.76671696e+00 -6.34171844e-01
  8.21392536e+00  8.59858513e+00  3.17632437e+00  2.96401882e+00
  4.87884188e+00  1.46060300e+00  7.75016904e-01  5.95933533e+00
  2.85177946e+00  4.70948601e+00  4.13632631e+00  9.87929535e+00
  1.08665876e+01  9.14421260e-01  8.82590580e+00  6.48437381e-01
  5.33739424e+00  2.29306793e+00  4.13651562e+00  6.47616768e+00
  4.24598312e+00  8.10760880e+00  7.45779276e+00  9.63120937e+00
  4.89558411e+00  8.02083302e+00  3.24438405e+00  3.59759498e+00
  4.09306526e+00  4.22914410e+00  9.30973232e-01  5.26565123e+00
  1.01665020e-01  9.00792408e+00  1.68276280e-01  4.04053593e+00
  2.87474799e+00  2.48327708e+00  7.51841593e+00  6.77074766e+00
  1.45175838e+00  6.70365572e+00  8.29291821e+00  7.64892721e+00
  1.97199440e+00  6.84292603e+00  7.54025269e+00  8.23593044e+00
  5.50070477e+00  3.45322466e+00  9.49723530e+00  8.45789242e+00
  5.81870270e+00  3.73827100e+00  8.91483879e+00  5.10812187e+00
  8.88178647e-01  4.46295691e+00  8.31669044e+00  4.49498892e+00
  5.43265700e-01  6.11975908e+00  9.95184231e+00  9.05875778e+00
  4.80623627e+00  9.23525047e+00  7.71717167e+00  5.08452940e+00
  4.99330044e+00  7.49071932e+00  7.34964132e+00  1.30181521e-01
  8.31134415e+00  4.43235731e+00  6.71253824e+00  7.91646671e+00
  4.65235710e+00  1.59666359e+00  4.05373240e+00  7.26302922e-01
  6.89302731e+00  9.45408058e+00  9.24543953e+00  4.22942209e+00
  5.97023821e+00  2.74910712e+00  5.68657923e+00  2.15732288e+00
  6.35400105e+00  2.63340044e+00  9.43256760e+00  6.54860783e+00
  9.90323901e-01  1.34424239e-01  9.92843723e+00  3.24450016e+00
  1.74554217e+00  3.04431581e+00  8.90259075e+00  7.87959576e+00
  5.06868839e+00  4.79971409e+00  2.77283359e+00  5.49839926e+00
  4.38119030e+00  2.14065003e+00  6.93851089e+00  4.98988056e+00
  7.73501110e+00  4.64208126e+00  4.79577160e+00  7.85067606e+00
  4.75812960e+00  9.79898643e+00  7.09330177e+00  8.39217281e+00
  9.55065346e+00  3.69468021e+00  5.54081106e+00  6.72979546e+00
  6.70603752e+00  9.38065147e+00  2.90241790e+00  2.52509665e+00
  8.94938564e+00  8.49423122e+00  2.13432574e+00  5.68594837e+00
  6.90361071e+00  4.37006855e+00  5.16254711e+00  5.72680283e+00
  5.19291306e+00  7.52211380e+00 -2.97093898e-01  2.55037498e+00
  2.54706359e+00  6.59417248e+00  7.53437948e+00  7.30045140e-01
  1.39935553e+00  6.93793297e+00  6.43762541e+00  5.88978815e+00
  9.83007526e+00  6.00890970e+00  6.66946948e-01  6.86854553e+00
  8.03701878e+00  6.11670637e+00  5.35453987e+00  9.57577324e+00
  1.49704802e+00  4.92001712e-01  1.53974497e+00  2.13410378e+00
  5.92641163e+00  6.05311298e+00  2.26492047e+00  4.60379982e+00
  4.60677052e+00  3.77398515e+00  7.27591801e+00  4.27604628e+00
  7.54144859e+00  1.66404676e+00  5.16442060e+00  8.22671890e+00
  1.71123445e+00  4.62352562e+00  6.08017778e+00  2.88557911e+00
  7.73235178e+00  6.43578827e-01  9.65846920e+00  4.54546928e+00
  3.51417971e+00  1.19948006e+00  5.29146385e+00  6.01765347e+00
  2.32322073e+00  2.89909482e+00  9.31757736e+00  6.46213627e+00
  2.06418347e+00  9.80539680e-01  8.66675568e+00  3.23060274e+00
  2.10251164e+00  9.88873005e+00  3.76415300e+00  8.50292981e-01
  5.07699108e+00  3.88809896e+00  9.39960575e+00  5.59372187e-01
 -5.31555176e-01  8.84760439e-01  9.86167717e+00  4.76715946e+00
  5.13683701e+00  5.85030556e+00  6.22967052e+00  1.53158998e+00
  6.54711390e+00  3.91384292e+00  2.45552421e+00  5.99128914e+00
  3.54900932e+00  2.41717911e+00  8.30238152e+00  1.64661229e+00
  2.59730220e+00  3.85882235e+00  2.47771192e+00  2.10063434e+00
  9.37286758e+00  3.17066669e+00  2.51538455e-01  5.38400126e+00
  1.23733842e+00  3.78326035e+00  9.20017338e+00  2.47332525e+00
  5.67668962e+00  5.17319727e+00  2.06113005e+00  6.44534540e+00
  8.66131496e+00  5.21378100e-01  6.50092697e+00  6.78853512e+00
  8.54306507e+00  1.80599499e+00  2.05283189e+00  6.85634184e+00
  1.15905434e-01  6.99897051e+00  8.32928753e+00  4.55407476e+00
  4.38788176e+00  3.54872870e+00 -4.62173462e-01  1.36794913e+00
  9.69468594e+00  8.38702488e+00  1.90114570e+00  7.94150639e+00
  3.36321092e+00  2.99465132e+00  1.99559021e+00  9.02903366e+00
  9.85151100e+00  3.23628950e+00  7.55823898e+00  3.70709014e+00
  1.74990630e+00  3.48457527e+00  5.45959711e+00  1.58513451e+00
  8.12769318e+00  4.73363543e+00  2.32966566e+00  7.44186544e+00
  7.94482851e+00  3.54043078e+00  7.04913664e+00  4.02170229e+00
  1.01847572e+01  8.72599220e+00  3.30791759e+00 -1.08366966e-01
  6.66013432e+00  1.91774535e+00  1.22588658e+00  1.02105055e+01
  9.86835957e+00  2.95132232e+00  3.01797199e+00  1.64833105e+00
  4.49072456e+00  8.49544430e+00  6.82054377e+00  9.07019711e+00
  1.68629599e+00  7.90870810e+00  9.09079742e+00  2.92365527e+00
  6.90887928e+00  4.17319202e+00  3.43049693e+00  9.02845764e+00
  4.29767895e+00  1.09342468e+00  3.74337578e+00  4.81818581e+00
  7.20623732e+00  1.07804976e+01  8.88462162e+00  9.30848122e+00
  4.87059927e+00  7.08490753e+00  2.95342398e+00  1.67907417e+00
  3.60964656e+00  7.03290796e+00  3.57637072e+00  8.74868298e+00
  8.52540398e+00  2.05524230e+00  3.42940140e+00  8.83274364e+00
  1.58101547e+00  3.33796239e+00  3.42331791e+00  6.96280146e+00
  3.25254261e-01 -1.25156724e+00  5.89129639e+00  4.46135187e+00
  6.51815891e+00  2.47705388e+00  9.10450840e+00  8.03964901e+00
  6.83334887e-01  8.24278164e+00  9.73476696e+00  1.00054388e+01
  7.11009216e+00  1.24967217e+00  5.68010473e+00  2.01691651e+00
  9.84855938e+00  7.23796368e+00  6.43789339e+00  6.97472811e+00
  4.51600838e+00  2.94856024e+00  9.13858128e+00  3.29138970e+00
  3.09904784e-01  4.01227283e+00  9.32983780e+00  5.93039370e+00
  1.10625067e+01  1.13827813e+00  1.01018476e+01  5.29424810e+00
  5.45073414e+00  5.82157803e+00  1.17043698e+00  3.22519493e+00
  7.34947968e+00  6.69572353e-01  1.62995577e+00  1.85301498e-01
  8.83085346e+00  5.87161255e+00  1.81599593e+00  7.31665230e+00
  9.78809166e+00  9.34503746e+00  9.14690876e+00  3.59607875e-01
  5.99414158e+00  2.59369469e+00  5.78580284e+00  3.18324995e+00
  1.74265718e+00  4.42781639e+00  4.24171114e+00  9.89190197e+00
  1.12621486e-02  4.74760723e+00  8.55551147e+00  7.21490955e+00
  8.37855244e+00  5.55442858e+00  7.92241478e+00  5.13917971e+00
  3.15440607e+00  8.63028622e+00  5.67239189e+00  1.22252262e+00
  7.74803591e+00  9.87097359e+00  4.27992439e+00  3.46016455e+00
  5.19145608e-01  6.58222771e+00  1.11442089e+01 -5.05199075e-01
  4.88900900e+00  1.03596199e+00  3.49793643e-01  6.70542479e+00
  8.65525723e+00  3.48391366e+00  8.85622883e+00  1.32473469e+00
  1.43105400e+00  9.27033043e+00  5.13401794e+00  6.83637190e+00
  3.17043185e-01  1.26743388e+00  5.01762807e-01  9.27802563e+00
  1.43500125e+00  8.44762039e+00  3.54408193e+00  7.80078506e+00
  3.90129399e+00  3.20132899e+00  1.04541802e+00  2.72584295e+00
  4.51778460e+00  7.11837149e+00  4.60630083e+00  9.86540890e+00
  6.96063471e+00 -3.97378445e-01  3.62382388e+00  6.64995527e+00
  1.94566798e+00  7.08949041e+00  5.43198347e+00  5.80618441e-01
  7.76860094e+00  6.34063721e+00  7.98214495e-01  4.38892889e+00
  2.10588074e+00  8.85570145e+00  2.82261276e+00  9.23659515e+00
  5.26395798e+00  3.50026894e+00  8.63844299e+00  6.71980190e+00]
Epoch 1/1000
2023-10-03 15:33:39.363 
Epoch 1/1000 
	 loss: 408.3663, MinusLogProbMetric: 408.3663, val_loss: 400.0996, val_MinusLogProbMetric: 400.0996

Epoch 1: val_loss improved from inf to 400.09958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 37s - loss: 408.3663 - MinusLogProbMetric: 408.3663 - val_loss: 400.0996 - val_MinusLogProbMetric: 400.0996 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 2/1000
2023-10-03 15:33:51.174 
Epoch 2/1000 
	 loss: 395.3007, MinusLogProbMetric: 395.3007, val_loss: 400.0177, val_MinusLogProbMetric: 400.0177

Epoch 2: val_loss improved from 400.09958 to 400.01767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 395.3007 - MinusLogProbMetric: 395.3007 - val_loss: 400.0177 - val_MinusLogProbMetric: 400.0177 - lr: 1.1111e-04 - 12s/epoch - 60ms/step
Epoch 3/1000
2023-10-03 15:34:02.883 
Epoch 3/1000 
	 loss: 395.2367, MinusLogProbMetric: 395.2367, val_loss: 399.6523, val_MinusLogProbMetric: 399.6523

Epoch 3: val_loss improved from 400.01767 to 399.65234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 395.2367 - MinusLogProbMetric: 395.2367 - val_loss: 399.6523 - val_MinusLogProbMetric: 399.6523 - lr: 1.1111e-04 - 12s/epoch - 60ms/step
Epoch 4/1000
2023-10-03 15:34:14.636 
Epoch 4/1000 
	 loss: 395.4048, MinusLogProbMetric: 395.4048, val_loss: 398.9567, val_MinusLogProbMetric: 398.9567

Epoch 4: val_loss improved from 399.65234 to 398.95670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 395.4048 - MinusLogProbMetric: 395.4048 - val_loss: 398.9567 - val_MinusLogProbMetric: 398.9567 - lr: 1.1111e-04 - 12s/epoch - 61ms/step
Epoch 5/1000
2023-10-03 15:34:26.488 
Epoch 5/1000 
	 loss: 395.0005, MinusLogProbMetric: 395.0005, val_loss: 399.4475, val_MinusLogProbMetric: 399.4475

Epoch 5: val_loss did not improve from 398.95670
196/196 - 11s - loss: 395.0005 - MinusLogProbMetric: 395.0005 - val_loss: 399.4475 - val_MinusLogProbMetric: 399.4475 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 6/1000
2023-10-03 15:34:37.820 
Epoch 6/1000 
	 loss: 395.0485, MinusLogProbMetric: 395.0485, val_loss: 401.9698, val_MinusLogProbMetric: 401.9698

Epoch 6: val_loss did not improve from 398.95670
196/196 - 11s - loss: 395.0485 - MinusLogProbMetric: 395.0485 - val_loss: 401.9698 - val_MinusLogProbMetric: 401.9698 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 7/1000
2023-10-03 15:34:49.059 
Epoch 7/1000 
	 loss: 394.9922, MinusLogProbMetric: 394.9922, val_loss: 398.9450, val_MinusLogProbMetric: 398.9450

Epoch 7: val_loss improved from 398.95670 to 398.94501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 394.9922 - MinusLogProbMetric: 394.9922 - val_loss: 398.9450 - val_MinusLogProbMetric: 398.9450 - lr: 1.1111e-04 - 12s/epoch - 60ms/step
Epoch 8/1000
2023-10-03 15:35:00.554 
Epoch 8/1000 
	 loss: 394.8347, MinusLogProbMetric: 394.8347, val_loss: 398.7688, val_MinusLogProbMetric: 398.7688

Epoch 8: val_loss improved from 398.94501 to 398.76877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 394.8347 - MinusLogProbMetric: 394.8347 - val_loss: 398.7688 - val_MinusLogProbMetric: 398.7688 - lr: 1.1111e-04 - 12s/epoch - 61ms/step
Epoch 9/1000
2023-10-03 15:35:12.560 
Epoch 9/1000 
	 loss: 394.6238, MinusLogProbMetric: 394.6238, val_loss: 404.4121, val_MinusLogProbMetric: 404.4121

Epoch 9: val_loss did not improve from 398.76877
196/196 - 11s - loss: 394.6238 - MinusLogProbMetric: 394.6238 - val_loss: 404.4121 - val_MinusLogProbMetric: 404.4121 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 10/1000
2023-10-03 15:35:23.982 
Epoch 10/1000 
	 loss: 394.7566, MinusLogProbMetric: 394.7566, val_loss: 398.6014, val_MinusLogProbMetric: 398.6014

Epoch 10: val_loss improved from 398.76877 to 398.60144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 394.7566 - MinusLogProbMetric: 394.7566 - val_loss: 398.6014 - val_MinusLogProbMetric: 398.6014 - lr: 1.1111e-04 - 12s/epoch - 61ms/step
Epoch 11/1000
2023-10-03 15:35:35.600 
Epoch 11/1000 
	 loss: 394.5992, MinusLogProbMetric: 394.5992, val_loss: 398.2673, val_MinusLogProbMetric: 398.2673

Epoch 11: val_loss improved from 398.60144 to 398.26733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 394.5992 - MinusLogProbMetric: 394.5992 - val_loss: 398.2673 - val_MinusLogProbMetric: 398.2673 - lr: 1.1111e-04 - 12s/epoch - 61ms/step
Epoch 12/1000
2023-10-03 15:35:47.791 
Epoch 12/1000 
	 loss: 394.5339, MinusLogProbMetric: 394.5339, val_loss: 401.0671, val_MinusLogProbMetric: 401.0671

Epoch 12: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.5339 - MinusLogProbMetric: 394.5339 - val_loss: 401.0671 - val_MinusLogProbMetric: 401.0671 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 13/1000
2023-10-03 15:35:59.076 
Epoch 13/1000 
	 loss: 394.4173, MinusLogProbMetric: 394.4173, val_loss: 398.4672, val_MinusLogProbMetric: 398.4672

Epoch 13: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.4173 - MinusLogProbMetric: 394.4173 - val_loss: 398.4672 - val_MinusLogProbMetric: 398.4672 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 14/1000
2023-10-03 15:36:10.274 
Epoch 14/1000 
	 loss: 394.5107, MinusLogProbMetric: 394.5107, val_loss: 398.9897, val_MinusLogProbMetric: 398.9897

Epoch 14: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.5107 - MinusLogProbMetric: 394.5107 - val_loss: 398.9897 - val_MinusLogProbMetric: 398.9897 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 15/1000
2023-10-03 15:36:21.548 
Epoch 15/1000 
	 loss: 394.2198, MinusLogProbMetric: 394.2198, val_loss: 398.6083, val_MinusLogProbMetric: 398.6083

Epoch 15: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.2198 - MinusLogProbMetric: 394.2198 - val_loss: 398.6083 - val_MinusLogProbMetric: 398.6083 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 16/1000
2023-10-03 15:36:32.576 
Epoch 16/1000 
	 loss: 394.1191, MinusLogProbMetric: 394.1191, val_loss: 401.4772, val_MinusLogProbMetric: 401.4772

Epoch 16: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.1191 - MinusLogProbMetric: 394.1191 - val_loss: 401.4772 - val_MinusLogProbMetric: 401.4772 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 17/1000
2023-10-03 15:36:43.833 
Epoch 17/1000 
	 loss: 394.1953, MinusLogProbMetric: 394.1953, val_loss: 402.0998, val_MinusLogProbMetric: 402.0998

Epoch 17: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.1953 - MinusLogProbMetric: 394.1953 - val_loss: 402.0998 - val_MinusLogProbMetric: 402.0998 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 18/1000
2023-10-03 15:36:55.016 
Epoch 18/1000 
	 loss: 394.0013, MinusLogProbMetric: 394.0013, val_loss: 398.7526, val_MinusLogProbMetric: 398.7526

Epoch 18: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.0013 - MinusLogProbMetric: 394.0013 - val_loss: 398.7526 - val_MinusLogProbMetric: 398.7526 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 19/1000
2023-10-03 15:37:06.091 
Epoch 19/1000 
	 loss: 394.2361, MinusLogProbMetric: 394.2361, val_loss: 398.4962, val_MinusLogProbMetric: 398.4962

Epoch 19: val_loss did not improve from 398.26733
196/196 - 11s - loss: 394.2361 - MinusLogProbMetric: 394.2361 - val_loss: 398.4962 - val_MinusLogProbMetric: 398.4962 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 20/1000
2023-10-03 15:37:17.576 
Epoch 20/1000 
	 loss: 393.9587, MinusLogProbMetric: 393.9587, val_loss: 398.0768, val_MinusLogProbMetric: 398.0768

Epoch 20: val_loss improved from 398.26733 to 398.07675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 393.9587 - MinusLogProbMetric: 393.9587 - val_loss: 398.0768 - val_MinusLogProbMetric: 398.0768 - lr: 1.1111e-04 - 12s/epoch - 62ms/step
Epoch 21/1000
2023-10-03 15:37:29.823 
Epoch 21/1000 
	 loss: 394.0472, MinusLogProbMetric: 394.0472, val_loss: 400.2454, val_MinusLogProbMetric: 400.2454

Epoch 21: val_loss did not improve from 398.07675
196/196 - 12s - loss: 394.0472 - MinusLogProbMetric: 394.0472 - val_loss: 400.2454 - val_MinusLogProbMetric: 400.2454 - lr: 1.1111e-04 - 12s/epoch - 59ms/step
Epoch 22/1000
2023-10-03 15:37:40.530 
Epoch 22/1000 
	 loss: 393.7153, MinusLogProbMetric: 393.7153, val_loss: 398.1661, val_MinusLogProbMetric: 398.1661

Epoch 22: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.7153 - MinusLogProbMetric: 393.7153 - val_loss: 398.1661 - val_MinusLogProbMetric: 398.1661 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 23/1000
2023-10-03 15:37:51.743 
Epoch 23/1000 
	 loss: 393.9612, MinusLogProbMetric: 393.9612, val_loss: 398.2007, val_MinusLogProbMetric: 398.2007

Epoch 23: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.9612 - MinusLogProbMetric: 393.9612 - val_loss: 398.2007 - val_MinusLogProbMetric: 398.2007 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 24/1000
2023-10-03 15:38:02.977 
Epoch 24/1000 
	 loss: 393.7744, MinusLogProbMetric: 393.7744, val_loss: 398.1603, val_MinusLogProbMetric: 398.1603

Epoch 24: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.7744 - MinusLogProbMetric: 393.7744 - val_loss: 398.1603 - val_MinusLogProbMetric: 398.1603 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 25/1000
2023-10-03 15:38:14.219 
Epoch 25/1000 
	 loss: 393.7416, MinusLogProbMetric: 393.7416, val_loss: 401.4239, val_MinusLogProbMetric: 401.4239

Epoch 25: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.7416 - MinusLogProbMetric: 393.7416 - val_loss: 401.4239 - val_MinusLogProbMetric: 401.4239 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 26/1000
2023-10-03 15:38:25.371 
Epoch 26/1000 
	 loss: 393.4948, MinusLogProbMetric: 393.4948, val_loss: 398.9345, val_MinusLogProbMetric: 398.9345

Epoch 26: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.4948 - MinusLogProbMetric: 393.4948 - val_loss: 398.9345 - val_MinusLogProbMetric: 398.9345 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 27/1000
2023-10-03 15:38:36.561 
Epoch 27/1000 
	 loss: 393.4516, MinusLogProbMetric: 393.4516, val_loss: 403.3563, val_MinusLogProbMetric: 403.3563

Epoch 27: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.4516 - MinusLogProbMetric: 393.4516 - val_loss: 403.3563 - val_MinusLogProbMetric: 403.3563 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 28/1000
2023-10-03 15:38:47.906 
Epoch 28/1000 
	 loss: 393.5877, MinusLogProbMetric: 393.5877, val_loss: 399.7047, val_MinusLogProbMetric: 399.7047

Epoch 28: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.5877 - MinusLogProbMetric: 393.5877 - val_loss: 399.7047 - val_MinusLogProbMetric: 399.7047 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 29/1000
2023-10-03 15:38:59.070 
Epoch 29/1000 
	 loss: 393.4667, MinusLogProbMetric: 393.4667, val_loss: 399.3988, val_MinusLogProbMetric: 399.3988

Epoch 29: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.4667 - MinusLogProbMetric: 393.4667 - val_loss: 399.3988 - val_MinusLogProbMetric: 399.3988 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 30/1000
2023-10-03 15:39:10.232 
Epoch 30/1000 
	 loss: 393.5583, MinusLogProbMetric: 393.5583, val_loss: 399.2040, val_MinusLogProbMetric: 399.2040

Epoch 30: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.5583 - MinusLogProbMetric: 393.5583 - val_loss: 399.2040 - val_MinusLogProbMetric: 399.2040 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 31/1000
2023-10-03 15:39:21.813 
Epoch 31/1000 
	 loss: 393.3239, MinusLogProbMetric: 393.3239, val_loss: 399.8169, val_MinusLogProbMetric: 399.8169

Epoch 31: val_loss did not improve from 398.07675
196/196 - 12s - loss: 393.3239 - MinusLogProbMetric: 393.3239 - val_loss: 399.8169 - val_MinusLogProbMetric: 399.8169 - lr: 1.1111e-04 - 12s/epoch - 59ms/step
Epoch 32/1000
2023-10-03 15:39:33.335 
Epoch 32/1000 
	 loss: 393.3768, MinusLogProbMetric: 393.3768, val_loss: 399.6204, val_MinusLogProbMetric: 399.6204

Epoch 32: val_loss did not improve from 398.07675
196/196 - 12s - loss: 393.3768 - MinusLogProbMetric: 393.3768 - val_loss: 399.6204 - val_MinusLogProbMetric: 399.6204 - lr: 1.1111e-04 - 12s/epoch - 59ms/step
Epoch 33/1000
2023-10-03 15:39:44.717 
Epoch 33/1000 
	 loss: 393.2721, MinusLogProbMetric: 393.2721, val_loss: 398.3147, val_MinusLogProbMetric: 398.3147

Epoch 33: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.2721 - MinusLogProbMetric: 393.2721 - val_loss: 398.3147 - val_MinusLogProbMetric: 398.3147 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 34/1000
2023-10-03 15:39:55.948 
Epoch 34/1000 
	 loss: 393.4217, MinusLogProbMetric: 393.4217, val_loss: 398.8899, val_MinusLogProbMetric: 398.8899

Epoch 34: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.4217 - MinusLogProbMetric: 393.4217 - val_loss: 398.8899 - val_MinusLogProbMetric: 398.8899 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 35/1000
2023-10-03 15:40:06.823 
Epoch 35/1000 
	 loss: 393.1970, MinusLogProbMetric: 393.1970, val_loss: 400.3801, val_MinusLogProbMetric: 400.3801

Epoch 35: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.1970 - MinusLogProbMetric: 393.1970 - val_loss: 400.3801 - val_MinusLogProbMetric: 400.3801 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 36/1000
2023-10-03 15:40:17.640 
Epoch 36/1000 
	 loss: 393.2617, MinusLogProbMetric: 393.2617, val_loss: 399.4682, val_MinusLogProbMetric: 399.4682

Epoch 36: val_loss did not improve from 398.07675
196/196 - 11s - loss: 393.2617 - MinusLogProbMetric: 393.2617 - val_loss: 399.4682 - val_MinusLogProbMetric: 399.4682 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 37/1000
2023-10-03 15:40:27.706 
Epoch 37/1000 
	 loss: 392.8745, MinusLogProbMetric: 392.8745, val_loss: 399.2100, val_MinusLogProbMetric: 399.2100

Epoch 37: val_loss did not improve from 398.07675
196/196 - 10s - loss: 392.8745 - MinusLogProbMetric: 392.8745 - val_loss: 399.2100 - val_MinusLogProbMetric: 399.2100 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 38/1000
2023-10-03 15:40:38.196 
Epoch 38/1000 
	 loss: 393.4009, MinusLogProbMetric: 393.4009, val_loss: 398.5015, val_MinusLogProbMetric: 398.5015

Epoch 38: val_loss did not improve from 398.07675
196/196 - 10s - loss: 393.4009 - MinusLogProbMetric: 393.4009 - val_loss: 398.5015 - val_MinusLogProbMetric: 398.5015 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 39/1000
2023-10-03 15:40:48.509 
Epoch 39/1000 
	 loss: 392.8669, MinusLogProbMetric: 392.8669, val_loss: 398.7858, val_MinusLogProbMetric: 398.7858

Epoch 39: val_loss did not improve from 398.07675
196/196 - 10s - loss: 392.8669 - MinusLogProbMetric: 392.8669 - val_loss: 398.7858 - val_MinusLogProbMetric: 398.7858 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 40/1000
2023-10-03 15:40:58.875 
Epoch 40/1000 
	 loss: 393.5028, MinusLogProbMetric: 393.5028, val_loss: 399.1686, val_MinusLogProbMetric: 399.1686

Epoch 40: val_loss did not improve from 398.07675
196/196 - 10s - loss: 393.5028 - MinusLogProbMetric: 393.5028 - val_loss: 399.1686 - val_MinusLogProbMetric: 399.1686 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 41/1000
2023-10-03 15:41:09.349 
Epoch 41/1000 
	 loss: 392.8058, MinusLogProbMetric: 392.8058, val_loss: 397.8655, val_MinusLogProbMetric: 397.8655

Epoch 41: val_loss improved from 398.07675 to 397.86551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 392.8058 - MinusLogProbMetric: 392.8058 - val_loss: 397.8655 - val_MinusLogProbMetric: 397.8655 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 42/1000
2023-10-03 15:41:20.037 
Epoch 42/1000 
	 loss: 393.0076, MinusLogProbMetric: 393.0076, val_loss: 399.6938, val_MinusLogProbMetric: 399.6938

Epoch 42: val_loss did not improve from 397.86551
196/196 - 10s - loss: 393.0076 - MinusLogProbMetric: 393.0076 - val_loss: 399.6938 - val_MinusLogProbMetric: 399.6938 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 43/1000
2023-10-03 15:41:30.254 
Epoch 43/1000 
	 loss: 393.4160, MinusLogProbMetric: 393.4160, val_loss: 398.3187, val_MinusLogProbMetric: 398.3187

Epoch 43: val_loss did not improve from 397.86551
196/196 - 10s - loss: 393.4160 - MinusLogProbMetric: 393.4160 - val_loss: 398.3187 - val_MinusLogProbMetric: 398.3187 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 44/1000
2023-10-03 15:41:40.495 
Epoch 44/1000 
	 loss: 392.7794, MinusLogProbMetric: 392.7794, val_loss: 401.1408, val_MinusLogProbMetric: 401.1408

Epoch 44: val_loss did not improve from 397.86551
196/196 - 10s - loss: 392.7794 - MinusLogProbMetric: 392.7794 - val_loss: 401.1408 - val_MinusLogProbMetric: 401.1408 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 45/1000
2023-10-03 15:41:50.815 
Epoch 45/1000 
	 loss: 392.7455, MinusLogProbMetric: 392.7455, val_loss: 398.2228, val_MinusLogProbMetric: 398.2228

Epoch 45: val_loss did not improve from 397.86551
196/196 - 10s - loss: 392.7455 - MinusLogProbMetric: 392.7455 - val_loss: 398.2228 - val_MinusLogProbMetric: 398.2228 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 46/1000
2023-10-03 15:42:01.277 
Epoch 46/1000 
	 loss: 392.6818, MinusLogProbMetric: 392.6818, val_loss: 398.2142, val_MinusLogProbMetric: 398.2142

Epoch 46: val_loss did not improve from 397.86551
196/196 - 10s - loss: 392.6818 - MinusLogProbMetric: 392.6818 - val_loss: 398.2142 - val_MinusLogProbMetric: 398.2142 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 47/1000
2023-10-03 15:42:11.626 
Epoch 47/1000 
	 loss: 392.8055, MinusLogProbMetric: 392.8055, val_loss: 400.0452, val_MinusLogProbMetric: 400.0452

Epoch 47: val_loss did not improve from 397.86551
196/196 - 10s - loss: 392.8055 - MinusLogProbMetric: 392.8055 - val_loss: 400.0452 - val_MinusLogProbMetric: 400.0452 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 48/1000
2023-10-03 15:42:22.305 
Epoch 48/1000 
	 loss: 392.5777, MinusLogProbMetric: 392.5777, val_loss: 398.9330, val_MinusLogProbMetric: 398.9330

Epoch 48: val_loss did not improve from 397.86551
196/196 - 11s - loss: 392.5777 - MinusLogProbMetric: 392.5777 - val_loss: 398.9330 - val_MinusLogProbMetric: 398.9330 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 49/1000
2023-10-03 15:42:32.533 
Epoch 49/1000 
	 loss: 392.6313, MinusLogProbMetric: 392.6313, val_loss: 399.1818, val_MinusLogProbMetric: 399.1818

Epoch 49: val_loss did not improve from 397.86551
196/196 - 10s - loss: 392.6313 - MinusLogProbMetric: 392.6313 - val_loss: 399.1818 - val_MinusLogProbMetric: 399.1818 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 50/1000
2023-10-03 15:42:42.950 
Epoch 50/1000 
	 loss: 393.1360, MinusLogProbMetric: 393.1360, val_loss: 399.3140, val_MinusLogProbMetric: 399.3140

Epoch 50: val_loss did not improve from 397.86551
196/196 - 10s - loss: 393.1360 - MinusLogProbMetric: 393.1360 - val_loss: 399.3140 - val_MinusLogProbMetric: 399.3140 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 51/1000
2023-10-03 15:42:53.354 
Epoch 51/1000 
	 loss: 392.5656, MinusLogProbMetric: 392.5656, val_loss: 399.7982, val_MinusLogProbMetric: 399.7982

Epoch 51: val_loss did not improve from 397.86551
196/196 - 10s - loss: 392.5656 - MinusLogProbMetric: 392.5656 - val_loss: 399.7982 - val_MinusLogProbMetric: 399.7982 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 52/1000
2023-10-03 15:43:03.763 
Epoch 52/1000 
	 loss: 392.3956, MinusLogProbMetric: 392.3956, val_loss: 399.0323, val_MinusLogProbMetric: 399.0323

Epoch 52: val_loss did not improve from 397.86551
196/196 - 10s - loss: 392.3956 - MinusLogProbMetric: 392.3956 - val_loss: 399.0323 - val_MinusLogProbMetric: 399.0323 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 53/1000
2023-10-03 15:43:14.063 
Epoch 53/1000 
	 loss: 392.6113, MinusLogProbMetric: 392.6113, val_loss: 397.7452, val_MinusLogProbMetric: 397.7452

Epoch 53: val_loss improved from 397.86551 to 397.74518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 392.6113 - MinusLogProbMetric: 392.6113 - val_loss: 397.7452 - val_MinusLogProbMetric: 397.7452 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 54/1000
2023-10-03 15:43:24.645 
Epoch 54/1000 
	 loss: 392.6136, MinusLogProbMetric: 392.6136, val_loss: 398.3361, val_MinusLogProbMetric: 398.3361

Epoch 54: val_loss did not improve from 397.74518
196/196 - 10s - loss: 392.6136 - MinusLogProbMetric: 392.6136 - val_loss: 398.3361 - val_MinusLogProbMetric: 398.3361 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 55/1000
2023-10-03 15:43:33.383 
Epoch 55/1000 
	 loss: 392.2457, MinusLogProbMetric: 392.2457, val_loss: 403.1735, val_MinusLogProbMetric: 403.1735

Epoch 55: val_loss did not improve from 397.74518
196/196 - 9s - loss: 392.2457 - MinusLogProbMetric: 392.2457 - val_loss: 403.1735 - val_MinusLogProbMetric: 403.1735 - lr: 1.1111e-04 - 9s/epoch - 45ms/step
Epoch 56/1000
2023-10-03 15:43:43.461 
Epoch 56/1000 
	 loss: 392.6324, MinusLogProbMetric: 392.6324, val_loss: 399.1772, val_MinusLogProbMetric: 399.1772

Epoch 56: val_loss did not improve from 397.74518
196/196 - 10s - loss: 392.6324 - MinusLogProbMetric: 392.6324 - val_loss: 399.1772 - val_MinusLogProbMetric: 399.1772 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 57/1000
2023-10-03 15:43:53.750 
Epoch 57/1000 
	 loss: 392.3584, MinusLogProbMetric: 392.3584, val_loss: 399.5242, val_MinusLogProbMetric: 399.5242

Epoch 57: val_loss did not improve from 397.74518
196/196 - 10s - loss: 392.3584 - MinusLogProbMetric: 392.3584 - val_loss: 399.5242 - val_MinusLogProbMetric: 399.5242 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 58/1000
2023-10-03 15:44:04.121 
Epoch 58/1000 
	 loss: 392.0950, MinusLogProbMetric: 392.0950, val_loss: 399.2045, val_MinusLogProbMetric: 399.2045

Epoch 58: val_loss did not improve from 397.74518
196/196 - 10s - loss: 392.0950 - MinusLogProbMetric: 392.0950 - val_loss: 399.2045 - val_MinusLogProbMetric: 399.2045 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 59/1000
2023-10-03 15:44:14.586 
Epoch 59/1000 
	 loss: 392.1392, MinusLogProbMetric: 392.1392, val_loss: 403.2333, val_MinusLogProbMetric: 403.2333

Epoch 59: val_loss did not improve from 397.74518
196/196 - 10s - loss: 392.1392 - MinusLogProbMetric: 392.1392 - val_loss: 403.2333 - val_MinusLogProbMetric: 403.2333 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 60/1000
2023-10-03 15:44:24.908 
Epoch 60/1000 
	 loss: 392.3854, MinusLogProbMetric: 392.3854, val_loss: 397.3236, val_MinusLogProbMetric: 397.3236

Epoch 60: val_loss improved from 397.74518 to 397.32358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 392.3854 - MinusLogProbMetric: 392.3854 - val_loss: 397.3236 - val_MinusLogProbMetric: 397.3236 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 61/1000
2023-10-03 15:44:36.189 
Epoch 61/1000 
	 loss: 392.2804, MinusLogProbMetric: 392.2804, val_loss: 399.2957, val_MinusLogProbMetric: 399.2957

Epoch 61: val_loss did not improve from 397.32358
196/196 - 11s - loss: 392.2804 - MinusLogProbMetric: 392.2804 - val_loss: 399.2957 - val_MinusLogProbMetric: 399.2957 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 62/1000
2023-10-03 15:44:46.571 
Epoch 62/1000 
	 loss: 392.1172, MinusLogProbMetric: 392.1172, val_loss: 398.5868, val_MinusLogProbMetric: 398.5868

Epoch 62: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.1172 - MinusLogProbMetric: 392.1172 - val_loss: 398.5868 - val_MinusLogProbMetric: 398.5868 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 63/1000
2023-10-03 15:44:56.794 
Epoch 63/1000 
	 loss: 392.2274, MinusLogProbMetric: 392.2274, val_loss: 397.5874, val_MinusLogProbMetric: 397.5874

Epoch 63: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.2274 - MinusLogProbMetric: 392.2274 - val_loss: 397.5874 - val_MinusLogProbMetric: 397.5874 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 64/1000
2023-10-03 15:45:07.152 
Epoch 64/1000 
	 loss: 392.0406, MinusLogProbMetric: 392.0406, val_loss: 398.3235, val_MinusLogProbMetric: 398.3235

Epoch 64: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.0406 - MinusLogProbMetric: 392.0406 - val_loss: 398.3235 - val_MinusLogProbMetric: 398.3235 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 65/1000
2023-10-03 15:45:17.549 
Epoch 65/1000 
	 loss: 392.2475, MinusLogProbMetric: 392.2475, val_loss: 400.6844, val_MinusLogProbMetric: 400.6844

Epoch 65: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.2475 - MinusLogProbMetric: 392.2475 - val_loss: 400.6844 - val_MinusLogProbMetric: 400.6844 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 66/1000
2023-10-03 15:45:28.142 
Epoch 66/1000 
	 loss: 392.1334, MinusLogProbMetric: 392.1334, val_loss: 398.8000, val_MinusLogProbMetric: 398.8000

Epoch 66: val_loss did not improve from 397.32358
196/196 - 11s - loss: 392.1334 - MinusLogProbMetric: 392.1334 - val_loss: 398.8000 - val_MinusLogProbMetric: 398.8000 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 67/1000
2023-10-03 15:45:38.597 
Epoch 67/1000 
	 loss: 392.0892, MinusLogProbMetric: 392.0892, val_loss: 397.6743, val_MinusLogProbMetric: 397.6743

Epoch 67: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.0892 - MinusLogProbMetric: 392.0892 - val_loss: 397.6743 - val_MinusLogProbMetric: 397.6743 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 68/1000
2023-10-03 15:45:48.957 
Epoch 68/1000 
	 loss: 392.0018, MinusLogProbMetric: 392.0018, val_loss: 400.3766, val_MinusLogProbMetric: 400.3766

Epoch 68: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.0018 - MinusLogProbMetric: 392.0018 - val_loss: 400.3766 - val_MinusLogProbMetric: 400.3766 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 69/1000
2023-10-03 15:45:59.119 
Epoch 69/1000 
	 loss: 391.9674, MinusLogProbMetric: 391.9674, val_loss: 402.0933, val_MinusLogProbMetric: 402.0933

Epoch 69: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.9674 - MinusLogProbMetric: 391.9674 - val_loss: 402.0933 - val_MinusLogProbMetric: 402.0933 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 70/1000
2023-10-03 15:46:09.323 
Epoch 70/1000 
	 loss: 391.8581, MinusLogProbMetric: 391.8581, val_loss: 402.9909, val_MinusLogProbMetric: 402.9909

Epoch 70: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.8581 - MinusLogProbMetric: 391.8581 - val_loss: 402.9909 - val_MinusLogProbMetric: 402.9909 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 71/1000
2023-10-03 15:46:19.596 
Epoch 71/1000 
	 loss: 391.8379, MinusLogProbMetric: 391.8379, val_loss: 399.7885, val_MinusLogProbMetric: 399.7885

Epoch 71: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.8379 - MinusLogProbMetric: 391.8379 - val_loss: 399.7885 - val_MinusLogProbMetric: 399.7885 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 72/1000
2023-10-03 15:46:29.919 
Epoch 72/1000 
	 loss: 392.8098, MinusLogProbMetric: 392.8098, val_loss: 398.3567, val_MinusLogProbMetric: 398.3567

Epoch 72: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.8098 - MinusLogProbMetric: 392.8098 - val_loss: 398.3567 - val_MinusLogProbMetric: 398.3567 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 73/1000
2023-10-03 15:46:40.197 
Epoch 73/1000 
	 loss: 391.5667, MinusLogProbMetric: 391.5667, val_loss: 397.7970, val_MinusLogProbMetric: 397.7970

Epoch 73: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.5667 - MinusLogProbMetric: 391.5667 - val_loss: 397.7970 - val_MinusLogProbMetric: 397.7970 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 74/1000
2023-10-03 15:46:50.170 
Epoch 74/1000 
	 loss: 391.9456, MinusLogProbMetric: 391.9456, val_loss: 398.0818, val_MinusLogProbMetric: 398.0818

Epoch 74: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.9456 - MinusLogProbMetric: 391.9456 - val_loss: 398.0818 - val_MinusLogProbMetric: 398.0818 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 75/1000
2023-10-03 15:47:00.970 
Epoch 75/1000 
	 loss: 391.6784, MinusLogProbMetric: 391.6784, val_loss: 400.4803, val_MinusLogProbMetric: 400.4803

Epoch 75: val_loss did not improve from 397.32358
196/196 - 11s - loss: 391.6784 - MinusLogProbMetric: 391.6784 - val_loss: 400.4803 - val_MinusLogProbMetric: 400.4803 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 76/1000
2023-10-03 15:47:11.336 
Epoch 76/1000 
	 loss: 391.9171, MinusLogProbMetric: 391.9171, val_loss: 399.3442, val_MinusLogProbMetric: 399.3442

Epoch 76: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.9171 - MinusLogProbMetric: 391.9171 - val_loss: 399.3442 - val_MinusLogProbMetric: 399.3442 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 77/1000
2023-10-03 15:47:21.890 
Epoch 77/1000 
	 loss: 392.0951, MinusLogProbMetric: 392.0951, val_loss: 400.1354, val_MinusLogProbMetric: 400.1354

Epoch 77: val_loss did not improve from 397.32358
196/196 - 11s - loss: 392.0951 - MinusLogProbMetric: 392.0951 - val_loss: 400.1354 - val_MinusLogProbMetric: 400.1354 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 78/1000
2023-10-03 15:47:32.590 
Epoch 78/1000 
	 loss: 391.4587, MinusLogProbMetric: 391.4587, val_loss: 399.9633, val_MinusLogProbMetric: 399.9633

Epoch 78: val_loss did not improve from 397.32358
196/196 - 11s - loss: 391.4587 - MinusLogProbMetric: 391.4587 - val_loss: 399.9633 - val_MinusLogProbMetric: 399.9633 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 79/1000
2023-10-03 15:47:43.069 
Epoch 79/1000 
	 loss: 392.0437, MinusLogProbMetric: 392.0437, val_loss: 398.2225, val_MinusLogProbMetric: 398.2225

Epoch 79: val_loss did not improve from 397.32358
196/196 - 10s - loss: 392.0437 - MinusLogProbMetric: 392.0437 - val_loss: 398.2225 - val_MinusLogProbMetric: 398.2225 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 80/1000
2023-10-03 15:47:53.293 
Epoch 80/1000 
	 loss: 391.6159, MinusLogProbMetric: 391.6159, val_loss: 397.6819, val_MinusLogProbMetric: 397.6819

Epoch 80: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.6159 - MinusLogProbMetric: 391.6159 - val_loss: 397.6819 - val_MinusLogProbMetric: 397.6819 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 81/1000
2023-10-03 15:48:03.484 
Epoch 81/1000 
	 loss: 391.5103, MinusLogProbMetric: 391.5103, val_loss: 398.5299, val_MinusLogProbMetric: 398.5299

Epoch 81: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.5103 - MinusLogProbMetric: 391.5103 - val_loss: 398.5299 - val_MinusLogProbMetric: 398.5299 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 82/1000
2023-10-03 15:48:13.776 
Epoch 82/1000 
	 loss: 391.5559, MinusLogProbMetric: 391.5559, val_loss: 398.5351, val_MinusLogProbMetric: 398.5351

Epoch 82: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.5559 - MinusLogProbMetric: 391.5559 - val_loss: 398.5351 - val_MinusLogProbMetric: 398.5351 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 83/1000
2023-10-03 15:48:23.891 
Epoch 83/1000 
	 loss: 391.6347, MinusLogProbMetric: 391.6347, val_loss: 399.6292, val_MinusLogProbMetric: 399.6292

Epoch 83: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.6347 - MinusLogProbMetric: 391.6347 - val_loss: 399.6292 - val_MinusLogProbMetric: 399.6292 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 84/1000
2023-10-03 15:48:33.960 
Epoch 84/1000 
	 loss: 391.4246, MinusLogProbMetric: 391.4246, val_loss: 399.3121, val_MinusLogProbMetric: 399.3121

Epoch 84: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.4246 - MinusLogProbMetric: 391.4246 - val_loss: 399.3121 - val_MinusLogProbMetric: 399.3121 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 85/1000
2023-10-03 15:48:44.107 
Epoch 85/1000 
	 loss: 391.3848, MinusLogProbMetric: 391.3848, val_loss: 398.4349, val_MinusLogProbMetric: 398.4349

Epoch 85: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.3848 - MinusLogProbMetric: 391.3848 - val_loss: 398.4349 - val_MinusLogProbMetric: 398.4349 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 86/1000
2023-10-03 15:48:54.308 
Epoch 86/1000 
	 loss: 391.3998, MinusLogProbMetric: 391.3998, val_loss: 399.2057, val_MinusLogProbMetric: 399.2057

Epoch 86: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.3998 - MinusLogProbMetric: 391.3998 - val_loss: 399.2057 - val_MinusLogProbMetric: 399.2057 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 87/1000
2023-10-03 15:49:04.676 
Epoch 87/1000 
	 loss: 391.5354, MinusLogProbMetric: 391.5354, val_loss: 401.2480, val_MinusLogProbMetric: 401.2480

Epoch 87: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.5354 - MinusLogProbMetric: 391.5354 - val_loss: 401.2480 - val_MinusLogProbMetric: 401.2480 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 88/1000
2023-10-03 15:49:14.825 
Epoch 88/1000 
	 loss: 391.4227, MinusLogProbMetric: 391.4227, val_loss: 398.4700, val_MinusLogProbMetric: 398.4700

Epoch 88: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.4227 - MinusLogProbMetric: 391.4227 - val_loss: 398.4700 - val_MinusLogProbMetric: 398.4700 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 89/1000
2023-10-03 15:49:25.185 
Epoch 89/1000 
	 loss: 391.4006, MinusLogProbMetric: 391.4006, val_loss: 398.8123, val_MinusLogProbMetric: 398.8123

Epoch 89: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.4006 - MinusLogProbMetric: 391.4006 - val_loss: 398.8123 - val_MinusLogProbMetric: 398.8123 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 90/1000
2023-10-03 15:49:35.403 
Epoch 90/1000 
	 loss: 391.3974, MinusLogProbMetric: 391.3974, val_loss: 398.8438, val_MinusLogProbMetric: 398.8438

Epoch 90: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.3974 - MinusLogProbMetric: 391.3974 - val_loss: 398.8438 - val_MinusLogProbMetric: 398.8438 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 91/1000
2023-10-03 15:49:45.641 
Epoch 91/1000 
	 loss: 391.3622, MinusLogProbMetric: 391.3622, val_loss: 398.4560, val_MinusLogProbMetric: 398.4560

Epoch 91: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.3622 - MinusLogProbMetric: 391.3622 - val_loss: 398.4560 - val_MinusLogProbMetric: 398.4560 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 92/1000
2023-10-03 15:49:55.969 
Epoch 92/1000 
	 loss: 391.3351, MinusLogProbMetric: 391.3351, val_loss: 399.1245, val_MinusLogProbMetric: 399.1245

Epoch 92: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.3351 - MinusLogProbMetric: 391.3351 - val_loss: 399.1245 - val_MinusLogProbMetric: 399.1245 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 93/1000
2023-10-03 15:50:06.290 
Epoch 93/1000 
	 loss: 391.4687, MinusLogProbMetric: 391.4687, val_loss: 398.4955, val_MinusLogProbMetric: 398.4955

Epoch 93: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.4687 - MinusLogProbMetric: 391.4687 - val_loss: 398.4955 - val_MinusLogProbMetric: 398.4955 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-10-03 15:50:16.595 
Epoch 94/1000 
	 loss: 391.1662, MinusLogProbMetric: 391.1662, val_loss: 397.8815, val_MinusLogProbMetric: 397.8815

Epoch 94: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.1662 - MinusLogProbMetric: 391.1662 - val_loss: 397.8815 - val_MinusLogProbMetric: 397.8815 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 95/1000
2023-10-03 15:50:26.805 
Epoch 95/1000 
	 loss: 391.4106, MinusLogProbMetric: 391.4106, val_loss: 398.8267, val_MinusLogProbMetric: 398.8267

Epoch 95: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.4106 - MinusLogProbMetric: 391.4106 - val_loss: 398.8267 - val_MinusLogProbMetric: 398.8267 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 96/1000
2023-10-03 15:50:36.988 
Epoch 96/1000 
	 loss: 391.2114, MinusLogProbMetric: 391.2114, val_loss: 400.2506, val_MinusLogProbMetric: 400.2506

Epoch 96: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.2114 - MinusLogProbMetric: 391.2114 - val_loss: 400.2506 - val_MinusLogProbMetric: 400.2506 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 97/1000
2023-10-03 15:50:47.273 
Epoch 97/1000 
	 loss: 391.4729, MinusLogProbMetric: 391.4729, val_loss: 399.4174, val_MinusLogProbMetric: 399.4174

Epoch 97: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.4729 - MinusLogProbMetric: 391.4729 - val_loss: 399.4174 - val_MinusLogProbMetric: 399.4174 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 98/1000
2023-10-03 15:50:57.584 
Epoch 98/1000 
	 loss: 390.9238, MinusLogProbMetric: 390.9238, val_loss: 399.9104, val_MinusLogProbMetric: 399.9104

Epoch 98: val_loss did not improve from 397.32358
196/196 - 10s - loss: 390.9238 - MinusLogProbMetric: 390.9238 - val_loss: 399.9104 - val_MinusLogProbMetric: 399.9104 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 99/1000
2023-10-03 15:51:08.100 
Epoch 99/1000 
	 loss: 390.9900, MinusLogProbMetric: 390.9900, val_loss: 401.2540, val_MinusLogProbMetric: 401.2540

Epoch 99: val_loss did not improve from 397.32358
196/196 - 11s - loss: 390.9900 - MinusLogProbMetric: 390.9900 - val_loss: 401.2540 - val_MinusLogProbMetric: 401.2540 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 100/1000
2023-10-03 15:51:18.489 
Epoch 100/1000 
	 loss: 391.1854, MinusLogProbMetric: 391.1854, val_loss: 398.3425, val_MinusLogProbMetric: 398.3425

Epoch 100: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.1854 - MinusLogProbMetric: 391.1854 - val_loss: 398.3425 - val_MinusLogProbMetric: 398.3425 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 101/1000
2023-10-03 15:51:28.757 
Epoch 101/1000 
	 loss: 391.3035, MinusLogProbMetric: 391.3035, val_loss: 398.3893, val_MinusLogProbMetric: 398.3893

Epoch 101: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.3035 - MinusLogProbMetric: 391.3035 - val_loss: 398.3893 - val_MinusLogProbMetric: 398.3893 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 102/1000
2023-10-03 15:51:39.128 
Epoch 102/1000 
	 loss: 391.1362, MinusLogProbMetric: 391.1362, val_loss: 401.0326, val_MinusLogProbMetric: 401.0326

Epoch 102: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.1362 - MinusLogProbMetric: 391.1362 - val_loss: 401.0326 - val_MinusLogProbMetric: 401.0326 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 103/1000
2023-10-03 15:51:49.448 
Epoch 103/1000 
	 loss: 390.8844, MinusLogProbMetric: 390.8844, val_loss: 398.9060, val_MinusLogProbMetric: 398.9060

Epoch 103: val_loss did not improve from 397.32358
196/196 - 10s - loss: 390.8844 - MinusLogProbMetric: 390.8844 - val_loss: 398.9060 - val_MinusLogProbMetric: 398.9060 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 104/1000
2023-10-03 15:51:59.688 
Epoch 104/1000 
	 loss: 391.1193, MinusLogProbMetric: 391.1193, val_loss: 397.8274, val_MinusLogProbMetric: 397.8274

Epoch 104: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.1193 - MinusLogProbMetric: 391.1193 - val_loss: 397.8274 - val_MinusLogProbMetric: 397.8274 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 105/1000
2023-10-03 15:52:10.024 
Epoch 105/1000 
	 loss: 390.9426, MinusLogProbMetric: 390.9426, val_loss: 399.6251, val_MinusLogProbMetric: 399.6251

Epoch 105: val_loss did not improve from 397.32358
196/196 - 10s - loss: 390.9426 - MinusLogProbMetric: 390.9426 - val_loss: 399.6251 - val_MinusLogProbMetric: 399.6251 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 106/1000
2023-10-03 15:52:20.304 
Epoch 106/1000 
	 loss: 391.2177, MinusLogProbMetric: 391.2177, val_loss: 399.2945, val_MinusLogProbMetric: 399.2945

Epoch 106: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.2177 - MinusLogProbMetric: 391.2177 - val_loss: 399.2945 - val_MinusLogProbMetric: 399.2945 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 107/1000
2023-10-03 15:52:30.449 
Epoch 107/1000 
	 loss: 390.6776, MinusLogProbMetric: 390.6776, val_loss: 398.6694, val_MinusLogProbMetric: 398.6694

Epoch 107: val_loss did not improve from 397.32358
196/196 - 10s - loss: 390.6776 - MinusLogProbMetric: 390.6776 - val_loss: 398.6694 - val_MinusLogProbMetric: 398.6694 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 108/1000
2023-10-03 15:52:40.814 
Epoch 108/1000 
	 loss: 391.3453, MinusLogProbMetric: 391.3453, val_loss: 398.2174, val_MinusLogProbMetric: 398.2174

Epoch 108: val_loss did not improve from 397.32358
196/196 - 10s - loss: 391.3453 - MinusLogProbMetric: 391.3453 - val_loss: 398.2174 - val_MinusLogProbMetric: 398.2174 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 109/1000
2023-10-03 15:52:51.061 
Epoch 109/1000 
	 loss: 390.8747, MinusLogProbMetric: 390.8747, val_loss: 398.5114, val_MinusLogProbMetric: 398.5114

Epoch 109: val_loss did not improve from 397.32358
196/196 - 10s - loss: 390.8747 - MinusLogProbMetric: 390.8747 - val_loss: 398.5114 - val_MinusLogProbMetric: 398.5114 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 110/1000
2023-10-03 15:53:01.193 
Epoch 110/1000 
	 loss: 390.6623, MinusLogProbMetric: 390.6623, val_loss: 421.3569, val_MinusLogProbMetric: 421.3569

Epoch 110: val_loss did not improve from 397.32358
196/196 - 10s - loss: 390.6623 - MinusLogProbMetric: 390.6623 - val_loss: 421.3569 - val_MinusLogProbMetric: 421.3569 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 111/1000
2023-10-03 15:53:11.306 
Epoch 111/1000 
	 loss: 388.5969, MinusLogProbMetric: 388.5969, val_loss: 396.5013, val_MinusLogProbMetric: 396.5013

Epoch 111: val_loss improved from 397.32358 to 396.50128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 388.5969 - MinusLogProbMetric: 388.5969 - val_loss: 396.5013 - val_MinusLogProbMetric: 396.5013 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 112/1000
2023-10-03 15:53:21.967 
Epoch 112/1000 
	 loss: 387.6438, MinusLogProbMetric: 387.6438, val_loss: 396.5314, val_MinusLogProbMetric: 396.5314

Epoch 112: val_loss did not improve from 396.50128
196/196 - 10s - loss: 387.6438 - MinusLogProbMetric: 387.6438 - val_loss: 396.5314 - val_MinusLogProbMetric: 396.5314 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 113/1000
2023-10-03 15:53:31.932 
Epoch 113/1000 
	 loss: 387.6254, MinusLogProbMetric: 387.6254, val_loss: 396.4020, val_MinusLogProbMetric: 396.4020

Epoch 113: val_loss improved from 396.50128 to 396.40204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 10s - loss: 387.6254 - MinusLogProbMetric: 387.6254 - val_loss: 396.4020 - val_MinusLogProbMetric: 396.4020 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 114/1000
2023-10-03 15:53:42.619 
Epoch 114/1000 
	 loss: 387.7518, MinusLogProbMetric: 387.7518, val_loss: 396.5274, val_MinusLogProbMetric: 396.5274

Epoch 114: val_loss did not improve from 396.40204
196/196 - 10s - loss: 387.7518 - MinusLogProbMetric: 387.7518 - val_loss: 396.5274 - val_MinusLogProbMetric: 396.5274 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 115/1000
2023-10-03 15:53:52.946 
Epoch 115/1000 
	 loss: 387.6711, MinusLogProbMetric: 387.6711, val_loss: 396.5141, val_MinusLogProbMetric: 396.5141

Epoch 115: val_loss did not improve from 396.40204
196/196 - 10s - loss: 387.6711 - MinusLogProbMetric: 387.6711 - val_loss: 396.5141 - val_MinusLogProbMetric: 396.5141 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 116/1000
2023-10-03 15:54:03.075 
Epoch 116/1000 
	 loss: 387.7686, MinusLogProbMetric: 387.7686, val_loss: 396.3501, val_MinusLogProbMetric: 396.3501

Epoch 116: val_loss improved from 396.40204 to 396.35010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 387.7686 - MinusLogProbMetric: 387.7686 - val_loss: 396.3501 - val_MinusLogProbMetric: 396.3501 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 117/1000
2023-10-03 15:54:14.185 
Epoch 117/1000 
	 loss: 387.6074, MinusLogProbMetric: 387.6074, val_loss: 396.8162, val_MinusLogProbMetric: 396.8162

Epoch 117: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.6074 - MinusLogProbMetric: 387.6074 - val_loss: 396.8162 - val_MinusLogProbMetric: 396.8162 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 118/1000
2023-10-03 15:54:24.445 
Epoch 118/1000 
	 loss: 387.8200, MinusLogProbMetric: 387.8200, val_loss: 399.2467, val_MinusLogProbMetric: 399.2467

Epoch 118: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.8200 - MinusLogProbMetric: 387.8200 - val_loss: 399.2467 - val_MinusLogProbMetric: 399.2467 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 119/1000
2023-10-03 15:54:34.719 
Epoch 119/1000 
	 loss: 387.8938, MinusLogProbMetric: 387.8938, val_loss: 396.7855, val_MinusLogProbMetric: 396.7855

Epoch 119: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.8938 - MinusLogProbMetric: 387.8938 - val_loss: 396.7855 - val_MinusLogProbMetric: 396.7855 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 120/1000
2023-10-03 15:54:44.782 
Epoch 120/1000 
	 loss: 387.6870, MinusLogProbMetric: 387.6870, val_loss: 396.4671, val_MinusLogProbMetric: 396.4671

Epoch 120: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.6870 - MinusLogProbMetric: 387.6870 - val_loss: 396.4671 - val_MinusLogProbMetric: 396.4671 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 121/1000
2023-10-03 15:54:55.100 
Epoch 121/1000 
	 loss: 387.8648, MinusLogProbMetric: 387.8648, val_loss: 396.8982, val_MinusLogProbMetric: 396.8982

Epoch 121: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.8648 - MinusLogProbMetric: 387.8648 - val_loss: 396.8982 - val_MinusLogProbMetric: 396.8982 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 122/1000
2023-10-03 15:55:05.399 
Epoch 122/1000 
	 loss: 387.6957, MinusLogProbMetric: 387.6957, val_loss: 396.5222, val_MinusLogProbMetric: 396.5222

Epoch 122: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.6957 - MinusLogProbMetric: 387.6957 - val_loss: 396.5222 - val_MinusLogProbMetric: 396.5222 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 123/1000
2023-10-03 15:55:15.474 
Epoch 123/1000 
	 loss: 387.7365, MinusLogProbMetric: 387.7365, val_loss: 396.5267, val_MinusLogProbMetric: 396.5267

Epoch 123: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.7365 - MinusLogProbMetric: 387.7365 - val_loss: 396.5267 - val_MinusLogProbMetric: 396.5267 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 124/1000
2023-10-03 15:55:25.575 
Epoch 124/1000 
	 loss: 387.8940, MinusLogProbMetric: 387.8940, val_loss: 396.5187, val_MinusLogProbMetric: 396.5187

Epoch 124: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.8940 - MinusLogProbMetric: 387.8940 - val_loss: 396.5187 - val_MinusLogProbMetric: 396.5187 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 125/1000
2023-10-03 15:55:35.946 
Epoch 125/1000 
	 loss: 387.5963, MinusLogProbMetric: 387.5963, val_loss: 397.2999, val_MinusLogProbMetric: 397.2999

Epoch 125: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.5963 - MinusLogProbMetric: 387.5963 - val_loss: 397.2999 - val_MinusLogProbMetric: 397.2999 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 126/1000
2023-10-03 15:55:46.114 
Epoch 126/1000 
	 loss: 387.6896, MinusLogProbMetric: 387.6896, val_loss: 396.5969, val_MinusLogProbMetric: 396.5969

Epoch 126: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.6896 - MinusLogProbMetric: 387.6896 - val_loss: 396.5969 - val_MinusLogProbMetric: 396.5969 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 127/1000
2023-10-03 15:55:56.481 
Epoch 127/1000 
	 loss: 387.7167, MinusLogProbMetric: 387.7167, val_loss: 396.3968, val_MinusLogProbMetric: 396.3968

Epoch 127: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.7167 - MinusLogProbMetric: 387.7167 - val_loss: 396.3968 - val_MinusLogProbMetric: 396.3968 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 128/1000
2023-10-03 15:56:06.594 
Epoch 128/1000 
	 loss: 387.6352, MinusLogProbMetric: 387.6352, val_loss: 397.2818, val_MinusLogProbMetric: 397.2818

Epoch 128: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.6352 - MinusLogProbMetric: 387.6352 - val_loss: 397.2818 - val_MinusLogProbMetric: 397.2818 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 129/1000
2023-10-03 15:56:16.809 
Epoch 129/1000 
	 loss: 387.4989, MinusLogProbMetric: 387.4989, val_loss: 396.4094, val_MinusLogProbMetric: 396.4094

Epoch 129: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.4989 - MinusLogProbMetric: 387.4989 - val_loss: 396.4094 - val_MinusLogProbMetric: 396.4094 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 130/1000
2023-10-03 15:56:26.976 
Epoch 130/1000 
	 loss: 387.6470, MinusLogProbMetric: 387.6470, val_loss: 397.6968, val_MinusLogProbMetric: 397.6968

Epoch 130: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.6470 - MinusLogProbMetric: 387.6470 - val_loss: 397.6968 - val_MinusLogProbMetric: 397.6968 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 131/1000
2023-10-03 15:56:37.239 
Epoch 131/1000 
	 loss: 387.5879, MinusLogProbMetric: 387.5879, val_loss: 396.7746, val_MinusLogProbMetric: 396.7746

Epoch 131: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.5879 - MinusLogProbMetric: 387.5879 - val_loss: 396.7746 - val_MinusLogProbMetric: 396.7746 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 132/1000
2023-10-03 15:56:47.493 
Epoch 132/1000 
	 loss: 387.4971, MinusLogProbMetric: 387.4971, val_loss: 397.2804, val_MinusLogProbMetric: 397.2804

Epoch 132: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.4971 - MinusLogProbMetric: 387.4971 - val_loss: 397.2804 - val_MinusLogProbMetric: 397.2804 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 133/1000
2023-10-03 15:56:57.961 
Epoch 133/1000 
	 loss: 387.4060, MinusLogProbMetric: 387.4060, val_loss: 397.1016, val_MinusLogProbMetric: 397.1016

Epoch 133: val_loss did not improve from 396.35010
196/196 - 10s - loss: 387.4060 - MinusLogProbMetric: 387.4060 - val_loss: 397.1016 - val_MinusLogProbMetric: 397.1016 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 134/1000
2023-10-03 15:57:08.296 
Epoch 134/1000 
	 loss: 387.6561, MinusLogProbMetric: 387.6561, val_loss: 396.2893, val_MinusLogProbMetric: 396.2893

Epoch 134: val_loss improved from 396.35010 to 396.28934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 12s - loss: 387.6561 - MinusLogProbMetric: 387.6561 - val_loss: 396.2893 - val_MinusLogProbMetric: 396.2893 - lr: 5.5556e-05 - 12s/epoch - 59ms/step
Epoch 135/1000
2023-10-03 15:57:19.749 
Epoch 135/1000 
	 loss: 387.5435, MinusLogProbMetric: 387.5435, val_loss: 397.2615, val_MinusLogProbMetric: 397.2615

Epoch 135: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.5435 - MinusLogProbMetric: 387.5435 - val_loss: 397.2615 - val_MinusLogProbMetric: 397.2615 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 136/1000
2023-10-03 15:57:30.052 
Epoch 136/1000 
	 loss: 387.5168, MinusLogProbMetric: 387.5168, val_loss: 396.3177, val_MinusLogProbMetric: 396.3177

Epoch 136: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.5168 - MinusLogProbMetric: 387.5168 - val_loss: 396.3177 - val_MinusLogProbMetric: 396.3177 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 137/1000
2023-10-03 15:57:40.090 
Epoch 137/1000 
	 loss: 387.5273, MinusLogProbMetric: 387.5273, val_loss: 396.5476, val_MinusLogProbMetric: 396.5476

Epoch 137: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.5273 - MinusLogProbMetric: 387.5273 - val_loss: 396.5476 - val_MinusLogProbMetric: 396.5476 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 138/1000
2023-10-03 15:57:50.269 
Epoch 138/1000 
	 loss: 387.6134, MinusLogProbMetric: 387.6134, val_loss: 396.9590, val_MinusLogProbMetric: 396.9590

Epoch 138: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.6134 - MinusLogProbMetric: 387.6134 - val_loss: 396.9590 - val_MinusLogProbMetric: 396.9590 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 139/1000
2023-10-03 15:58:00.563 
Epoch 139/1000 
	 loss: 387.4800, MinusLogProbMetric: 387.4800, val_loss: 396.3847, val_MinusLogProbMetric: 396.3847

Epoch 139: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.4800 - MinusLogProbMetric: 387.4800 - val_loss: 396.3847 - val_MinusLogProbMetric: 396.3847 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 140/1000
2023-10-03 15:58:10.728 
Epoch 140/1000 
	 loss: 387.4675, MinusLogProbMetric: 387.4675, val_loss: 397.1467, val_MinusLogProbMetric: 397.1467

Epoch 140: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.4675 - MinusLogProbMetric: 387.4675 - val_loss: 397.1467 - val_MinusLogProbMetric: 397.1467 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 141/1000
2023-10-03 15:58:20.954 
Epoch 141/1000 
	 loss: 387.4264, MinusLogProbMetric: 387.4264, val_loss: 396.9756, val_MinusLogProbMetric: 396.9756

Epoch 141: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.4264 - MinusLogProbMetric: 387.4264 - val_loss: 396.9756 - val_MinusLogProbMetric: 396.9756 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 142/1000
2023-10-03 15:58:31.385 
Epoch 142/1000 
	 loss: 387.4148, MinusLogProbMetric: 387.4148, val_loss: 396.7899, val_MinusLogProbMetric: 396.7899

Epoch 142: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.4148 - MinusLogProbMetric: 387.4148 - val_loss: 396.7899 - val_MinusLogProbMetric: 396.7899 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 143/1000
2023-10-03 15:58:41.641 
Epoch 143/1000 
	 loss: 387.5045, MinusLogProbMetric: 387.5045, val_loss: 397.5533, val_MinusLogProbMetric: 397.5533

Epoch 143: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.5045 - MinusLogProbMetric: 387.5045 - val_loss: 397.5533 - val_MinusLogProbMetric: 397.5533 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 144/1000
2023-10-03 15:58:51.844 
Epoch 144/1000 
	 loss: 387.4305, MinusLogProbMetric: 387.4305, val_loss: 396.3289, val_MinusLogProbMetric: 396.3289

Epoch 144: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.4305 - MinusLogProbMetric: 387.4305 - val_loss: 396.3289 - val_MinusLogProbMetric: 396.3289 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 145/1000
2023-10-03 15:59:02.218 
Epoch 145/1000 
	 loss: 387.4246, MinusLogProbMetric: 387.4246, val_loss: 397.6364, val_MinusLogProbMetric: 397.6364

Epoch 145: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.4246 - MinusLogProbMetric: 387.4246 - val_loss: 397.6364 - val_MinusLogProbMetric: 397.6364 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 146/1000
2023-10-03 15:59:12.553 
Epoch 146/1000 
	 loss: 387.3532, MinusLogProbMetric: 387.3532, val_loss: 396.6455, val_MinusLogProbMetric: 396.6455

Epoch 146: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.3532 - MinusLogProbMetric: 387.3532 - val_loss: 396.6455 - val_MinusLogProbMetric: 396.6455 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 147/1000
2023-10-03 15:59:22.851 
Epoch 147/1000 
	 loss: 387.6386, MinusLogProbMetric: 387.6386, val_loss: 396.8018, val_MinusLogProbMetric: 396.8018

Epoch 147: val_loss did not improve from 396.28934
196/196 - 10s - loss: 387.6386 - MinusLogProbMetric: 387.6386 - val_loss: 396.8018 - val_MinusLogProbMetric: 396.8018 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 148/1000
2023-10-03 15:59:32.873 
Epoch 148/1000 
	 loss: 387.2285, MinusLogProbMetric: 387.2285, val_loss: 396.2626, val_MinusLogProbMetric: 396.2626

Epoch 148: val_loss improved from 396.28934 to 396.26260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 10s - loss: 387.2285 - MinusLogProbMetric: 387.2285 - val_loss: 396.2626 - val_MinusLogProbMetric: 396.2626 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 149/1000
2023-10-03 15:59:43.408 
Epoch 149/1000 
	 loss: 387.2873, MinusLogProbMetric: 387.2873, val_loss: 398.2288, val_MinusLogProbMetric: 398.2288

Epoch 149: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.2873 - MinusLogProbMetric: 387.2873 - val_loss: 398.2288 - val_MinusLogProbMetric: 398.2288 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 150/1000
2023-10-03 15:59:53.405 
Epoch 150/1000 
	 loss: 387.5345, MinusLogProbMetric: 387.5345, val_loss: 398.2098, val_MinusLogProbMetric: 398.2098

Epoch 150: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.5345 - MinusLogProbMetric: 387.5345 - val_loss: 398.2098 - val_MinusLogProbMetric: 398.2098 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 151/1000
2023-10-03 16:00:03.736 
Epoch 151/1000 
	 loss: 387.1290, MinusLogProbMetric: 387.1290, val_loss: 396.4572, val_MinusLogProbMetric: 396.4572

Epoch 151: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.1290 - MinusLogProbMetric: 387.1290 - val_loss: 396.4572 - val_MinusLogProbMetric: 396.4572 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 152/1000
2023-10-03 16:00:14.581 
Epoch 152/1000 
	 loss: 387.3088, MinusLogProbMetric: 387.3088, val_loss: 397.4173, val_MinusLogProbMetric: 397.4173

Epoch 152: val_loss did not improve from 396.26260
196/196 - 11s - loss: 387.3088 - MinusLogProbMetric: 387.3088 - val_loss: 397.4173 - val_MinusLogProbMetric: 397.4173 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 153/1000
2023-10-03 16:00:25.052 
Epoch 153/1000 
	 loss: 387.2661, MinusLogProbMetric: 387.2661, val_loss: 396.7444, val_MinusLogProbMetric: 396.7444

Epoch 153: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.2661 - MinusLogProbMetric: 387.2661 - val_loss: 396.7444 - val_MinusLogProbMetric: 396.7444 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 154/1000
2023-10-03 16:00:35.076 
Epoch 154/1000 
	 loss: 387.3089, MinusLogProbMetric: 387.3089, val_loss: 396.4180, val_MinusLogProbMetric: 396.4180

Epoch 154: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.3089 - MinusLogProbMetric: 387.3089 - val_loss: 396.4180 - val_MinusLogProbMetric: 396.4180 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 155/1000
2023-10-03 16:00:45.114 
Epoch 155/1000 
	 loss: 387.4920, MinusLogProbMetric: 387.4920, val_loss: 397.0322, val_MinusLogProbMetric: 397.0322

Epoch 155: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.4920 - MinusLogProbMetric: 387.4920 - val_loss: 397.0322 - val_MinusLogProbMetric: 397.0322 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 156/1000
2023-10-03 16:00:54.865 
Epoch 156/1000 
	 loss: 387.3398, MinusLogProbMetric: 387.3398, val_loss: 397.8351, val_MinusLogProbMetric: 397.8351

Epoch 156: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.3398 - MinusLogProbMetric: 387.3398 - val_loss: 397.8351 - val_MinusLogProbMetric: 397.8351 - lr: 5.5556e-05 - 10s/epoch - 50ms/step
Epoch 157/1000
2023-10-03 16:01:05.061 
Epoch 157/1000 
	 loss: 387.2643, MinusLogProbMetric: 387.2643, val_loss: 396.7647, val_MinusLogProbMetric: 396.7647

Epoch 157: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.2643 - MinusLogProbMetric: 387.2643 - val_loss: 396.7647 - val_MinusLogProbMetric: 396.7647 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 158/1000
2023-10-03 16:01:15.021 
Epoch 158/1000 
	 loss: 387.0506, MinusLogProbMetric: 387.0506, val_loss: 396.8515, val_MinusLogProbMetric: 396.8515

Epoch 158: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.0506 - MinusLogProbMetric: 387.0506 - val_loss: 396.8515 - val_MinusLogProbMetric: 396.8515 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 159/1000
2023-10-03 16:01:25.252 
Epoch 159/1000 
	 loss: 387.1547, MinusLogProbMetric: 387.1547, val_loss: 397.1136, val_MinusLogProbMetric: 397.1136

Epoch 159: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.1547 - MinusLogProbMetric: 387.1547 - val_loss: 397.1136 - val_MinusLogProbMetric: 397.1136 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 160/1000
2023-10-03 16:01:35.478 
Epoch 160/1000 
	 loss: 387.1916, MinusLogProbMetric: 387.1916, val_loss: 396.3506, val_MinusLogProbMetric: 396.3506

Epoch 160: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.1916 - MinusLogProbMetric: 387.1916 - val_loss: 396.3506 - val_MinusLogProbMetric: 396.3506 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 161/1000
2023-10-03 16:01:45.748 
Epoch 161/1000 
	 loss: 387.4519, MinusLogProbMetric: 387.4519, val_loss: 396.7407, val_MinusLogProbMetric: 396.7407

Epoch 161: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.4519 - MinusLogProbMetric: 387.4519 - val_loss: 396.7407 - val_MinusLogProbMetric: 396.7407 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 162/1000
2023-10-03 16:01:56.095 
Epoch 162/1000 
	 loss: 387.1371, MinusLogProbMetric: 387.1371, val_loss: 397.6597, val_MinusLogProbMetric: 397.6597

Epoch 162: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.1371 - MinusLogProbMetric: 387.1371 - val_loss: 397.6597 - val_MinusLogProbMetric: 397.6597 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 163/1000
2023-10-03 16:02:06.403 
Epoch 163/1000 
	 loss: 387.2121, MinusLogProbMetric: 387.2121, val_loss: 396.6345, val_MinusLogProbMetric: 396.6345

Epoch 163: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.2121 - MinusLogProbMetric: 387.2121 - val_loss: 396.6345 - val_MinusLogProbMetric: 396.6345 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 164/1000
2023-10-03 16:02:16.620 
Epoch 164/1000 
	 loss: 387.2913, MinusLogProbMetric: 387.2913, val_loss: 398.0169, val_MinusLogProbMetric: 398.0169

Epoch 164: val_loss did not improve from 396.26260
196/196 - 10s - loss: 387.2913 - MinusLogProbMetric: 387.2913 - val_loss: 398.0169 - val_MinusLogProbMetric: 398.0169 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 165/1000
2023-10-03 16:02:26.938 
Epoch 165/1000 
	 loss: 387.1830, MinusLogProbMetric: 387.1830, val_loss: 396.2113, val_MinusLogProbMetric: 396.2113

Epoch 165: val_loss improved from 396.26260 to 396.21127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 387.1830 - MinusLogProbMetric: 387.1830 - val_loss: 396.2113 - val_MinusLogProbMetric: 396.2113 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 166/1000
2023-10-03 16:02:37.477 
Epoch 166/1000 
	 loss: 387.5245, MinusLogProbMetric: 387.5245, val_loss: 398.3289, val_MinusLogProbMetric: 398.3289

Epoch 166: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.5245 - MinusLogProbMetric: 387.5245 - val_loss: 398.3289 - val_MinusLogProbMetric: 398.3289 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 167/1000
2023-10-03 16:02:47.653 
Epoch 167/1000 
	 loss: 387.0438, MinusLogProbMetric: 387.0438, val_loss: 396.2597, val_MinusLogProbMetric: 396.2597

Epoch 167: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.0438 - MinusLogProbMetric: 387.0438 - val_loss: 396.2597 - val_MinusLogProbMetric: 396.2597 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 168/1000
2023-10-03 16:02:58.070 
Epoch 168/1000 
	 loss: 387.2068, MinusLogProbMetric: 387.2068, val_loss: 396.3184, val_MinusLogProbMetric: 396.3184

Epoch 168: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.2068 - MinusLogProbMetric: 387.2068 - val_loss: 396.3184 - val_MinusLogProbMetric: 396.3184 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 169/1000
2023-10-03 16:03:08.419 
Epoch 169/1000 
	 loss: 386.9458, MinusLogProbMetric: 386.9458, val_loss: 396.8193, val_MinusLogProbMetric: 396.8193

Epoch 169: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.9458 - MinusLogProbMetric: 386.9458 - val_loss: 396.8193 - val_MinusLogProbMetric: 396.8193 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 170/1000
2023-10-03 16:03:18.659 
Epoch 170/1000 
	 loss: 387.2989, MinusLogProbMetric: 387.2989, val_loss: 397.0220, val_MinusLogProbMetric: 397.0220

Epoch 170: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.2989 - MinusLogProbMetric: 387.2989 - val_loss: 397.0220 - val_MinusLogProbMetric: 397.0220 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 171/1000
2023-10-03 16:03:28.727 
Epoch 171/1000 
	 loss: 387.1301, MinusLogProbMetric: 387.1301, val_loss: 397.3504, val_MinusLogProbMetric: 397.3504

Epoch 171: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.1301 - MinusLogProbMetric: 387.1301 - val_loss: 397.3504 - val_MinusLogProbMetric: 397.3504 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 172/1000
2023-10-03 16:03:39.064 
Epoch 172/1000 
	 loss: 387.0210, MinusLogProbMetric: 387.0210, val_loss: 398.6971, val_MinusLogProbMetric: 398.6971

Epoch 172: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.0210 - MinusLogProbMetric: 387.0210 - val_loss: 398.6971 - val_MinusLogProbMetric: 398.6971 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 173/1000
2023-10-03 16:03:49.388 
Epoch 173/1000 
	 loss: 387.1721, MinusLogProbMetric: 387.1721, val_loss: 396.8212, val_MinusLogProbMetric: 396.8212

Epoch 173: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.1721 - MinusLogProbMetric: 387.1721 - val_loss: 396.8212 - val_MinusLogProbMetric: 396.8212 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 174/1000
2023-10-03 16:03:59.884 
Epoch 174/1000 
	 loss: 386.9597, MinusLogProbMetric: 386.9597, val_loss: 397.2975, val_MinusLogProbMetric: 397.2975

Epoch 174: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.9597 - MinusLogProbMetric: 386.9597 - val_loss: 397.2975 - val_MinusLogProbMetric: 397.2975 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 175/1000
2023-10-03 16:04:10.480 
Epoch 175/1000 
	 loss: 387.2908, MinusLogProbMetric: 387.2908, val_loss: 398.0025, val_MinusLogProbMetric: 398.0025

Epoch 175: val_loss did not improve from 396.21127
196/196 - 11s - loss: 387.2908 - MinusLogProbMetric: 387.2908 - val_loss: 398.0025 - val_MinusLogProbMetric: 398.0025 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 176/1000
2023-10-03 16:04:20.786 
Epoch 176/1000 
	 loss: 387.0240, MinusLogProbMetric: 387.0240, val_loss: 396.6194, val_MinusLogProbMetric: 396.6194

Epoch 176: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.0240 - MinusLogProbMetric: 387.0240 - val_loss: 396.6194 - val_MinusLogProbMetric: 396.6194 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 177/1000
2023-10-03 16:04:31.061 
Epoch 177/1000 
	 loss: 387.0761, MinusLogProbMetric: 387.0761, val_loss: 396.5734, val_MinusLogProbMetric: 396.5734

Epoch 177: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.0761 - MinusLogProbMetric: 387.0761 - val_loss: 396.5734 - val_MinusLogProbMetric: 396.5734 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 178/1000
2023-10-03 16:04:41.245 
Epoch 178/1000 
	 loss: 387.2365, MinusLogProbMetric: 387.2365, val_loss: 396.9960, val_MinusLogProbMetric: 396.9960

Epoch 178: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.2365 - MinusLogProbMetric: 387.2365 - val_loss: 396.9960 - val_MinusLogProbMetric: 396.9960 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 179/1000
2023-10-03 16:04:51.528 
Epoch 179/1000 
	 loss: 386.8483, MinusLogProbMetric: 386.8483, val_loss: 397.2242, val_MinusLogProbMetric: 397.2242

Epoch 179: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.8483 - MinusLogProbMetric: 386.8483 - val_loss: 397.2242 - val_MinusLogProbMetric: 397.2242 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 180/1000
2023-10-03 16:05:01.742 
Epoch 180/1000 
	 loss: 387.1744, MinusLogProbMetric: 387.1744, val_loss: 397.3626, val_MinusLogProbMetric: 397.3626

Epoch 180: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.1744 - MinusLogProbMetric: 387.1744 - val_loss: 397.3626 - val_MinusLogProbMetric: 397.3626 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 181/1000
2023-10-03 16:05:12.198 
Epoch 181/1000 
	 loss: 386.9120, MinusLogProbMetric: 386.9120, val_loss: 396.5144, val_MinusLogProbMetric: 396.5144

Epoch 181: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.9120 - MinusLogProbMetric: 386.9120 - val_loss: 396.5144 - val_MinusLogProbMetric: 396.5144 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 182/1000
2023-10-03 16:05:22.734 
Epoch 182/1000 
	 loss: 387.0472, MinusLogProbMetric: 387.0472, val_loss: 396.4593, val_MinusLogProbMetric: 396.4593

Epoch 182: val_loss did not improve from 396.21127
196/196 - 11s - loss: 387.0472 - MinusLogProbMetric: 387.0472 - val_loss: 396.4593 - val_MinusLogProbMetric: 396.4593 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 183/1000
2023-10-03 16:05:33.118 
Epoch 183/1000 
	 loss: 387.0280, MinusLogProbMetric: 387.0280, val_loss: 397.9456, val_MinusLogProbMetric: 397.9456

Epoch 183: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.0280 - MinusLogProbMetric: 387.0280 - val_loss: 397.9456 - val_MinusLogProbMetric: 397.9456 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 184/1000
2023-10-03 16:05:43.611 
Epoch 184/1000 
	 loss: 386.8881, MinusLogProbMetric: 386.8881, val_loss: 397.1163, val_MinusLogProbMetric: 397.1163

Epoch 184: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.8881 - MinusLogProbMetric: 386.8881 - val_loss: 397.1163 - val_MinusLogProbMetric: 397.1163 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 185/1000
2023-10-03 16:05:54.069 
Epoch 185/1000 
	 loss: 387.1260, MinusLogProbMetric: 387.1260, val_loss: 396.2635, val_MinusLogProbMetric: 396.2635

Epoch 185: val_loss did not improve from 396.21127
196/196 - 10s - loss: 387.1260 - MinusLogProbMetric: 387.1260 - val_loss: 396.2635 - val_MinusLogProbMetric: 396.2635 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 186/1000
2023-10-03 16:06:04.293 
Epoch 186/1000 
	 loss: 386.8214, MinusLogProbMetric: 386.8214, val_loss: 397.8698, val_MinusLogProbMetric: 397.8698

Epoch 186: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.8214 - MinusLogProbMetric: 386.8214 - val_loss: 397.8698 - val_MinusLogProbMetric: 397.8698 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 187/1000
2023-10-03 16:06:14.661 
Epoch 187/1000 
	 loss: 386.9706, MinusLogProbMetric: 386.9706, val_loss: 396.8973, val_MinusLogProbMetric: 396.8973

Epoch 187: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.9706 - MinusLogProbMetric: 386.9706 - val_loss: 396.8973 - val_MinusLogProbMetric: 396.8973 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 188/1000
2023-10-03 16:06:25.152 
Epoch 188/1000 
	 loss: 386.9951, MinusLogProbMetric: 386.9951, val_loss: 397.3364, val_MinusLogProbMetric: 397.3364

Epoch 188: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.9951 - MinusLogProbMetric: 386.9951 - val_loss: 397.3364 - val_MinusLogProbMetric: 397.3364 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 189/1000
2023-10-03 16:06:36.047 
Epoch 189/1000 
	 loss: 387.1335, MinusLogProbMetric: 387.1335, val_loss: 396.7440, val_MinusLogProbMetric: 396.7440

Epoch 189: val_loss did not improve from 396.21127
196/196 - 11s - loss: 387.1335 - MinusLogProbMetric: 387.1335 - val_loss: 396.7440 - val_MinusLogProbMetric: 396.7440 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 190/1000
2023-10-03 16:06:46.198 
Epoch 190/1000 
	 loss: 386.6759, MinusLogProbMetric: 386.6759, val_loss: 396.9339, val_MinusLogProbMetric: 396.9339

Epoch 190: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.6759 - MinusLogProbMetric: 386.6759 - val_loss: 396.9339 - val_MinusLogProbMetric: 396.9339 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 191/1000
2023-10-03 16:06:56.441 
Epoch 191/1000 
	 loss: 386.6859, MinusLogProbMetric: 386.6859, val_loss: 396.3393, val_MinusLogProbMetric: 396.3393

Epoch 191: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.6859 - MinusLogProbMetric: 386.6859 - val_loss: 396.3393 - val_MinusLogProbMetric: 396.3393 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 192/1000
2023-10-03 16:07:06.683 
Epoch 192/1000 
	 loss: 386.6053, MinusLogProbMetric: 386.6053, val_loss: 396.6700, val_MinusLogProbMetric: 396.6700

Epoch 192: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.6053 - MinusLogProbMetric: 386.6053 - val_loss: 396.6700 - val_MinusLogProbMetric: 396.6700 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 193/1000
2023-10-03 16:07:16.727 
Epoch 193/1000 
	 loss: 386.8094, MinusLogProbMetric: 386.8094, val_loss: 396.9368, val_MinusLogProbMetric: 396.9368

Epoch 193: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.8094 - MinusLogProbMetric: 386.8094 - val_loss: 396.9368 - val_MinusLogProbMetric: 396.9368 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 194/1000
2023-10-03 16:07:27.012 
Epoch 194/1000 
	 loss: 386.8955, MinusLogProbMetric: 386.8955, val_loss: 396.6336, val_MinusLogProbMetric: 396.6336

Epoch 194: val_loss did not improve from 396.21127
196/196 - 10s - loss: 386.8955 - MinusLogProbMetric: 386.8955 - val_loss: 396.6336 - val_MinusLogProbMetric: 396.6336 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 195/1000
2023-10-03 16:07:37.518 
Epoch 195/1000 
	 loss: 386.7417, MinusLogProbMetric: 386.7417, val_loss: 397.0479, val_MinusLogProbMetric: 397.0479

Epoch 195: val_loss did not improve from 396.21127
196/196 - 11s - loss: 386.7417 - MinusLogProbMetric: 386.7417 - val_loss: 397.0479 - val_MinusLogProbMetric: 397.0479 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 196/1000
2023-10-03 16:07:47.757 
Epoch 196/1000 
	 loss: 387.1584, MinusLogProbMetric: 387.1584, val_loss: 395.9114, val_MinusLogProbMetric: 395.9114

Epoch 196: val_loss improved from 396.21127 to 395.91138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_352/weights/best_weights.h5
196/196 - 11s - loss: 387.1584 - MinusLogProbMetric: 387.1584 - val_loss: 395.9114 - val_MinusLogProbMetric: 395.9114 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 197/1000
2023-10-03 16:07:58.279 
Epoch 197/1000 
	 loss: 386.6598, MinusLogProbMetric: 386.6598, val_loss: 397.9263, val_MinusLogProbMetric: 397.9263

Epoch 197: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6598 - MinusLogProbMetric: 386.6598 - val_loss: 397.9263 - val_MinusLogProbMetric: 397.9263 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 198/1000
2023-10-03 16:08:08.845 
Epoch 198/1000 
	 loss: 386.5921, MinusLogProbMetric: 386.5921, val_loss: 396.5411, val_MinusLogProbMetric: 396.5411

Epoch 198: val_loss did not improve from 395.91138
196/196 - 11s - loss: 386.5921 - MinusLogProbMetric: 386.5921 - val_loss: 396.5411 - val_MinusLogProbMetric: 396.5411 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 199/1000
2023-10-03 16:08:19.221 
Epoch 199/1000 
	 loss: 386.8938, MinusLogProbMetric: 386.8938, val_loss: 396.9643, val_MinusLogProbMetric: 396.9643

Epoch 199: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.8938 - MinusLogProbMetric: 386.8938 - val_loss: 396.9643 - val_MinusLogProbMetric: 396.9643 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 200/1000
2023-10-03 16:08:29.463 
Epoch 200/1000 
	 loss: 386.6463, MinusLogProbMetric: 386.6463, val_loss: 396.7338, val_MinusLogProbMetric: 396.7338

Epoch 200: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6463 - MinusLogProbMetric: 386.6463 - val_loss: 396.7338 - val_MinusLogProbMetric: 396.7338 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 201/1000
2023-10-03 16:08:39.947 
Epoch 201/1000 
	 loss: 386.6752, MinusLogProbMetric: 386.6752, val_loss: 397.3887, val_MinusLogProbMetric: 397.3887

Epoch 201: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6752 - MinusLogProbMetric: 386.6752 - val_loss: 397.3887 - val_MinusLogProbMetric: 397.3887 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 202/1000
2023-10-03 16:08:50.186 
Epoch 202/1000 
	 loss: 386.8683, MinusLogProbMetric: 386.8683, val_loss: 397.0931, val_MinusLogProbMetric: 397.0931

Epoch 202: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.8683 - MinusLogProbMetric: 386.8683 - val_loss: 397.0931 - val_MinusLogProbMetric: 397.0931 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 203/1000
2023-10-03 16:09:00.384 
Epoch 203/1000 
	 loss: 386.6461, MinusLogProbMetric: 386.6461, val_loss: 396.5739, val_MinusLogProbMetric: 396.5739

Epoch 203: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6461 - MinusLogProbMetric: 386.6461 - val_loss: 396.5739 - val_MinusLogProbMetric: 396.5739 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 204/1000
2023-10-03 16:09:10.674 
Epoch 204/1000 
	 loss: 386.8358, MinusLogProbMetric: 386.8358, val_loss: 397.1981, val_MinusLogProbMetric: 397.1981

Epoch 204: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.8358 - MinusLogProbMetric: 386.8358 - val_loss: 397.1981 - val_MinusLogProbMetric: 397.1981 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 205/1000
2023-10-03 16:09:20.963 
Epoch 205/1000 
	 loss: 386.8626, MinusLogProbMetric: 386.8626, val_loss: 398.3043, val_MinusLogProbMetric: 398.3043

Epoch 205: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.8626 - MinusLogProbMetric: 386.8626 - val_loss: 398.3043 - val_MinusLogProbMetric: 398.3043 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 206/1000
2023-10-03 16:09:30.910 
Epoch 206/1000 
	 loss: 386.5831, MinusLogProbMetric: 386.5831, val_loss: 397.0845, val_MinusLogProbMetric: 397.0845

Epoch 206: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5831 - MinusLogProbMetric: 386.5831 - val_loss: 397.0845 - val_MinusLogProbMetric: 397.0845 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 207/1000
2023-10-03 16:09:41.337 
Epoch 207/1000 
	 loss: 386.3665, MinusLogProbMetric: 386.3665, val_loss: 398.7129, val_MinusLogProbMetric: 398.7129

Epoch 207: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.3665 - MinusLogProbMetric: 386.3665 - val_loss: 398.7129 - val_MinusLogProbMetric: 398.7129 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 208/1000
2023-10-03 16:09:51.584 
Epoch 208/1000 
	 loss: 386.5515, MinusLogProbMetric: 386.5515, val_loss: 397.4596, val_MinusLogProbMetric: 397.4596

Epoch 208: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5515 - MinusLogProbMetric: 386.5515 - val_loss: 397.4596 - val_MinusLogProbMetric: 397.4596 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 209/1000
2023-10-03 16:10:01.813 
Epoch 209/1000 
	 loss: 386.7291, MinusLogProbMetric: 386.7291, val_loss: 396.4043, val_MinusLogProbMetric: 396.4043

Epoch 209: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.7291 - MinusLogProbMetric: 386.7291 - val_loss: 396.4043 - val_MinusLogProbMetric: 396.4043 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 210/1000
2023-10-03 16:10:12.122 
Epoch 210/1000 
	 loss: 386.7858, MinusLogProbMetric: 386.7858, val_loss: 397.4608, val_MinusLogProbMetric: 397.4608

Epoch 210: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.7858 - MinusLogProbMetric: 386.7858 - val_loss: 397.4608 - val_MinusLogProbMetric: 397.4608 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 211/1000
2023-10-03 16:10:22.452 
Epoch 211/1000 
	 loss: 386.8973, MinusLogProbMetric: 386.8973, val_loss: 396.9383, val_MinusLogProbMetric: 396.9383

Epoch 211: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.8973 - MinusLogProbMetric: 386.8973 - val_loss: 396.9383 - val_MinusLogProbMetric: 396.9383 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 212/1000
2023-10-03 16:10:32.786 
Epoch 212/1000 
	 loss: 386.5662, MinusLogProbMetric: 386.5662, val_loss: 397.3510, val_MinusLogProbMetric: 397.3510

Epoch 212: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5662 - MinusLogProbMetric: 386.5662 - val_loss: 397.3510 - val_MinusLogProbMetric: 397.3510 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 213/1000
2023-10-03 16:10:43.098 
Epoch 213/1000 
	 loss: 386.4987, MinusLogProbMetric: 386.4987, val_loss: 397.5904, val_MinusLogProbMetric: 397.5904

Epoch 213: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.4987 - MinusLogProbMetric: 386.4987 - val_loss: 397.5904 - val_MinusLogProbMetric: 397.5904 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 214/1000
2023-10-03 16:10:53.243 
Epoch 214/1000 
	 loss: 386.8078, MinusLogProbMetric: 386.8078, val_loss: 397.8540, val_MinusLogProbMetric: 397.8540

Epoch 214: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.8078 - MinusLogProbMetric: 386.8078 - val_loss: 397.8540 - val_MinusLogProbMetric: 397.8540 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 215/1000
2023-10-03 16:11:03.513 
Epoch 215/1000 
	 loss: 386.5167, MinusLogProbMetric: 386.5167, val_loss: 396.6045, val_MinusLogProbMetric: 396.6045

Epoch 215: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5167 - MinusLogProbMetric: 386.5167 - val_loss: 396.6045 - val_MinusLogProbMetric: 396.6045 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 216/1000
2023-10-03 16:11:13.779 
Epoch 216/1000 
	 loss: 386.5404, MinusLogProbMetric: 386.5404, val_loss: 396.8840, val_MinusLogProbMetric: 396.8840

Epoch 216: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5404 - MinusLogProbMetric: 386.5404 - val_loss: 396.8840 - val_MinusLogProbMetric: 396.8840 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 217/1000
2023-10-03 16:11:24.106 
Epoch 217/1000 
	 loss: 386.3077, MinusLogProbMetric: 386.3077, val_loss: 397.7033, val_MinusLogProbMetric: 397.7033

Epoch 217: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.3077 - MinusLogProbMetric: 386.3077 - val_loss: 397.7033 - val_MinusLogProbMetric: 397.7033 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 218/1000
2023-10-03 16:11:34.566 
Epoch 218/1000 
	 loss: 386.5015, MinusLogProbMetric: 386.5015, val_loss: 401.2795, val_MinusLogProbMetric: 401.2795

Epoch 218: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5015 - MinusLogProbMetric: 386.5015 - val_loss: 401.2795 - val_MinusLogProbMetric: 401.2795 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 219/1000
2023-10-03 16:11:44.854 
Epoch 219/1000 
	 loss: 386.6562, MinusLogProbMetric: 386.6562, val_loss: 397.5362, val_MinusLogProbMetric: 397.5362

Epoch 219: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6562 - MinusLogProbMetric: 386.6562 - val_loss: 397.5362 - val_MinusLogProbMetric: 397.5362 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 220/1000
2023-10-03 16:11:55.204 
Epoch 220/1000 
	 loss: 386.5842, MinusLogProbMetric: 386.5842, val_loss: 397.4076, val_MinusLogProbMetric: 397.4076

Epoch 220: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5842 - MinusLogProbMetric: 386.5842 - val_loss: 397.4076 - val_MinusLogProbMetric: 397.4076 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 221/1000
2023-10-03 16:12:05.557 
Epoch 221/1000 
	 loss: 386.4824, MinusLogProbMetric: 386.4824, val_loss: 397.9840, val_MinusLogProbMetric: 397.9840

Epoch 221: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.4824 - MinusLogProbMetric: 386.4824 - val_loss: 397.9840 - val_MinusLogProbMetric: 397.9840 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 222/1000
2023-10-03 16:12:15.953 
Epoch 222/1000 
	 loss: 386.5252, MinusLogProbMetric: 386.5252, val_loss: 397.1347, val_MinusLogProbMetric: 397.1347

Epoch 222: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5252 - MinusLogProbMetric: 386.5252 - val_loss: 397.1347 - val_MinusLogProbMetric: 397.1347 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 223/1000
2023-10-03 16:12:26.159 
Epoch 223/1000 
	 loss: 386.2756, MinusLogProbMetric: 386.2756, val_loss: 396.8012, val_MinusLogProbMetric: 396.8012

Epoch 223: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.2756 - MinusLogProbMetric: 386.2756 - val_loss: 396.8012 - val_MinusLogProbMetric: 396.8012 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 224/1000
2023-10-03 16:12:36.424 
Epoch 224/1000 
	 loss: 386.3677, MinusLogProbMetric: 386.3677, val_loss: 396.6036, val_MinusLogProbMetric: 396.6036

Epoch 224: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.3677 - MinusLogProbMetric: 386.3677 - val_loss: 396.6036 - val_MinusLogProbMetric: 396.6036 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 225/1000
2023-10-03 16:12:46.864 
Epoch 225/1000 
	 loss: 386.3651, MinusLogProbMetric: 386.3651, val_loss: 396.7101, val_MinusLogProbMetric: 396.7101

Epoch 225: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.3651 - MinusLogProbMetric: 386.3651 - val_loss: 396.7101 - val_MinusLogProbMetric: 396.7101 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 226/1000
2023-10-03 16:12:57.164 
Epoch 226/1000 
	 loss: 386.6274, MinusLogProbMetric: 386.6274, val_loss: 397.4713, val_MinusLogProbMetric: 397.4713

Epoch 226: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6274 - MinusLogProbMetric: 386.6274 - val_loss: 397.4713 - val_MinusLogProbMetric: 397.4713 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 227/1000
2023-10-03 16:13:07.539 
Epoch 227/1000 
	 loss: 386.5607, MinusLogProbMetric: 386.5607, val_loss: 397.6413, val_MinusLogProbMetric: 397.6413

Epoch 227: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5607 - MinusLogProbMetric: 386.5607 - val_loss: 397.6413 - val_MinusLogProbMetric: 397.6413 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 228/1000
2023-10-03 16:13:17.774 
Epoch 228/1000 
	 loss: 386.4475, MinusLogProbMetric: 386.4475, val_loss: 396.6592, val_MinusLogProbMetric: 396.6592

Epoch 228: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.4475 - MinusLogProbMetric: 386.4475 - val_loss: 396.6592 - val_MinusLogProbMetric: 396.6592 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 229/1000
2023-10-03 16:13:27.975 
Epoch 229/1000 
	 loss: 386.5439, MinusLogProbMetric: 386.5439, val_loss: 396.9997, val_MinusLogProbMetric: 396.9997

Epoch 229: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5439 - MinusLogProbMetric: 386.5439 - val_loss: 396.9997 - val_MinusLogProbMetric: 396.9997 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 230/1000
2023-10-03 16:13:38.213 
Epoch 230/1000 
	 loss: 386.4338, MinusLogProbMetric: 386.4338, val_loss: 397.5158, val_MinusLogProbMetric: 397.5158

Epoch 230: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.4338 - MinusLogProbMetric: 386.4338 - val_loss: 397.5158 - val_MinusLogProbMetric: 397.5158 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 231/1000
2023-10-03 16:13:48.650 
Epoch 231/1000 
	 loss: 386.6739, MinusLogProbMetric: 386.6739, val_loss: 397.8040, val_MinusLogProbMetric: 397.8040

Epoch 231: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6739 - MinusLogProbMetric: 386.6739 - val_loss: 397.8040 - val_MinusLogProbMetric: 397.8040 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 232/1000
2023-10-03 16:13:59.097 
Epoch 232/1000 
	 loss: 386.3487, MinusLogProbMetric: 386.3487, val_loss: 397.0946, val_MinusLogProbMetric: 397.0946

Epoch 232: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.3487 - MinusLogProbMetric: 386.3487 - val_loss: 397.0946 - val_MinusLogProbMetric: 397.0946 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 233/1000
2023-10-03 16:14:09.354 
Epoch 233/1000 
	 loss: 386.6559, MinusLogProbMetric: 386.6559, val_loss: 398.3129, val_MinusLogProbMetric: 398.3129

Epoch 233: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.6559 - MinusLogProbMetric: 386.6559 - val_loss: 398.3129 - val_MinusLogProbMetric: 398.3129 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 234/1000
2023-10-03 16:14:19.715 
Epoch 234/1000 
	 loss: 386.1572, MinusLogProbMetric: 386.1572, val_loss: 398.3863, val_MinusLogProbMetric: 398.3863

Epoch 234: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.1572 - MinusLogProbMetric: 386.1572 - val_loss: 398.3863 - val_MinusLogProbMetric: 398.3863 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 235/1000
2023-10-03 16:14:30.039 
Epoch 235/1000 
	 loss: 386.4323, MinusLogProbMetric: 386.4323, val_loss: 397.2022, val_MinusLogProbMetric: 397.2022

Epoch 235: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.4323 - MinusLogProbMetric: 386.4323 - val_loss: 397.2022 - val_MinusLogProbMetric: 397.2022 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 236/1000
2023-10-03 16:14:40.333 
Epoch 236/1000 
	 loss: 386.4964, MinusLogProbMetric: 386.4964, val_loss: 397.9151, val_MinusLogProbMetric: 397.9151

Epoch 236: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.4964 - MinusLogProbMetric: 386.4964 - val_loss: 397.9151 - val_MinusLogProbMetric: 397.9151 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 237/1000
2023-10-03 16:14:50.583 
Epoch 237/1000 
	 loss: 386.3380, MinusLogProbMetric: 386.3380, val_loss: 397.1088, val_MinusLogProbMetric: 397.1088

Epoch 237: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.3380 - MinusLogProbMetric: 386.3380 - val_loss: 397.1088 - val_MinusLogProbMetric: 397.1088 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 238/1000
2023-10-03 16:15:00.898 
Epoch 238/1000 
	 loss: 386.2336, MinusLogProbMetric: 386.2336, val_loss: 397.5843, val_MinusLogProbMetric: 397.5843

Epoch 238: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.2336 - MinusLogProbMetric: 386.2336 - val_loss: 397.5843 - val_MinusLogProbMetric: 397.5843 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 239/1000
2023-10-03 16:15:11.135 
Epoch 239/1000 
	 loss: 386.7453, MinusLogProbMetric: 386.7453, val_loss: 397.8235, val_MinusLogProbMetric: 397.8235

Epoch 239: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.7453 - MinusLogProbMetric: 386.7453 - val_loss: 397.8235 - val_MinusLogProbMetric: 397.8235 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 240/1000
2023-10-03 16:15:21.283 
Epoch 240/1000 
	 loss: 386.5242, MinusLogProbMetric: 386.5242, val_loss: 397.1983, val_MinusLogProbMetric: 397.1983

Epoch 240: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5242 - MinusLogProbMetric: 386.5242 - val_loss: 397.1983 - val_MinusLogProbMetric: 397.1983 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 241/1000
2023-10-03 16:15:31.659 
Epoch 241/1000 
	 loss: 386.1621, MinusLogProbMetric: 386.1621, val_loss: 396.8841, val_MinusLogProbMetric: 396.8841

Epoch 241: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.1621 - MinusLogProbMetric: 386.1621 - val_loss: 396.8841 - val_MinusLogProbMetric: 396.8841 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 242/1000
2023-10-03 16:15:41.910 
Epoch 242/1000 
	 loss: 386.4869, MinusLogProbMetric: 386.4869, val_loss: 397.1812, val_MinusLogProbMetric: 397.1812

Epoch 242: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.4869 - MinusLogProbMetric: 386.4869 - val_loss: 397.1812 - val_MinusLogProbMetric: 397.1812 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 243/1000
2023-10-03 16:15:52.388 
Epoch 243/1000 
	 loss: 386.3619, MinusLogProbMetric: 386.3619, val_loss: 397.3537, val_MinusLogProbMetric: 397.3537

Epoch 243: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.3619 - MinusLogProbMetric: 386.3619 - val_loss: 397.3537 - val_MinusLogProbMetric: 397.3537 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 244/1000
2023-10-03 16:16:02.689 
Epoch 244/1000 
	 loss: 386.2664, MinusLogProbMetric: 386.2664, val_loss: 398.0656, val_MinusLogProbMetric: 398.0656

Epoch 244: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.2664 - MinusLogProbMetric: 386.2664 - val_loss: 398.0656 - val_MinusLogProbMetric: 398.0656 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 245/1000
2023-10-03 16:16:12.977 
Epoch 245/1000 
	 loss: 386.5246, MinusLogProbMetric: 386.5246, val_loss: 397.3786, val_MinusLogProbMetric: 397.3786

Epoch 245: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.5246 - MinusLogProbMetric: 386.5246 - val_loss: 397.3786 - val_MinusLogProbMetric: 397.3786 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 246/1000
2023-10-03 16:16:23.216 
Epoch 246/1000 
	 loss: 386.1729, MinusLogProbMetric: 386.1729, val_loss: 398.0004, val_MinusLogProbMetric: 398.0004

Epoch 246: val_loss did not improve from 395.91138
196/196 - 10s - loss: 386.1729 - MinusLogProbMetric: 386.1729 - val_loss: 398.0004 - val_MinusLogProbMetric: 398.0004 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 247/1000
2023-10-03 16:16:33.421 
Epoch 247/1000 
	 loss: 384.8477, MinusLogProbMetric: 384.8477, val_loss: 396.4877, val_MinusLogProbMetric: 396.4877

Epoch 247: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.8477 - MinusLogProbMetric: 384.8477 - val_loss: 396.4877 - val_MinusLogProbMetric: 396.4877 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 248/1000
2023-10-03 16:16:43.619 
Epoch 248/1000 
	 loss: 384.6708, MinusLogProbMetric: 384.6708, val_loss: 396.5146, val_MinusLogProbMetric: 396.5146

Epoch 248: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6708 - MinusLogProbMetric: 384.6708 - val_loss: 396.5146 - val_MinusLogProbMetric: 396.5146 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 249/1000
2023-10-03 16:16:53.890 
Epoch 249/1000 
	 loss: 384.6246, MinusLogProbMetric: 384.6246, val_loss: 396.8627, val_MinusLogProbMetric: 396.8627

Epoch 249: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6246 - MinusLogProbMetric: 384.6246 - val_loss: 396.8627 - val_MinusLogProbMetric: 396.8627 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 250/1000
2023-10-03 16:17:04.029 
Epoch 250/1000 
	 loss: 384.6450, MinusLogProbMetric: 384.6450, val_loss: 396.4818, val_MinusLogProbMetric: 396.4818

Epoch 250: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6450 - MinusLogProbMetric: 384.6450 - val_loss: 396.4818 - val_MinusLogProbMetric: 396.4818 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 251/1000
2023-10-03 16:17:14.336 
Epoch 251/1000 
	 loss: 384.5924, MinusLogProbMetric: 384.5924, val_loss: 396.5212, val_MinusLogProbMetric: 396.5212

Epoch 251: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5924 - MinusLogProbMetric: 384.5924 - val_loss: 396.5212 - val_MinusLogProbMetric: 396.5212 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 252/1000
2023-10-03 16:17:24.383 
Epoch 252/1000 
	 loss: 384.5772, MinusLogProbMetric: 384.5772, val_loss: 396.6565, val_MinusLogProbMetric: 396.6565

Epoch 252: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5772 - MinusLogProbMetric: 384.5772 - val_loss: 396.6565 - val_MinusLogProbMetric: 396.6565 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 253/1000
2023-10-03 16:17:34.469 
Epoch 253/1000 
	 loss: 384.6479, MinusLogProbMetric: 384.6479, val_loss: 396.6546, val_MinusLogProbMetric: 396.6546

Epoch 253: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6479 - MinusLogProbMetric: 384.6479 - val_loss: 396.6546 - val_MinusLogProbMetric: 396.6546 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 254/1000
2023-10-03 16:17:44.499 
Epoch 254/1000 
	 loss: 384.5972, MinusLogProbMetric: 384.5972, val_loss: 396.2217, val_MinusLogProbMetric: 396.2217

Epoch 254: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5972 - MinusLogProbMetric: 384.5972 - val_loss: 396.2217 - val_MinusLogProbMetric: 396.2217 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 255/1000
2023-10-03 16:17:54.996 
Epoch 255/1000 
	 loss: 384.7141, MinusLogProbMetric: 384.7141, val_loss: 396.5504, val_MinusLogProbMetric: 396.5504

Epoch 255: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.7141 - MinusLogProbMetric: 384.7141 - val_loss: 396.5504 - val_MinusLogProbMetric: 396.5504 - lr: 2.7778e-05 - 10s/epoch - 54ms/step
Epoch 256/1000
2023-10-03 16:18:05.676 
Epoch 256/1000 
	 loss: 384.5735, MinusLogProbMetric: 384.5735, val_loss: 396.6031, val_MinusLogProbMetric: 396.6031

Epoch 256: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.5735 - MinusLogProbMetric: 384.5735 - val_loss: 396.6031 - val_MinusLogProbMetric: 396.6031 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 257/1000
2023-10-03 16:18:16.452 
Epoch 257/1000 
	 loss: 384.6775, MinusLogProbMetric: 384.6775, val_loss: 396.7299, val_MinusLogProbMetric: 396.7299

Epoch 257: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.6775 - MinusLogProbMetric: 384.6775 - val_loss: 396.7299 - val_MinusLogProbMetric: 396.7299 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 258/1000
2023-10-03 16:18:26.936 
Epoch 258/1000 
	 loss: 384.6072, MinusLogProbMetric: 384.6072, val_loss: 396.4808, val_MinusLogProbMetric: 396.4808

Epoch 258: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6072 - MinusLogProbMetric: 384.6072 - val_loss: 396.4808 - val_MinusLogProbMetric: 396.4808 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 259/1000
2023-10-03 16:18:37.492 
Epoch 259/1000 
	 loss: 384.6330, MinusLogProbMetric: 384.6330, val_loss: 396.7621, val_MinusLogProbMetric: 396.7621

Epoch 259: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.6330 - MinusLogProbMetric: 384.6330 - val_loss: 396.7621 - val_MinusLogProbMetric: 396.7621 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 260/1000
2023-10-03 16:18:48.127 
Epoch 260/1000 
	 loss: 384.6147, MinusLogProbMetric: 384.6147, val_loss: 396.4143, val_MinusLogProbMetric: 396.4143

Epoch 260: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.6147 - MinusLogProbMetric: 384.6147 - val_loss: 396.4143 - val_MinusLogProbMetric: 396.4143 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 261/1000
2023-10-03 16:18:58.547 
Epoch 261/1000 
	 loss: 384.5545, MinusLogProbMetric: 384.5545, val_loss: 396.4179, val_MinusLogProbMetric: 396.4179

Epoch 261: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5545 - MinusLogProbMetric: 384.5545 - val_loss: 396.4179 - val_MinusLogProbMetric: 396.4179 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 262/1000
2023-10-03 16:19:09.181 
Epoch 262/1000 
	 loss: 384.5896, MinusLogProbMetric: 384.5896, val_loss: 396.8983, val_MinusLogProbMetric: 396.8983

Epoch 262: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.5896 - MinusLogProbMetric: 384.5896 - val_loss: 396.8983 - val_MinusLogProbMetric: 396.8983 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 263/1000
2023-10-03 16:19:19.462 
Epoch 263/1000 
	 loss: 384.5860, MinusLogProbMetric: 384.5860, val_loss: 396.6468, val_MinusLogProbMetric: 396.6468

Epoch 263: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5860 - MinusLogProbMetric: 384.5860 - val_loss: 396.6468 - val_MinusLogProbMetric: 396.6468 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 264/1000
2023-10-03 16:19:30.472 
Epoch 264/1000 
	 loss: 384.6007, MinusLogProbMetric: 384.6007, val_loss: 396.7841, val_MinusLogProbMetric: 396.7841

Epoch 264: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.6007 - MinusLogProbMetric: 384.6007 - val_loss: 396.7841 - val_MinusLogProbMetric: 396.7841 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 265/1000
2023-10-03 16:19:40.957 
Epoch 265/1000 
	 loss: 384.5194, MinusLogProbMetric: 384.5194, val_loss: 396.4882, val_MinusLogProbMetric: 396.4882

Epoch 265: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5194 - MinusLogProbMetric: 384.5194 - val_loss: 396.4882 - val_MinusLogProbMetric: 396.4882 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 266/1000
2023-10-03 16:19:51.405 
Epoch 266/1000 
	 loss: 384.5099, MinusLogProbMetric: 384.5099, val_loss: 396.6364, val_MinusLogProbMetric: 396.6364

Epoch 266: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5099 - MinusLogProbMetric: 384.5099 - val_loss: 396.6364 - val_MinusLogProbMetric: 396.6364 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 267/1000
2023-10-03 16:20:01.838 
Epoch 267/1000 
	 loss: 384.5540, MinusLogProbMetric: 384.5540, val_loss: 396.9888, val_MinusLogProbMetric: 396.9888

Epoch 267: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5540 - MinusLogProbMetric: 384.5540 - val_loss: 396.9888 - val_MinusLogProbMetric: 396.9888 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 268/1000
2023-10-03 16:20:12.326 
Epoch 268/1000 
	 loss: 384.5896, MinusLogProbMetric: 384.5896, val_loss: 397.0075, val_MinusLogProbMetric: 397.0075

Epoch 268: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5896 - MinusLogProbMetric: 384.5896 - val_loss: 397.0075 - val_MinusLogProbMetric: 397.0075 - lr: 2.7778e-05 - 10s/epoch - 54ms/step
Epoch 269/1000
2023-10-03 16:20:22.715 
Epoch 269/1000 
	 loss: 384.5540, MinusLogProbMetric: 384.5540, val_loss: 396.9075, val_MinusLogProbMetric: 396.9075

Epoch 269: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5540 - MinusLogProbMetric: 384.5540 - val_loss: 396.9075 - val_MinusLogProbMetric: 396.9075 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 270/1000
2023-10-03 16:20:33.254 
Epoch 270/1000 
	 loss: 384.5479, MinusLogProbMetric: 384.5479, val_loss: 396.6424, val_MinusLogProbMetric: 396.6424

Epoch 270: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.5479 - MinusLogProbMetric: 384.5479 - val_loss: 396.6424 - val_MinusLogProbMetric: 396.6424 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 271/1000
2023-10-03 16:20:43.647 
Epoch 271/1000 
	 loss: 384.4729, MinusLogProbMetric: 384.4729, val_loss: 396.5693, val_MinusLogProbMetric: 396.5693

Epoch 271: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.4729 - MinusLogProbMetric: 384.4729 - val_loss: 396.5693 - val_MinusLogProbMetric: 396.5693 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 272/1000
2023-10-03 16:20:53.987 
Epoch 272/1000 
	 loss: 384.5234, MinusLogProbMetric: 384.5234, val_loss: 396.7607, val_MinusLogProbMetric: 396.7607

Epoch 272: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5234 - MinusLogProbMetric: 384.5234 - val_loss: 396.7607 - val_MinusLogProbMetric: 396.7607 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 273/1000
2023-10-03 16:21:04.266 
Epoch 273/1000 
	 loss: 384.4894, MinusLogProbMetric: 384.4894, val_loss: 396.6802, val_MinusLogProbMetric: 396.6802

Epoch 273: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.4894 - MinusLogProbMetric: 384.4894 - val_loss: 396.6802 - val_MinusLogProbMetric: 396.6802 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 274/1000
2023-10-03 16:21:14.584 
Epoch 274/1000 
	 loss: 384.6242, MinusLogProbMetric: 384.6242, val_loss: 397.1917, val_MinusLogProbMetric: 397.1917

Epoch 274: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6242 - MinusLogProbMetric: 384.6242 - val_loss: 397.1917 - val_MinusLogProbMetric: 397.1917 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 275/1000
2023-10-03 16:21:25.065 
Epoch 275/1000 
	 loss: 384.7003, MinusLogProbMetric: 384.7003, val_loss: 396.8065, val_MinusLogProbMetric: 396.8065

Epoch 275: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.7003 - MinusLogProbMetric: 384.7003 - val_loss: 396.8065 - val_MinusLogProbMetric: 396.8065 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 276/1000
2023-10-03 16:21:35.596 
Epoch 276/1000 
	 loss: 384.5505, MinusLogProbMetric: 384.5505, val_loss: 397.0037, val_MinusLogProbMetric: 397.0037

Epoch 276: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.5505 - MinusLogProbMetric: 384.5505 - val_loss: 397.0037 - val_MinusLogProbMetric: 397.0037 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 277/1000
2023-10-03 16:21:46.148 
Epoch 277/1000 
	 loss: 384.7308, MinusLogProbMetric: 384.7308, val_loss: 396.2880, val_MinusLogProbMetric: 396.2880

Epoch 277: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.7308 - MinusLogProbMetric: 384.7308 - val_loss: 396.2880 - val_MinusLogProbMetric: 396.2880 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 278/1000
2023-10-03 16:21:56.967 
Epoch 278/1000 
	 loss: 384.5846, MinusLogProbMetric: 384.5846, val_loss: 396.5555, val_MinusLogProbMetric: 396.5555

Epoch 278: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.5846 - MinusLogProbMetric: 384.5846 - val_loss: 396.5555 - val_MinusLogProbMetric: 396.5555 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 279/1000
2023-10-03 16:22:07.506 
Epoch 279/1000 
	 loss: 384.4703, MinusLogProbMetric: 384.4703, val_loss: 396.6274, val_MinusLogProbMetric: 396.6274

Epoch 279: val_loss did not improve from 395.91138
196/196 - 11s - loss: 384.4703 - MinusLogProbMetric: 384.4703 - val_loss: 396.6274 - val_MinusLogProbMetric: 396.6274 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 280/1000
2023-10-03 16:22:17.892 
Epoch 280/1000 
	 loss: 384.5159, MinusLogProbMetric: 384.5159, val_loss: 396.7753, val_MinusLogProbMetric: 396.7753

Epoch 280: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5159 - MinusLogProbMetric: 384.5159 - val_loss: 396.7753 - val_MinusLogProbMetric: 396.7753 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 281/1000
2023-10-03 16:22:28.299 
Epoch 281/1000 
	 loss: 384.5135, MinusLogProbMetric: 384.5135, val_loss: 396.7421, val_MinusLogProbMetric: 396.7421

Epoch 281: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5135 - MinusLogProbMetric: 384.5135 - val_loss: 396.7421 - val_MinusLogProbMetric: 396.7421 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 282/1000
2023-10-03 16:22:38.588 
Epoch 282/1000 
	 loss: 384.4439, MinusLogProbMetric: 384.4439, val_loss: 396.6131, val_MinusLogProbMetric: 396.6131

Epoch 282: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.4439 - MinusLogProbMetric: 384.4439 - val_loss: 396.6131 - val_MinusLogProbMetric: 396.6131 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 283/1000
2023-10-03 16:22:48.965 
Epoch 283/1000 
	 loss: 384.4984, MinusLogProbMetric: 384.4984, val_loss: 396.7802, val_MinusLogProbMetric: 396.7802

Epoch 283: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.4984 - MinusLogProbMetric: 384.4984 - val_loss: 396.7802 - val_MinusLogProbMetric: 396.7802 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 284/1000
2023-10-03 16:22:59.307 
Epoch 284/1000 
	 loss: 384.5089, MinusLogProbMetric: 384.5089, val_loss: 397.0069, val_MinusLogProbMetric: 397.0069

Epoch 284: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5089 - MinusLogProbMetric: 384.5089 - val_loss: 397.0069 - val_MinusLogProbMetric: 397.0069 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 285/1000
2023-10-03 16:23:09.751 
Epoch 285/1000 
	 loss: 384.4100, MinusLogProbMetric: 384.4100, val_loss: 396.5027, val_MinusLogProbMetric: 396.5027

Epoch 285: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.4100 - MinusLogProbMetric: 384.4100 - val_loss: 396.5027 - val_MinusLogProbMetric: 396.5027 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 286/1000
2023-10-03 16:23:19.793 
Epoch 286/1000 
	 loss: 384.4087, MinusLogProbMetric: 384.4087, val_loss: 396.9481, val_MinusLogProbMetric: 396.9481

Epoch 286: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.4087 - MinusLogProbMetric: 384.4087 - val_loss: 396.9481 - val_MinusLogProbMetric: 396.9481 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 287/1000
2023-10-03 16:23:29.945 
Epoch 287/1000 
	 loss: 384.6475, MinusLogProbMetric: 384.6475, val_loss: 397.0035, val_MinusLogProbMetric: 397.0035

Epoch 287: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6475 - MinusLogProbMetric: 384.6475 - val_loss: 397.0035 - val_MinusLogProbMetric: 397.0035 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 288/1000
2023-10-03 16:23:40.586 
Epoch 288/1000 
	 loss: 384.6620, MinusLogProbMetric: 384.6620, val_loss: 397.2210, val_MinusLogProbMetric: 397.2210

Epoch 288: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6620 - MinusLogProbMetric: 384.6620 - val_loss: 397.2210 - val_MinusLogProbMetric: 397.2210 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 289/1000
2023-10-03 16:23:51.015 
Epoch 289/1000 
	 loss: 384.6695, MinusLogProbMetric: 384.6695, val_loss: 397.0610, val_MinusLogProbMetric: 397.0610

Epoch 289: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6695 - MinusLogProbMetric: 384.6695 - val_loss: 397.0610 - val_MinusLogProbMetric: 397.0610 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 290/1000
2023-10-03 16:24:01.511 
Epoch 290/1000 
	 loss: 384.6599, MinusLogProbMetric: 384.6599, val_loss: 396.4790, val_MinusLogProbMetric: 396.4790

Epoch 290: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6599 - MinusLogProbMetric: 384.6599 - val_loss: 396.4790 - val_MinusLogProbMetric: 396.4790 - lr: 2.7778e-05 - 10s/epoch - 54ms/step
Epoch 291/1000
2023-10-03 16:24:11.897 
Epoch 291/1000 
	 loss: 384.6555, MinusLogProbMetric: 384.6555, val_loss: 398.9482, val_MinusLogProbMetric: 398.9482

Epoch 291: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.6555 - MinusLogProbMetric: 384.6555 - val_loss: 398.9482 - val_MinusLogProbMetric: 398.9482 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 292/1000
2023-10-03 16:24:22.147 
Epoch 292/1000 
	 loss: 384.5632, MinusLogProbMetric: 384.5632, val_loss: 396.5483, val_MinusLogProbMetric: 396.5483

Epoch 292: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.5632 - MinusLogProbMetric: 384.5632 - val_loss: 396.5483 - val_MinusLogProbMetric: 396.5483 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 293/1000
2023-10-03 16:24:32.481 
Epoch 293/1000 
	 loss: 384.4066, MinusLogProbMetric: 384.4066, val_loss: 396.7020, val_MinusLogProbMetric: 396.7020

Epoch 293: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.4066 - MinusLogProbMetric: 384.4066 - val_loss: 396.7020 - val_MinusLogProbMetric: 396.7020 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 294/1000
2023-10-03 16:24:42.632 
Epoch 294/1000 
	 loss: 384.3654, MinusLogProbMetric: 384.3654, val_loss: 396.6632, val_MinusLogProbMetric: 396.6632

Epoch 294: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.3654 - MinusLogProbMetric: 384.3654 - val_loss: 396.6632 - val_MinusLogProbMetric: 396.6632 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 295/1000
2023-10-03 16:24:53.087 
Epoch 295/1000 
	 loss: 384.3374, MinusLogProbMetric: 384.3374, val_loss: 396.6069, val_MinusLogProbMetric: 396.6069

Epoch 295: val_loss did not improve from 395.91138
196/196 - 10s - loss: 384.3374 - MinusLogProbMetric: 384.3374 - val_loss: 396.6069 - val_MinusLogProbMetric: 396.6069 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 296/1000
2023-10-03 16:25:03.532 
Epoch 296/1000 
	 loss: 384.5261, MinusLogProbMetric: 384.5261, val_loss: 397.0901, val_MinusLogProbMetric: 397.0901

Epoch 296: val_loss did not improve from 395.91138
Restoring model weights from the end of the best epoch: 196.
196/196 - 11s - loss: 384.5261 - MinusLogProbMetric: 384.5261 - val_loss: 397.0901 - val_MinusLogProbMetric: 397.0901 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 296: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7fb9b77cadd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 5388.55151443393 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:6 out of the last 6 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7fb9b77c9120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 5541.023461353034 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7fb9b77c9120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 5492.447710692068 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
WARNING:tensorflow:6 out of the last 6 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7fb9b77c9240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 5579.662422748981 seconds.
Training succeeded with seed 869.
Model trained in 3121.48 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 22068.25 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 22068.51 s.
===========
Run 352/360 done in 26451.50 s.
===========

Directory ../../results/MAFN_new/run_353/ already exists.
Skipping it.
===========
Run 353/360 already exists. Skipping it.
===========

===========
Generating train data for run 354.
===========
Train data generated in 0.71 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_354/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_354/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_354
self.data_kwargs: {'seed': 926}
self.x_data: [[7.8542657  4.4182515  5.3175983  ... 3.8539157  8.104709   6.7665896 ]
 [8.06671    5.087758   5.108798   ... 2.7856822  7.9267573  7.1754713 ]
 [5.9009757  0.26195854 4.63319    ... 5.096487   6.4324465  2.782943  ]
 ...
 [5.7334824  0.57263243 4.688254   ... 4.731608   6.577419   3.9972575 ]
 [5.8251185  0.91118604 4.8986015  ... 4.9331145  6.9940166  4.170423  ]
 [5.469847   7.5352907  5.8636546  ... 8.857828   2.8293529  6.7579055 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_134 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  4509200   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,509,200
Trainable params: 4,509,200
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7fbac6c2d990>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbac6caee90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbac6caee90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb0817b940>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbad733b790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb0817a110>, <keras.callbacks.ModelCheckpoint object at 0x7fbad7356f50>, <keras.callbacks.EarlyStopping object at 0x7fbb08179fc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbad7354130>, <keras.callbacks.TerminateOnNaN object at 0x7fbb0817ac80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_354/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 354/360 with hyperparameters:
timestamp = 2023-10-03 22:32:54.253323
ndims = 1000
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 5
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 4509200
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.85426569e+00  4.41825151e+00  5.31759834e+00  2.60887671e+00
  6.41178656e+00  2.13207030e+00  5.74208021e+00  1.84043932e+00
  1.69763756e+00  4.54294538e+00  3.11069012e+00  2.37418079e+00
  2.54046345e+00  1.62041679e-01  9.34055448e-02  2.05885458e+00
  5.24263716e+00  7.68947506e+00  4.56616545e+00  9.68113995e+00
  2.26570651e-01  1.77447128e+00  3.29641533e+00  6.17277622e+00
  8.20737648e+00  3.43033528e+00  2.55136061e+00  5.97014785e-01
  8.28674316e+00  3.98282480e+00  5.93197346e+00  1.06245613e+01
  9.49379206e-01  1.63838947e+00  7.06365585e+00  7.94115591e+00
  7.53974438e+00  5.61496639e+00  9.80136871e+00  3.15971422e+00
  6.21875811e+00 -9.54353809e-03  4.26025915e+00  7.39404678e+00
  2.21483946e+00  9.84185696e+00  7.24155045e+00  3.30283046e+00
  7.74830770e+00  9.08916414e-01  9.71360779e+00  6.58768594e-01
  8.35239983e+00  2.32051826e+00  7.30240631e+00 -8.83946955e-01
  3.03193045e+00  7.32728100e+00  3.74522591e+00  6.38918638e-01
  3.79207659e+00  4.00098944e+00  5.66429996e+00  5.85767746e+00
  8.47701550e+00  5.04558372e+00  3.84120035e+00  1.24103403e+00
  7.94665098e+00  7.07438660e+00  3.71784091e+00  6.12205219e+00
  6.66771460e+00  2.12367511e+00 -9.60745335e-01  3.74312615e+00
  5.94952774e+00  4.78941059e+00  6.61200619e+00  8.09659863e+00
  4.87202358e+00  2.70784044e+00  9.03696346e+00  5.63844013e+00
  7.53536034e+00  7.15711594e+00  9.41573143e+00  2.42348766e+00
  7.42718220e+00  3.36646557e+00  3.73524284e+00  1.32205629e+00
  3.59988236e+00  9.48313904e+00  1.93991113e+00  2.75920010e+00
  5.01511669e+00  3.30604935e+00  5.06413269e+00  7.73147202e+00
  2.30829501e+00  4.39429140e+00  6.82420444e+00  6.79297352e+00
  9.61048603e+00  6.80823135e+00  7.26028967e+00  9.56877518e+00
  3.79452395e+00  4.15086555e+00 -2.60416776e-01  1.66208482e+00
  3.06919241e+00  9.93813455e-01  7.33946943e+00  5.69343281e+00
  5.63299084e+00  7.84667826e+00  2.26034689e+00  3.43404484e+00
  7.00420618e+00  3.05020475e+00 -7.25471079e-02  3.36938071e+00
  9.09626770e+00  4.11609411e+00  2.85297346e+00  9.84171486e+00
  6.31026506e+00  4.01078224e+00  1.21615183e+00  7.14538717e+00
  6.33219719e+00  9.81515121e+00  2.49839687e+00  6.05599999e-01
  7.24084187e+00  2.75359464e+00  2.50772285e+00  4.09680367e+00
  8.30523300e+00  6.01748800e+00  4.66583490e-01  4.52141905e+00
  5.89312696e+00  5.51575184e+00  5.69468737e+00  1.91655934e+00
  8.48398876e+00  5.63048315e+00  7.34936619e+00  9.74814224e+00
  3.48965788e+00  8.88919735e+00  9.60582942e-02  1.66400623e+00
  5.99077606e+00  3.11164737e+00  2.75282717e+00  2.60087824e+00
  2.64426851e+00  7.44998503e+00  5.85604477e+00  7.84842968e+00
  8.28961468e+00  8.40444946e+00  4.12819386e+00  4.58752203e+00
  9.98953819e+00  6.43158293e+00  5.55915296e-01  5.72953415e+00
  4.68271637e+00  9.90587902e+00 -8.83717835e-01  5.94018364e+00
  7.98547173e+00  2.84753513e+00  9.44544220e+00  9.53206062e+00
  9.07542324e+00  6.41640186e-01  6.17845011e+00  3.03050613e+00
  8.23334789e+00  6.30862665e+00  5.43327713e+00  1.29359305e+00
  3.25533032e+00  6.31515408e+00  1.03074417e+01  5.75671196e+00
  9.49255085e+00  9.50892544e+00  6.79545212e+00  4.71281195e+00
  3.50111294e+00  9.18763638e+00  4.49652076e-01  1.14515615e+00
  2.40005207e+00  7.48550272e+00  3.02852464e+00  6.13900280e+00
  7.07990456e+00  4.02328634e+00  5.32941818e+00  5.56215000e+00
  9.59676170e+00  6.84960365e+00  6.14611101e+00  7.01535404e-01
  9.08370018e+00  5.82812595e+00  7.74594402e+00  6.01755202e-01
  3.96469641e+00  2.35852933e+00  2.45156860e+00  8.29704666e+00
  6.53238392e+00  4.18872261e+00 -6.58399761e-01  9.65485573e+00
  9.80379200e+00  1.98804641e+00 -2.41938114e-01  5.28435040e+00
  9.89102745e+00  8.85664177e+00  4.39452076e+00  3.62702298e+00
  2.62890053e+00  8.97751904e+00  6.36042309e+00  1.33491313e+00
  2.77312636e+00  2.77005053e+00  1.88997674e+00  5.14272547e+00
  8.35700703e+00  6.68333673e+00  1.91080523e+00 -7.04806745e-01
  1.96866465e+00  3.76307774e+00  7.95638275e+00  9.92790127e+00
  3.73964596e+00  7.23839140e+00  9.96518135e+00  7.50328255e+00
  6.60035801e+00  8.07895303e-01  3.62572813e+00  9.14682579e+00
  2.86480689e+00  8.59156322e+00  7.98565483e+00  4.01899433e+00
  4.55118990e+00  5.28791046e+00  7.33071327e+00  5.27818727e+00
  8.85012031e-01  7.58879042e+00  3.19855785e+00  9.85190296e+00
  2.27252841e+00  2.54628229e+00  2.68598413e+00  5.91966152e+00
  8.05706692e+00  1.65711355e+00  1.67708433e+00  6.08699369e+00
  4.28031015e+00  5.60096979e+00  6.40451479e+00  1.06371555e+01
  5.36539984e+00  6.85709620e+00  8.92924786e+00  3.32924414e+00
  2.92118073e+00  6.10644913e+00  5.32210112e+00  9.56133556e+00
  4.79739636e-01  9.31261921e+00  5.15149164e+00  1.02548325e+00
  2.37651587e+00  6.48900795e+00  4.59978104e+00  9.12724590e+00
  4.42423201e+00  4.07489395e+00  8.31999683e+00  5.35771799e+00
  6.72994852e+00  7.68310690e+00  6.69467068e+00  4.67777395e+00
  6.99015021e-01  4.14359522e+00  8.36619568e+00  3.67691922e+00
  9.04692936e+00  4.49189425e+00  6.32296133e+00  8.04836750e+00
  4.41118336e+00  8.30536187e-01  9.79877853e+00  2.71371508e+00
  5.12246799e+00  4.35286856e+00  5.49793339e+00  3.22211933e+00
  7.62125874e+00  1.31961989e+00  1.10699387e+01  8.59894085e+00
  5.24368238e+00  4.61356401e+00  3.45124292e+00  5.36580896e+00
  8.41000843e+00  2.77613783e+00  4.29664612e+00  1.90059173e+00
  3.43093586e+00  2.01634741e+00  1.07569265e+01  5.71786928e+00
  7.56941891e+00  1.09229112e+00  2.52849770e+00  8.91465664e+00
  9.52245593e-01  1.17926216e+00  6.83719635e+00  6.82608938e+00
  7.53233051e+00  1.01758318e+01  1.57473803e+00  4.45485210e+00
  4.28795290e+00  1.64892748e-01  1.81490600e+00  2.76026893e+00
  4.74909878e+00  3.11362815e+00  8.00123119e+00  3.19517183e+00
  9.50560093e+00  1.03588438e+01  4.30221748e+00  6.96166873e-01
  3.03467369e+00  7.79732800e+00  2.67352748e+00  3.50514221e+00
  5.08668947e+00  7.38722420e+00  9.63334274e+00  2.62670517e+00
  4.05484772e+00  2.31513214e+00  2.64107323e+00  2.18680382e-01
  4.29812479e+00  8.41921329e+00  4.19078636e+00  3.98049355e+00
  8.93853092e+00  7.08515024e+00  5.22109699e+00  7.64430141e+00
  2.03226995e+00  3.94330883e+00  1.36629438e+00  2.33601141e+00
  3.89655709e-01  3.61157084e+00  4.44274712e+00  9.43988323e+00
  2.50003672e+00  5.12204170e+00  8.95978451e+00  7.98188019e+00
  7.24716902e+00  2.88545752e+00  4.57083321e+00  2.54753900e+00
  1.84977734e+00  4.41161156e+00  7.37936544e+00  5.14107323e+00
  7.78119135e+00  8.40401840e+00  1.04103088e-02  5.93060875e+00
  5.76526022e+00  9.53269196e+00  7.67608523e-01  6.57992554e+00
  1.95678222e+00  5.49429357e-01  5.23544788e+00  7.59472132e+00
  7.29799318e+00 -1.20772636e+00  4.30894661e+00  4.65965390e-01
  2.60425591e+00  1.97165930e+00  1.86237991e+00  2.43441749e+00
  1.47088110e+00  9.78774834e+00  3.94406772e+00  8.90942097e+00
  1.22915721e+00  7.60721827e+00  8.06412756e-01  4.82014179e+00
  5.46107650e-01  7.98665380e+00  9.84689426e+00  5.38293898e-01
  8.54449463e+00  8.00841713e+00  3.39391780e+00  9.08403206e+00
  2.01289654e+00  4.27781343e+00  9.96862292e-01  6.87161112e+00
 -9.90051091e-01  4.79714537e+00  6.87879229e+00  8.84089375e+00
  2.07931614e+00  8.38031006e+00  5.88058186e+00  5.93655205e+00
  5.64989281e+00  6.21854544e-01  6.68466759e+00  4.26185799e+00
  8.59449577e+00  3.42455196e+00  5.77914000e+00  4.20325327e+00
  8.32858276e+00  2.54622722e+00  3.04043859e-01  8.33305359e+00
  4.29302406e+00  5.85027218e+00  3.44100654e-01  1.02574406e+01
  9.13591194e+00  6.44509363e+00  2.23200083e+00  3.65651631e+00
  9.79177284e+00  6.80626631e+00  1.36182153e+00  4.29894447e+00
  9.91862965e+00  6.15662336e+00  8.39294195e-02  5.73490715e+00
  3.36321139e+00  9.89277542e-01  4.06990290e+00  4.61008358e+00
 -7.16332495e-01  6.55716562e+00  5.19925833e+00  7.17309570e+00
  4.03727007e+00  5.57426548e+00  6.92208004e+00  8.72025681e+00
  3.84482765e+00  6.02073288e+00  7.86660480e+00 -5.73046148e-01
  7.67636967e+00  3.58273864e+00  7.47981644e+00  5.36535072e+00
  7.16784859e+00  7.85914564e+00  1.76731563e+00  6.72368050e+00
  8.51148510e+00  1.61090291e+00  6.08754253e+00  6.35778236e+00
  9.91369438e+00  2.55167031e+00  1.66099370e+00  3.64644337e+00
  2.42382288e+00  7.45638466e+00  7.33168173e+00  6.45663881e+00
  2.01684594e+00  8.60406494e+00  1.10975132e+01  5.38561678e+00
  5.44827414e+00  4.94278336e+00  1.86474085e+00  1.97823441e+00
  4.30962515e+00  1.54604924e+00  9.07867241e+00  7.96630621e+00
  5.26732063e+00  3.58175659e+00  9.51251125e+00  1.04863870e+00
  5.07879162e+00  1.05271053e+00  1.87499964e+00  6.65355206e+00
  6.66385412e+00  6.33094311e+00  8.71027946e+00  9.82982922e+00
  6.55875683e+00  5.20563507e+00  8.00854397e+00  2.41466331e+00
  7.00420952e+00  4.88513613e+00  1.61634290e+00  9.31071377e+00
  3.35792899e+00  5.86867237e+00  8.60531521e+00  5.60204506e+00
  1.54713285e+00  7.74277306e+00  3.66394973e+00  9.42229176e+00
  8.48596764e+00  6.48666763e+00  3.23076034e+00  7.68814230e+00
  9.35188389e+00  4.08446217e+00  5.02816010e+00  3.38081270e-01
  6.36648226e+00  8.51817703e+00  2.80446982e+00  3.20807362e+00
  4.51574850e+00  5.93633711e-01  9.46782529e-01  2.16139364e+00
  2.60937262e+00  5.28547049e+00  4.84176254e+00  9.81929684e+00
  7.88902140e+00  1.66553545e+00  8.82484341e+00  7.09167838e-01
  5.03990793e+00  2.42374206e+00  2.91808867e+00  7.50122595e+00
  4.18691397e+00  9.02093601e+00  7.10776806e+00  9.57430172e+00
  5.59825611e+00  8.02043629e+00  3.25235939e+00  4.47115898e+00
  4.30808973e+00  3.80606079e+00  1.12467217e+00  5.29313755e+00
  1.50810623e+00  8.85228539e+00 -4.06661093e-01  5.10796404e+00
  3.78489470e+00  7.05098629e-01  7.91552544e+00  6.63939571e+00
  1.46795654e+00  6.65862942e+00  8.33073235e+00  8.50840759e+00
  1.30628014e+00  6.92401600e+00  7.72493744e+00  8.51644993e+00
  5.43707657e+00  4.83005619e+00  8.90700626e+00  8.11882019e+00
  5.73599482e+00  2.80633116e+00  9.65988350e+00  4.51777983e+00
  9.21329618e-01  4.88313866e+00  9.77910519e+00  4.40435410e+00
 -3.27194422e-01  6.12769270e+00  9.89487934e+00  7.31696033e+00
  6.35276508e+00  9.55050945e+00  8.07680893e+00  6.34605026e+00
  4.77183819e+00  7.15873861e+00  8.06319714e+00  5.61736882e-01
  7.67201567e+00  3.85464573e+00  6.69436502e+00  7.86791897e+00
  4.83945560e+00  1.62425935e+00  4.19299459e+00  9.26880896e-01
  6.88019085e+00  9.74193478e+00  8.84968662e+00  3.65187788e+00
  6.33452368e+00  3.75078416e+00  4.67366219e+00  3.54604197e+00
  8.16938019e+00  2.72105742e+00  9.75844955e+00  6.57792234e+00
  2.28564978e-01  4.99803901e-01  1.02100105e+01  8.88365149e-01
  3.53124762e+00  1.72824395e+00  9.85693932e+00  5.20375729e+00
  5.34282923e+00  6.05618429e+00  2.67098927e+00  5.57222414e+00
  4.29868984e+00  1.93069673e+00  6.95643902e+00  3.56284547e+00
  7.73937511e+00  4.35557556e+00  5.20385551e+00  7.79240131e+00
  4.75181484e+00  9.50519943e+00  8.01542187e+00  8.55453110e+00
  9.82915688e+00  3.75745511e+00  5.48529482e+00  7.18655014e+00
  6.00739861e+00  1.01745186e+01  2.86505723e+00  3.40535784e+00
  8.88479424e+00  7.66841078e+00  2.33434486e+00  6.26787615e+00
  7.08852863e+00  2.91594267e+00  5.48495722e+00  5.74615335e+00
  4.40000677e+00  7.55737638e+00 -8.34494978e-02  4.98475075e+00
  3.07788324e+00  5.51033068e+00  7.23143101e+00  7.69217253e-01
  1.29760706e+00  6.85467720e+00  7.40828228e+00  5.57670832e+00
  9.81540394e+00  5.34229517e+00  6.78031504e-01  6.85619688e+00
  5.78618479e+00  5.76328468e+00  5.48964214e+00  8.51025105e+00
  1.36436164e+00  1.25749195e+00  1.53151178e+00  2.31167674e+00
  5.93736410e+00  6.02586079e+00  2.10990143e+00  3.74104166e+00
  4.49994564e+00  4.30085993e+00  8.55859375e+00  4.00177526e+00
  6.96290779e+00  1.65811288e+00  5.44697285e+00  8.24122715e+00
  1.78438163e+00  4.33162642e+00  5.78468227e+00  2.05096912e+00
  7.39810944e+00  1.59008896e+00  8.76768684e+00  4.69711876e+00
  4.04790258e+00  1.24405193e+00  2.83710051e+00  6.33698320e+00
  1.33927023e+00  3.15551066e+00  9.67255783e+00  6.40811014e+00
  2.06277108e+00  2.24199677e+00  8.81563950e+00  3.05780077e+00
  1.87078035e+00  9.16118813e+00  3.53771424e+00  1.65168977e+00
  5.19690895e+00  4.59403133e+00  9.05195808e+00  1.09238386e+00
  1.04556847e+00  8.68545175e-01  9.68726254e+00  5.04982138e+00
  2.69507694e+00  5.98866844e+00  4.93495703e+00  2.08913898e+00
  6.94658279e+00  2.16514921e+00  2.17784786e+00  5.89404345e+00
  2.47186089e+00  3.07393289e+00  9.32638359e+00  1.50767267e+00
  3.44327784e+00  3.86774182e+00  2.78355002e+00  2.11643291e+00
  8.07786083e+00  3.65396214e+00 -2.35649347e-01  5.44787359e+00
  8.69826198e-01  2.77893639e+00  9.38866138e+00  3.60090685e+00
  5.78489542e+00  5.85346889e+00  2.58838773e-01  6.52211618e+00
  9.64319706e+00  8.25448573e-01  5.58079672e+00  7.69353580e+00
  8.50068855e+00  1.93690157e+00  2.22951698e+00  6.91337490e+00
  3.25202256e-01  7.09594727e+00  8.97951889e+00  4.25646448e+00
  4.87320185e+00  3.06530523e+00  6.00203633e-01  1.48462224e+00
  8.84535408e+00  8.94914532e+00  1.44960713e+00  9.55700684e+00
  3.84997725e+00  2.66661191e+00  9.85832453e-01  9.06340885e+00
  9.97899628e+00  3.32584810e+00  6.95174551e+00  3.19924808e+00
 -1.51607394e-01  4.97948170e+00  5.40087318e+00  1.11839056e+00
  8.18654537e+00  5.18570328e+00  2.13246346e+00  6.35160780e+00
  8.03942680e+00  3.31687546e+00  8.01695156e+00  4.56763649e+00
  9.25021172e+00  7.31373024e+00  3.17991471e+00  1.24928892e-01
  5.86790705e+00  2.48283291e+00  1.13595665e+00  9.80613613e+00
  1.05967150e+01  2.69507051e+00  3.01809096e+00  1.64316809e+00
  4.14577913e+00  8.58723545e+00  6.37656307e+00  8.65282822e+00
  1.23624945e+00  6.83144760e+00  9.12307358e+00  2.77780533e+00
  6.90577936e+00  3.74633145e+00  2.03699780e+00  9.43050480e+00
  3.88212705e+00  1.03552914e+00  3.94930816e+00  3.95189595e+00
  7.43439579e+00  8.88795757e+00  7.81051493e+00  9.53783226e+00
  4.64317465e+00  7.03867435e+00  3.24562931e+00  9.52776432e-01
  3.66223240e+00  7.14639282e+00  3.33262825e+00  7.86202145e+00
  7.74663401e+00  1.78269148e+00  2.58091092e+00  8.37615490e+00
  3.35916471e+00  3.20344758e+00  3.59131765e+00  8.01384544e+00
 -2.43629217e-02  7.42378175e-01  5.56940269e+00  4.90014553e+00
  6.48054647e+00  3.32681608e+00  9.08345699e+00  7.95236826e+00
  6.34802222e-01  6.87756348e+00  9.49590397e+00  1.01780138e+01
  7.24633646e+00  1.21293211e+00  6.68064976e+00  3.20778751e+00
  1.00395985e+01  7.21336079e+00  6.02273321e+00  6.93427706e+00
  4.42568588e+00  4.00593805e+00  9.61300564e+00  3.34010863e+00
  5.83148003e-01  4.11973238e+00  8.45203018e+00  6.38991165e+00
  1.04141502e+01  7.16550827e-01  8.68817711e+00  4.16127014e+00
  3.57046032e+00  2.75189900e+00  1.17473054e+00  3.15659904e+00
  7.35806704e+00  4.37037677e-01  1.24299979e+00 -1.54236555e-02
  8.57396030e+00  5.91643810e+00  1.83133376e+00  7.77049971e+00
  1.06167221e+01  9.25513077e+00  9.58188915e+00 -5.68707407e-01
  5.92560768e+00  2.55895495e+00  5.55885506e+00  2.23880720e+00
  1.15490997e+00  5.06085348e+00  5.79838181e+00  1.02773275e+01
 -8.31137300e-02  3.83537984e+00  8.54463768e+00  5.31149960e+00
  8.12730598e+00  6.55586195e+00  7.92252541e+00  5.34070778e+00
  2.95670152e+00  8.34237576e+00  5.68467236e+00  1.28557813e+00
  8.03099155e+00  9.61913395e+00  4.18909931e+00  3.39216042e+00
  6.58131719e-01  6.45680571e+00  8.95029449e+00  1.41747653e+00
  4.75601006e+00  2.90228367e-01  5.12265801e-01  6.72031784e+00
  8.77206898e+00  3.71332788e+00  8.48167229e+00  1.21904695e+00
  1.87795568e+00  9.97635460e+00  4.99329329e+00  6.38564539e+00
  2.54908800e-01  1.29785419e+00  5.73886812e-01  8.37132168e+00
  1.09254289e+00  8.28661823e+00  3.38485265e+00  7.73170853e+00
  4.09941435e+00  2.49423814e+00  1.25597119e+00  2.96604967e+00
  3.78301477e+00  8.97054768e+00  4.46885729e+00  9.85267830e+00
  5.88528252e+00 -5.40345371e-01  4.29398537e+00  7.70487690e+00
  1.43975925e+00  8.53642845e+00  5.70689344e+00 -4.84625697e-02
  7.89377451e+00  5.98359823e+00  8.12033236e-01  4.98253584e+00
  3.64299822e+00  9.16150284e+00  2.32410598e+00  9.28726101e+00
  5.53273964e+00  3.85391569e+00  8.10470867e+00  6.76658964e+00]
Epoch 1/1000
2023-10-03 22:33:15.217 
Epoch 1/1000 
	 loss: 1756.7766, MinusLogProbMetric: 1756.7766, val_loss: 643.6048, val_MinusLogProbMetric: 643.6048

Epoch 1: val_loss improved from inf to 643.60480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 21s - loss: 1756.7766 - MinusLogProbMetric: 1756.7766 - val_loss: 643.6048 - val_MinusLogProbMetric: 643.6048 - lr: 0.0010 - 21s/epoch - 107ms/step
Epoch 2/1000
2023-10-03 22:33:22.280 
Epoch 2/1000 
	 loss: 597.4961, MinusLogProbMetric: 597.4961, val_loss: 556.2346, val_MinusLogProbMetric: 556.2346

Epoch 2: val_loss improved from 643.60480 to 556.23462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 597.4961 - MinusLogProbMetric: 597.4961 - val_loss: 556.2346 - val_MinusLogProbMetric: 556.2346 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 3/1000
2023-10-03 22:33:29.135 
Epoch 3/1000 
	 loss: 568.4553, MinusLogProbMetric: 568.4553, val_loss: 526.7653, val_MinusLogProbMetric: 526.7653

Epoch 3: val_loss improved from 556.23462 to 526.76532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 568.4553 - MinusLogProbMetric: 568.4553 - val_loss: 526.7653 - val_MinusLogProbMetric: 526.7653 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 4/1000
2023-10-03 22:33:36.536 
Epoch 4/1000 
	 loss: 771.1385, MinusLogProbMetric: 771.1385, val_loss: 561.4177, val_MinusLogProbMetric: 561.4177

Epoch 4: val_loss did not improve from 526.76532
196/196 - 7s - loss: 771.1385 - MinusLogProbMetric: 771.1385 - val_loss: 561.4177 - val_MinusLogProbMetric: 561.4177 - lr: 0.0010 - 7s/epoch - 37ms/step
Epoch 5/1000
2023-10-03 22:33:43.772 
Epoch 5/1000 
	 loss: 544.7369, MinusLogProbMetric: 544.7369, val_loss: 497.2168, val_MinusLogProbMetric: 497.2168

Epoch 5: val_loss improved from 526.76532 to 497.21683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 544.7369 - MinusLogProbMetric: 544.7369 - val_loss: 497.2168 - val_MinusLogProbMetric: 497.2168 - lr: 0.0010 - 7s/epoch - 38ms/step
Epoch 6/1000
2023-10-03 22:33:51.302 
Epoch 6/1000 
	 loss: 485.5510, MinusLogProbMetric: 485.5510, val_loss: 478.7581, val_MinusLogProbMetric: 478.7581

Epoch 6: val_loss improved from 497.21683 to 478.75809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 485.5510 - MinusLogProbMetric: 485.5510 - val_loss: 478.7581 - val_MinusLogProbMetric: 478.7581 - lr: 0.0010 - 8s/epoch - 39ms/step
Epoch 7/1000
2023-10-03 22:33:58.964 
Epoch 7/1000 
	 loss: 472.2292, MinusLogProbMetric: 472.2292, val_loss: 468.3245, val_MinusLogProbMetric: 468.3245

Epoch 7: val_loss improved from 478.75809 to 468.32452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 472.2292 - MinusLogProbMetric: 472.2292 - val_loss: 468.3245 - val_MinusLogProbMetric: 468.3245 - lr: 0.0010 - 8s/epoch - 39ms/step
Epoch 8/1000
2023-10-03 22:34:06.226 
Epoch 8/1000 
	 loss: 468.6292, MinusLogProbMetric: 468.6292, val_loss: 459.1765, val_MinusLogProbMetric: 459.1765

Epoch 8: val_loss improved from 468.32452 to 459.17654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 468.6292 - MinusLogProbMetric: 468.6292 - val_loss: 459.1765 - val_MinusLogProbMetric: 459.1765 - lr: 0.0010 - 8s/epoch - 39ms/step
Epoch 9/1000
2023-10-03 22:34:14.163 
Epoch 9/1000 
	 loss: 458.4013, MinusLogProbMetric: 458.4013, val_loss: 457.6673, val_MinusLogProbMetric: 457.6673

Epoch 9: val_loss improved from 459.17654 to 457.66727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 458.4013 - MinusLogProbMetric: 458.4013 - val_loss: 457.6673 - val_MinusLogProbMetric: 457.6673 - lr: 0.0010 - 8s/epoch - 39ms/step
Epoch 10/1000
2023-10-03 22:34:21.761 
Epoch 10/1000 
	 loss: 461.8282, MinusLogProbMetric: 461.8282, val_loss: 474.3369, val_MinusLogProbMetric: 474.3369

Epoch 10: val_loss did not improve from 457.66727
196/196 - 7s - loss: 461.8282 - MinusLogProbMetric: 461.8282 - val_loss: 474.3369 - val_MinusLogProbMetric: 474.3369 - lr: 0.0010 - 7s/epoch - 38ms/step
Epoch 11/1000
2023-10-03 22:34:29.152 
Epoch 11/1000 
	 loss: 447.5042, MinusLogProbMetric: 447.5042, val_loss: 441.0163, val_MinusLogProbMetric: 441.0163

Epoch 11: val_loss improved from 457.66727 to 441.01630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 447.5042 - MinusLogProbMetric: 447.5042 - val_loss: 441.0163 - val_MinusLogProbMetric: 441.0163 - lr: 0.0010 - 8s/epoch - 39ms/step
Epoch 12/1000
2023-10-03 22:34:36.589 
Epoch 12/1000 
	 loss: 447.9739, MinusLogProbMetric: 447.9739, val_loss: 438.5415, val_MinusLogProbMetric: 438.5415

Epoch 12: val_loss improved from 441.01630 to 438.54150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 447.9739 - MinusLogProbMetric: 447.9739 - val_loss: 438.5415 - val_MinusLogProbMetric: 438.5415 - lr: 0.0010 - 7s/epoch - 38ms/step
Epoch 13/1000
2023-10-03 22:34:44.252 
Epoch 13/1000 
	 loss: 441.8381, MinusLogProbMetric: 441.8381, val_loss: 445.3476, val_MinusLogProbMetric: 445.3476

Epoch 13: val_loss did not improve from 438.54150
196/196 - 7s - loss: 441.8381 - MinusLogProbMetric: 441.8381 - val_loss: 445.3476 - val_MinusLogProbMetric: 445.3476 - lr: 0.0010 - 7s/epoch - 38ms/step
Epoch 14/1000
2023-10-03 22:34:51.611 
Epoch 14/1000 
	 loss: 440.1273, MinusLogProbMetric: 440.1273, val_loss: 436.4646, val_MinusLogProbMetric: 436.4646

Epoch 14: val_loss improved from 438.54150 to 436.46457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 440.1273 - MinusLogProbMetric: 440.1273 - val_loss: 436.4646 - val_MinusLogProbMetric: 436.4646 - lr: 0.0010 - 8s/epoch - 39ms/step
Epoch 15/1000
2023-10-03 22:34:59.088 
Epoch 15/1000 
	 loss: 439.9386, MinusLogProbMetric: 439.9386, val_loss: 442.9009, val_MinusLogProbMetric: 442.9009

Epoch 15: val_loss did not improve from 436.46457
196/196 - 7s - loss: 439.9386 - MinusLogProbMetric: 439.9386 - val_loss: 442.9009 - val_MinusLogProbMetric: 442.9009 - lr: 0.0010 - 7s/epoch - 37ms/step
Epoch 16/1000
2023-10-03 22:35:05.904 
Epoch 16/1000 
	 loss: 436.0322, MinusLogProbMetric: 436.0322, val_loss: 441.4371, val_MinusLogProbMetric: 441.4371

Epoch 16: val_loss did not improve from 436.46457
196/196 - 7s - loss: 436.0322 - MinusLogProbMetric: 436.0322 - val_loss: 441.4371 - val_MinusLogProbMetric: 441.4371 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 17/1000
2023-10-03 22:35:12.699 
Epoch 17/1000 
	 loss: 434.6093, MinusLogProbMetric: 434.6093, val_loss: 438.7769, val_MinusLogProbMetric: 438.7769

Epoch 17: val_loss did not improve from 436.46457
196/196 - 7s - loss: 434.6093 - MinusLogProbMetric: 434.6093 - val_loss: 438.7769 - val_MinusLogProbMetric: 438.7769 - lr: 0.0010 - 7s/epoch - 35ms/step
Epoch 18/1000
2023-10-03 22:35:19.826 
Epoch 18/1000 
	 loss: 436.3669, MinusLogProbMetric: 436.3669, val_loss: 436.9892, val_MinusLogProbMetric: 436.9892

Epoch 18: val_loss did not improve from 436.46457
196/196 - 7s - loss: 436.3669 - MinusLogProbMetric: 436.3669 - val_loss: 436.9892 - val_MinusLogProbMetric: 436.9892 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 19/1000
2023-10-03 22:35:26.992 
Epoch 19/1000 
	 loss: 432.8578, MinusLogProbMetric: 432.8578, val_loss: 435.3716, val_MinusLogProbMetric: 435.3716

Epoch 19: val_loss improved from 436.46457 to 435.37161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 432.8578 - MinusLogProbMetric: 432.8578 - val_loss: 435.3716 - val_MinusLogProbMetric: 435.3716 - lr: 0.0010 - 7s/epoch - 38ms/step
Epoch 20/1000
2023-10-03 22:35:34.017 
Epoch 20/1000 
	 loss: 429.1833, MinusLogProbMetric: 429.1833, val_loss: 427.5492, val_MinusLogProbMetric: 427.5492

Epoch 20: val_loss improved from 435.37161 to 427.54919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 429.1833 - MinusLogProbMetric: 429.1833 - val_loss: 427.5492 - val_MinusLogProbMetric: 427.5492 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 21/1000
2023-10-03 22:35:40.923 
Epoch 21/1000 
	 loss: 428.2751, MinusLogProbMetric: 428.2751, val_loss: 428.4462, val_MinusLogProbMetric: 428.4462

Epoch 21: val_loss did not improve from 427.54919
196/196 - 7s - loss: 428.2751 - MinusLogProbMetric: 428.2751 - val_loss: 428.4462 - val_MinusLogProbMetric: 428.4462 - lr: 0.0010 - 7s/epoch - 34ms/step
Epoch 22/1000
2023-10-03 22:35:47.703 
Epoch 22/1000 
	 loss: 426.9532, MinusLogProbMetric: 426.9532, val_loss: 424.9444, val_MinusLogProbMetric: 424.9444

Epoch 22: val_loss improved from 427.54919 to 424.94437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 426.9532 - MinusLogProbMetric: 426.9532 - val_loss: 424.9444 - val_MinusLogProbMetric: 424.9444 - lr: 0.0010 - 7s/epoch - 36ms/step
Epoch 23/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 32: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-03 22:35:50.007 
Epoch 23/1000 
	 loss: nan, MinusLogProbMetric: 93590.0391, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 23: val_loss did not improve from 424.94437
196/196 - 2s - loss: nan - MinusLogProbMetric: 93590.0391 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 2s/epoch - 10ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 354.
===========
Train data generated in 0.69 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_354/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_354/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_354
self.data_kwargs: {'seed': 926}
self.x_data: [[7.8542657  4.4182515  5.3175983  ... 3.8539157  8.104709   6.7665896 ]
 [8.06671    5.087758   5.108798   ... 2.7856822  7.9267573  7.1754713 ]
 [5.9009757  0.26195854 4.63319    ... 5.096487   6.4324465  2.782943  ]
 ...
 [5.7334824  0.57263243 4.688254   ... 4.731608   6.577419   3.9972575 ]
 [5.8251185  0.91118604 4.8986015  ... 4.9331145  6.9940166  4.170423  ]
 [5.469847   7.5352907  5.8636546  ... 8.857828   2.8293529  6.7579055 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_14"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_140 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  4509200   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,509,200
Trainable params: 4,509,200
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7fbb08404520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb22bbf280>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb22bbf280>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb2bfe92d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb2bc50100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb72c6d9600>, <keras.callbacks.ModelCheckpoint object at 0x7fb72c6d8910>, <keras.callbacks.EarlyStopping object at 0x7fb72c6dbf70>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb72c6dba90>, <keras.callbacks.TerminateOnNaN object at 0x7fb72c6db610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 354/360 with hyperparameters:
timestamp = 2023-10-03 22:35:51.945981
ndims = 1000
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 5
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 4509200
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.85426569e+00  4.41825151e+00  5.31759834e+00  2.60887671e+00
  6.41178656e+00  2.13207030e+00  5.74208021e+00  1.84043932e+00
  1.69763756e+00  4.54294538e+00  3.11069012e+00  2.37418079e+00
  2.54046345e+00  1.62041679e-01  9.34055448e-02  2.05885458e+00
  5.24263716e+00  7.68947506e+00  4.56616545e+00  9.68113995e+00
  2.26570651e-01  1.77447128e+00  3.29641533e+00  6.17277622e+00
  8.20737648e+00  3.43033528e+00  2.55136061e+00  5.97014785e-01
  8.28674316e+00  3.98282480e+00  5.93197346e+00  1.06245613e+01
  9.49379206e-01  1.63838947e+00  7.06365585e+00  7.94115591e+00
  7.53974438e+00  5.61496639e+00  9.80136871e+00  3.15971422e+00
  6.21875811e+00 -9.54353809e-03  4.26025915e+00  7.39404678e+00
  2.21483946e+00  9.84185696e+00  7.24155045e+00  3.30283046e+00
  7.74830770e+00  9.08916414e-01  9.71360779e+00  6.58768594e-01
  8.35239983e+00  2.32051826e+00  7.30240631e+00 -8.83946955e-01
  3.03193045e+00  7.32728100e+00  3.74522591e+00  6.38918638e-01
  3.79207659e+00  4.00098944e+00  5.66429996e+00  5.85767746e+00
  8.47701550e+00  5.04558372e+00  3.84120035e+00  1.24103403e+00
  7.94665098e+00  7.07438660e+00  3.71784091e+00  6.12205219e+00
  6.66771460e+00  2.12367511e+00 -9.60745335e-01  3.74312615e+00
  5.94952774e+00  4.78941059e+00  6.61200619e+00  8.09659863e+00
  4.87202358e+00  2.70784044e+00  9.03696346e+00  5.63844013e+00
  7.53536034e+00  7.15711594e+00  9.41573143e+00  2.42348766e+00
  7.42718220e+00  3.36646557e+00  3.73524284e+00  1.32205629e+00
  3.59988236e+00  9.48313904e+00  1.93991113e+00  2.75920010e+00
  5.01511669e+00  3.30604935e+00  5.06413269e+00  7.73147202e+00
  2.30829501e+00  4.39429140e+00  6.82420444e+00  6.79297352e+00
  9.61048603e+00  6.80823135e+00  7.26028967e+00  9.56877518e+00
  3.79452395e+00  4.15086555e+00 -2.60416776e-01  1.66208482e+00
  3.06919241e+00  9.93813455e-01  7.33946943e+00  5.69343281e+00
  5.63299084e+00  7.84667826e+00  2.26034689e+00  3.43404484e+00
  7.00420618e+00  3.05020475e+00 -7.25471079e-02  3.36938071e+00
  9.09626770e+00  4.11609411e+00  2.85297346e+00  9.84171486e+00
  6.31026506e+00  4.01078224e+00  1.21615183e+00  7.14538717e+00
  6.33219719e+00  9.81515121e+00  2.49839687e+00  6.05599999e-01
  7.24084187e+00  2.75359464e+00  2.50772285e+00  4.09680367e+00
  8.30523300e+00  6.01748800e+00  4.66583490e-01  4.52141905e+00
  5.89312696e+00  5.51575184e+00  5.69468737e+00  1.91655934e+00
  8.48398876e+00  5.63048315e+00  7.34936619e+00  9.74814224e+00
  3.48965788e+00  8.88919735e+00  9.60582942e-02  1.66400623e+00
  5.99077606e+00  3.11164737e+00  2.75282717e+00  2.60087824e+00
  2.64426851e+00  7.44998503e+00  5.85604477e+00  7.84842968e+00
  8.28961468e+00  8.40444946e+00  4.12819386e+00  4.58752203e+00
  9.98953819e+00  6.43158293e+00  5.55915296e-01  5.72953415e+00
  4.68271637e+00  9.90587902e+00 -8.83717835e-01  5.94018364e+00
  7.98547173e+00  2.84753513e+00  9.44544220e+00  9.53206062e+00
  9.07542324e+00  6.41640186e-01  6.17845011e+00  3.03050613e+00
  8.23334789e+00  6.30862665e+00  5.43327713e+00  1.29359305e+00
  3.25533032e+00  6.31515408e+00  1.03074417e+01  5.75671196e+00
  9.49255085e+00  9.50892544e+00  6.79545212e+00  4.71281195e+00
  3.50111294e+00  9.18763638e+00  4.49652076e-01  1.14515615e+00
  2.40005207e+00  7.48550272e+00  3.02852464e+00  6.13900280e+00
  7.07990456e+00  4.02328634e+00  5.32941818e+00  5.56215000e+00
  9.59676170e+00  6.84960365e+00  6.14611101e+00  7.01535404e-01
  9.08370018e+00  5.82812595e+00  7.74594402e+00  6.01755202e-01
  3.96469641e+00  2.35852933e+00  2.45156860e+00  8.29704666e+00
  6.53238392e+00  4.18872261e+00 -6.58399761e-01  9.65485573e+00
  9.80379200e+00  1.98804641e+00 -2.41938114e-01  5.28435040e+00
  9.89102745e+00  8.85664177e+00  4.39452076e+00  3.62702298e+00
  2.62890053e+00  8.97751904e+00  6.36042309e+00  1.33491313e+00
  2.77312636e+00  2.77005053e+00  1.88997674e+00  5.14272547e+00
  8.35700703e+00  6.68333673e+00  1.91080523e+00 -7.04806745e-01
  1.96866465e+00  3.76307774e+00  7.95638275e+00  9.92790127e+00
  3.73964596e+00  7.23839140e+00  9.96518135e+00  7.50328255e+00
  6.60035801e+00  8.07895303e-01  3.62572813e+00  9.14682579e+00
  2.86480689e+00  8.59156322e+00  7.98565483e+00  4.01899433e+00
  4.55118990e+00  5.28791046e+00  7.33071327e+00  5.27818727e+00
  8.85012031e-01  7.58879042e+00  3.19855785e+00  9.85190296e+00
  2.27252841e+00  2.54628229e+00  2.68598413e+00  5.91966152e+00
  8.05706692e+00  1.65711355e+00  1.67708433e+00  6.08699369e+00
  4.28031015e+00  5.60096979e+00  6.40451479e+00  1.06371555e+01
  5.36539984e+00  6.85709620e+00  8.92924786e+00  3.32924414e+00
  2.92118073e+00  6.10644913e+00  5.32210112e+00  9.56133556e+00
  4.79739636e-01  9.31261921e+00  5.15149164e+00  1.02548325e+00
  2.37651587e+00  6.48900795e+00  4.59978104e+00  9.12724590e+00
  4.42423201e+00  4.07489395e+00  8.31999683e+00  5.35771799e+00
  6.72994852e+00  7.68310690e+00  6.69467068e+00  4.67777395e+00
  6.99015021e-01  4.14359522e+00  8.36619568e+00  3.67691922e+00
  9.04692936e+00  4.49189425e+00  6.32296133e+00  8.04836750e+00
  4.41118336e+00  8.30536187e-01  9.79877853e+00  2.71371508e+00
  5.12246799e+00  4.35286856e+00  5.49793339e+00  3.22211933e+00
  7.62125874e+00  1.31961989e+00  1.10699387e+01  8.59894085e+00
  5.24368238e+00  4.61356401e+00  3.45124292e+00  5.36580896e+00
  8.41000843e+00  2.77613783e+00  4.29664612e+00  1.90059173e+00
  3.43093586e+00  2.01634741e+00  1.07569265e+01  5.71786928e+00
  7.56941891e+00  1.09229112e+00  2.52849770e+00  8.91465664e+00
  9.52245593e-01  1.17926216e+00  6.83719635e+00  6.82608938e+00
  7.53233051e+00  1.01758318e+01  1.57473803e+00  4.45485210e+00
  4.28795290e+00  1.64892748e-01  1.81490600e+00  2.76026893e+00
  4.74909878e+00  3.11362815e+00  8.00123119e+00  3.19517183e+00
  9.50560093e+00  1.03588438e+01  4.30221748e+00  6.96166873e-01
  3.03467369e+00  7.79732800e+00  2.67352748e+00  3.50514221e+00
  5.08668947e+00  7.38722420e+00  9.63334274e+00  2.62670517e+00
  4.05484772e+00  2.31513214e+00  2.64107323e+00  2.18680382e-01
  4.29812479e+00  8.41921329e+00  4.19078636e+00  3.98049355e+00
  8.93853092e+00  7.08515024e+00  5.22109699e+00  7.64430141e+00
  2.03226995e+00  3.94330883e+00  1.36629438e+00  2.33601141e+00
  3.89655709e-01  3.61157084e+00  4.44274712e+00  9.43988323e+00
  2.50003672e+00  5.12204170e+00  8.95978451e+00  7.98188019e+00
  7.24716902e+00  2.88545752e+00  4.57083321e+00  2.54753900e+00
  1.84977734e+00  4.41161156e+00  7.37936544e+00  5.14107323e+00
  7.78119135e+00  8.40401840e+00  1.04103088e-02  5.93060875e+00
  5.76526022e+00  9.53269196e+00  7.67608523e-01  6.57992554e+00
  1.95678222e+00  5.49429357e-01  5.23544788e+00  7.59472132e+00
  7.29799318e+00 -1.20772636e+00  4.30894661e+00  4.65965390e-01
  2.60425591e+00  1.97165930e+00  1.86237991e+00  2.43441749e+00
  1.47088110e+00  9.78774834e+00  3.94406772e+00  8.90942097e+00
  1.22915721e+00  7.60721827e+00  8.06412756e-01  4.82014179e+00
  5.46107650e-01  7.98665380e+00  9.84689426e+00  5.38293898e-01
  8.54449463e+00  8.00841713e+00  3.39391780e+00  9.08403206e+00
  2.01289654e+00  4.27781343e+00  9.96862292e-01  6.87161112e+00
 -9.90051091e-01  4.79714537e+00  6.87879229e+00  8.84089375e+00
  2.07931614e+00  8.38031006e+00  5.88058186e+00  5.93655205e+00
  5.64989281e+00  6.21854544e-01  6.68466759e+00  4.26185799e+00
  8.59449577e+00  3.42455196e+00  5.77914000e+00  4.20325327e+00
  8.32858276e+00  2.54622722e+00  3.04043859e-01  8.33305359e+00
  4.29302406e+00  5.85027218e+00  3.44100654e-01  1.02574406e+01
  9.13591194e+00  6.44509363e+00  2.23200083e+00  3.65651631e+00
  9.79177284e+00  6.80626631e+00  1.36182153e+00  4.29894447e+00
  9.91862965e+00  6.15662336e+00  8.39294195e-02  5.73490715e+00
  3.36321139e+00  9.89277542e-01  4.06990290e+00  4.61008358e+00
 -7.16332495e-01  6.55716562e+00  5.19925833e+00  7.17309570e+00
  4.03727007e+00  5.57426548e+00  6.92208004e+00  8.72025681e+00
  3.84482765e+00  6.02073288e+00  7.86660480e+00 -5.73046148e-01
  7.67636967e+00  3.58273864e+00  7.47981644e+00  5.36535072e+00
  7.16784859e+00  7.85914564e+00  1.76731563e+00  6.72368050e+00
  8.51148510e+00  1.61090291e+00  6.08754253e+00  6.35778236e+00
  9.91369438e+00  2.55167031e+00  1.66099370e+00  3.64644337e+00
  2.42382288e+00  7.45638466e+00  7.33168173e+00  6.45663881e+00
  2.01684594e+00  8.60406494e+00  1.10975132e+01  5.38561678e+00
  5.44827414e+00  4.94278336e+00  1.86474085e+00  1.97823441e+00
  4.30962515e+00  1.54604924e+00  9.07867241e+00  7.96630621e+00
  5.26732063e+00  3.58175659e+00  9.51251125e+00  1.04863870e+00
  5.07879162e+00  1.05271053e+00  1.87499964e+00  6.65355206e+00
  6.66385412e+00  6.33094311e+00  8.71027946e+00  9.82982922e+00
  6.55875683e+00  5.20563507e+00  8.00854397e+00  2.41466331e+00
  7.00420952e+00  4.88513613e+00  1.61634290e+00  9.31071377e+00
  3.35792899e+00  5.86867237e+00  8.60531521e+00  5.60204506e+00
  1.54713285e+00  7.74277306e+00  3.66394973e+00  9.42229176e+00
  8.48596764e+00  6.48666763e+00  3.23076034e+00  7.68814230e+00
  9.35188389e+00  4.08446217e+00  5.02816010e+00  3.38081270e-01
  6.36648226e+00  8.51817703e+00  2.80446982e+00  3.20807362e+00
  4.51574850e+00  5.93633711e-01  9.46782529e-01  2.16139364e+00
  2.60937262e+00  5.28547049e+00  4.84176254e+00  9.81929684e+00
  7.88902140e+00  1.66553545e+00  8.82484341e+00  7.09167838e-01
  5.03990793e+00  2.42374206e+00  2.91808867e+00  7.50122595e+00
  4.18691397e+00  9.02093601e+00  7.10776806e+00  9.57430172e+00
  5.59825611e+00  8.02043629e+00  3.25235939e+00  4.47115898e+00
  4.30808973e+00  3.80606079e+00  1.12467217e+00  5.29313755e+00
  1.50810623e+00  8.85228539e+00 -4.06661093e-01  5.10796404e+00
  3.78489470e+00  7.05098629e-01  7.91552544e+00  6.63939571e+00
  1.46795654e+00  6.65862942e+00  8.33073235e+00  8.50840759e+00
  1.30628014e+00  6.92401600e+00  7.72493744e+00  8.51644993e+00
  5.43707657e+00  4.83005619e+00  8.90700626e+00  8.11882019e+00
  5.73599482e+00  2.80633116e+00  9.65988350e+00  4.51777983e+00
  9.21329618e-01  4.88313866e+00  9.77910519e+00  4.40435410e+00
 -3.27194422e-01  6.12769270e+00  9.89487934e+00  7.31696033e+00
  6.35276508e+00  9.55050945e+00  8.07680893e+00  6.34605026e+00
  4.77183819e+00  7.15873861e+00  8.06319714e+00  5.61736882e-01
  7.67201567e+00  3.85464573e+00  6.69436502e+00  7.86791897e+00
  4.83945560e+00  1.62425935e+00  4.19299459e+00  9.26880896e-01
  6.88019085e+00  9.74193478e+00  8.84968662e+00  3.65187788e+00
  6.33452368e+00  3.75078416e+00  4.67366219e+00  3.54604197e+00
  8.16938019e+00  2.72105742e+00  9.75844955e+00  6.57792234e+00
  2.28564978e-01  4.99803901e-01  1.02100105e+01  8.88365149e-01
  3.53124762e+00  1.72824395e+00  9.85693932e+00  5.20375729e+00
  5.34282923e+00  6.05618429e+00  2.67098927e+00  5.57222414e+00
  4.29868984e+00  1.93069673e+00  6.95643902e+00  3.56284547e+00
  7.73937511e+00  4.35557556e+00  5.20385551e+00  7.79240131e+00
  4.75181484e+00  9.50519943e+00  8.01542187e+00  8.55453110e+00
  9.82915688e+00  3.75745511e+00  5.48529482e+00  7.18655014e+00
  6.00739861e+00  1.01745186e+01  2.86505723e+00  3.40535784e+00
  8.88479424e+00  7.66841078e+00  2.33434486e+00  6.26787615e+00
  7.08852863e+00  2.91594267e+00  5.48495722e+00  5.74615335e+00
  4.40000677e+00  7.55737638e+00 -8.34494978e-02  4.98475075e+00
  3.07788324e+00  5.51033068e+00  7.23143101e+00  7.69217253e-01
  1.29760706e+00  6.85467720e+00  7.40828228e+00  5.57670832e+00
  9.81540394e+00  5.34229517e+00  6.78031504e-01  6.85619688e+00
  5.78618479e+00  5.76328468e+00  5.48964214e+00  8.51025105e+00
  1.36436164e+00  1.25749195e+00  1.53151178e+00  2.31167674e+00
  5.93736410e+00  6.02586079e+00  2.10990143e+00  3.74104166e+00
  4.49994564e+00  4.30085993e+00  8.55859375e+00  4.00177526e+00
  6.96290779e+00  1.65811288e+00  5.44697285e+00  8.24122715e+00
  1.78438163e+00  4.33162642e+00  5.78468227e+00  2.05096912e+00
  7.39810944e+00  1.59008896e+00  8.76768684e+00  4.69711876e+00
  4.04790258e+00  1.24405193e+00  2.83710051e+00  6.33698320e+00
  1.33927023e+00  3.15551066e+00  9.67255783e+00  6.40811014e+00
  2.06277108e+00  2.24199677e+00  8.81563950e+00  3.05780077e+00
  1.87078035e+00  9.16118813e+00  3.53771424e+00  1.65168977e+00
  5.19690895e+00  4.59403133e+00  9.05195808e+00  1.09238386e+00
  1.04556847e+00  8.68545175e-01  9.68726254e+00  5.04982138e+00
  2.69507694e+00  5.98866844e+00  4.93495703e+00  2.08913898e+00
  6.94658279e+00  2.16514921e+00  2.17784786e+00  5.89404345e+00
  2.47186089e+00  3.07393289e+00  9.32638359e+00  1.50767267e+00
  3.44327784e+00  3.86774182e+00  2.78355002e+00  2.11643291e+00
  8.07786083e+00  3.65396214e+00 -2.35649347e-01  5.44787359e+00
  8.69826198e-01  2.77893639e+00  9.38866138e+00  3.60090685e+00
  5.78489542e+00  5.85346889e+00  2.58838773e-01  6.52211618e+00
  9.64319706e+00  8.25448573e-01  5.58079672e+00  7.69353580e+00
  8.50068855e+00  1.93690157e+00  2.22951698e+00  6.91337490e+00
  3.25202256e-01  7.09594727e+00  8.97951889e+00  4.25646448e+00
  4.87320185e+00  3.06530523e+00  6.00203633e-01  1.48462224e+00
  8.84535408e+00  8.94914532e+00  1.44960713e+00  9.55700684e+00
  3.84997725e+00  2.66661191e+00  9.85832453e-01  9.06340885e+00
  9.97899628e+00  3.32584810e+00  6.95174551e+00  3.19924808e+00
 -1.51607394e-01  4.97948170e+00  5.40087318e+00  1.11839056e+00
  8.18654537e+00  5.18570328e+00  2.13246346e+00  6.35160780e+00
  8.03942680e+00  3.31687546e+00  8.01695156e+00  4.56763649e+00
  9.25021172e+00  7.31373024e+00  3.17991471e+00  1.24928892e-01
  5.86790705e+00  2.48283291e+00  1.13595665e+00  9.80613613e+00
  1.05967150e+01  2.69507051e+00  3.01809096e+00  1.64316809e+00
  4.14577913e+00  8.58723545e+00  6.37656307e+00  8.65282822e+00
  1.23624945e+00  6.83144760e+00  9.12307358e+00  2.77780533e+00
  6.90577936e+00  3.74633145e+00  2.03699780e+00  9.43050480e+00
  3.88212705e+00  1.03552914e+00  3.94930816e+00  3.95189595e+00
  7.43439579e+00  8.88795757e+00  7.81051493e+00  9.53783226e+00
  4.64317465e+00  7.03867435e+00  3.24562931e+00  9.52776432e-01
  3.66223240e+00  7.14639282e+00  3.33262825e+00  7.86202145e+00
  7.74663401e+00  1.78269148e+00  2.58091092e+00  8.37615490e+00
  3.35916471e+00  3.20344758e+00  3.59131765e+00  8.01384544e+00
 -2.43629217e-02  7.42378175e-01  5.56940269e+00  4.90014553e+00
  6.48054647e+00  3.32681608e+00  9.08345699e+00  7.95236826e+00
  6.34802222e-01  6.87756348e+00  9.49590397e+00  1.01780138e+01
  7.24633646e+00  1.21293211e+00  6.68064976e+00  3.20778751e+00
  1.00395985e+01  7.21336079e+00  6.02273321e+00  6.93427706e+00
  4.42568588e+00  4.00593805e+00  9.61300564e+00  3.34010863e+00
  5.83148003e-01  4.11973238e+00  8.45203018e+00  6.38991165e+00
  1.04141502e+01  7.16550827e-01  8.68817711e+00  4.16127014e+00
  3.57046032e+00  2.75189900e+00  1.17473054e+00  3.15659904e+00
  7.35806704e+00  4.37037677e-01  1.24299979e+00 -1.54236555e-02
  8.57396030e+00  5.91643810e+00  1.83133376e+00  7.77049971e+00
  1.06167221e+01  9.25513077e+00  9.58188915e+00 -5.68707407e-01
  5.92560768e+00  2.55895495e+00  5.55885506e+00  2.23880720e+00
  1.15490997e+00  5.06085348e+00  5.79838181e+00  1.02773275e+01
 -8.31137300e-02  3.83537984e+00  8.54463768e+00  5.31149960e+00
  8.12730598e+00  6.55586195e+00  7.92252541e+00  5.34070778e+00
  2.95670152e+00  8.34237576e+00  5.68467236e+00  1.28557813e+00
  8.03099155e+00  9.61913395e+00  4.18909931e+00  3.39216042e+00
  6.58131719e-01  6.45680571e+00  8.95029449e+00  1.41747653e+00
  4.75601006e+00  2.90228367e-01  5.12265801e-01  6.72031784e+00
  8.77206898e+00  3.71332788e+00  8.48167229e+00  1.21904695e+00
  1.87795568e+00  9.97635460e+00  4.99329329e+00  6.38564539e+00
  2.54908800e-01  1.29785419e+00  5.73886812e-01  8.37132168e+00
  1.09254289e+00  8.28661823e+00  3.38485265e+00  7.73170853e+00
  4.09941435e+00  2.49423814e+00  1.25597119e+00  2.96604967e+00
  3.78301477e+00  8.97054768e+00  4.46885729e+00  9.85267830e+00
  5.88528252e+00 -5.40345371e-01  4.29398537e+00  7.70487690e+00
  1.43975925e+00  8.53642845e+00  5.70689344e+00 -4.84625697e-02
  7.89377451e+00  5.98359823e+00  8.12033236e-01  4.98253584e+00
  3.64299822e+00  9.16150284e+00  2.32410598e+00  9.28726101e+00
  5.53273964e+00  3.85391569e+00  8.10470867e+00  6.76658964e+00]
Epoch 1/1000
2023-10-03 22:36:12.362 
Epoch 1/1000 
	 loss: 450.0901, MinusLogProbMetric: 450.0901, val_loss: 414.2248, val_MinusLogProbMetric: 414.2248

Epoch 1: val_loss improved from inf to 414.22482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 20s - loss: 450.0901 - MinusLogProbMetric: 450.0901 - val_loss: 414.2248 - val_MinusLogProbMetric: 414.2248 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 2/1000
2023-10-03 22:36:19.428 
Epoch 2/1000 
	 loss: 412.3188, MinusLogProbMetric: 412.3188, val_loss: 414.1353, val_MinusLogProbMetric: 414.1353

Epoch 2: val_loss improved from 414.22482 to 414.13535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 412.3188 - MinusLogProbMetric: 412.3188 - val_loss: 414.1353 - val_MinusLogProbMetric: 414.1353 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 3/1000
2023-10-03 22:36:26.496 
Epoch 3/1000 
	 loss: 411.7958, MinusLogProbMetric: 411.7958, val_loss: 411.6948, val_MinusLogProbMetric: 411.6948

Epoch 3: val_loss improved from 414.13535 to 411.69482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 411.7958 - MinusLogProbMetric: 411.7958 - val_loss: 411.6948 - val_MinusLogProbMetric: 411.6948 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 4/1000
2023-10-03 22:36:33.405 
Epoch 4/1000 
	 loss: 411.5619, MinusLogProbMetric: 411.5619, val_loss: 411.0557, val_MinusLogProbMetric: 411.0557

Epoch 4: val_loss improved from 411.69482 to 411.05573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 411.5619 - MinusLogProbMetric: 411.5619 - val_loss: 411.0557 - val_MinusLogProbMetric: 411.0557 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 5/1000
2023-10-03 22:36:40.458 
Epoch 5/1000 
	 loss: 411.2655, MinusLogProbMetric: 411.2655, val_loss: 411.1402, val_MinusLogProbMetric: 411.1402

Epoch 5: val_loss did not improve from 411.05573
196/196 - 7s - loss: 411.2655 - MinusLogProbMetric: 411.2655 - val_loss: 411.1402 - val_MinusLogProbMetric: 411.1402 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 6/1000
2023-10-03 22:36:47.157 
Epoch 6/1000 
	 loss: 410.6729, MinusLogProbMetric: 410.6729, val_loss: 409.9260, val_MinusLogProbMetric: 409.9260

Epoch 6: val_loss improved from 411.05573 to 409.92603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 410.6729 - MinusLogProbMetric: 410.6729 - val_loss: 409.9260 - val_MinusLogProbMetric: 409.9260 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 7/1000
2023-10-03 22:36:54.093 
Epoch 7/1000 
	 loss: 410.9198, MinusLogProbMetric: 410.9198, val_loss: 409.4392, val_MinusLogProbMetric: 409.4392

Epoch 7: val_loss improved from 409.92603 to 409.43918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 410.9198 - MinusLogProbMetric: 410.9198 - val_loss: 409.4392 - val_MinusLogProbMetric: 409.4392 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 8/1000
2023-10-03 22:37:00.900 
Epoch 8/1000 
	 loss: 409.9716, MinusLogProbMetric: 409.9716, val_loss: 411.4151, val_MinusLogProbMetric: 411.4151

Epoch 8: val_loss did not improve from 409.43918
196/196 - 7s - loss: 409.9716 - MinusLogProbMetric: 409.9716 - val_loss: 411.4151 - val_MinusLogProbMetric: 411.4151 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 9/1000
2023-10-03 22:37:07.621 
Epoch 9/1000 
	 loss: 410.7113, MinusLogProbMetric: 410.7113, val_loss: 410.3503, val_MinusLogProbMetric: 410.3503

Epoch 9: val_loss did not improve from 409.43918
196/196 - 7s - loss: 410.7113 - MinusLogProbMetric: 410.7113 - val_loss: 410.3503 - val_MinusLogProbMetric: 410.3503 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 10/1000
2023-10-03 22:37:14.404 
Epoch 10/1000 
	 loss: 409.9552, MinusLogProbMetric: 409.9552, val_loss: 408.1007, val_MinusLogProbMetric: 408.1007

Epoch 10: val_loss improved from 409.43918 to 408.10074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 409.9552 - MinusLogProbMetric: 409.9552 - val_loss: 408.1007 - val_MinusLogProbMetric: 408.1007 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 11/1000
2023-10-03 22:37:21.453 
Epoch 11/1000 
	 loss: 408.8983, MinusLogProbMetric: 408.8983, val_loss: 409.8934, val_MinusLogProbMetric: 409.8934

Epoch 11: val_loss did not improve from 408.10074
196/196 - 7s - loss: 408.8983 - MinusLogProbMetric: 408.8983 - val_loss: 409.8934 - val_MinusLogProbMetric: 409.8934 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 12/1000
2023-10-03 22:37:28.147 
Epoch 12/1000 
	 loss: 408.8168, MinusLogProbMetric: 408.8168, val_loss: 408.9305, val_MinusLogProbMetric: 408.9305

Epoch 12: val_loss did not improve from 408.10074
196/196 - 7s - loss: 408.8168 - MinusLogProbMetric: 408.8168 - val_loss: 408.9305 - val_MinusLogProbMetric: 408.9305 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 13/1000
2023-10-03 22:37:34.847 
Epoch 13/1000 
	 loss: 408.8752, MinusLogProbMetric: 408.8752, val_loss: 408.3991, val_MinusLogProbMetric: 408.3991

Epoch 13: val_loss did not improve from 408.10074
196/196 - 7s - loss: 408.8752 - MinusLogProbMetric: 408.8752 - val_loss: 408.3991 - val_MinusLogProbMetric: 408.3991 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 14/1000
2023-10-03 22:37:41.639 
Epoch 14/1000 
	 loss: 407.9845, MinusLogProbMetric: 407.9845, val_loss: 405.8076, val_MinusLogProbMetric: 405.8076

Epoch 14: val_loss improved from 408.10074 to 405.80765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 407.9845 - MinusLogProbMetric: 407.9845 - val_loss: 405.8076 - val_MinusLogProbMetric: 405.8076 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 15/1000
2023-10-03 22:37:48.545 
Epoch 15/1000 
	 loss: 408.0288, MinusLogProbMetric: 408.0288, val_loss: 408.5687, val_MinusLogProbMetric: 408.5687

Epoch 15: val_loss did not improve from 405.80765
196/196 - 7s - loss: 408.0288 - MinusLogProbMetric: 408.0288 - val_loss: 408.5687 - val_MinusLogProbMetric: 408.5687 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 16/1000
2023-10-03 22:37:55.156 
Epoch 16/1000 
	 loss: 407.3692, MinusLogProbMetric: 407.3692, val_loss: 406.4589, val_MinusLogProbMetric: 406.4589

Epoch 16: val_loss did not improve from 405.80765
196/196 - 7s - loss: 407.3692 - MinusLogProbMetric: 407.3692 - val_loss: 406.4589 - val_MinusLogProbMetric: 406.4589 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 17/1000
2023-10-03 22:38:01.816 
Epoch 17/1000 
	 loss: 408.7412, MinusLogProbMetric: 408.7412, val_loss: 408.6922, val_MinusLogProbMetric: 408.6922

Epoch 17: val_loss did not improve from 405.80765
196/196 - 7s - loss: 408.7412 - MinusLogProbMetric: 408.7412 - val_loss: 408.6922 - val_MinusLogProbMetric: 408.6922 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 18/1000
2023-10-03 22:38:08.929 
Epoch 18/1000 
	 loss: 408.5246, MinusLogProbMetric: 408.5246, val_loss: 406.2372, val_MinusLogProbMetric: 406.2372

Epoch 18: val_loss did not improve from 405.80765
196/196 - 7s - loss: 408.5246 - MinusLogProbMetric: 408.5246 - val_loss: 406.2372 - val_MinusLogProbMetric: 406.2372 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 19/1000
2023-10-03 22:38:16.230 
Epoch 19/1000 
	 loss: 409.4830, MinusLogProbMetric: 409.4830, val_loss: 405.1788, val_MinusLogProbMetric: 405.1788

Epoch 19: val_loss improved from 405.80765 to 405.17883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 409.4830 - MinusLogProbMetric: 409.4830 - val_loss: 405.1788 - val_MinusLogProbMetric: 405.1788 - lr: 3.3333e-04 - 8s/epoch - 39ms/step
Epoch 20/1000
2023-10-03 22:38:23.907 
Epoch 20/1000 
	 loss: 406.5798, MinusLogProbMetric: 406.5798, val_loss: 412.8594, val_MinusLogProbMetric: 412.8594

Epoch 20: val_loss did not improve from 405.17883
196/196 - 7s - loss: 406.5798 - MinusLogProbMetric: 406.5798 - val_loss: 412.8594 - val_MinusLogProbMetric: 412.8594 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 21/1000
2023-10-03 22:38:30.709 
Epoch 21/1000 
	 loss: 405.2814, MinusLogProbMetric: 405.2814, val_loss: 407.5504, val_MinusLogProbMetric: 407.5504

Epoch 21: val_loss did not improve from 405.17883
196/196 - 7s - loss: 405.2814 - MinusLogProbMetric: 405.2814 - val_loss: 407.5504 - val_MinusLogProbMetric: 407.5504 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 22/1000
2023-10-03 22:38:37.432 
Epoch 22/1000 
	 loss: 405.8152, MinusLogProbMetric: 405.8152, val_loss: 407.9783, val_MinusLogProbMetric: 407.9783

Epoch 22: val_loss did not improve from 405.17883
196/196 - 7s - loss: 405.8152 - MinusLogProbMetric: 405.8152 - val_loss: 407.9783 - val_MinusLogProbMetric: 407.9783 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 23/1000
2023-10-03 22:38:44.153 
Epoch 23/1000 
	 loss: 406.1120, MinusLogProbMetric: 406.1120, val_loss: 409.7579, val_MinusLogProbMetric: 409.7579

Epoch 23: val_loss did not improve from 405.17883
196/196 - 7s - loss: 406.1120 - MinusLogProbMetric: 406.1120 - val_loss: 409.7579 - val_MinusLogProbMetric: 409.7579 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 24/1000
2023-10-03 22:38:50.892 
Epoch 24/1000 
	 loss: 404.9941, MinusLogProbMetric: 404.9941, val_loss: 404.0829, val_MinusLogProbMetric: 404.0829

Epoch 24: val_loss improved from 405.17883 to 404.08289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 404.9941 - MinusLogProbMetric: 404.9941 - val_loss: 404.0829 - val_MinusLogProbMetric: 404.0829 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 25/1000
2023-10-03 22:38:57.930 
Epoch 25/1000 
	 loss: 404.8722, MinusLogProbMetric: 404.8722, val_loss: 409.3229, val_MinusLogProbMetric: 409.3229

Epoch 25: val_loss did not improve from 404.08289
196/196 - 7s - loss: 404.8722 - MinusLogProbMetric: 404.8722 - val_loss: 409.3229 - val_MinusLogProbMetric: 409.3229 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 26/1000
2023-10-03 22:39:04.646 
Epoch 26/1000 
	 loss: 406.1881, MinusLogProbMetric: 406.1881, val_loss: 406.1034, val_MinusLogProbMetric: 406.1034

Epoch 26: val_loss did not improve from 404.08289
196/196 - 7s - loss: 406.1881 - MinusLogProbMetric: 406.1881 - val_loss: 406.1034 - val_MinusLogProbMetric: 406.1034 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 27/1000
2023-10-03 22:39:11.466 
Epoch 27/1000 
	 loss: 404.5942, MinusLogProbMetric: 404.5942, val_loss: 404.5508, val_MinusLogProbMetric: 404.5508

Epoch 27: val_loss did not improve from 404.08289
196/196 - 7s - loss: 404.5942 - MinusLogProbMetric: 404.5942 - val_loss: 404.5508 - val_MinusLogProbMetric: 404.5508 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 28/1000
2023-10-03 22:39:18.210 
Epoch 28/1000 
	 loss: 405.6866, MinusLogProbMetric: 405.6866, val_loss: 404.6468, val_MinusLogProbMetric: 404.6468

Epoch 28: val_loss did not improve from 404.08289
196/196 - 7s - loss: 405.6866 - MinusLogProbMetric: 405.6866 - val_loss: 404.6468 - val_MinusLogProbMetric: 404.6468 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 29/1000
2023-10-03 22:39:24.916 
Epoch 29/1000 
	 loss: 404.0925, MinusLogProbMetric: 404.0925, val_loss: 414.3957, val_MinusLogProbMetric: 414.3957

Epoch 29: val_loss did not improve from 404.08289
196/196 - 7s - loss: 404.0925 - MinusLogProbMetric: 404.0925 - val_loss: 414.3957 - val_MinusLogProbMetric: 414.3957 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 30/1000
2023-10-03 22:39:31.812 
Epoch 30/1000 
	 loss: 405.1196, MinusLogProbMetric: 405.1196, val_loss: 403.3607, val_MinusLogProbMetric: 403.3607

Epoch 30: val_loss improved from 404.08289 to 403.36069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 405.1196 - MinusLogProbMetric: 405.1196 - val_loss: 403.3607 - val_MinusLogProbMetric: 403.3607 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 31/1000
2023-10-03 22:39:38.823 
Epoch 31/1000 
	 loss: 403.9362, MinusLogProbMetric: 403.9362, val_loss: 406.8827, val_MinusLogProbMetric: 406.8827

Epoch 31: val_loss did not improve from 403.36069
196/196 - 7s - loss: 403.9362 - MinusLogProbMetric: 403.9362 - val_loss: 406.8827 - val_MinusLogProbMetric: 406.8827 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 32/1000
2023-10-03 22:39:45.481 
Epoch 32/1000 
	 loss: 403.4332, MinusLogProbMetric: 403.4332, val_loss: 402.7189, val_MinusLogProbMetric: 402.7189

Epoch 32: val_loss improved from 403.36069 to 402.71887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 403.4332 - MinusLogProbMetric: 403.4332 - val_loss: 402.7189 - val_MinusLogProbMetric: 402.7189 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 33/1000
2023-10-03 22:39:52.336 
Epoch 33/1000 
	 loss: 403.5205, MinusLogProbMetric: 403.5205, val_loss: 409.2217, val_MinusLogProbMetric: 409.2217

Epoch 33: val_loss did not improve from 402.71887
196/196 - 7s - loss: 403.5205 - MinusLogProbMetric: 403.5205 - val_loss: 409.2217 - val_MinusLogProbMetric: 409.2217 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 34/1000
2023-10-03 22:39:59.108 
Epoch 34/1000 
	 loss: 404.6595, MinusLogProbMetric: 404.6595, val_loss: 403.0711, val_MinusLogProbMetric: 403.0711

Epoch 34: val_loss did not improve from 402.71887
196/196 - 7s - loss: 404.6595 - MinusLogProbMetric: 404.6595 - val_loss: 403.0711 - val_MinusLogProbMetric: 403.0711 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 35/1000
2023-10-03 22:40:05.799 
Epoch 35/1000 
	 loss: 403.8418, MinusLogProbMetric: 403.8418, val_loss: 406.0440, val_MinusLogProbMetric: 406.0440

Epoch 35: val_loss did not improve from 402.71887
196/196 - 7s - loss: 403.8418 - MinusLogProbMetric: 403.8418 - val_loss: 406.0440 - val_MinusLogProbMetric: 406.0440 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 36/1000
2023-10-03 22:40:12.516 
Epoch 36/1000 
	 loss: 402.5998, MinusLogProbMetric: 402.5998, val_loss: 402.8066, val_MinusLogProbMetric: 402.8066

Epoch 36: val_loss did not improve from 402.71887
196/196 - 7s - loss: 402.5998 - MinusLogProbMetric: 402.5998 - val_loss: 402.8066 - val_MinusLogProbMetric: 402.8066 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 37/1000
2023-10-03 22:40:19.314 
Epoch 37/1000 
	 loss: 404.6591, MinusLogProbMetric: 404.6591, val_loss: 407.5509, val_MinusLogProbMetric: 407.5509

Epoch 37: val_loss did not improve from 402.71887
196/196 - 7s - loss: 404.6591 - MinusLogProbMetric: 404.6591 - val_loss: 407.5509 - val_MinusLogProbMetric: 407.5509 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 38/1000
2023-10-03 22:40:26.074 
Epoch 38/1000 
	 loss: 402.6471, MinusLogProbMetric: 402.6471, val_loss: 407.2305, val_MinusLogProbMetric: 407.2305

Epoch 38: val_loss did not improve from 402.71887
196/196 - 7s - loss: 402.6471 - MinusLogProbMetric: 402.6471 - val_loss: 407.2305 - val_MinusLogProbMetric: 407.2305 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 39/1000
2023-10-03 22:40:32.710 
Epoch 39/1000 
	 loss: 406.2890, MinusLogProbMetric: 406.2890, val_loss: 404.4344, val_MinusLogProbMetric: 404.4344

Epoch 39: val_loss did not improve from 402.71887
196/196 - 7s - loss: 406.2890 - MinusLogProbMetric: 406.2890 - val_loss: 404.4344 - val_MinusLogProbMetric: 404.4344 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 40/1000
2023-10-03 22:40:39.399 
Epoch 40/1000 
	 loss: 402.1789, MinusLogProbMetric: 402.1789, val_loss: 403.2140, val_MinusLogProbMetric: 403.2140

Epoch 40: val_loss did not improve from 402.71887
196/196 - 7s - loss: 402.1789 - MinusLogProbMetric: 402.1789 - val_loss: 403.2140 - val_MinusLogProbMetric: 403.2140 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 41/1000
2023-10-03 22:40:46.223 
Epoch 41/1000 
	 loss: 402.2978, MinusLogProbMetric: 402.2978, val_loss: 402.9742, val_MinusLogProbMetric: 402.9742

Epoch 41: val_loss did not improve from 402.71887
196/196 - 7s - loss: 402.2978 - MinusLogProbMetric: 402.2978 - val_loss: 402.9742 - val_MinusLogProbMetric: 402.9742 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 42/1000
2023-10-03 22:40:52.934 
Epoch 42/1000 
	 loss: 402.2888, MinusLogProbMetric: 402.2888, val_loss: 402.9844, val_MinusLogProbMetric: 402.9844

Epoch 42: val_loss did not improve from 402.71887
196/196 - 7s - loss: 402.2888 - MinusLogProbMetric: 402.2888 - val_loss: 402.9844 - val_MinusLogProbMetric: 402.9844 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 43/1000
2023-10-03 22:40:59.716 
Epoch 43/1000 
	 loss: 402.1658, MinusLogProbMetric: 402.1658, val_loss: 402.1704, val_MinusLogProbMetric: 402.1704

Epoch 43: val_loss improved from 402.71887 to 402.17041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 402.1658 - MinusLogProbMetric: 402.1658 - val_loss: 402.1704 - val_MinusLogProbMetric: 402.1704 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 44/1000
2023-10-03 22:41:06.652 
Epoch 44/1000 
	 loss: 403.4426, MinusLogProbMetric: 403.4426, val_loss: 402.7642, val_MinusLogProbMetric: 402.7642

Epoch 44: val_loss did not improve from 402.17041
196/196 - 7s - loss: 403.4426 - MinusLogProbMetric: 403.4426 - val_loss: 402.7642 - val_MinusLogProbMetric: 402.7642 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 45/1000
2023-10-03 22:41:13.510 
Epoch 45/1000 
	 loss: 402.0464, MinusLogProbMetric: 402.0464, val_loss: 402.6966, val_MinusLogProbMetric: 402.6966

Epoch 45: val_loss did not improve from 402.17041
196/196 - 7s - loss: 402.0464 - MinusLogProbMetric: 402.0464 - val_loss: 402.6966 - val_MinusLogProbMetric: 402.6966 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 46/1000
2023-10-03 22:41:20.295 
Epoch 46/1000 
	 loss: 403.9597, MinusLogProbMetric: 403.9597, val_loss: 401.9309, val_MinusLogProbMetric: 401.9309

Epoch 46: val_loss improved from 402.17041 to 401.93094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 403.9597 - MinusLogProbMetric: 403.9597 - val_loss: 401.9309 - val_MinusLogProbMetric: 401.9309 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 47/1000
2023-10-03 22:41:28.031 
Epoch 47/1000 
	 loss: 401.1156, MinusLogProbMetric: 401.1156, val_loss: 403.6170, val_MinusLogProbMetric: 403.6170

Epoch 47: val_loss did not improve from 401.93094
196/196 - 7s - loss: 401.1156 - MinusLogProbMetric: 401.1156 - val_loss: 403.6170 - val_MinusLogProbMetric: 403.6170 - lr: 3.3333e-04 - 7s/epoch - 38ms/step
Epoch 48/1000
2023-10-03 22:41:35.356 
Epoch 48/1000 
	 loss: 403.4073, MinusLogProbMetric: 403.4073, val_loss: 405.8528, val_MinusLogProbMetric: 405.8528

Epoch 48: val_loss did not improve from 401.93094
196/196 - 7s - loss: 403.4073 - MinusLogProbMetric: 403.4073 - val_loss: 405.8528 - val_MinusLogProbMetric: 405.8528 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 49/1000
2023-10-03 22:41:42.424 
Epoch 49/1000 
	 loss: 400.9174, MinusLogProbMetric: 400.9174, val_loss: 414.1257, val_MinusLogProbMetric: 414.1257

Epoch 49: val_loss did not improve from 401.93094
196/196 - 7s - loss: 400.9174 - MinusLogProbMetric: 400.9174 - val_loss: 414.1257 - val_MinusLogProbMetric: 414.1257 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 50/1000
2023-10-03 22:41:49.113 
Epoch 50/1000 
	 loss: 402.2831, MinusLogProbMetric: 402.2831, val_loss: 405.8916, val_MinusLogProbMetric: 405.8916

Epoch 50: val_loss did not improve from 401.93094
196/196 - 7s - loss: 402.2831 - MinusLogProbMetric: 402.2831 - val_loss: 405.8916 - val_MinusLogProbMetric: 405.8916 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 51/1000
2023-10-03 22:41:55.915 
Epoch 51/1000 
	 loss: 403.3044, MinusLogProbMetric: 403.3044, val_loss: 401.2233, val_MinusLogProbMetric: 401.2233

Epoch 51: val_loss improved from 401.93094 to 401.22330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 403.3044 - MinusLogProbMetric: 403.3044 - val_loss: 401.2233 - val_MinusLogProbMetric: 401.2233 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 52/1000
2023-10-03 22:42:02.883 
Epoch 52/1000 
	 loss: 400.9257, MinusLogProbMetric: 400.9257, val_loss: 401.2228, val_MinusLogProbMetric: 401.2228

Epoch 52: val_loss improved from 401.22330 to 401.22281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 400.9257 - MinusLogProbMetric: 400.9257 - val_loss: 401.2228 - val_MinusLogProbMetric: 401.2228 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 53/1000
2023-10-03 22:42:09.873 
Epoch 53/1000 
	 loss: 401.2646, MinusLogProbMetric: 401.2646, val_loss: 404.3118, val_MinusLogProbMetric: 404.3118

Epoch 53: val_loss did not improve from 401.22281
196/196 - 7s - loss: 401.2646 - MinusLogProbMetric: 401.2646 - val_loss: 404.3118 - val_MinusLogProbMetric: 404.3118 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 54/1000
2023-10-03 22:42:16.580 
Epoch 54/1000 
	 loss: 402.0311, MinusLogProbMetric: 402.0311, val_loss: 404.3884, val_MinusLogProbMetric: 404.3884

Epoch 54: val_loss did not improve from 401.22281
196/196 - 7s - loss: 402.0311 - MinusLogProbMetric: 402.0311 - val_loss: 404.3884 - val_MinusLogProbMetric: 404.3884 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 55/1000
2023-10-03 22:42:23.408 
Epoch 55/1000 
	 loss: 400.8653, MinusLogProbMetric: 400.8653, val_loss: 400.2627, val_MinusLogProbMetric: 400.2627

Epoch 55: val_loss improved from 401.22281 to 400.26273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 400.8653 - MinusLogProbMetric: 400.8653 - val_loss: 400.2627 - val_MinusLogProbMetric: 400.2627 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 56/1000
2023-10-03 22:42:30.399 
Epoch 56/1000 
	 loss: 401.7386, MinusLogProbMetric: 401.7386, val_loss: 401.3229, val_MinusLogProbMetric: 401.3229

Epoch 56: val_loss did not improve from 400.26273
196/196 - 7s - loss: 401.7386 - MinusLogProbMetric: 401.7386 - val_loss: 401.3229 - val_MinusLogProbMetric: 401.3229 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 57/1000
2023-10-03 22:42:37.121 
Epoch 57/1000 
	 loss: 401.8990, MinusLogProbMetric: 401.8990, val_loss: 400.6340, val_MinusLogProbMetric: 400.6340

Epoch 57: val_loss did not improve from 400.26273
196/196 - 7s - loss: 401.8990 - MinusLogProbMetric: 401.8990 - val_loss: 400.6340 - val_MinusLogProbMetric: 400.6340 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 58/1000
2023-10-03 22:42:43.886 
Epoch 58/1000 
	 loss: 400.3443, MinusLogProbMetric: 400.3443, val_loss: 400.7867, val_MinusLogProbMetric: 400.7867

Epoch 58: val_loss did not improve from 400.26273
196/196 - 7s - loss: 400.3443 - MinusLogProbMetric: 400.3443 - val_loss: 400.7867 - val_MinusLogProbMetric: 400.7867 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 59/1000
2023-10-03 22:42:50.570 
Epoch 59/1000 
	 loss: 400.6196, MinusLogProbMetric: 400.6196, val_loss: 402.3059, val_MinusLogProbMetric: 402.3059

Epoch 59: val_loss did not improve from 400.26273
196/196 - 7s - loss: 400.6196 - MinusLogProbMetric: 400.6196 - val_loss: 402.3059 - val_MinusLogProbMetric: 402.3059 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 60/1000
2023-10-03 22:42:57.216 
Epoch 60/1000 
	 loss: 400.5077, MinusLogProbMetric: 400.5077, val_loss: 414.6792, val_MinusLogProbMetric: 414.6792

Epoch 60: val_loss did not improve from 400.26273
196/196 - 7s - loss: 400.5077 - MinusLogProbMetric: 400.5077 - val_loss: 414.6792 - val_MinusLogProbMetric: 414.6792 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 61/1000
2023-10-03 22:43:04.070 
Epoch 61/1000 
	 loss: 401.0009, MinusLogProbMetric: 401.0009, val_loss: 404.1897, val_MinusLogProbMetric: 404.1897

Epoch 61: val_loss did not improve from 400.26273
196/196 - 7s - loss: 401.0009 - MinusLogProbMetric: 401.0009 - val_loss: 404.1897 - val_MinusLogProbMetric: 404.1897 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 62/1000
2023-10-03 22:43:10.744 
Epoch 62/1000 
	 loss: 401.2866, MinusLogProbMetric: 401.2866, val_loss: 401.2143, val_MinusLogProbMetric: 401.2143

Epoch 62: val_loss did not improve from 400.26273
196/196 - 7s - loss: 401.2866 - MinusLogProbMetric: 401.2866 - val_loss: 401.2143 - val_MinusLogProbMetric: 401.2143 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 63/1000
2023-10-03 22:43:17.365 
Epoch 63/1000 
	 loss: 400.1555, MinusLogProbMetric: 400.1555, val_loss: 400.3259, val_MinusLogProbMetric: 400.3259

Epoch 63: val_loss did not improve from 400.26273
196/196 - 7s - loss: 400.1555 - MinusLogProbMetric: 400.1555 - val_loss: 400.3259 - val_MinusLogProbMetric: 400.3259 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 64/1000
2023-10-03 22:43:24.225 
Epoch 64/1000 
	 loss: 400.7976, MinusLogProbMetric: 400.7976, val_loss: 401.2570, val_MinusLogProbMetric: 401.2570

Epoch 64: val_loss did not improve from 400.26273
196/196 - 7s - loss: 400.7976 - MinusLogProbMetric: 400.7976 - val_loss: 401.2570 - val_MinusLogProbMetric: 401.2570 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 65/1000
2023-10-03 22:43:30.914 
Epoch 65/1000 
	 loss: 399.4115, MinusLogProbMetric: 399.4115, val_loss: 400.1277, val_MinusLogProbMetric: 400.1277

Epoch 65: val_loss improved from 400.26273 to 400.12775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 399.4115 - MinusLogProbMetric: 399.4115 - val_loss: 400.1277 - val_MinusLogProbMetric: 400.1277 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 66/1000
2023-10-03 22:43:37.789 
Epoch 66/1000 
	 loss: 400.0613, MinusLogProbMetric: 400.0613, val_loss: 401.8484, val_MinusLogProbMetric: 401.8484

Epoch 66: val_loss did not improve from 400.12775
196/196 - 7s - loss: 400.0613 - MinusLogProbMetric: 400.0613 - val_loss: 401.8484 - val_MinusLogProbMetric: 401.8484 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 67/1000
2023-10-03 22:43:44.570 
Epoch 67/1000 
	 loss: 400.1626, MinusLogProbMetric: 400.1626, val_loss: 400.9838, val_MinusLogProbMetric: 400.9838

Epoch 67: val_loss did not improve from 400.12775
196/196 - 7s - loss: 400.1626 - MinusLogProbMetric: 400.1626 - val_loss: 400.9838 - val_MinusLogProbMetric: 400.9838 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 68/1000
2023-10-03 22:43:51.325 
Epoch 68/1000 
	 loss: 400.3630, MinusLogProbMetric: 400.3630, val_loss: 404.1759, val_MinusLogProbMetric: 404.1759

Epoch 68: val_loss did not improve from 400.12775
196/196 - 7s - loss: 400.3630 - MinusLogProbMetric: 400.3630 - val_loss: 404.1759 - val_MinusLogProbMetric: 404.1759 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 69/1000
2023-10-03 22:43:58.079 
Epoch 69/1000 
	 loss: 399.1658, MinusLogProbMetric: 399.1658, val_loss: 400.6325, val_MinusLogProbMetric: 400.6325

Epoch 69: val_loss did not improve from 400.12775
196/196 - 7s - loss: 399.1658 - MinusLogProbMetric: 399.1658 - val_loss: 400.6325 - val_MinusLogProbMetric: 400.6325 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 70/1000
2023-10-03 22:44:04.783 
Epoch 70/1000 
	 loss: 400.1675, MinusLogProbMetric: 400.1675, val_loss: 399.3752, val_MinusLogProbMetric: 399.3752

Epoch 70: val_loss improved from 400.12775 to 399.37515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 400.1675 - MinusLogProbMetric: 400.1675 - val_loss: 399.3752 - val_MinusLogProbMetric: 399.3752 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 71/1000
2023-10-03 22:44:11.821 
Epoch 71/1000 
	 loss: 401.0262, MinusLogProbMetric: 401.0262, val_loss: 399.7830, val_MinusLogProbMetric: 399.7830

Epoch 71: val_loss did not improve from 399.37515
196/196 - 7s - loss: 401.0262 - MinusLogProbMetric: 401.0262 - val_loss: 399.7830 - val_MinusLogProbMetric: 399.7830 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 72/1000
2023-10-03 22:44:18.558 
Epoch 72/1000 
	 loss: 398.9492, MinusLogProbMetric: 398.9492, val_loss: 401.1049, val_MinusLogProbMetric: 401.1049

Epoch 72: val_loss did not improve from 399.37515
196/196 - 7s - loss: 398.9492 - MinusLogProbMetric: 398.9492 - val_loss: 401.1049 - val_MinusLogProbMetric: 401.1049 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 73/1000
2023-10-03 22:44:25.252 
Epoch 73/1000 
	 loss: 399.6518, MinusLogProbMetric: 399.6518, val_loss: 402.4862, val_MinusLogProbMetric: 402.4862

Epoch 73: val_loss did not improve from 399.37515
196/196 - 7s - loss: 399.6518 - MinusLogProbMetric: 399.6518 - val_loss: 402.4862 - val_MinusLogProbMetric: 402.4862 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 74/1000
2023-10-03 22:44:32.110 
Epoch 74/1000 
	 loss: 399.4317, MinusLogProbMetric: 399.4317, val_loss: 400.6301, val_MinusLogProbMetric: 400.6301

Epoch 74: val_loss did not improve from 399.37515
196/196 - 7s - loss: 399.4317 - MinusLogProbMetric: 399.4317 - val_loss: 400.6301 - val_MinusLogProbMetric: 400.6301 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 75/1000
2023-10-03 22:44:39.423 
Epoch 75/1000 
	 loss: 398.8993, MinusLogProbMetric: 398.8993, val_loss: 399.0960, val_MinusLogProbMetric: 399.0960

Epoch 75: val_loss improved from 399.37515 to 399.09601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 398.8993 - MinusLogProbMetric: 398.8993 - val_loss: 399.0960 - val_MinusLogProbMetric: 399.0960 - lr: 3.3333e-04 - 8s/epoch - 39ms/step
Epoch 76/1000
2023-10-03 22:44:46.965 
Epoch 76/1000 
	 loss: 399.8689, MinusLogProbMetric: 399.8689, val_loss: 414.6580, val_MinusLogProbMetric: 414.6580

Epoch 76: val_loss did not improve from 399.09601
196/196 - 7s - loss: 399.8689 - MinusLogProbMetric: 399.8689 - val_loss: 414.6580 - val_MinusLogProbMetric: 414.6580 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 77/1000
2023-10-03 22:44:53.909 
Epoch 77/1000 
	 loss: 399.4976, MinusLogProbMetric: 399.4976, val_loss: 400.3907, val_MinusLogProbMetric: 400.3907

Epoch 77: val_loss did not improve from 399.09601
196/196 - 7s - loss: 399.4976 - MinusLogProbMetric: 399.4976 - val_loss: 400.3907 - val_MinusLogProbMetric: 400.3907 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 78/1000
2023-10-03 22:45:00.609 
Epoch 78/1000 
	 loss: 399.1353, MinusLogProbMetric: 399.1353, val_loss: 400.1710, val_MinusLogProbMetric: 400.1710

Epoch 78: val_loss did not improve from 399.09601
196/196 - 7s - loss: 399.1353 - MinusLogProbMetric: 399.1353 - val_loss: 400.1710 - val_MinusLogProbMetric: 400.1710 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 79/1000
2023-10-03 22:45:07.320 
Epoch 79/1000 
	 loss: 399.3248, MinusLogProbMetric: 399.3248, val_loss: 404.7935, val_MinusLogProbMetric: 404.7935

Epoch 79: val_loss did not improve from 399.09601
196/196 - 7s - loss: 399.3248 - MinusLogProbMetric: 399.3248 - val_loss: 404.7935 - val_MinusLogProbMetric: 404.7935 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 80/1000
2023-10-03 22:45:14.099 
Epoch 80/1000 
	 loss: 398.5864, MinusLogProbMetric: 398.5864, val_loss: 401.9411, val_MinusLogProbMetric: 401.9411

Epoch 80: val_loss did not improve from 399.09601
196/196 - 7s - loss: 398.5864 - MinusLogProbMetric: 398.5864 - val_loss: 401.9411 - val_MinusLogProbMetric: 401.9411 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 81/1000
2023-10-03 22:45:20.743 
Epoch 81/1000 
	 loss: 403.7321, MinusLogProbMetric: 403.7321, val_loss: 400.6004, val_MinusLogProbMetric: 400.6004

Epoch 81: val_loss did not improve from 399.09601
196/196 - 7s - loss: 403.7321 - MinusLogProbMetric: 403.7321 - val_loss: 400.6004 - val_MinusLogProbMetric: 400.6004 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 82/1000
2023-10-03 22:45:27.497 
Epoch 82/1000 
	 loss: 398.5748, MinusLogProbMetric: 398.5748, val_loss: 410.4060, val_MinusLogProbMetric: 410.4060

Epoch 82: val_loss did not improve from 399.09601
196/196 - 7s - loss: 398.5748 - MinusLogProbMetric: 398.5748 - val_loss: 410.4060 - val_MinusLogProbMetric: 410.4060 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 83/1000
2023-10-03 22:45:34.202 
Epoch 83/1000 
	 loss: 398.3579, MinusLogProbMetric: 398.3579, val_loss: 402.2054, val_MinusLogProbMetric: 402.2054

Epoch 83: val_loss did not improve from 399.09601
196/196 - 7s - loss: 398.3579 - MinusLogProbMetric: 398.3579 - val_loss: 402.2054 - val_MinusLogProbMetric: 402.2054 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 84/1000
2023-10-03 22:45:40.865 
Epoch 84/1000 
	 loss: 398.0824, MinusLogProbMetric: 398.0824, val_loss: 401.6349, val_MinusLogProbMetric: 401.6349

Epoch 84: val_loss did not improve from 399.09601
196/196 - 7s - loss: 398.0824 - MinusLogProbMetric: 398.0824 - val_loss: 401.6349 - val_MinusLogProbMetric: 401.6349 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 85/1000
2023-10-03 22:45:47.615 
Epoch 85/1000 
	 loss: 398.2964, MinusLogProbMetric: 398.2964, val_loss: 399.8791, val_MinusLogProbMetric: 399.8791

Epoch 85: val_loss did not improve from 399.09601
196/196 - 7s - loss: 398.2964 - MinusLogProbMetric: 398.2964 - val_loss: 399.8791 - val_MinusLogProbMetric: 399.8791 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 86/1000
2023-10-03 22:45:54.314 
Epoch 86/1000 
	 loss: 398.6878, MinusLogProbMetric: 398.6878, val_loss: 399.8661, val_MinusLogProbMetric: 399.8661

Epoch 86: val_loss did not improve from 399.09601
196/196 - 7s - loss: 398.6878 - MinusLogProbMetric: 398.6878 - val_loss: 399.8661 - val_MinusLogProbMetric: 399.8661 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 87/1000
2023-10-03 22:46:00.988 
Epoch 87/1000 
	 loss: 1099.8148, MinusLogProbMetric: 1099.8148, val_loss: 549.6118, val_MinusLogProbMetric: 549.6118

Epoch 87: val_loss did not improve from 399.09601
196/196 - 7s - loss: 1099.8148 - MinusLogProbMetric: 1099.8148 - val_loss: 549.6118 - val_MinusLogProbMetric: 549.6118 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 88/1000
2023-10-03 22:46:07.828 
Epoch 88/1000 
	 loss: 470.2859, MinusLogProbMetric: 470.2859, val_loss: 445.5716, val_MinusLogProbMetric: 445.5716

Epoch 88: val_loss did not improve from 399.09601
196/196 - 7s - loss: 470.2859 - MinusLogProbMetric: 470.2859 - val_loss: 445.5716 - val_MinusLogProbMetric: 445.5716 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 89/1000
2023-10-03 22:46:14.553 
Epoch 89/1000 
	 loss: 436.6994, MinusLogProbMetric: 436.6994, val_loss: 432.9808, val_MinusLogProbMetric: 432.9808

Epoch 89: val_loss did not improve from 399.09601
196/196 - 7s - loss: 436.6994 - MinusLogProbMetric: 436.6994 - val_loss: 432.9808 - val_MinusLogProbMetric: 432.9808 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 90/1000
2023-10-03 22:46:21.249 
Epoch 90/1000 
	 loss: 427.7277, MinusLogProbMetric: 427.7277, val_loss: 426.4979, val_MinusLogProbMetric: 426.4979

Epoch 90: val_loss did not improve from 399.09601
196/196 - 7s - loss: 427.7277 - MinusLogProbMetric: 427.7277 - val_loss: 426.4979 - val_MinusLogProbMetric: 426.4979 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 91/1000
2023-10-03 22:46:28.081 
Epoch 91/1000 
	 loss: 422.7076, MinusLogProbMetric: 422.7076, val_loss: 422.7307, val_MinusLogProbMetric: 422.7307

Epoch 91: val_loss did not improve from 399.09601
196/196 - 7s - loss: 422.7076 - MinusLogProbMetric: 422.7076 - val_loss: 422.7307 - val_MinusLogProbMetric: 422.7307 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 92/1000
2023-10-03 22:46:34.766 
Epoch 92/1000 
	 loss: 419.3379, MinusLogProbMetric: 419.3379, val_loss: 419.8492, val_MinusLogProbMetric: 419.8492

Epoch 92: val_loss did not improve from 399.09601
196/196 - 7s - loss: 419.3379 - MinusLogProbMetric: 419.3379 - val_loss: 419.8492 - val_MinusLogProbMetric: 419.8492 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 93/1000
2023-10-03 22:46:41.490 
Epoch 93/1000 
	 loss: 416.6288, MinusLogProbMetric: 416.6288, val_loss: 417.2780, val_MinusLogProbMetric: 417.2780

Epoch 93: val_loss did not improve from 399.09601
196/196 - 7s - loss: 416.6288 - MinusLogProbMetric: 416.6288 - val_loss: 417.2780 - val_MinusLogProbMetric: 417.2780 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 94/1000
2023-10-03 22:46:48.281 
Epoch 94/1000 
	 loss: 414.9270, MinusLogProbMetric: 414.9270, val_loss: 414.8931, val_MinusLogProbMetric: 414.8931

Epoch 94: val_loss did not improve from 399.09601
196/196 - 7s - loss: 414.9270 - MinusLogProbMetric: 414.9270 - val_loss: 414.8931 - val_MinusLogProbMetric: 414.8931 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 95/1000
2023-10-03 22:46:54.959 
Epoch 95/1000 
	 loss: 413.1786, MinusLogProbMetric: 413.1786, val_loss: 413.8450, val_MinusLogProbMetric: 413.8450

Epoch 95: val_loss did not improve from 399.09601
196/196 - 7s - loss: 413.1786 - MinusLogProbMetric: 413.1786 - val_loss: 413.8450 - val_MinusLogProbMetric: 413.8450 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 96/1000
2023-10-03 22:47:01.696 
Epoch 96/1000 
	 loss: 411.8707, MinusLogProbMetric: 411.8707, val_loss: 412.8916, val_MinusLogProbMetric: 412.8916

Epoch 96: val_loss did not improve from 399.09601
196/196 - 7s - loss: 411.8707 - MinusLogProbMetric: 411.8707 - val_loss: 412.8916 - val_MinusLogProbMetric: 412.8916 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 97/1000
2023-10-03 22:47:08.482 
Epoch 97/1000 
	 loss: 410.7112, MinusLogProbMetric: 410.7112, val_loss: 411.2589, val_MinusLogProbMetric: 411.2589

Epoch 97: val_loss did not improve from 399.09601
196/196 - 7s - loss: 410.7112 - MinusLogProbMetric: 410.7112 - val_loss: 411.2589 - val_MinusLogProbMetric: 411.2589 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 98/1000
2023-10-03 22:47:15.140 
Epoch 98/1000 
	 loss: 409.6916, MinusLogProbMetric: 409.6916, val_loss: 410.4108, val_MinusLogProbMetric: 410.4108

Epoch 98: val_loss did not improve from 399.09601
196/196 - 7s - loss: 409.6916 - MinusLogProbMetric: 409.6916 - val_loss: 410.4108 - val_MinusLogProbMetric: 410.4108 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 99/1000
2023-10-03 22:47:21.916 
Epoch 99/1000 
	 loss: 408.9384, MinusLogProbMetric: 408.9384, val_loss: 409.3019, val_MinusLogProbMetric: 409.3019

Epoch 99: val_loss did not improve from 399.09601
196/196 - 7s - loss: 408.9384 - MinusLogProbMetric: 408.9384 - val_loss: 409.3019 - val_MinusLogProbMetric: 409.3019 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 100/1000
2023-10-03 22:47:28.642 
Epoch 100/1000 
	 loss: 408.2112, MinusLogProbMetric: 408.2112, val_loss: 408.4851, val_MinusLogProbMetric: 408.4851

Epoch 100: val_loss did not improve from 399.09601
196/196 - 7s - loss: 408.2112 - MinusLogProbMetric: 408.2112 - val_loss: 408.4851 - val_MinusLogProbMetric: 408.4851 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 101/1000
2023-10-03 22:47:35.376 
Epoch 101/1000 
	 loss: 407.5139, MinusLogProbMetric: 407.5139, val_loss: 409.8519, val_MinusLogProbMetric: 409.8519

Epoch 101: val_loss did not improve from 399.09601
196/196 - 7s - loss: 407.5139 - MinusLogProbMetric: 407.5139 - val_loss: 409.8519 - val_MinusLogProbMetric: 409.8519 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 102/1000
2023-10-03 22:47:42.156 
Epoch 102/1000 
	 loss: 406.9227, MinusLogProbMetric: 406.9227, val_loss: 407.7568, val_MinusLogProbMetric: 407.7568

Epoch 102: val_loss did not improve from 399.09601
196/196 - 7s - loss: 406.9227 - MinusLogProbMetric: 406.9227 - val_loss: 407.7568 - val_MinusLogProbMetric: 407.7568 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 103/1000
2023-10-03 22:47:49.277 
Epoch 103/1000 
	 loss: 406.4700, MinusLogProbMetric: 406.4700, val_loss: 407.2861, val_MinusLogProbMetric: 407.2861

Epoch 103: val_loss did not improve from 399.09601
196/196 - 7s - loss: 406.4700 - MinusLogProbMetric: 406.4700 - val_loss: 407.2861 - val_MinusLogProbMetric: 407.2861 - lr: 3.3333e-04 - 7s/epoch - 36ms/step
Epoch 104/1000
2023-10-03 22:47:56.578 
Epoch 104/1000 
	 loss: 405.7141, MinusLogProbMetric: 405.7141, val_loss: 407.1050, val_MinusLogProbMetric: 407.1050

Epoch 104: val_loss did not improve from 399.09601
196/196 - 7s - loss: 405.7141 - MinusLogProbMetric: 405.7141 - val_loss: 407.1050 - val_MinusLogProbMetric: 407.1050 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 105/1000
2023-10-03 22:48:03.807 
Epoch 105/1000 
	 loss: 405.5062, MinusLogProbMetric: 405.5062, val_loss: 407.3547, val_MinusLogProbMetric: 407.3547

Epoch 105: val_loss did not improve from 399.09601
196/196 - 7s - loss: 405.5062 - MinusLogProbMetric: 405.5062 - val_loss: 407.3547 - val_MinusLogProbMetric: 407.3547 - lr: 3.3333e-04 - 7s/epoch - 37ms/step
Epoch 106/1000
2023-10-03 22:48:10.652 
Epoch 106/1000 
	 loss: 405.0221, MinusLogProbMetric: 405.0221, val_loss: 405.3223, val_MinusLogProbMetric: 405.3223

Epoch 106: val_loss did not improve from 399.09601
196/196 - 7s - loss: 405.0221 - MinusLogProbMetric: 405.0221 - val_loss: 405.3223 - val_MinusLogProbMetric: 405.3223 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 107/1000
2023-10-03 22:48:17.338 
Epoch 107/1000 
	 loss: 404.5572, MinusLogProbMetric: 404.5572, val_loss: 405.9600, val_MinusLogProbMetric: 405.9600

Epoch 107: val_loss did not improve from 399.09601
196/196 - 7s - loss: 404.5572 - MinusLogProbMetric: 404.5572 - val_loss: 405.9600 - val_MinusLogProbMetric: 405.9600 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 108/1000
2023-10-03 22:48:24.054 
Epoch 108/1000 
	 loss: 404.0746, MinusLogProbMetric: 404.0746, val_loss: 405.1503, val_MinusLogProbMetric: 405.1503

Epoch 108: val_loss did not improve from 399.09601
196/196 - 7s - loss: 404.0746 - MinusLogProbMetric: 404.0746 - val_loss: 405.1503 - val_MinusLogProbMetric: 405.1503 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 109/1000
2023-10-03 22:48:30.897 
Epoch 109/1000 
	 loss: 404.0580, MinusLogProbMetric: 404.0580, val_loss: 405.2764, val_MinusLogProbMetric: 405.2764

Epoch 109: val_loss did not improve from 399.09601
196/196 - 7s - loss: 404.0580 - MinusLogProbMetric: 404.0580 - val_loss: 405.2764 - val_MinusLogProbMetric: 405.2764 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 110/1000
2023-10-03 22:48:37.782 
Epoch 110/1000 
	 loss: 403.5153, MinusLogProbMetric: 403.5153, val_loss: 404.8593, val_MinusLogProbMetric: 404.8593

Epoch 110: val_loss did not improve from 399.09601
196/196 - 7s - loss: 403.5153 - MinusLogProbMetric: 403.5153 - val_loss: 404.8593 - val_MinusLogProbMetric: 404.8593 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 111/1000
2023-10-03 22:48:44.611 
Epoch 111/1000 
	 loss: 403.2187, MinusLogProbMetric: 403.2187, val_loss: 404.1935, val_MinusLogProbMetric: 404.1935

Epoch 111: val_loss did not improve from 399.09601
196/196 - 7s - loss: 403.2187 - MinusLogProbMetric: 403.2187 - val_loss: 404.1935 - val_MinusLogProbMetric: 404.1935 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 112/1000
2023-10-03 22:48:51.383 
Epoch 112/1000 
	 loss: 402.9993, MinusLogProbMetric: 402.9993, val_loss: 404.0431, val_MinusLogProbMetric: 404.0431

Epoch 112: val_loss did not improve from 399.09601
196/196 - 7s - loss: 402.9993 - MinusLogProbMetric: 402.9993 - val_loss: 404.0431 - val_MinusLogProbMetric: 404.0431 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 113/1000
2023-10-03 22:48:58.089 
Epoch 113/1000 
	 loss: 402.8407, MinusLogProbMetric: 402.8407, val_loss: 403.8614, val_MinusLogProbMetric: 403.8614

Epoch 113: val_loss did not improve from 399.09601
196/196 - 7s - loss: 402.8407 - MinusLogProbMetric: 402.8407 - val_loss: 403.8614 - val_MinusLogProbMetric: 403.8614 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 114/1000
2023-10-03 22:49:04.897 
Epoch 114/1000 
	 loss: 402.4496, MinusLogProbMetric: 402.4496, val_loss: 402.7007, val_MinusLogProbMetric: 402.7007

Epoch 114: val_loss did not improve from 399.09601
196/196 - 7s - loss: 402.4496 - MinusLogProbMetric: 402.4496 - val_loss: 402.7007 - val_MinusLogProbMetric: 402.7007 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 115/1000
2023-10-03 22:49:11.612 
Epoch 115/1000 
	 loss: 402.2506, MinusLogProbMetric: 402.2506, val_loss: 405.0872, val_MinusLogProbMetric: 405.0872

Epoch 115: val_loss did not improve from 399.09601
196/196 - 7s - loss: 402.2506 - MinusLogProbMetric: 402.2506 - val_loss: 405.0872 - val_MinusLogProbMetric: 405.0872 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 116/1000
2023-10-03 22:49:18.391 
Epoch 116/1000 
	 loss: 402.6109, MinusLogProbMetric: 402.6109, val_loss: 402.8473, val_MinusLogProbMetric: 402.8473

Epoch 116: val_loss did not improve from 399.09601
196/196 - 7s - loss: 402.6109 - MinusLogProbMetric: 402.6109 - val_loss: 402.8473 - val_MinusLogProbMetric: 402.8473 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 117/1000
2023-10-03 22:49:24.951 
Epoch 117/1000 
	 loss: 401.7670, MinusLogProbMetric: 401.7670, val_loss: 402.6599, val_MinusLogProbMetric: 402.6599

Epoch 117: val_loss did not improve from 399.09601
196/196 - 7s - loss: 401.7670 - MinusLogProbMetric: 401.7670 - val_loss: 402.6599 - val_MinusLogProbMetric: 402.6599 - lr: 3.3333e-04 - 7s/epoch - 33ms/step
Epoch 118/1000
2023-10-03 22:49:31.699 
Epoch 118/1000 
	 loss: 401.5204, MinusLogProbMetric: 401.5204, val_loss: 402.9119, val_MinusLogProbMetric: 402.9119

Epoch 118: val_loss did not improve from 399.09601
196/196 - 7s - loss: 401.5204 - MinusLogProbMetric: 401.5204 - val_loss: 402.9119 - val_MinusLogProbMetric: 402.9119 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 119/1000
2023-10-03 22:49:38.560 
Epoch 119/1000 
	 loss: 401.3939, MinusLogProbMetric: 401.3939, val_loss: 403.4909, val_MinusLogProbMetric: 403.4909

Epoch 119: val_loss did not improve from 399.09601
196/196 - 7s - loss: 401.3939 - MinusLogProbMetric: 401.3939 - val_loss: 403.4909 - val_MinusLogProbMetric: 403.4909 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 120/1000
2023-10-03 22:49:45.258 
Epoch 120/1000 
	 loss: 401.0787, MinusLogProbMetric: 401.0787, val_loss: 402.4269, val_MinusLogProbMetric: 402.4269

Epoch 120: val_loss did not improve from 399.09601
196/196 - 7s - loss: 401.0787 - MinusLogProbMetric: 401.0787 - val_loss: 402.4269 - val_MinusLogProbMetric: 402.4269 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 121/1000
2023-10-03 22:49:52.127 
Epoch 121/1000 
	 loss: 400.9178, MinusLogProbMetric: 400.9178, val_loss: 402.5960, val_MinusLogProbMetric: 402.5960

Epoch 121: val_loss did not improve from 399.09601
196/196 - 7s - loss: 400.9178 - MinusLogProbMetric: 400.9178 - val_loss: 402.5960 - val_MinusLogProbMetric: 402.5960 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 122/1000
2023-10-03 22:49:58.881 
Epoch 122/1000 
	 loss: 400.8918, MinusLogProbMetric: 400.8918, val_loss: 405.2493, val_MinusLogProbMetric: 405.2493

Epoch 122: val_loss did not improve from 399.09601
196/196 - 7s - loss: 400.8918 - MinusLogProbMetric: 400.8918 - val_loss: 405.2493 - val_MinusLogProbMetric: 405.2493 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 123/1000
2023-10-03 22:50:05.568 
Epoch 123/1000 
	 loss: 400.8495, MinusLogProbMetric: 400.8495, val_loss: 409.5632, val_MinusLogProbMetric: 409.5632

Epoch 123: val_loss did not improve from 399.09601
196/196 - 7s - loss: 400.8495 - MinusLogProbMetric: 400.8495 - val_loss: 409.5632 - val_MinusLogProbMetric: 409.5632 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 124/1000
2023-10-03 22:50:12.364 
Epoch 124/1000 
	 loss: 400.7949, MinusLogProbMetric: 400.7949, val_loss: 401.0729, val_MinusLogProbMetric: 401.0729

Epoch 124: val_loss did not improve from 399.09601
196/196 - 7s - loss: 400.7949 - MinusLogProbMetric: 400.7949 - val_loss: 401.0729 - val_MinusLogProbMetric: 401.0729 - lr: 3.3333e-04 - 7s/epoch - 35ms/step
Epoch 125/1000
2023-10-03 22:50:18.962 
Epoch 125/1000 
	 loss: 400.4124, MinusLogProbMetric: 400.4124, val_loss: 402.1705, val_MinusLogProbMetric: 402.1705

Epoch 125: val_loss did not improve from 399.09601
196/196 - 7s - loss: 400.4124 - MinusLogProbMetric: 400.4124 - val_loss: 402.1705 - val_MinusLogProbMetric: 402.1705 - lr: 3.3333e-04 - 7s/epoch - 34ms/step
Epoch 126/1000
2023-10-03 22:50:25.723 
Epoch 126/1000 
	 loss: 397.4467, MinusLogProbMetric: 397.4467, val_loss: 398.4066, val_MinusLogProbMetric: 398.4066

Epoch 126: val_loss improved from 399.09601 to 398.40659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 397.4467 - MinusLogProbMetric: 397.4467 - val_loss: 398.4066 - val_MinusLogProbMetric: 398.4066 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 127/1000
2023-10-03 22:50:32.678 
Epoch 127/1000 
	 loss: 397.1111, MinusLogProbMetric: 397.1111, val_loss: 398.7051, val_MinusLogProbMetric: 398.7051

Epoch 127: val_loss did not improve from 398.40659
196/196 - 7s - loss: 397.1111 - MinusLogProbMetric: 397.1111 - val_loss: 398.7051 - val_MinusLogProbMetric: 398.7051 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 128/1000
2023-10-03 22:50:39.434 
Epoch 128/1000 
	 loss: 397.1048, MinusLogProbMetric: 397.1048, val_loss: 398.3414, val_MinusLogProbMetric: 398.3414

Epoch 128: val_loss improved from 398.40659 to 398.34140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 397.1048 - MinusLogProbMetric: 397.1048 - val_loss: 398.3414 - val_MinusLogProbMetric: 398.3414 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 129/1000
2023-10-03 22:50:46.544 
Epoch 129/1000 
	 loss: 397.0031, MinusLogProbMetric: 397.0031, val_loss: 398.3673, val_MinusLogProbMetric: 398.3673

Epoch 129: val_loss did not improve from 398.34140
196/196 - 7s - loss: 397.0031 - MinusLogProbMetric: 397.0031 - val_loss: 398.3673 - val_MinusLogProbMetric: 398.3673 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 130/1000
2023-10-03 22:50:53.233 
Epoch 130/1000 
	 loss: 397.0478, MinusLogProbMetric: 397.0478, val_loss: 398.4352, val_MinusLogProbMetric: 398.4352

Epoch 130: val_loss did not improve from 398.34140
196/196 - 7s - loss: 397.0478 - MinusLogProbMetric: 397.0478 - val_loss: 398.4352 - val_MinusLogProbMetric: 398.4352 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 131/1000
2023-10-03 22:51:00.279 
Epoch 131/1000 
	 loss: 397.1144, MinusLogProbMetric: 397.1144, val_loss: 398.6564, val_MinusLogProbMetric: 398.6564

Epoch 131: val_loss did not improve from 398.34140
196/196 - 7s - loss: 397.1144 - MinusLogProbMetric: 397.1144 - val_loss: 398.6564 - val_MinusLogProbMetric: 398.6564 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 132/1000
2023-10-03 22:51:07.554 
Epoch 132/1000 
	 loss: 396.8439, MinusLogProbMetric: 396.8439, val_loss: 398.3059, val_MinusLogProbMetric: 398.3059

Epoch 132: val_loss improved from 398.34140 to 398.30588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 396.8439 - MinusLogProbMetric: 396.8439 - val_loss: 398.3059 - val_MinusLogProbMetric: 398.3059 - lr: 1.6667e-04 - 8s/epoch - 38ms/step
Epoch 133/1000
2023-10-03 22:51:15.164 
Epoch 133/1000 
	 loss: 396.9067, MinusLogProbMetric: 396.9067, val_loss: 398.1483, val_MinusLogProbMetric: 398.1483

Epoch 133: val_loss improved from 398.30588 to 398.14828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 396.9067 - MinusLogProbMetric: 396.9067 - val_loss: 398.1483 - val_MinusLogProbMetric: 398.1483 - lr: 1.6667e-04 - 8s/epoch - 39ms/step
Epoch 134/1000
2023-10-03 22:51:22.251 
Epoch 134/1000 
	 loss: 396.8006, MinusLogProbMetric: 396.8006, val_loss: 398.2926, val_MinusLogProbMetric: 398.2926

Epoch 134: val_loss did not improve from 398.14828
196/196 - 7s - loss: 396.8006 - MinusLogProbMetric: 396.8006 - val_loss: 398.2926 - val_MinusLogProbMetric: 398.2926 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 135/1000
2023-10-03 22:51:28.946 
Epoch 135/1000 
	 loss: 396.7545, MinusLogProbMetric: 396.7545, val_loss: 398.4393, val_MinusLogProbMetric: 398.4393

Epoch 135: val_loss did not improve from 398.14828
196/196 - 7s - loss: 396.7545 - MinusLogProbMetric: 396.7545 - val_loss: 398.4393 - val_MinusLogProbMetric: 398.4393 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 136/1000
2023-10-03 22:51:35.668 
Epoch 136/1000 
	 loss: 396.8108, MinusLogProbMetric: 396.8108, val_loss: 398.1463, val_MinusLogProbMetric: 398.1463

Epoch 136: val_loss improved from 398.14828 to 398.14633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 396.8108 - MinusLogProbMetric: 396.8108 - val_loss: 398.1463 - val_MinusLogProbMetric: 398.1463 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 137/1000
2023-10-03 22:51:42.595 
Epoch 137/1000 
	 loss: 396.6537, MinusLogProbMetric: 396.6537, val_loss: 399.0524, val_MinusLogProbMetric: 399.0524

Epoch 137: val_loss did not improve from 398.14633
196/196 - 7s - loss: 396.6537 - MinusLogProbMetric: 396.6537 - val_loss: 399.0524 - val_MinusLogProbMetric: 399.0524 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 138/1000
2023-10-03 22:51:49.202 
Epoch 138/1000 
	 loss: 396.4945, MinusLogProbMetric: 396.4945, val_loss: 397.8381, val_MinusLogProbMetric: 397.8381

Epoch 138: val_loss improved from 398.14633 to 397.83813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 396.4945 - MinusLogProbMetric: 396.4945 - val_loss: 397.8381 - val_MinusLogProbMetric: 397.8381 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 139/1000
2023-10-03 22:51:56.335 
Epoch 139/1000 
	 loss: 396.5072, MinusLogProbMetric: 396.5072, val_loss: 398.9325, val_MinusLogProbMetric: 398.9325

Epoch 139: val_loss did not improve from 397.83813
196/196 - 7s - loss: 396.5072 - MinusLogProbMetric: 396.5072 - val_loss: 398.9325 - val_MinusLogProbMetric: 398.9325 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 140/1000
2023-10-03 22:52:03.102 
Epoch 140/1000 
	 loss: 396.5553, MinusLogProbMetric: 396.5553, val_loss: 397.5052, val_MinusLogProbMetric: 397.5052

Epoch 140: val_loss improved from 397.83813 to 397.50519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 396.5553 - MinusLogProbMetric: 396.5553 - val_loss: 397.5052 - val_MinusLogProbMetric: 397.5052 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 141/1000
2023-10-03 22:52:10.220 
Epoch 141/1000 
	 loss: 396.3578, MinusLogProbMetric: 396.3578, val_loss: 397.6510, val_MinusLogProbMetric: 397.6510

Epoch 141: val_loss did not improve from 397.50519
196/196 - 7s - loss: 396.3578 - MinusLogProbMetric: 396.3578 - val_loss: 397.6510 - val_MinusLogProbMetric: 397.6510 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 142/1000
2023-10-03 22:52:16.875 
Epoch 142/1000 
	 loss: 396.4622, MinusLogProbMetric: 396.4622, val_loss: 397.2152, val_MinusLogProbMetric: 397.2152

Epoch 142: val_loss improved from 397.50519 to 397.21521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 396.4622 - MinusLogProbMetric: 396.4622 - val_loss: 397.2152 - val_MinusLogProbMetric: 397.2152 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 143/1000
2023-10-03 22:52:23.938 
Epoch 143/1000 
	 loss: 396.2741, MinusLogProbMetric: 396.2741, val_loss: 396.9893, val_MinusLogProbMetric: 396.9893

Epoch 143: val_loss improved from 397.21521 to 396.98929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 396.2741 - MinusLogProbMetric: 396.2741 - val_loss: 396.9893 - val_MinusLogProbMetric: 396.9893 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 144/1000
2023-10-03 22:52:30.913 
Epoch 144/1000 
	 loss: 396.1785, MinusLogProbMetric: 396.1785, val_loss: 397.4725, val_MinusLogProbMetric: 397.4725

Epoch 144: val_loss did not improve from 396.98929
196/196 - 7s - loss: 396.1785 - MinusLogProbMetric: 396.1785 - val_loss: 397.4725 - val_MinusLogProbMetric: 397.4725 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 145/1000
2023-10-03 22:52:37.622 
Epoch 145/1000 
	 loss: 396.2292, MinusLogProbMetric: 396.2292, val_loss: 397.9727, val_MinusLogProbMetric: 397.9727

Epoch 145: val_loss did not improve from 396.98929
196/196 - 7s - loss: 396.2292 - MinusLogProbMetric: 396.2292 - val_loss: 397.9727 - val_MinusLogProbMetric: 397.9727 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 146/1000
2023-10-03 22:52:44.423 
Epoch 146/1000 
	 loss: 395.9855, MinusLogProbMetric: 395.9855, val_loss: 398.3645, val_MinusLogProbMetric: 398.3645

Epoch 146: val_loss did not improve from 396.98929
196/196 - 7s - loss: 395.9855 - MinusLogProbMetric: 395.9855 - val_loss: 398.3645 - val_MinusLogProbMetric: 398.3645 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 147/1000
2023-10-03 22:52:51.111 
Epoch 147/1000 
	 loss: 396.2850, MinusLogProbMetric: 396.2850, val_loss: 397.0390, val_MinusLogProbMetric: 397.0390

Epoch 147: val_loss did not improve from 396.98929
196/196 - 7s - loss: 396.2850 - MinusLogProbMetric: 396.2850 - val_loss: 397.0390 - val_MinusLogProbMetric: 397.0390 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 148/1000
2023-10-03 22:52:57.755 
Epoch 148/1000 
	 loss: 396.0327, MinusLogProbMetric: 396.0327, val_loss: 398.1351, val_MinusLogProbMetric: 398.1351

Epoch 148: val_loss did not improve from 396.98929
196/196 - 7s - loss: 396.0327 - MinusLogProbMetric: 396.0327 - val_loss: 398.1351 - val_MinusLogProbMetric: 398.1351 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 149/1000
2023-10-03 22:53:04.627 
Epoch 149/1000 
	 loss: 395.9032, MinusLogProbMetric: 395.9032, val_loss: 399.4429, val_MinusLogProbMetric: 399.4429

Epoch 149: val_loss did not improve from 396.98929
196/196 - 7s - loss: 395.9032 - MinusLogProbMetric: 395.9032 - val_loss: 399.4429 - val_MinusLogProbMetric: 399.4429 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 150/1000
2023-10-03 22:53:11.429 
Epoch 150/1000 
	 loss: 396.0262, MinusLogProbMetric: 396.0262, val_loss: 397.5475, val_MinusLogProbMetric: 397.5475

Epoch 150: val_loss did not improve from 396.98929
196/196 - 7s - loss: 396.0262 - MinusLogProbMetric: 396.0262 - val_loss: 397.5475 - val_MinusLogProbMetric: 397.5475 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 151/1000
2023-10-03 22:53:18.156 
Epoch 151/1000 
	 loss: 396.0105, MinusLogProbMetric: 396.0105, val_loss: 397.1665, val_MinusLogProbMetric: 397.1665

Epoch 151: val_loss did not improve from 396.98929
196/196 - 7s - loss: 396.0105 - MinusLogProbMetric: 396.0105 - val_loss: 397.1665 - val_MinusLogProbMetric: 397.1665 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 152/1000
2023-10-03 22:53:24.843 
Epoch 152/1000 
	 loss: 395.6659, MinusLogProbMetric: 395.6659, val_loss: 396.7447, val_MinusLogProbMetric: 396.7447

Epoch 152: val_loss improved from 396.98929 to 396.74469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 395.6659 - MinusLogProbMetric: 395.6659 - val_loss: 396.7447 - val_MinusLogProbMetric: 396.7447 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 153/1000
2023-10-03 22:53:31.936 
Epoch 153/1000 
	 loss: 395.7631, MinusLogProbMetric: 395.7631, val_loss: 397.7573, val_MinusLogProbMetric: 397.7573

Epoch 153: val_loss did not improve from 396.74469
196/196 - 7s - loss: 395.7631 - MinusLogProbMetric: 395.7631 - val_loss: 397.7573 - val_MinusLogProbMetric: 397.7573 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 154/1000
2023-10-03 22:53:38.597 
Epoch 154/1000 
	 loss: 395.6602, MinusLogProbMetric: 395.6602, val_loss: 397.6506, val_MinusLogProbMetric: 397.6506

Epoch 154: val_loss did not improve from 396.74469
196/196 - 7s - loss: 395.6602 - MinusLogProbMetric: 395.6602 - val_loss: 397.6506 - val_MinusLogProbMetric: 397.6506 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 155/1000
2023-10-03 22:53:45.221 
Epoch 155/1000 
	 loss: 395.5500, MinusLogProbMetric: 395.5500, val_loss: 398.2403, val_MinusLogProbMetric: 398.2403

Epoch 155: val_loss did not improve from 396.74469
196/196 - 7s - loss: 395.5500 - MinusLogProbMetric: 395.5500 - val_loss: 398.2403 - val_MinusLogProbMetric: 398.2403 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 156/1000
2023-10-03 22:53:52.101 
Epoch 156/1000 
	 loss: 395.7806, MinusLogProbMetric: 395.7806, val_loss: 396.9177, val_MinusLogProbMetric: 396.9177

Epoch 156: val_loss did not improve from 396.74469
196/196 - 7s - loss: 395.7806 - MinusLogProbMetric: 395.7806 - val_loss: 396.9177 - val_MinusLogProbMetric: 396.9177 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 157/1000
2023-10-03 22:53:58.924 
Epoch 157/1000 
	 loss: 395.5557, MinusLogProbMetric: 395.5557, val_loss: 397.0165, val_MinusLogProbMetric: 397.0165

Epoch 157: val_loss did not improve from 396.74469
196/196 - 7s - loss: 395.5557 - MinusLogProbMetric: 395.5557 - val_loss: 397.0165 - val_MinusLogProbMetric: 397.0165 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 158/1000
2023-10-03 22:54:06.185 
Epoch 158/1000 
	 loss: 395.5836, MinusLogProbMetric: 395.5836, val_loss: 396.4819, val_MinusLogProbMetric: 396.4819

Epoch 158: val_loss improved from 396.74469 to 396.48190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 395.5836 - MinusLogProbMetric: 395.5836 - val_loss: 396.4819 - val_MinusLogProbMetric: 396.4819 - lr: 1.6667e-04 - 8s/epoch - 38ms/step
Epoch 159/1000
2023-10-03 22:54:13.256 
Epoch 159/1000 
	 loss: 395.4354, MinusLogProbMetric: 395.4354, val_loss: 396.6332, val_MinusLogProbMetric: 396.6332

Epoch 159: val_loss did not improve from 396.48190
196/196 - 7s - loss: 395.4354 - MinusLogProbMetric: 395.4354 - val_loss: 396.6332 - val_MinusLogProbMetric: 396.6332 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 160/1000
2023-10-03 22:54:20.152 
Epoch 160/1000 
	 loss: 395.5280, MinusLogProbMetric: 395.5280, val_loss: 396.7285, val_MinusLogProbMetric: 396.7285

Epoch 160: val_loss did not improve from 396.48190
196/196 - 7s - loss: 395.5280 - MinusLogProbMetric: 395.5280 - val_loss: 396.7285 - val_MinusLogProbMetric: 396.7285 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 161/1000
2023-10-03 22:54:26.771 
Epoch 161/1000 
	 loss: 395.3311, MinusLogProbMetric: 395.3311, val_loss: 397.2191, val_MinusLogProbMetric: 397.2191

Epoch 161: val_loss did not improve from 396.48190
196/196 - 7s - loss: 395.3311 - MinusLogProbMetric: 395.3311 - val_loss: 397.2191 - val_MinusLogProbMetric: 397.2191 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 162/1000
2023-10-03 22:54:33.461 
Epoch 162/1000 
	 loss: 395.5630, MinusLogProbMetric: 395.5630, val_loss: 399.0376, val_MinusLogProbMetric: 399.0376

Epoch 162: val_loss did not improve from 396.48190
196/196 - 7s - loss: 395.5630 - MinusLogProbMetric: 395.5630 - val_loss: 399.0376 - val_MinusLogProbMetric: 399.0376 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 163/1000
2023-10-03 22:54:40.255 
Epoch 163/1000 
	 loss: 395.3400, MinusLogProbMetric: 395.3400, val_loss: 396.9189, val_MinusLogProbMetric: 396.9189

Epoch 163: val_loss did not improve from 396.48190
196/196 - 7s - loss: 395.3400 - MinusLogProbMetric: 395.3400 - val_loss: 396.9189 - val_MinusLogProbMetric: 396.9189 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 164/1000
2023-10-03 22:54:46.897 
Epoch 164/1000 
	 loss: 395.1678, MinusLogProbMetric: 395.1678, val_loss: 396.7716, val_MinusLogProbMetric: 396.7716

Epoch 164: val_loss did not improve from 396.48190
196/196 - 7s - loss: 395.1678 - MinusLogProbMetric: 395.1678 - val_loss: 396.7716 - val_MinusLogProbMetric: 396.7716 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 165/1000
2023-10-03 22:54:53.539 
Epoch 165/1000 
	 loss: 395.1567, MinusLogProbMetric: 395.1567, val_loss: 396.4788, val_MinusLogProbMetric: 396.4788

Epoch 165: val_loss improved from 396.48190 to 396.47879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 395.1567 - MinusLogProbMetric: 395.1567 - val_loss: 396.4788 - val_MinusLogProbMetric: 396.4788 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 166/1000
2023-10-03 22:55:00.382 
Epoch 166/1000 
	 loss: 395.1338, MinusLogProbMetric: 395.1338, val_loss: 396.5767, val_MinusLogProbMetric: 396.5767

Epoch 166: val_loss did not improve from 396.47879
196/196 - 7s - loss: 395.1338 - MinusLogProbMetric: 395.1338 - val_loss: 396.5767 - val_MinusLogProbMetric: 396.5767 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 167/1000
2023-10-03 22:55:07.208 
Epoch 167/1000 
	 loss: 395.1965, MinusLogProbMetric: 395.1965, val_loss: 396.4661, val_MinusLogProbMetric: 396.4661

Epoch 167: val_loss improved from 396.47879 to 396.46606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 395.1965 - MinusLogProbMetric: 395.1965 - val_loss: 396.4661 - val_MinusLogProbMetric: 396.4661 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 168/1000
2023-10-03 22:55:14.092 
Epoch 168/1000 
	 loss: 395.5431, MinusLogProbMetric: 395.5431, val_loss: 397.4070, val_MinusLogProbMetric: 397.4070

Epoch 168: val_loss did not improve from 396.46606
196/196 - 7s - loss: 395.5431 - MinusLogProbMetric: 395.5431 - val_loss: 397.4070 - val_MinusLogProbMetric: 397.4070 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 169/1000
2023-10-03 22:55:20.731 
Epoch 169/1000 
	 loss: 394.9380, MinusLogProbMetric: 394.9380, val_loss: 396.9651, val_MinusLogProbMetric: 396.9651

Epoch 169: val_loss did not improve from 396.46606
196/196 - 7s - loss: 394.9380 - MinusLogProbMetric: 394.9380 - val_loss: 396.9651 - val_MinusLogProbMetric: 396.9651 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 170/1000
2023-10-03 22:55:27.668 
Epoch 170/1000 
	 loss: 395.1358, MinusLogProbMetric: 395.1358, val_loss: 395.7200, val_MinusLogProbMetric: 395.7200

Epoch 170: val_loss improved from 396.46606 to 395.72003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 395.1358 - MinusLogProbMetric: 395.1358 - val_loss: 395.7200 - val_MinusLogProbMetric: 395.7200 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 171/1000
2023-10-03 22:55:34.632 
Epoch 171/1000 
	 loss: 394.9081, MinusLogProbMetric: 394.9081, val_loss: 396.8337, val_MinusLogProbMetric: 396.8337

Epoch 171: val_loss did not improve from 395.72003
196/196 - 7s - loss: 394.9081 - MinusLogProbMetric: 394.9081 - val_loss: 396.8337 - val_MinusLogProbMetric: 396.8337 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 172/1000
2023-10-03 22:55:41.368 
Epoch 172/1000 
	 loss: 394.9483, MinusLogProbMetric: 394.9483, val_loss: 398.1254, val_MinusLogProbMetric: 398.1254

Epoch 172: val_loss did not improve from 395.72003
196/196 - 7s - loss: 394.9483 - MinusLogProbMetric: 394.9483 - val_loss: 398.1254 - val_MinusLogProbMetric: 398.1254 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 173/1000
2023-10-03 22:55:48.047 
Epoch 173/1000 
	 loss: 394.9529, MinusLogProbMetric: 394.9529, val_loss: 395.7079, val_MinusLogProbMetric: 395.7079

Epoch 173: val_loss improved from 395.72003 to 395.70786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 394.9529 - MinusLogProbMetric: 394.9529 - val_loss: 395.7079 - val_MinusLogProbMetric: 395.7079 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 174/1000
2023-10-03 22:55:54.923 
Epoch 174/1000 
	 loss: 394.9663, MinusLogProbMetric: 394.9663, val_loss: 397.2562, val_MinusLogProbMetric: 397.2562

Epoch 174: val_loss did not improve from 395.70786
196/196 - 7s - loss: 394.9663 - MinusLogProbMetric: 394.9663 - val_loss: 397.2562 - val_MinusLogProbMetric: 397.2562 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 175/1000
2023-10-03 22:56:01.688 
Epoch 175/1000 
	 loss: 394.9675, MinusLogProbMetric: 394.9675, val_loss: 397.4052, val_MinusLogProbMetric: 397.4052

Epoch 175: val_loss did not improve from 395.70786
196/196 - 7s - loss: 394.9675 - MinusLogProbMetric: 394.9675 - val_loss: 397.4052 - val_MinusLogProbMetric: 397.4052 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 176/1000
2023-10-03 22:56:08.365 
Epoch 176/1000 
	 loss: 395.0032, MinusLogProbMetric: 395.0032, val_loss: 396.4649, val_MinusLogProbMetric: 396.4649

Epoch 176: val_loss did not improve from 395.70786
196/196 - 7s - loss: 395.0032 - MinusLogProbMetric: 395.0032 - val_loss: 396.4649 - val_MinusLogProbMetric: 396.4649 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 177/1000
2023-10-03 22:56:15.070 
Epoch 177/1000 
	 loss: 394.6353, MinusLogProbMetric: 394.6353, val_loss: 395.6023, val_MinusLogProbMetric: 395.6023

Epoch 177: val_loss improved from 395.70786 to 395.60226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 394.6353 - MinusLogProbMetric: 394.6353 - val_loss: 395.6023 - val_MinusLogProbMetric: 395.6023 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 178/1000
2023-10-03 22:56:22.019 
Epoch 178/1000 
	 loss: 394.6961, MinusLogProbMetric: 394.6961, val_loss: 395.8407, val_MinusLogProbMetric: 395.8407

Epoch 178: val_loss did not improve from 395.60226
196/196 - 7s - loss: 394.6961 - MinusLogProbMetric: 394.6961 - val_loss: 395.8407 - val_MinusLogProbMetric: 395.8407 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 179/1000
2023-10-03 22:56:28.790 
Epoch 179/1000 
	 loss: 394.8192, MinusLogProbMetric: 394.8192, val_loss: 398.3893, val_MinusLogProbMetric: 398.3893

Epoch 179: val_loss did not improve from 395.60226
196/196 - 7s - loss: 394.8192 - MinusLogProbMetric: 394.8192 - val_loss: 398.3893 - val_MinusLogProbMetric: 398.3893 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 180/1000
2023-10-03 22:56:35.593 
Epoch 180/1000 
	 loss: 394.6577, MinusLogProbMetric: 394.6577, val_loss: 395.5759, val_MinusLogProbMetric: 395.5759

Epoch 180: val_loss improved from 395.60226 to 395.57590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 394.6577 - MinusLogProbMetric: 394.6577 - val_loss: 395.5759 - val_MinusLogProbMetric: 395.5759 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 181/1000
2023-10-03 22:56:42.681 
Epoch 181/1000 
	 loss: 394.5621, MinusLogProbMetric: 394.5621, val_loss: 397.0302, val_MinusLogProbMetric: 397.0302

Epoch 181: val_loss did not improve from 395.57590
196/196 - 7s - loss: 394.5621 - MinusLogProbMetric: 394.5621 - val_loss: 397.0302 - val_MinusLogProbMetric: 397.0302 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 182/1000
2023-10-03 22:56:49.435 
Epoch 182/1000 
	 loss: 394.6868, MinusLogProbMetric: 394.6868, val_loss: 398.5199, val_MinusLogProbMetric: 398.5199

Epoch 182: val_loss did not improve from 395.57590
196/196 - 7s - loss: 394.6868 - MinusLogProbMetric: 394.6868 - val_loss: 398.5199 - val_MinusLogProbMetric: 398.5199 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 183/1000
2023-10-03 22:56:56.284 
Epoch 183/1000 
	 loss: 394.6459, MinusLogProbMetric: 394.6459, val_loss: 396.0051, val_MinusLogProbMetric: 396.0051

Epoch 183: val_loss did not improve from 395.57590
196/196 - 7s - loss: 394.6459 - MinusLogProbMetric: 394.6459 - val_loss: 396.0051 - val_MinusLogProbMetric: 396.0051 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 184/1000
2023-10-03 22:57:02.863 
Epoch 184/1000 
	 loss: 394.5478, MinusLogProbMetric: 394.5478, val_loss: 395.9619, val_MinusLogProbMetric: 395.9619

Epoch 184: val_loss did not improve from 395.57590
196/196 - 7s - loss: 394.5478 - MinusLogProbMetric: 394.5478 - val_loss: 395.9619 - val_MinusLogProbMetric: 395.9619 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 185/1000
2023-10-03 22:57:09.611 
Epoch 185/1000 
	 loss: 394.5967, MinusLogProbMetric: 394.5967, val_loss: 397.4357, val_MinusLogProbMetric: 397.4357

Epoch 185: val_loss did not improve from 395.57590
196/196 - 7s - loss: 394.5967 - MinusLogProbMetric: 394.5967 - val_loss: 397.4357 - val_MinusLogProbMetric: 397.4357 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 186/1000
2023-10-03 22:57:16.813 
Epoch 186/1000 
	 loss: 394.3960, MinusLogProbMetric: 394.3960, val_loss: 396.0124, val_MinusLogProbMetric: 396.0124

Epoch 186: val_loss did not improve from 395.57590
196/196 - 7s - loss: 394.3960 - MinusLogProbMetric: 394.3960 - val_loss: 396.0124 - val_MinusLogProbMetric: 396.0124 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 187/1000
2023-10-03 22:57:24.171 
Epoch 187/1000 
	 loss: 394.6456, MinusLogProbMetric: 394.6456, val_loss: 395.3595, val_MinusLogProbMetric: 395.3595

Epoch 187: val_loss improved from 395.57590 to 395.35950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 394.6456 - MinusLogProbMetric: 394.6456 - val_loss: 395.3595 - val_MinusLogProbMetric: 395.3595 - lr: 1.6667e-04 - 8s/epoch - 39ms/step
Epoch 188/1000
2023-10-03 22:57:31.567 
Epoch 188/1000 
	 loss: 394.3896, MinusLogProbMetric: 394.3896, val_loss: 396.2939, val_MinusLogProbMetric: 396.2939

Epoch 188: val_loss did not improve from 395.35950
196/196 - 7s - loss: 394.3896 - MinusLogProbMetric: 394.3896 - val_loss: 396.2939 - val_MinusLogProbMetric: 396.2939 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 189/1000
2023-10-03 22:57:38.337 
Epoch 189/1000 
	 loss: 394.5283, MinusLogProbMetric: 394.5283, val_loss: 395.0525, val_MinusLogProbMetric: 395.0525

Epoch 189: val_loss improved from 395.35950 to 395.05249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 394.5283 - MinusLogProbMetric: 394.5283 - val_loss: 395.0525 - val_MinusLogProbMetric: 395.0525 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 190/1000
2023-10-03 22:57:45.305 
Epoch 190/1000 
	 loss: 394.1434, MinusLogProbMetric: 394.1434, val_loss: 395.4054, val_MinusLogProbMetric: 395.4054

Epoch 190: val_loss did not improve from 395.05249
196/196 - 7s - loss: 394.1434 - MinusLogProbMetric: 394.1434 - val_loss: 395.4054 - val_MinusLogProbMetric: 395.4054 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 191/1000
2023-10-03 22:57:51.982 
Epoch 191/1000 
	 loss: 394.4855, MinusLogProbMetric: 394.4855, val_loss: 395.1378, val_MinusLogProbMetric: 395.1378

Epoch 191: val_loss did not improve from 395.05249
196/196 - 7s - loss: 394.4855 - MinusLogProbMetric: 394.4855 - val_loss: 395.1378 - val_MinusLogProbMetric: 395.1378 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 192/1000
2023-10-03 22:57:58.705 
Epoch 192/1000 
	 loss: 394.2629, MinusLogProbMetric: 394.2629, val_loss: 396.0552, val_MinusLogProbMetric: 396.0552

Epoch 192: val_loss did not improve from 395.05249
196/196 - 7s - loss: 394.2629 - MinusLogProbMetric: 394.2629 - val_loss: 396.0552 - val_MinusLogProbMetric: 396.0552 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 193/1000
2023-10-03 22:58:05.444 
Epoch 193/1000 
	 loss: 394.3013, MinusLogProbMetric: 394.3013, val_loss: 396.7840, val_MinusLogProbMetric: 396.7840

Epoch 193: val_loss did not improve from 395.05249
196/196 - 7s - loss: 394.3013 - MinusLogProbMetric: 394.3013 - val_loss: 396.7840 - val_MinusLogProbMetric: 396.7840 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 194/1000
2023-10-03 22:58:12.140 
Epoch 194/1000 
	 loss: 394.5154, MinusLogProbMetric: 394.5154, val_loss: 394.9719, val_MinusLogProbMetric: 394.9719

Epoch 194: val_loss improved from 395.05249 to 394.97186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 394.5154 - MinusLogProbMetric: 394.5154 - val_loss: 394.9719 - val_MinusLogProbMetric: 394.9719 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 195/1000
2023-10-03 22:58:19.240 
Epoch 195/1000 
	 loss: 394.1087, MinusLogProbMetric: 394.1087, val_loss: 395.5504, val_MinusLogProbMetric: 395.5504

Epoch 195: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.1087 - MinusLogProbMetric: 394.1087 - val_loss: 395.5504 - val_MinusLogProbMetric: 395.5504 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 196/1000
2023-10-03 22:58:25.872 
Epoch 196/1000 
	 loss: 394.5255, MinusLogProbMetric: 394.5255, val_loss: 395.1739, val_MinusLogProbMetric: 395.1739

Epoch 196: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.5255 - MinusLogProbMetric: 394.5255 - val_loss: 395.1739 - val_MinusLogProbMetric: 395.1739 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 197/1000
2023-10-03 22:58:32.542 
Epoch 197/1000 
	 loss: 394.0653, MinusLogProbMetric: 394.0653, val_loss: 395.5429, val_MinusLogProbMetric: 395.5429

Epoch 197: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.0653 - MinusLogProbMetric: 394.0653 - val_loss: 395.5429 - val_MinusLogProbMetric: 395.5429 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 198/1000
2023-10-03 22:58:39.336 
Epoch 198/1000 
	 loss: 394.2531, MinusLogProbMetric: 394.2531, val_loss: 395.2314, val_MinusLogProbMetric: 395.2314

Epoch 198: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.2531 - MinusLogProbMetric: 394.2531 - val_loss: 395.2314 - val_MinusLogProbMetric: 395.2314 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 199/1000
2023-10-03 22:58:46.067 
Epoch 199/1000 
	 loss: 394.1502, MinusLogProbMetric: 394.1502, val_loss: 396.0501, val_MinusLogProbMetric: 396.0501

Epoch 199: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.1502 - MinusLogProbMetric: 394.1502 - val_loss: 396.0501 - val_MinusLogProbMetric: 396.0501 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 200/1000
2023-10-03 22:58:52.716 
Epoch 200/1000 
	 loss: 394.2368, MinusLogProbMetric: 394.2368, val_loss: 395.8972, val_MinusLogProbMetric: 395.8972

Epoch 200: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.2368 - MinusLogProbMetric: 394.2368 - val_loss: 395.8972 - val_MinusLogProbMetric: 395.8972 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 201/1000
2023-10-03 22:58:59.554 
Epoch 201/1000 
	 loss: 394.1013, MinusLogProbMetric: 394.1013, val_loss: 395.9155, val_MinusLogProbMetric: 395.9155

Epoch 201: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.1013 - MinusLogProbMetric: 394.1013 - val_loss: 395.9155 - val_MinusLogProbMetric: 395.9155 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 202/1000
2023-10-03 22:59:06.235 
Epoch 202/1000 
	 loss: 394.0140, MinusLogProbMetric: 394.0140, val_loss: 395.1765, val_MinusLogProbMetric: 395.1765

Epoch 202: val_loss did not improve from 394.97186
196/196 - 7s - loss: 394.0140 - MinusLogProbMetric: 394.0140 - val_loss: 395.1765 - val_MinusLogProbMetric: 395.1765 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 203/1000
2023-10-03 22:59:13.045 
Epoch 203/1000 
	 loss: 394.0892, MinusLogProbMetric: 394.0892, val_loss: 394.9301, val_MinusLogProbMetric: 394.9301

Epoch 203: val_loss improved from 394.97186 to 394.93015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 394.0892 - MinusLogProbMetric: 394.0892 - val_loss: 394.9301 - val_MinusLogProbMetric: 394.9301 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 204/1000
2023-10-03 22:59:20.202 
Epoch 204/1000 
	 loss: 393.9595, MinusLogProbMetric: 393.9595, val_loss: 398.2416, val_MinusLogProbMetric: 398.2416

Epoch 204: val_loss did not improve from 394.93015
196/196 - 7s - loss: 393.9595 - MinusLogProbMetric: 393.9595 - val_loss: 398.2416 - val_MinusLogProbMetric: 398.2416 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 205/1000
2023-10-03 22:59:27.088 
Epoch 205/1000 
	 loss: 394.0347, MinusLogProbMetric: 394.0347, val_loss: 395.8888, val_MinusLogProbMetric: 395.8888

Epoch 205: val_loss did not improve from 394.93015
196/196 - 7s - loss: 394.0347 - MinusLogProbMetric: 394.0347 - val_loss: 395.8888 - val_MinusLogProbMetric: 395.8888 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 206/1000
2023-10-03 22:59:33.781 
Epoch 206/1000 
	 loss: 393.9938, MinusLogProbMetric: 393.9938, val_loss: 395.0510, val_MinusLogProbMetric: 395.0510

Epoch 206: val_loss did not improve from 394.93015
196/196 - 7s - loss: 393.9938 - MinusLogProbMetric: 393.9938 - val_loss: 395.0510 - val_MinusLogProbMetric: 395.0510 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 207/1000
2023-10-03 22:59:40.612 
Epoch 207/1000 
	 loss: 393.8417, MinusLogProbMetric: 393.8417, val_loss: 395.4852, val_MinusLogProbMetric: 395.4852

Epoch 207: val_loss did not improve from 394.93015
196/196 - 7s - loss: 393.8417 - MinusLogProbMetric: 393.8417 - val_loss: 395.4852 - val_MinusLogProbMetric: 395.4852 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 208/1000
2023-10-03 22:59:47.329 
Epoch 208/1000 
	 loss: 393.7267, MinusLogProbMetric: 393.7267, val_loss: 395.2178, val_MinusLogProbMetric: 395.2178

Epoch 208: val_loss did not improve from 394.93015
196/196 - 7s - loss: 393.7267 - MinusLogProbMetric: 393.7267 - val_loss: 395.2178 - val_MinusLogProbMetric: 395.2178 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 209/1000
2023-10-03 22:59:54.037 
Epoch 209/1000 
	 loss: 394.0894, MinusLogProbMetric: 394.0894, val_loss: 396.4767, val_MinusLogProbMetric: 396.4767

Epoch 209: val_loss did not improve from 394.93015
196/196 - 7s - loss: 394.0894 - MinusLogProbMetric: 394.0894 - val_loss: 396.4767 - val_MinusLogProbMetric: 396.4767 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 210/1000
2023-10-03 23:00:00.881 
Epoch 210/1000 
	 loss: 394.0092, MinusLogProbMetric: 394.0092, val_loss: 398.0626, val_MinusLogProbMetric: 398.0626

Epoch 210: val_loss did not improve from 394.93015
196/196 - 7s - loss: 394.0092 - MinusLogProbMetric: 394.0092 - val_loss: 398.0626 - val_MinusLogProbMetric: 398.0626 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 211/1000
2023-10-03 23:00:07.575 
Epoch 211/1000 
	 loss: 393.7933, MinusLogProbMetric: 393.7933, val_loss: 396.4808, val_MinusLogProbMetric: 396.4808

Epoch 211: val_loss did not improve from 394.93015
196/196 - 7s - loss: 393.7933 - MinusLogProbMetric: 393.7933 - val_loss: 396.4808 - val_MinusLogProbMetric: 396.4808 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 212/1000
2023-10-03 23:00:14.261 
Epoch 212/1000 
	 loss: 393.9308, MinusLogProbMetric: 393.9308, val_loss: 394.7236, val_MinusLogProbMetric: 394.7236

Epoch 212: val_loss improved from 394.93015 to 394.72363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 393.9308 - MinusLogProbMetric: 393.9308 - val_loss: 394.7236 - val_MinusLogProbMetric: 394.7236 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 213/1000
2023-10-03 23:00:21.375 
Epoch 213/1000 
	 loss: 394.1099, MinusLogProbMetric: 394.1099, val_loss: 396.2855, val_MinusLogProbMetric: 396.2855

Epoch 213: val_loss did not improve from 394.72363
196/196 - 7s - loss: 394.1099 - MinusLogProbMetric: 394.1099 - val_loss: 396.2855 - val_MinusLogProbMetric: 396.2855 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 214/1000
2023-10-03 23:00:28.259 
Epoch 214/1000 
	 loss: 393.6531, MinusLogProbMetric: 393.6531, val_loss: 395.3580, val_MinusLogProbMetric: 395.3580

Epoch 214: val_loss did not improve from 394.72363
196/196 - 7s - loss: 393.6531 - MinusLogProbMetric: 393.6531 - val_loss: 395.3580 - val_MinusLogProbMetric: 395.3580 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 215/1000
2023-10-03 23:00:35.420 
Epoch 215/1000 
	 loss: 393.9086, MinusLogProbMetric: 393.9086, val_loss: 397.5820, val_MinusLogProbMetric: 397.5820

Epoch 215: val_loss did not improve from 394.72363
196/196 - 7s - loss: 393.9086 - MinusLogProbMetric: 393.9086 - val_loss: 397.5820 - val_MinusLogProbMetric: 397.5820 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 216/1000
2023-10-03 23:00:42.891 
Epoch 216/1000 
	 loss: 393.5443, MinusLogProbMetric: 393.5443, val_loss: 394.8470, val_MinusLogProbMetric: 394.8470

Epoch 216: val_loss did not improve from 394.72363
196/196 - 7s - loss: 393.5443 - MinusLogProbMetric: 393.5443 - val_loss: 394.8470 - val_MinusLogProbMetric: 394.8470 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 217/1000
2023-10-03 23:00:49.621 
Epoch 217/1000 
	 loss: 393.6132, MinusLogProbMetric: 393.6132, val_loss: 395.0458, val_MinusLogProbMetric: 395.0458

Epoch 217: val_loss did not improve from 394.72363
196/196 - 7s - loss: 393.6132 - MinusLogProbMetric: 393.6132 - val_loss: 395.0458 - val_MinusLogProbMetric: 395.0458 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 218/1000
2023-10-03 23:00:56.324 
Epoch 218/1000 
	 loss: 393.7997, MinusLogProbMetric: 393.7997, val_loss: 394.6713, val_MinusLogProbMetric: 394.6713

Epoch 218: val_loss improved from 394.72363 to 394.67126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 393.7997 - MinusLogProbMetric: 393.7997 - val_loss: 394.6713 - val_MinusLogProbMetric: 394.6713 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 219/1000
2023-10-03 23:01:03.222 
Epoch 219/1000 
	 loss: 393.6631, MinusLogProbMetric: 393.6631, val_loss: 395.4464, val_MinusLogProbMetric: 395.4464

Epoch 219: val_loss did not improve from 394.67126
196/196 - 7s - loss: 393.6631 - MinusLogProbMetric: 393.6631 - val_loss: 395.4464 - val_MinusLogProbMetric: 395.4464 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 220/1000
2023-10-03 23:01:09.929 
Epoch 220/1000 
	 loss: 393.6496, MinusLogProbMetric: 393.6496, val_loss: 394.7541, val_MinusLogProbMetric: 394.7541

Epoch 220: val_loss did not improve from 394.67126
196/196 - 7s - loss: 393.6496 - MinusLogProbMetric: 393.6496 - val_loss: 394.7541 - val_MinusLogProbMetric: 394.7541 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 221/1000
2023-10-03 23:01:16.830 
Epoch 221/1000 
	 loss: 393.3223, MinusLogProbMetric: 393.3223, val_loss: 395.4917, val_MinusLogProbMetric: 395.4917

Epoch 221: val_loss did not improve from 394.67126
196/196 - 7s - loss: 393.3223 - MinusLogProbMetric: 393.3223 - val_loss: 395.4917 - val_MinusLogProbMetric: 395.4917 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 222/1000
2023-10-03 23:01:23.557 
Epoch 222/1000 
	 loss: 393.8844, MinusLogProbMetric: 393.8844, val_loss: 395.2995, val_MinusLogProbMetric: 395.2995

Epoch 222: val_loss did not improve from 394.67126
196/196 - 7s - loss: 393.8844 - MinusLogProbMetric: 393.8844 - val_loss: 395.2995 - val_MinusLogProbMetric: 395.2995 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 223/1000
2023-10-03 23:01:30.238 
Epoch 223/1000 
	 loss: 393.6456, MinusLogProbMetric: 393.6456, val_loss: 395.2789, val_MinusLogProbMetric: 395.2789

Epoch 223: val_loss did not improve from 394.67126
196/196 - 7s - loss: 393.6456 - MinusLogProbMetric: 393.6456 - val_loss: 395.2789 - val_MinusLogProbMetric: 395.2789 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 224/1000
2023-10-03 23:01:36.989 
Epoch 224/1000 
	 loss: 393.7492, MinusLogProbMetric: 393.7492, val_loss: 395.0431, val_MinusLogProbMetric: 395.0431

Epoch 224: val_loss did not improve from 394.67126
196/196 - 7s - loss: 393.7492 - MinusLogProbMetric: 393.7492 - val_loss: 395.0431 - val_MinusLogProbMetric: 395.0431 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 225/1000
2023-10-03 23:01:43.696 
Epoch 225/1000 
	 loss: 393.6650, MinusLogProbMetric: 393.6650, val_loss: 394.5899, val_MinusLogProbMetric: 394.5899

Epoch 225: val_loss improved from 394.67126 to 394.58987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 393.6650 - MinusLogProbMetric: 393.6650 - val_loss: 394.5899 - val_MinusLogProbMetric: 394.5899 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 226/1000
2023-10-03 23:01:50.779 
Epoch 226/1000 
	 loss: 393.3476, MinusLogProbMetric: 393.3476, val_loss: 395.5481, val_MinusLogProbMetric: 395.5481

Epoch 226: val_loss did not improve from 394.58987
196/196 - 7s - loss: 393.3476 - MinusLogProbMetric: 393.3476 - val_loss: 395.5481 - val_MinusLogProbMetric: 395.5481 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 227/1000
2023-10-03 23:01:57.326 
Epoch 227/1000 
	 loss: 393.5408, MinusLogProbMetric: 393.5408, val_loss: 394.9925, val_MinusLogProbMetric: 394.9925

Epoch 227: val_loss did not improve from 394.58987
196/196 - 7s - loss: 393.5408 - MinusLogProbMetric: 393.5408 - val_loss: 394.9925 - val_MinusLogProbMetric: 394.9925 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 228/1000
2023-10-03 23:02:04.228 
Epoch 228/1000 
	 loss: 393.4372, MinusLogProbMetric: 393.4372, val_loss: 395.6933, val_MinusLogProbMetric: 395.6933

Epoch 228: val_loss did not improve from 394.58987
196/196 - 7s - loss: 393.4372 - MinusLogProbMetric: 393.4372 - val_loss: 395.6933 - val_MinusLogProbMetric: 395.6933 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 229/1000
2023-10-03 23:02:10.892 
Epoch 229/1000 
	 loss: 393.4005, MinusLogProbMetric: 393.4005, val_loss: 394.9978, val_MinusLogProbMetric: 394.9978

Epoch 229: val_loss did not improve from 394.58987
196/196 - 7s - loss: 393.4005 - MinusLogProbMetric: 393.4005 - val_loss: 394.9978 - val_MinusLogProbMetric: 394.9978 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 230/1000
2023-10-03 23:02:17.604 
Epoch 230/1000 
	 loss: 393.3244, MinusLogProbMetric: 393.3244, val_loss: 394.4841, val_MinusLogProbMetric: 394.4841

Epoch 230: val_loss improved from 394.58987 to 394.48413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 393.3244 - MinusLogProbMetric: 393.3244 - val_loss: 394.4841 - val_MinusLogProbMetric: 394.4841 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 231/1000
2023-10-03 23:02:24.578 
Epoch 231/1000 
	 loss: 393.5072, MinusLogProbMetric: 393.5072, val_loss: 394.5195, val_MinusLogProbMetric: 394.5195

Epoch 231: val_loss did not improve from 394.48413
196/196 - 7s - loss: 393.5072 - MinusLogProbMetric: 393.5072 - val_loss: 394.5195 - val_MinusLogProbMetric: 394.5195 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 232/1000
2023-10-03 23:02:31.318 
Epoch 232/1000 
	 loss: 393.1982, MinusLogProbMetric: 393.1982, val_loss: 395.3307, val_MinusLogProbMetric: 395.3307

Epoch 232: val_loss did not improve from 394.48413
196/196 - 7s - loss: 393.1982 - MinusLogProbMetric: 393.1982 - val_loss: 395.3307 - val_MinusLogProbMetric: 395.3307 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 233/1000
2023-10-03 23:02:38.037 
Epoch 233/1000 
	 loss: 393.2760, MinusLogProbMetric: 393.2760, val_loss: 395.7853, val_MinusLogProbMetric: 395.7853

Epoch 233: val_loss did not improve from 394.48413
196/196 - 7s - loss: 393.2760 - MinusLogProbMetric: 393.2760 - val_loss: 395.7853 - val_MinusLogProbMetric: 395.7853 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 234/1000
2023-10-03 23:02:44.713 
Epoch 234/1000 
	 loss: 393.5221, MinusLogProbMetric: 393.5221, val_loss: 396.0740, val_MinusLogProbMetric: 396.0740

Epoch 234: val_loss did not improve from 394.48413
196/196 - 7s - loss: 393.5221 - MinusLogProbMetric: 393.5221 - val_loss: 396.0740 - val_MinusLogProbMetric: 396.0740 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 235/1000
2023-10-03 23:02:51.566 
Epoch 235/1000 
	 loss: 393.2948, MinusLogProbMetric: 393.2948, val_loss: 395.5056, val_MinusLogProbMetric: 395.5056

Epoch 235: val_loss did not improve from 394.48413
196/196 - 7s - loss: 393.2948 - MinusLogProbMetric: 393.2948 - val_loss: 395.5056 - val_MinusLogProbMetric: 395.5056 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 236/1000
2023-10-03 23:02:58.271 
Epoch 236/1000 
	 loss: 393.5464, MinusLogProbMetric: 393.5464, val_loss: 394.2672, val_MinusLogProbMetric: 394.2672

Epoch 236: val_loss improved from 394.48413 to 394.26724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 393.5464 - MinusLogProbMetric: 393.5464 - val_loss: 394.2672 - val_MinusLogProbMetric: 394.2672 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 237/1000
2023-10-03 23:03:05.546 
Epoch 237/1000 
	 loss: 393.0187, MinusLogProbMetric: 393.0187, val_loss: 394.2026, val_MinusLogProbMetric: 394.2026

Epoch 237: val_loss improved from 394.26724 to 394.20258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 393.0187 - MinusLogProbMetric: 393.0187 - val_loss: 394.2026 - val_MinusLogProbMetric: 394.2026 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 238/1000
2023-10-03 23:03:12.455 
Epoch 238/1000 
	 loss: 393.4605, MinusLogProbMetric: 393.4605, val_loss: 394.8210, val_MinusLogProbMetric: 394.8210

Epoch 238: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.4605 - MinusLogProbMetric: 393.4605 - val_loss: 394.8210 - val_MinusLogProbMetric: 394.8210 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 239/1000
2023-10-03 23:03:19.252 
Epoch 239/1000 
	 loss: 393.3008, MinusLogProbMetric: 393.3008, val_loss: 395.3602, val_MinusLogProbMetric: 395.3602

Epoch 239: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.3008 - MinusLogProbMetric: 393.3008 - val_loss: 395.3602 - val_MinusLogProbMetric: 395.3602 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 240/1000
2023-10-03 23:03:25.969 
Epoch 240/1000 
	 loss: 393.2524, MinusLogProbMetric: 393.2524, val_loss: 395.4704, val_MinusLogProbMetric: 395.4704

Epoch 240: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.2524 - MinusLogProbMetric: 393.2524 - val_loss: 395.4704 - val_MinusLogProbMetric: 395.4704 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 241/1000
2023-10-03 23:03:32.835 
Epoch 241/1000 
	 loss: 393.4153, MinusLogProbMetric: 393.4153, val_loss: 394.5665, val_MinusLogProbMetric: 394.5665

Epoch 241: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.4153 - MinusLogProbMetric: 393.4153 - val_loss: 394.5665 - val_MinusLogProbMetric: 394.5665 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 242/1000
2023-10-03 23:03:39.821 
Epoch 242/1000 
	 loss: 393.2928, MinusLogProbMetric: 393.2928, val_loss: 396.1491, val_MinusLogProbMetric: 396.1491

Epoch 242: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.2928 - MinusLogProbMetric: 393.2928 - val_loss: 396.1491 - val_MinusLogProbMetric: 396.1491 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 243/1000
2023-10-03 23:03:46.919 
Epoch 243/1000 
	 loss: 393.2778, MinusLogProbMetric: 393.2778, val_loss: 396.7513, val_MinusLogProbMetric: 396.7513

Epoch 243: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.2778 - MinusLogProbMetric: 393.2778 - val_loss: 396.7513 - val_MinusLogProbMetric: 396.7513 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 244/1000
2023-10-03 23:03:54.241 
Epoch 244/1000 
	 loss: 393.2092, MinusLogProbMetric: 393.2092, val_loss: 395.1498, val_MinusLogProbMetric: 395.1498

Epoch 244: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.2092 - MinusLogProbMetric: 393.2092 - val_loss: 395.1498 - val_MinusLogProbMetric: 395.1498 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 245/1000
2023-10-03 23:04:01.223 
Epoch 245/1000 
	 loss: 392.9723, MinusLogProbMetric: 392.9723, val_loss: 394.8709, val_MinusLogProbMetric: 394.8709

Epoch 245: val_loss did not improve from 394.20258
196/196 - 7s - loss: 392.9723 - MinusLogProbMetric: 392.9723 - val_loss: 394.8709 - val_MinusLogProbMetric: 394.8709 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 246/1000
2023-10-03 23:04:08.038 
Epoch 246/1000 
	 loss: 393.1584, MinusLogProbMetric: 393.1584, val_loss: 395.0035, val_MinusLogProbMetric: 395.0035

Epoch 246: val_loss did not improve from 394.20258
196/196 - 7s - loss: 393.1584 - MinusLogProbMetric: 393.1584 - val_loss: 395.0035 - val_MinusLogProbMetric: 395.0035 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 247/1000
2023-10-03 23:04:14.791 
Epoch 247/1000 
	 loss: 392.9545, MinusLogProbMetric: 392.9545, val_loss: 393.9295, val_MinusLogProbMetric: 393.9295

Epoch 247: val_loss improved from 394.20258 to 393.92953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.9545 - MinusLogProbMetric: 392.9545 - val_loss: 393.9295 - val_MinusLogProbMetric: 393.9295 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 248/1000
2023-10-03 23:04:21.776 
Epoch 248/1000 
	 loss: 393.3648, MinusLogProbMetric: 393.3648, val_loss: 395.1598, val_MinusLogProbMetric: 395.1598

Epoch 248: val_loss did not improve from 393.92953
196/196 - 7s - loss: 393.3648 - MinusLogProbMetric: 393.3648 - val_loss: 395.1598 - val_MinusLogProbMetric: 395.1598 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 249/1000
2023-10-03 23:04:28.520 
Epoch 249/1000 
	 loss: 392.9708, MinusLogProbMetric: 392.9708, val_loss: 394.3601, val_MinusLogProbMetric: 394.3601

Epoch 249: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.9708 - MinusLogProbMetric: 392.9708 - val_loss: 394.3601 - val_MinusLogProbMetric: 394.3601 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 250/1000
2023-10-03 23:04:35.327 
Epoch 250/1000 
	 loss: 392.9436, MinusLogProbMetric: 392.9436, val_loss: 395.1132, val_MinusLogProbMetric: 395.1132

Epoch 250: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.9436 - MinusLogProbMetric: 392.9436 - val_loss: 395.1132 - val_MinusLogProbMetric: 395.1132 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 251/1000
2023-10-03 23:04:42.000 
Epoch 251/1000 
	 loss: 392.9954, MinusLogProbMetric: 392.9954, val_loss: 394.8367, val_MinusLogProbMetric: 394.8367

Epoch 251: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.9954 - MinusLogProbMetric: 392.9954 - val_loss: 394.8367 - val_MinusLogProbMetric: 394.8367 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 252/1000
2023-10-03 23:04:48.708 
Epoch 252/1000 
	 loss: 392.9723, MinusLogProbMetric: 392.9723, val_loss: 394.7662, val_MinusLogProbMetric: 394.7662

Epoch 252: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.9723 - MinusLogProbMetric: 392.9723 - val_loss: 394.7662 - val_MinusLogProbMetric: 394.7662 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 253/1000
2023-10-03 23:04:55.599 
Epoch 253/1000 
	 loss: 392.9164, MinusLogProbMetric: 392.9164, val_loss: 394.7703, val_MinusLogProbMetric: 394.7703

Epoch 253: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.9164 - MinusLogProbMetric: 392.9164 - val_loss: 394.7703 - val_MinusLogProbMetric: 394.7703 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 254/1000
2023-10-03 23:05:02.308 
Epoch 254/1000 
	 loss: 393.1786, MinusLogProbMetric: 393.1786, val_loss: 395.0236, val_MinusLogProbMetric: 395.0236

Epoch 254: val_loss did not improve from 393.92953
196/196 - 7s - loss: 393.1786 - MinusLogProbMetric: 393.1786 - val_loss: 395.0236 - val_MinusLogProbMetric: 395.0236 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 255/1000
2023-10-03 23:05:09.001 
Epoch 255/1000 
	 loss: 392.9797, MinusLogProbMetric: 392.9797, val_loss: 394.4213, val_MinusLogProbMetric: 394.4213

Epoch 255: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.9797 - MinusLogProbMetric: 392.9797 - val_loss: 394.4213 - val_MinusLogProbMetric: 394.4213 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 256/1000
2023-10-03 23:05:15.808 
Epoch 256/1000 
	 loss: 392.9173, MinusLogProbMetric: 392.9173, val_loss: 395.2331, val_MinusLogProbMetric: 395.2331

Epoch 256: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.9173 - MinusLogProbMetric: 392.9173 - val_loss: 395.2331 - val_MinusLogProbMetric: 395.2331 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 257/1000
2023-10-03 23:05:22.506 
Epoch 257/1000 
	 loss: 393.1254, MinusLogProbMetric: 393.1254, val_loss: 395.0577, val_MinusLogProbMetric: 395.0577

Epoch 257: val_loss did not improve from 393.92953
196/196 - 7s - loss: 393.1254 - MinusLogProbMetric: 393.1254 - val_loss: 395.0577 - val_MinusLogProbMetric: 395.0577 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 258/1000
2023-10-03 23:05:29.372 
Epoch 258/1000 
	 loss: 392.8195, MinusLogProbMetric: 392.8195, val_loss: 394.9436, val_MinusLogProbMetric: 394.9436

Epoch 258: val_loss did not improve from 393.92953
196/196 - 7s - loss: 392.8195 - MinusLogProbMetric: 392.8195 - val_loss: 394.9436 - val_MinusLogProbMetric: 394.9436 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 259/1000
2023-10-03 23:05:36.183 
Epoch 259/1000 
	 loss: 392.9826, MinusLogProbMetric: 392.9826, val_loss: 393.6145, val_MinusLogProbMetric: 393.6145

Epoch 259: val_loss improved from 393.92953 to 393.61450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.9826 - MinusLogProbMetric: 392.9826 - val_loss: 393.6145 - val_MinusLogProbMetric: 393.6145 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 260/1000
2023-10-03 23:05:42.927 
Epoch 260/1000 
	 loss: 392.8971, MinusLogProbMetric: 392.8971, val_loss: 394.6953, val_MinusLogProbMetric: 394.6953

Epoch 260: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.8971 - MinusLogProbMetric: 392.8971 - val_loss: 394.6953 - val_MinusLogProbMetric: 394.6953 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 261/1000
2023-10-03 23:05:49.788 
Epoch 261/1000 
	 loss: 392.7044, MinusLogProbMetric: 392.7044, val_loss: 395.6039, val_MinusLogProbMetric: 395.6039

Epoch 261: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.7044 - MinusLogProbMetric: 392.7044 - val_loss: 395.6039 - val_MinusLogProbMetric: 395.6039 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 262/1000
2023-10-03 23:05:56.445 
Epoch 262/1000 
	 loss: 392.7596, MinusLogProbMetric: 392.7596, val_loss: 396.6846, val_MinusLogProbMetric: 396.6846

Epoch 262: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.7596 - MinusLogProbMetric: 392.7596 - val_loss: 396.6846 - val_MinusLogProbMetric: 396.6846 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 263/1000
2023-10-03 23:06:03.215 
Epoch 263/1000 
	 loss: 392.9465, MinusLogProbMetric: 392.9465, val_loss: 394.6393, val_MinusLogProbMetric: 394.6393

Epoch 263: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.9465 - MinusLogProbMetric: 392.9465 - val_loss: 394.6393 - val_MinusLogProbMetric: 394.6393 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 264/1000
2023-10-03 23:06:09.970 
Epoch 264/1000 
	 loss: 392.7728, MinusLogProbMetric: 392.7728, val_loss: 394.3208, val_MinusLogProbMetric: 394.3208

Epoch 264: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.7728 - MinusLogProbMetric: 392.7728 - val_loss: 394.3208 - val_MinusLogProbMetric: 394.3208 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 265/1000
2023-10-03 23:06:16.722 
Epoch 265/1000 
	 loss: 392.8355, MinusLogProbMetric: 392.8355, val_loss: 394.8159, val_MinusLogProbMetric: 394.8159

Epoch 265: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.8355 - MinusLogProbMetric: 392.8355 - val_loss: 394.8159 - val_MinusLogProbMetric: 394.8159 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 266/1000
2023-10-03 23:06:23.555 
Epoch 266/1000 
	 loss: 392.7199, MinusLogProbMetric: 392.7199, val_loss: 394.5241, val_MinusLogProbMetric: 394.5241

Epoch 266: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.7199 - MinusLogProbMetric: 392.7199 - val_loss: 394.5241 - val_MinusLogProbMetric: 394.5241 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 267/1000
2023-10-03 23:06:30.324 
Epoch 267/1000 
	 loss: 392.6244, MinusLogProbMetric: 392.6244, val_loss: 394.4080, val_MinusLogProbMetric: 394.4080

Epoch 267: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.6244 - MinusLogProbMetric: 392.6244 - val_loss: 394.4080 - val_MinusLogProbMetric: 394.4080 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 268/1000
2023-10-03 23:06:37.101 
Epoch 268/1000 
	 loss: 393.1015, MinusLogProbMetric: 393.1015, val_loss: 395.4171, val_MinusLogProbMetric: 395.4171

Epoch 268: val_loss did not improve from 393.61450
196/196 - 7s - loss: 393.1015 - MinusLogProbMetric: 393.1015 - val_loss: 395.4171 - val_MinusLogProbMetric: 395.4171 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 269/1000
2023-10-03 23:06:43.882 
Epoch 269/1000 
	 loss: 392.6584, MinusLogProbMetric: 392.6584, val_loss: 395.6238, val_MinusLogProbMetric: 395.6238

Epoch 269: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.6584 - MinusLogProbMetric: 392.6584 - val_loss: 395.6238 - val_MinusLogProbMetric: 395.6238 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 270/1000
2023-10-03 23:06:50.598 
Epoch 270/1000 
	 loss: 393.0734, MinusLogProbMetric: 393.0734, val_loss: 394.1493, val_MinusLogProbMetric: 394.1493

Epoch 270: val_loss did not improve from 393.61450
196/196 - 7s - loss: 393.0734 - MinusLogProbMetric: 393.0734 - val_loss: 394.1493 - val_MinusLogProbMetric: 394.1493 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 271/1000
2023-10-03 23:06:57.678 
Epoch 271/1000 
	 loss: 392.5936, MinusLogProbMetric: 392.5936, val_loss: 396.5836, val_MinusLogProbMetric: 396.5836

Epoch 271: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.5936 - MinusLogProbMetric: 392.5936 - val_loss: 396.5836 - val_MinusLogProbMetric: 396.5836 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 272/1000
2023-10-03 23:07:04.641 
Epoch 272/1000 
	 loss: 393.0693, MinusLogProbMetric: 393.0693, val_loss: 394.5534, val_MinusLogProbMetric: 394.5534

Epoch 272: val_loss did not improve from 393.61450
196/196 - 7s - loss: 393.0693 - MinusLogProbMetric: 393.0693 - val_loss: 394.5534 - val_MinusLogProbMetric: 394.5534 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 273/1000
2023-10-03 23:07:11.406 
Epoch 273/1000 
	 loss: 392.4604, MinusLogProbMetric: 392.4604, val_loss: 394.4430, val_MinusLogProbMetric: 394.4430

Epoch 273: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.4604 - MinusLogProbMetric: 392.4604 - val_loss: 394.4430 - val_MinusLogProbMetric: 394.4430 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 274/1000
2023-10-03 23:07:18.161 
Epoch 274/1000 
	 loss: 392.7270, MinusLogProbMetric: 392.7270, val_loss: 394.4054, val_MinusLogProbMetric: 394.4054

Epoch 274: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.7270 - MinusLogProbMetric: 392.7270 - val_loss: 394.4054 - val_MinusLogProbMetric: 394.4054 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 275/1000
2023-10-03 23:07:24.938 
Epoch 275/1000 
	 loss: 392.4775, MinusLogProbMetric: 392.4775, val_loss: 396.2448, val_MinusLogProbMetric: 396.2448

Epoch 275: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.4775 - MinusLogProbMetric: 392.4775 - val_loss: 396.2448 - val_MinusLogProbMetric: 396.2448 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 276/1000
2023-10-03 23:07:31.722 
Epoch 276/1000 
	 loss: 392.5507, MinusLogProbMetric: 392.5507, val_loss: 396.1502, val_MinusLogProbMetric: 396.1502

Epoch 276: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.5507 - MinusLogProbMetric: 392.5507 - val_loss: 396.1502 - val_MinusLogProbMetric: 396.1502 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 277/1000
2023-10-03 23:07:39.009 
Epoch 277/1000 
	 loss: 392.7695, MinusLogProbMetric: 392.7695, val_loss: 395.1378, val_MinusLogProbMetric: 395.1378

Epoch 277: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.7695 - MinusLogProbMetric: 392.7695 - val_loss: 395.1378 - val_MinusLogProbMetric: 395.1378 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 278/1000
2023-10-03 23:07:45.706 
Epoch 278/1000 
	 loss: 392.7056, MinusLogProbMetric: 392.7056, val_loss: 405.8140, val_MinusLogProbMetric: 405.8140

Epoch 278: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.7056 - MinusLogProbMetric: 392.7056 - val_loss: 405.8140 - val_MinusLogProbMetric: 405.8140 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 279/1000
2023-10-03 23:07:53.382 
Epoch 279/1000 
	 loss: 392.9773, MinusLogProbMetric: 392.9773, val_loss: 394.3458, val_MinusLogProbMetric: 394.3458

Epoch 279: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.9773 - MinusLogProbMetric: 392.9773 - val_loss: 394.3458 - val_MinusLogProbMetric: 394.3458 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 280/1000
2023-10-03 23:08:00.104 
Epoch 280/1000 
	 loss: 392.6412, MinusLogProbMetric: 392.6412, val_loss: 394.2314, val_MinusLogProbMetric: 394.2314

Epoch 280: val_loss did not improve from 393.61450
196/196 - 7s - loss: 392.6412 - MinusLogProbMetric: 392.6412 - val_loss: 394.2314 - val_MinusLogProbMetric: 394.2314 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 281/1000
2023-10-03 23:08:06.788 
Epoch 281/1000 
	 loss: 392.7870, MinusLogProbMetric: 392.7870, val_loss: 393.5127, val_MinusLogProbMetric: 393.5127

Epoch 281: val_loss improved from 393.61450 to 393.51273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.7870 - MinusLogProbMetric: 392.7870 - val_loss: 393.5127 - val_MinusLogProbMetric: 393.5127 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 282/1000
2023-10-03 23:08:14.086 
Epoch 282/1000 
	 loss: 392.6781, MinusLogProbMetric: 392.6781, val_loss: 393.6240, val_MinusLogProbMetric: 393.6240

Epoch 282: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.6781 - MinusLogProbMetric: 392.6781 - val_loss: 393.6240 - val_MinusLogProbMetric: 393.6240 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 283/1000
2023-10-03 23:08:21.076 
Epoch 283/1000 
	 loss: 392.2995, MinusLogProbMetric: 392.2995, val_loss: 394.5702, val_MinusLogProbMetric: 394.5702

Epoch 283: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.2995 - MinusLogProbMetric: 392.2995 - val_loss: 394.5702 - val_MinusLogProbMetric: 394.5702 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 284/1000
2023-10-03 23:08:27.747 
Epoch 284/1000 
	 loss: 392.4042, MinusLogProbMetric: 392.4042, val_loss: 393.8538, val_MinusLogProbMetric: 393.8538

Epoch 284: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.4042 - MinusLogProbMetric: 392.4042 - val_loss: 393.8538 - val_MinusLogProbMetric: 393.8538 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 285/1000
2023-10-03 23:08:34.592 
Epoch 285/1000 
	 loss: 392.5740, MinusLogProbMetric: 392.5740, val_loss: 394.2378, val_MinusLogProbMetric: 394.2378

Epoch 285: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.5740 - MinusLogProbMetric: 392.5740 - val_loss: 394.2378 - val_MinusLogProbMetric: 394.2378 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 286/1000
2023-10-03 23:08:41.350 
Epoch 286/1000 
	 loss: 392.5400, MinusLogProbMetric: 392.5400, val_loss: 393.6221, val_MinusLogProbMetric: 393.6221

Epoch 286: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.5400 - MinusLogProbMetric: 392.5400 - val_loss: 393.6221 - val_MinusLogProbMetric: 393.6221 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 287/1000
2023-10-03 23:08:48.083 
Epoch 287/1000 
	 loss: 392.3132, MinusLogProbMetric: 392.3132, val_loss: 393.6682, val_MinusLogProbMetric: 393.6682

Epoch 287: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.3132 - MinusLogProbMetric: 392.3132 - val_loss: 393.6682 - val_MinusLogProbMetric: 393.6682 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 288/1000
2023-10-03 23:08:54.982 
Epoch 288/1000 
	 loss: 392.3637, MinusLogProbMetric: 392.3637, val_loss: 394.2928, val_MinusLogProbMetric: 394.2928

Epoch 288: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.3637 - MinusLogProbMetric: 392.3637 - val_loss: 394.2928 - val_MinusLogProbMetric: 394.2928 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 289/1000
2023-10-03 23:09:01.680 
Epoch 289/1000 
	 loss: 392.4477, MinusLogProbMetric: 392.4477, val_loss: 394.4786, val_MinusLogProbMetric: 394.4786

Epoch 289: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.4477 - MinusLogProbMetric: 392.4477 - val_loss: 394.4786 - val_MinusLogProbMetric: 394.4786 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 290/1000
2023-10-03 23:09:08.479 
Epoch 290/1000 
	 loss: 392.3574, MinusLogProbMetric: 392.3574, val_loss: 393.6187, val_MinusLogProbMetric: 393.6187

Epoch 290: val_loss did not improve from 393.51273
196/196 - 7s - loss: 392.3574 - MinusLogProbMetric: 392.3574 - val_loss: 393.6187 - val_MinusLogProbMetric: 393.6187 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 291/1000
2023-10-03 23:09:15.208 
Epoch 291/1000 
	 loss: 392.3719, MinusLogProbMetric: 392.3719, val_loss: 393.4682, val_MinusLogProbMetric: 393.4682

Epoch 291: val_loss improved from 393.51273 to 393.46820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.3719 - MinusLogProbMetric: 392.3719 - val_loss: 393.4682 - val_MinusLogProbMetric: 393.4682 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 292/1000
2023-10-03 23:09:22.202 
Epoch 292/1000 
	 loss: 392.4235, MinusLogProbMetric: 392.4235, val_loss: 395.0632, val_MinusLogProbMetric: 395.0632

Epoch 292: val_loss did not improve from 393.46820
196/196 - 7s - loss: 392.4235 - MinusLogProbMetric: 392.4235 - val_loss: 395.0632 - val_MinusLogProbMetric: 395.0632 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 293/1000
2023-10-03 23:09:29.191 
Epoch 293/1000 
	 loss: 392.3853, MinusLogProbMetric: 392.3853, val_loss: 393.5632, val_MinusLogProbMetric: 393.5632

Epoch 293: val_loss did not improve from 393.46820
196/196 - 7s - loss: 392.3853 - MinusLogProbMetric: 392.3853 - val_loss: 393.5632 - val_MinusLogProbMetric: 393.5632 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 294/1000
2023-10-03 23:09:35.889 
Epoch 294/1000 
	 loss: 392.3976, MinusLogProbMetric: 392.3976, val_loss: 394.1196, val_MinusLogProbMetric: 394.1196

Epoch 294: val_loss did not improve from 393.46820
196/196 - 7s - loss: 392.3976 - MinusLogProbMetric: 392.3976 - val_loss: 394.1196 - val_MinusLogProbMetric: 394.1196 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 295/1000
2023-10-03 23:09:42.671 
Epoch 295/1000 
	 loss: 392.3215, MinusLogProbMetric: 392.3215, val_loss: 394.7609, val_MinusLogProbMetric: 394.7609

Epoch 295: val_loss did not improve from 393.46820
196/196 - 7s - loss: 392.3215 - MinusLogProbMetric: 392.3215 - val_loss: 394.7609 - val_MinusLogProbMetric: 394.7609 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 296/1000
2023-10-03 23:09:49.462 
Epoch 296/1000 
	 loss: 392.3149, MinusLogProbMetric: 392.3149, val_loss: 395.0369, val_MinusLogProbMetric: 395.0369

Epoch 296: val_loss did not improve from 393.46820
196/196 - 7s - loss: 392.3149 - MinusLogProbMetric: 392.3149 - val_loss: 395.0369 - val_MinusLogProbMetric: 395.0369 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 297/1000
2023-10-03 23:09:56.176 
Epoch 297/1000 
	 loss: 392.9966, MinusLogProbMetric: 392.9966, val_loss: 396.0768, val_MinusLogProbMetric: 396.0768

Epoch 297: val_loss did not improve from 393.46820
196/196 - 7s - loss: 392.9966 - MinusLogProbMetric: 392.9966 - val_loss: 396.0768 - val_MinusLogProbMetric: 396.0768 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 298/1000
2023-10-03 23:10:03.044 
Epoch 298/1000 
	 loss: 392.1188, MinusLogProbMetric: 392.1188, val_loss: 393.4404, val_MinusLogProbMetric: 393.4404

Epoch 298: val_loss improved from 393.46820 to 393.44040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.1188 - MinusLogProbMetric: 392.1188 - val_loss: 393.4404 - val_MinusLogProbMetric: 393.4404 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 299/1000
2023-10-03 23:10:10.224 
Epoch 299/1000 
	 loss: 392.7300, MinusLogProbMetric: 392.7300, val_loss: 406.4116, val_MinusLogProbMetric: 406.4116

Epoch 299: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.7300 - MinusLogProbMetric: 392.7300 - val_loss: 406.4116 - val_MinusLogProbMetric: 406.4116 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 300/1000
2023-10-03 23:10:17.042 
Epoch 300/1000 
	 loss: 392.2845, MinusLogProbMetric: 392.2845, val_loss: 393.6808, val_MinusLogProbMetric: 393.6808

Epoch 300: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.2845 - MinusLogProbMetric: 392.2845 - val_loss: 393.6808 - val_MinusLogProbMetric: 393.6808 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 301/1000
2023-10-03 23:10:23.773 
Epoch 301/1000 
	 loss: 392.2075, MinusLogProbMetric: 392.2075, val_loss: 394.3592, val_MinusLogProbMetric: 394.3592

Epoch 301: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.2075 - MinusLogProbMetric: 392.2075 - val_loss: 394.3592 - val_MinusLogProbMetric: 394.3592 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 302/1000
2023-10-03 23:10:30.431 
Epoch 302/1000 
	 loss: 392.3602, MinusLogProbMetric: 392.3602, val_loss: 396.2636, val_MinusLogProbMetric: 396.2636

Epoch 302: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.3602 - MinusLogProbMetric: 392.3602 - val_loss: 396.2636 - val_MinusLogProbMetric: 396.2636 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 303/1000
2023-10-03 23:10:37.298 
Epoch 303/1000 
	 loss: 392.2668, MinusLogProbMetric: 392.2668, val_loss: 394.8838, val_MinusLogProbMetric: 394.8838

Epoch 303: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.2668 - MinusLogProbMetric: 392.2668 - val_loss: 394.8838 - val_MinusLogProbMetric: 394.8838 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 304/1000
2023-10-03 23:10:44.028 
Epoch 304/1000 
	 loss: 392.1065, MinusLogProbMetric: 392.1065, val_loss: 393.6289, val_MinusLogProbMetric: 393.6289

Epoch 304: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.1065 - MinusLogProbMetric: 392.1065 - val_loss: 393.6289 - val_MinusLogProbMetric: 393.6289 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 305/1000
2023-10-03 23:10:51.236 
Epoch 305/1000 
	 loss: 392.2046, MinusLogProbMetric: 392.2046, val_loss: 393.9885, val_MinusLogProbMetric: 393.9885

Epoch 305: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.2046 - MinusLogProbMetric: 392.2046 - val_loss: 393.9885 - val_MinusLogProbMetric: 393.9885 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 306/1000
2023-10-03 23:10:57.871 
Epoch 306/1000 
	 loss: 392.2633, MinusLogProbMetric: 392.2633, val_loss: 393.8674, val_MinusLogProbMetric: 393.8674

Epoch 306: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.2633 - MinusLogProbMetric: 392.2633 - val_loss: 393.8674 - val_MinusLogProbMetric: 393.8674 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 307/1000
2023-10-03 23:11:04.729 
Epoch 307/1000 
	 loss: 392.4152, MinusLogProbMetric: 392.4152, val_loss: 393.7791, val_MinusLogProbMetric: 393.7791

Epoch 307: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.4152 - MinusLogProbMetric: 392.4152 - val_loss: 393.7791 - val_MinusLogProbMetric: 393.7791 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 308/1000
2023-10-03 23:11:11.435 
Epoch 308/1000 
	 loss: 392.0445, MinusLogProbMetric: 392.0445, val_loss: 393.9280, val_MinusLogProbMetric: 393.9280

Epoch 308: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.0445 - MinusLogProbMetric: 392.0445 - val_loss: 393.9280 - val_MinusLogProbMetric: 393.9280 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 309/1000
2023-10-03 23:11:18.241 
Epoch 309/1000 
	 loss: 392.2114, MinusLogProbMetric: 392.2114, val_loss: 393.9468, val_MinusLogProbMetric: 393.9468

Epoch 309: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.2114 - MinusLogProbMetric: 392.2114 - val_loss: 393.9468 - val_MinusLogProbMetric: 393.9468 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 310/1000
2023-10-03 23:11:25.142 
Epoch 310/1000 
	 loss: 392.0302, MinusLogProbMetric: 392.0302, val_loss: 394.8223, val_MinusLogProbMetric: 394.8223

Epoch 310: val_loss did not improve from 393.44040
196/196 - 7s - loss: 392.0302 - MinusLogProbMetric: 392.0302 - val_loss: 394.8223 - val_MinusLogProbMetric: 394.8223 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 311/1000
2023-10-03 23:11:31.829 
Epoch 311/1000 
	 loss: 392.2155, MinusLogProbMetric: 392.2155, val_loss: 393.3237, val_MinusLogProbMetric: 393.3237

Epoch 311: val_loss improved from 393.44040 to 393.32370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.2155 - MinusLogProbMetric: 392.2155 - val_loss: 393.3237 - val_MinusLogProbMetric: 393.3237 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 312/1000
2023-10-03 23:11:38.865 
Epoch 312/1000 
	 loss: 392.6097, MinusLogProbMetric: 392.6097, val_loss: 393.8077, val_MinusLogProbMetric: 393.8077

Epoch 312: val_loss did not improve from 393.32370
196/196 - 7s - loss: 392.6097 - MinusLogProbMetric: 392.6097 - val_loss: 393.8077 - val_MinusLogProbMetric: 393.8077 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 313/1000
2023-10-03 23:11:45.518 
Epoch 313/1000 
	 loss: 392.1096, MinusLogProbMetric: 392.1096, val_loss: 393.5857, val_MinusLogProbMetric: 393.5857

Epoch 313: val_loss did not improve from 393.32370
196/196 - 7s - loss: 392.1096 - MinusLogProbMetric: 392.1096 - val_loss: 393.5857 - val_MinusLogProbMetric: 393.5857 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 314/1000
2023-10-03 23:11:52.316 
Epoch 314/1000 
	 loss: 392.2668, MinusLogProbMetric: 392.2668, val_loss: 394.2353, val_MinusLogProbMetric: 394.2353

Epoch 314: val_loss did not improve from 393.32370
196/196 - 7s - loss: 392.2668 - MinusLogProbMetric: 392.2668 - val_loss: 394.2353 - val_MinusLogProbMetric: 394.2353 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 315/1000
2023-10-03 23:11:59.032 
Epoch 315/1000 
	 loss: 392.3466, MinusLogProbMetric: 392.3466, val_loss: 393.7039, val_MinusLogProbMetric: 393.7039

Epoch 315: val_loss did not improve from 393.32370
196/196 - 7s - loss: 392.3466 - MinusLogProbMetric: 392.3466 - val_loss: 393.7039 - val_MinusLogProbMetric: 393.7039 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 316/1000
2023-10-03 23:12:05.882 
Epoch 316/1000 
	 loss: 392.1266, MinusLogProbMetric: 392.1266, val_loss: 393.3598, val_MinusLogProbMetric: 393.3598

Epoch 316: val_loss did not improve from 393.32370
196/196 - 7s - loss: 392.1266 - MinusLogProbMetric: 392.1266 - val_loss: 393.3598 - val_MinusLogProbMetric: 393.3598 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 317/1000
2023-10-03 23:12:12.723 
Epoch 317/1000 
	 loss: 391.9571, MinusLogProbMetric: 391.9571, val_loss: 394.0652, val_MinusLogProbMetric: 394.0652

Epoch 317: val_loss did not improve from 393.32370
196/196 - 7s - loss: 391.9571 - MinusLogProbMetric: 391.9571 - val_loss: 394.0652 - val_MinusLogProbMetric: 394.0652 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 318/1000
2023-10-03 23:12:19.430 
Epoch 318/1000 
	 loss: 392.0811, MinusLogProbMetric: 392.0811, val_loss: 393.5161, val_MinusLogProbMetric: 393.5161

Epoch 318: val_loss did not improve from 393.32370
196/196 - 7s - loss: 392.0811 - MinusLogProbMetric: 392.0811 - val_loss: 393.5161 - val_MinusLogProbMetric: 393.5161 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 319/1000
2023-10-03 23:12:26.223 
Epoch 319/1000 
	 loss: 392.1387, MinusLogProbMetric: 392.1387, val_loss: 393.5961, val_MinusLogProbMetric: 393.5961

Epoch 319: val_loss did not improve from 393.32370
196/196 - 7s - loss: 392.1387 - MinusLogProbMetric: 392.1387 - val_loss: 393.5961 - val_MinusLogProbMetric: 393.5961 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 320/1000
2023-10-03 23:12:32.913 
Epoch 320/1000 
	 loss: 391.9960, MinusLogProbMetric: 391.9960, val_loss: 394.5453, val_MinusLogProbMetric: 394.5453

Epoch 320: val_loss did not improve from 393.32370
196/196 - 7s - loss: 391.9960 - MinusLogProbMetric: 391.9960 - val_loss: 394.5453 - val_MinusLogProbMetric: 394.5453 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 321/1000
2023-10-03 23:12:39.749 
Epoch 321/1000 
	 loss: 392.0842, MinusLogProbMetric: 392.0842, val_loss: 393.2315, val_MinusLogProbMetric: 393.2315

Epoch 321: val_loss improved from 393.32370 to 393.23154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.0842 - MinusLogProbMetric: 392.0842 - val_loss: 393.2315 - val_MinusLogProbMetric: 393.2315 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 322/1000
2023-10-03 23:12:46.641 
Epoch 322/1000 
	 loss: 392.2026, MinusLogProbMetric: 392.2026, val_loss: 393.1710, val_MinusLogProbMetric: 393.1710

Epoch 322: val_loss improved from 393.23154 to 393.17099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.2026 - MinusLogProbMetric: 392.2026 - val_loss: 393.1710 - val_MinusLogProbMetric: 393.1710 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 323/1000
2023-10-03 23:12:53.696 
Epoch 323/1000 
	 loss: 391.9579, MinusLogProbMetric: 391.9579, val_loss: 395.2492, val_MinusLogProbMetric: 395.2492

Epoch 323: val_loss did not improve from 393.17099
196/196 - 7s - loss: 391.9579 - MinusLogProbMetric: 391.9579 - val_loss: 395.2492 - val_MinusLogProbMetric: 395.2492 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 324/1000
2023-10-03 23:13:00.348 
Epoch 324/1000 
	 loss: 392.0786, MinusLogProbMetric: 392.0786, val_loss: 393.6066, val_MinusLogProbMetric: 393.6066

Epoch 324: val_loss did not improve from 393.17099
196/196 - 7s - loss: 392.0786 - MinusLogProbMetric: 392.0786 - val_loss: 393.6066 - val_MinusLogProbMetric: 393.6066 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 325/1000
2023-10-03 23:13:07.110 
Epoch 325/1000 
	 loss: 392.1274, MinusLogProbMetric: 392.1274, val_loss: 395.1810, val_MinusLogProbMetric: 395.1810

Epoch 325: val_loss did not improve from 393.17099
196/196 - 7s - loss: 392.1274 - MinusLogProbMetric: 392.1274 - val_loss: 395.1810 - val_MinusLogProbMetric: 395.1810 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 326/1000
2023-10-03 23:13:13.832 
Epoch 326/1000 
	 loss: 391.9874, MinusLogProbMetric: 391.9874, val_loss: 396.3083, val_MinusLogProbMetric: 396.3083

Epoch 326: val_loss did not improve from 393.17099
196/196 - 7s - loss: 391.9874 - MinusLogProbMetric: 391.9874 - val_loss: 396.3083 - val_MinusLogProbMetric: 396.3083 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 327/1000
2023-10-03 23:13:20.760 
Epoch 327/1000 
	 loss: 392.1398, MinusLogProbMetric: 392.1398, val_loss: 393.5432, val_MinusLogProbMetric: 393.5432

Epoch 327: val_loss did not improve from 393.17099
196/196 - 7s - loss: 392.1398 - MinusLogProbMetric: 392.1398 - val_loss: 393.5432 - val_MinusLogProbMetric: 393.5432 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 328/1000
2023-10-03 23:13:27.607 
Epoch 328/1000 
	 loss: 392.0123, MinusLogProbMetric: 392.0123, val_loss: 393.3298, val_MinusLogProbMetric: 393.3298

Epoch 328: val_loss did not improve from 393.17099
196/196 - 7s - loss: 392.0123 - MinusLogProbMetric: 392.0123 - val_loss: 393.3298 - val_MinusLogProbMetric: 393.3298 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 329/1000
2023-10-03 23:13:34.473 
Epoch 329/1000 
	 loss: 391.9939, MinusLogProbMetric: 391.9939, val_loss: 394.1359, val_MinusLogProbMetric: 394.1359

Epoch 329: val_loss did not improve from 393.17099
196/196 - 7s - loss: 391.9939 - MinusLogProbMetric: 391.9939 - val_loss: 394.1359 - val_MinusLogProbMetric: 394.1359 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 330/1000
2023-10-03 23:13:41.245 
Epoch 330/1000 
	 loss: 392.0919, MinusLogProbMetric: 392.0919, val_loss: 393.0144, val_MinusLogProbMetric: 393.0144

Epoch 330: val_loss improved from 393.17099 to 393.01437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 392.0919 - MinusLogProbMetric: 392.0919 - val_loss: 393.0144 - val_MinusLogProbMetric: 393.0144 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 331/1000
2023-10-03 23:13:48.398 
Epoch 331/1000 
	 loss: 391.8926, MinusLogProbMetric: 391.8926, val_loss: 395.6173, val_MinusLogProbMetric: 395.6173

Epoch 331: val_loss did not improve from 393.01437
196/196 - 7s - loss: 391.8926 - MinusLogProbMetric: 391.8926 - val_loss: 395.6173 - val_MinusLogProbMetric: 395.6173 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 332/1000
2023-10-03 23:13:55.062 
Epoch 332/1000 
	 loss: 391.7952, MinusLogProbMetric: 391.7952, val_loss: 393.1842, val_MinusLogProbMetric: 393.1842

Epoch 332: val_loss did not improve from 393.01437
196/196 - 7s - loss: 391.7952 - MinusLogProbMetric: 391.7952 - val_loss: 393.1842 - val_MinusLogProbMetric: 393.1842 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 333/1000
2023-10-03 23:14:02.052 
Epoch 333/1000 
	 loss: 391.9763, MinusLogProbMetric: 391.9763, val_loss: 393.2159, val_MinusLogProbMetric: 393.2159

Epoch 333: val_loss did not improve from 393.01437
196/196 - 7s - loss: 391.9763 - MinusLogProbMetric: 391.9763 - val_loss: 393.2159 - val_MinusLogProbMetric: 393.2159 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 334/1000
2023-10-03 23:14:08.694 
Epoch 334/1000 
	 loss: 392.0587, MinusLogProbMetric: 392.0587, val_loss: 393.6490, val_MinusLogProbMetric: 393.6490

Epoch 334: val_loss did not improve from 393.01437
196/196 - 7s - loss: 392.0587 - MinusLogProbMetric: 392.0587 - val_loss: 393.6490 - val_MinusLogProbMetric: 393.6490 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 335/1000
2023-10-03 23:14:15.534 
Epoch 335/1000 
	 loss: 391.7120, MinusLogProbMetric: 391.7120, val_loss: 392.8027, val_MinusLogProbMetric: 392.8027

Epoch 335: val_loss improved from 393.01437 to 392.80273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 391.7120 - MinusLogProbMetric: 391.7120 - val_loss: 392.8027 - val_MinusLogProbMetric: 392.8027 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 336/1000
2023-10-03 23:14:22.643 
Epoch 336/1000 
	 loss: 392.0669, MinusLogProbMetric: 392.0669, val_loss: 393.1424, val_MinusLogProbMetric: 393.1424

Epoch 336: val_loss did not improve from 392.80273
196/196 - 7s - loss: 392.0669 - MinusLogProbMetric: 392.0669 - val_loss: 393.1424 - val_MinusLogProbMetric: 393.1424 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 337/1000
2023-10-03 23:14:29.351 
Epoch 337/1000 
	 loss: 391.7354, MinusLogProbMetric: 391.7354, val_loss: 393.1654, val_MinusLogProbMetric: 393.1654

Epoch 337: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7354 - MinusLogProbMetric: 391.7354 - val_loss: 393.1654 - val_MinusLogProbMetric: 393.1654 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 338/1000
2023-10-03 23:14:36.130 
Epoch 338/1000 
	 loss: 391.9091, MinusLogProbMetric: 391.9091, val_loss: 393.3458, val_MinusLogProbMetric: 393.3458

Epoch 338: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.9091 - MinusLogProbMetric: 391.9091 - val_loss: 393.3458 - val_MinusLogProbMetric: 393.3458 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 339/1000
2023-10-03 23:14:42.937 
Epoch 339/1000 
	 loss: 391.7372, MinusLogProbMetric: 391.7372, val_loss: 393.1536, val_MinusLogProbMetric: 393.1536

Epoch 339: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7372 - MinusLogProbMetric: 391.7372 - val_loss: 393.1536 - val_MinusLogProbMetric: 393.1536 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 340/1000
2023-10-03 23:14:49.646 
Epoch 340/1000 
	 loss: 392.0080, MinusLogProbMetric: 392.0080, val_loss: 394.7299, val_MinusLogProbMetric: 394.7299

Epoch 340: val_loss did not improve from 392.80273
196/196 - 7s - loss: 392.0080 - MinusLogProbMetric: 392.0080 - val_loss: 394.7299 - val_MinusLogProbMetric: 394.7299 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 341/1000
2023-10-03 23:14:56.255 
Epoch 341/1000 
	 loss: 391.8624, MinusLogProbMetric: 391.8624, val_loss: 393.3621, val_MinusLogProbMetric: 393.3621

Epoch 341: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.8624 - MinusLogProbMetric: 391.8624 - val_loss: 393.3621 - val_MinusLogProbMetric: 393.3621 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 342/1000
2023-10-03 23:15:03.004 
Epoch 342/1000 
	 loss: 391.7828, MinusLogProbMetric: 391.7828, val_loss: 393.7398, val_MinusLogProbMetric: 393.7398

Epoch 342: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7828 - MinusLogProbMetric: 391.7828 - val_loss: 393.7398 - val_MinusLogProbMetric: 393.7398 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 343/1000
2023-10-03 23:15:09.974 
Epoch 343/1000 
	 loss: 391.7152, MinusLogProbMetric: 391.7152, val_loss: 394.1699, val_MinusLogProbMetric: 394.1699

Epoch 343: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7152 - MinusLogProbMetric: 391.7152 - val_loss: 394.1699 - val_MinusLogProbMetric: 394.1699 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 344/1000
2023-10-03 23:15:16.814 
Epoch 344/1000 
	 loss: 391.7208, MinusLogProbMetric: 391.7208, val_loss: 393.9157, val_MinusLogProbMetric: 393.9157

Epoch 344: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7208 - MinusLogProbMetric: 391.7208 - val_loss: 393.9157 - val_MinusLogProbMetric: 393.9157 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 345/1000
2023-10-03 23:15:23.546 
Epoch 345/1000 
	 loss: 391.7744, MinusLogProbMetric: 391.7744, val_loss: 394.7794, val_MinusLogProbMetric: 394.7794

Epoch 345: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7744 - MinusLogProbMetric: 391.7744 - val_loss: 394.7794 - val_MinusLogProbMetric: 394.7794 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 346/1000
2023-10-03 23:15:30.175 
Epoch 346/1000 
	 loss: 391.8965, MinusLogProbMetric: 391.8965, val_loss: 393.5847, val_MinusLogProbMetric: 393.5847

Epoch 346: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.8965 - MinusLogProbMetric: 391.8965 - val_loss: 393.5847 - val_MinusLogProbMetric: 393.5847 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 347/1000
2023-10-03 23:15:36.982 
Epoch 347/1000 
	 loss: 391.7243, MinusLogProbMetric: 391.7243, val_loss: 393.4662, val_MinusLogProbMetric: 393.4662

Epoch 347: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7243 - MinusLogProbMetric: 391.7243 - val_loss: 393.4662 - val_MinusLogProbMetric: 393.4662 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 348/1000
2023-10-03 23:15:43.710 
Epoch 348/1000 
	 loss: 391.7330, MinusLogProbMetric: 391.7330, val_loss: 396.1286, val_MinusLogProbMetric: 396.1286

Epoch 348: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7330 - MinusLogProbMetric: 391.7330 - val_loss: 396.1286 - val_MinusLogProbMetric: 396.1286 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 349/1000
2023-10-03 23:15:50.614 
Epoch 349/1000 
	 loss: 391.8286, MinusLogProbMetric: 391.8286, val_loss: 393.4858, val_MinusLogProbMetric: 393.4858

Epoch 349: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.8286 - MinusLogProbMetric: 391.8286 - val_loss: 393.4858 - val_MinusLogProbMetric: 393.4858 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 350/1000
2023-10-03 23:15:57.390 
Epoch 350/1000 
	 loss: 391.7119, MinusLogProbMetric: 391.7119, val_loss: 394.5838, val_MinusLogProbMetric: 394.5838

Epoch 350: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.7119 - MinusLogProbMetric: 391.7119 - val_loss: 394.5838 - val_MinusLogProbMetric: 394.5838 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 351/1000
2023-10-03 23:16:04.108 
Epoch 351/1000 
	 loss: 391.8756, MinusLogProbMetric: 391.8756, val_loss: 393.4065, val_MinusLogProbMetric: 393.4065

Epoch 351: val_loss did not improve from 392.80273
196/196 - 7s - loss: 391.8756 - MinusLogProbMetric: 391.8756 - val_loss: 393.4065 - val_MinusLogProbMetric: 393.4065 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 352/1000
2023-10-03 23:16:10.952 
Epoch 352/1000 
	 loss: 391.9619, MinusLogProbMetric: 391.9619, val_loss: 392.7796, val_MinusLogProbMetric: 392.7796

Epoch 352: val_loss improved from 392.80273 to 392.77963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 391.9619 - MinusLogProbMetric: 391.9619 - val_loss: 392.7796 - val_MinusLogProbMetric: 392.7796 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 353/1000
2023-10-03 23:16:18.037 
Epoch 353/1000 
	 loss: 391.6316, MinusLogProbMetric: 391.6316, val_loss: 393.3071, val_MinusLogProbMetric: 393.3071

Epoch 353: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6316 - MinusLogProbMetric: 391.6316 - val_loss: 393.3071 - val_MinusLogProbMetric: 393.3071 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 354/1000
2023-10-03 23:16:24.856 
Epoch 354/1000 
	 loss: 391.5816, MinusLogProbMetric: 391.5816, val_loss: 393.5342, val_MinusLogProbMetric: 393.5342

Epoch 354: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.5816 - MinusLogProbMetric: 391.5816 - val_loss: 393.5342 - val_MinusLogProbMetric: 393.5342 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 355/1000
2023-10-03 23:16:31.861 
Epoch 355/1000 
	 loss: 391.6564, MinusLogProbMetric: 391.6564, val_loss: 394.9981, val_MinusLogProbMetric: 394.9981

Epoch 355: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6564 - MinusLogProbMetric: 391.6564 - val_loss: 394.9981 - val_MinusLogProbMetric: 394.9981 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 356/1000
2023-10-03 23:16:38.710 
Epoch 356/1000 
	 loss: 391.8765, MinusLogProbMetric: 391.8765, val_loss: 394.6580, val_MinusLogProbMetric: 394.6580

Epoch 356: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.8765 - MinusLogProbMetric: 391.8765 - val_loss: 394.6580 - val_MinusLogProbMetric: 394.6580 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 357/1000
2023-10-03 23:16:45.365 
Epoch 357/1000 
	 loss: 391.6002, MinusLogProbMetric: 391.6002, val_loss: 393.3652, val_MinusLogProbMetric: 393.3652

Epoch 357: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6002 - MinusLogProbMetric: 391.6002 - val_loss: 393.3652 - val_MinusLogProbMetric: 393.3652 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 358/1000
2023-10-03 23:16:52.084 
Epoch 358/1000 
	 loss: 391.6550, MinusLogProbMetric: 391.6550, val_loss: 392.9396, val_MinusLogProbMetric: 392.9396

Epoch 358: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6550 - MinusLogProbMetric: 391.6550 - val_loss: 392.9396 - val_MinusLogProbMetric: 392.9396 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 359/1000
2023-10-03 23:16:58.883 
Epoch 359/1000 
	 loss: 391.6148, MinusLogProbMetric: 391.6148, val_loss: 392.8944, val_MinusLogProbMetric: 392.8944

Epoch 359: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6148 - MinusLogProbMetric: 391.6148 - val_loss: 392.8944 - val_MinusLogProbMetric: 392.8944 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 360/1000
2023-10-03 23:17:05.884 
Epoch 360/1000 
	 loss: 391.4340, MinusLogProbMetric: 391.4340, val_loss: 393.5100, val_MinusLogProbMetric: 393.5100

Epoch 360: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.4340 - MinusLogProbMetric: 391.4340 - val_loss: 393.5100 - val_MinusLogProbMetric: 393.5100 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 361/1000
2023-10-03 23:17:12.821 
Epoch 361/1000 
	 loss: 392.0934, MinusLogProbMetric: 392.0934, val_loss: 393.8108, val_MinusLogProbMetric: 393.8108

Epoch 361: val_loss did not improve from 392.77963
196/196 - 7s - loss: 392.0934 - MinusLogProbMetric: 392.0934 - val_loss: 393.8108 - val_MinusLogProbMetric: 393.8108 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 362/1000
2023-10-03 23:17:19.516 
Epoch 362/1000 
	 loss: 391.7535, MinusLogProbMetric: 391.7535, val_loss: 392.8587, val_MinusLogProbMetric: 392.8587

Epoch 362: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.7535 - MinusLogProbMetric: 391.7535 - val_loss: 392.8587 - val_MinusLogProbMetric: 392.8587 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 363/1000
2023-10-03 23:17:26.140 
Epoch 363/1000 
	 loss: 391.6505, MinusLogProbMetric: 391.6505, val_loss: 393.5477, val_MinusLogProbMetric: 393.5477

Epoch 363: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6505 - MinusLogProbMetric: 391.6505 - val_loss: 393.5477 - val_MinusLogProbMetric: 393.5477 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 364/1000
2023-10-03 23:17:32.903 
Epoch 364/1000 
	 loss: 391.5769, MinusLogProbMetric: 391.5769, val_loss: 394.2533, val_MinusLogProbMetric: 394.2533

Epoch 364: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.5769 - MinusLogProbMetric: 391.5769 - val_loss: 394.2533 - val_MinusLogProbMetric: 394.2533 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 365/1000
2023-10-03 23:17:39.694 
Epoch 365/1000 
	 loss: 391.3145, MinusLogProbMetric: 391.3145, val_loss: 393.3473, val_MinusLogProbMetric: 393.3473

Epoch 365: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.3145 - MinusLogProbMetric: 391.3145 - val_loss: 393.3473 - val_MinusLogProbMetric: 393.3473 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 366/1000
2023-10-03 23:17:46.770 
Epoch 366/1000 
	 loss: 391.5914, MinusLogProbMetric: 391.5914, val_loss: 393.2107, val_MinusLogProbMetric: 393.2107

Epoch 366: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.5914 - MinusLogProbMetric: 391.5914 - val_loss: 393.2107 - val_MinusLogProbMetric: 393.2107 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 367/1000
2023-10-03 23:17:53.610 
Epoch 367/1000 
	 loss: 391.7304, MinusLogProbMetric: 391.7304, val_loss: 394.4540, val_MinusLogProbMetric: 394.4540

Epoch 367: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.7304 - MinusLogProbMetric: 391.7304 - val_loss: 394.4540 - val_MinusLogProbMetric: 394.4540 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 368/1000
2023-10-03 23:18:00.265 
Epoch 368/1000 
	 loss: 391.5793, MinusLogProbMetric: 391.5793, val_loss: 394.9822, val_MinusLogProbMetric: 394.9822

Epoch 368: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.5793 - MinusLogProbMetric: 391.5793 - val_loss: 394.9822 - val_MinusLogProbMetric: 394.9822 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 369/1000
2023-10-03 23:18:07.099 
Epoch 369/1000 
	 loss: 391.7168, MinusLogProbMetric: 391.7168, val_loss: 393.8543, val_MinusLogProbMetric: 393.8543

Epoch 369: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.7168 - MinusLogProbMetric: 391.7168 - val_loss: 393.8543 - val_MinusLogProbMetric: 393.8543 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 370/1000
2023-10-03 23:18:13.772 
Epoch 370/1000 
	 loss: 391.5586, MinusLogProbMetric: 391.5586, val_loss: 393.4589, val_MinusLogProbMetric: 393.4589

Epoch 370: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.5586 - MinusLogProbMetric: 391.5586 - val_loss: 393.4589 - val_MinusLogProbMetric: 393.4589 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 371/1000
2023-10-03 23:18:20.651 
Epoch 371/1000 
	 loss: 391.5663, MinusLogProbMetric: 391.5663, val_loss: 394.9652, val_MinusLogProbMetric: 394.9652

Epoch 371: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.5663 - MinusLogProbMetric: 391.5663 - val_loss: 394.9652 - val_MinusLogProbMetric: 394.9652 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 372/1000
2023-10-03 23:18:27.705 
Epoch 372/1000 
	 loss: 391.7616, MinusLogProbMetric: 391.7616, val_loss: 394.2353, val_MinusLogProbMetric: 394.2353

Epoch 372: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.7616 - MinusLogProbMetric: 391.7616 - val_loss: 394.2353 - val_MinusLogProbMetric: 394.2353 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 373/1000
2023-10-03 23:18:34.389 
Epoch 373/1000 
	 loss: 391.4188, MinusLogProbMetric: 391.4188, val_loss: 392.9964, val_MinusLogProbMetric: 392.9964

Epoch 373: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.4188 - MinusLogProbMetric: 391.4188 - val_loss: 392.9964 - val_MinusLogProbMetric: 392.9964 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 374/1000
2023-10-03 23:18:41.173 
Epoch 374/1000 
	 loss: 391.5984, MinusLogProbMetric: 391.5984, val_loss: 395.2259, val_MinusLogProbMetric: 395.2259

Epoch 374: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.5984 - MinusLogProbMetric: 391.5984 - val_loss: 395.2259 - val_MinusLogProbMetric: 395.2259 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 375/1000
2023-10-03 23:18:47.798 
Epoch 375/1000 
	 loss: 391.6232, MinusLogProbMetric: 391.6232, val_loss: 393.9302, val_MinusLogProbMetric: 393.9302

Epoch 375: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6232 - MinusLogProbMetric: 391.6232 - val_loss: 393.9302 - val_MinusLogProbMetric: 393.9302 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 376/1000
2023-10-03 23:18:54.469 
Epoch 376/1000 
	 loss: 391.6758, MinusLogProbMetric: 391.6758, val_loss: 394.0140, val_MinusLogProbMetric: 394.0140

Epoch 376: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.6758 - MinusLogProbMetric: 391.6758 - val_loss: 394.0140 - val_MinusLogProbMetric: 394.0140 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 377/1000
2023-10-03 23:19:01.370 
Epoch 377/1000 
	 loss: 391.2255, MinusLogProbMetric: 391.2255, val_loss: 394.1056, val_MinusLogProbMetric: 394.1056

Epoch 377: val_loss did not improve from 392.77963
196/196 - 7s - loss: 391.2255 - MinusLogProbMetric: 391.2255 - val_loss: 394.1056 - val_MinusLogProbMetric: 394.1056 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 378/1000
2023-10-03 23:19:08.200 
Epoch 378/1000 
	 loss: 391.6424, MinusLogProbMetric: 391.6424, val_loss: 392.6976, val_MinusLogProbMetric: 392.6976

Epoch 378: val_loss improved from 392.77963 to 392.69757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 391.6424 - MinusLogProbMetric: 391.6424 - val_loss: 392.6976 - val_MinusLogProbMetric: 392.6976 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 379/1000
2023-10-03 23:19:15.040 
Epoch 379/1000 
	 loss: 391.8438, MinusLogProbMetric: 391.8438, val_loss: 392.7916, val_MinusLogProbMetric: 392.7916

Epoch 379: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.8438 - MinusLogProbMetric: 391.8438 - val_loss: 392.7916 - val_MinusLogProbMetric: 392.7916 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 380/1000
2023-10-03 23:19:21.768 
Epoch 380/1000 
	 loss: 391.3284, MinusLogProbMetric: 391.3284, val_loss: 394.1215, val_MinusLogProbMetric: 394.1215

Epoch 380: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.3284 - MinusLogProbMetric: 391.3284 - val_loss: 394.1215 - val_MinusLogProbMetric: 394.1215 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 381/1000
2023-10-03 23:19:28.453 
Epoch 381/1000 
	 loss: 391.1473, MinusLogProbMetric: 391.1473, val_loss: 393.8836, val_MinusLogProbMetric: 393.8836

Epoch 381: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.1473 - MinusLogProbMetric: 391.1473 - val_loss: 393.8836 - val_MinusLogProbMetric: 393.8836 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 382/1000
2023-10-03 23:19:35.088 
Epoch 382/1000 
	 loss: 392.0715, MinusLogProbMetric: 392.0715, val_loss: 393.0748, val_MinusLogProbMetric: 393.0748

Epoch 382: val_loss did not improve from 392.69757
196/196 - 7s - loss: 392.0715 - MinusLogProbMetric: 392.0715 - val_loss: 393.0748 - val_MinusLogProbMetric: 393.0748 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 383/1000
2023-10-03 23:19:42.416 
Epoch 383/1000 
	 loss: 391.4951, MinusLogProbMetric: 391.4951, val_loss: 393.4857, val_MinusLogProbMetric: 393.4857

Epoch 383: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.4951 - MinusLogProbMetric: 391.4951 - val_loss: 393.4857 - val_MinusLogProbMetric: 393.4857 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 384/1000
2023-10-03 23:19:49.257 
Epoch 384/1000 
	 loss: 391.3525, MinusLogProbMetric: 391.3525, val_loss: 394.0901, val_MinusLogProbMetric: 394.0901

Epoch 384: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.3525 - MinusLogProbMetric: 391.3525 - val_loss: 394.0901 - val_MinusLogProbMetric: 394.0901 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 385/1000
2023-10-03 23:19:56.023 
Epoch 385/1000 
	 loss: 391.1832, MinusLogProbMetric: 391.1832, val_loss: 394.0754, val_MinusLogProbMetric: 394.0754

Epoch 385: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.1832 - MinusLogProbMetric: 391.1832 - val_loss: 394.0754 - val_MinusLogProbMetric: 394.0754 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 386/1000
2023-10-03 23:20:02.871 
Epoch 386/1000 
	 loss: 391.7065, MinusLogProbMetric: 391.7065, val_loss: 393.1593, val_MinusLogProbMetric: 393.1593

Epoch 386: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.7065 - MinusLogProbMetric: 391.7065 - val_loss: 393.1593 - val_MinusLogProbMetric: 393.1593 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 387/1000
2023-10-03 23:20:09.535 
Epoch 387/1000 
	 loss: 391.4679, MinusLogProbMetric: 391.4679, val_loss: 396.3193, val_MinusLogProbMetric: 396.3193

Epoch 387: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.4679 - MinusLogProbMetric: 391.4679 - val_loss: 396.3193 - val_MinusLogProbMetric: 396.3193 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 388/1000
2023-10-03 23:20:16.550 
Epoch 388/1000 
	 loss: 391.5021, MinusLogProbMetric: 391.5021, val_loss: 394.0878, val_MinusLogProbMetric: 394.0878

Epoch 388: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.5021 - MinusLogProbMetric: 391.5021 - val_loss: 394.0878 - val_MinusLogProbMetric: 394.0878 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 389/1000
2023-10-03 23:20:23.447 
Epoch 389/1000 
	 loss: 391.3310, MinusLogProbMetric: 391.3310, val_loss: 394.9838, val_MinusLogProbMetric: 394.9838

Epoch 389: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.3310 - MinusLogProbMetric: 391.3310 - val_loss: 394.9838 - val_MinusLogProbMetric: 394.9838 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 390/1000
2023-10-03 23:20:30.133 
Epoch 390/1000 
	 loss: 391.2945, MinusLogProbMetric: 391.2945, val_loss: 393.2694, val_MinusLogProbMetric: 393.2694

Epoch 390: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.2945 - MinusLogProbMetric: 391.2945 - val_loss: 393.2694 - val_MinusLogProbMetric: 393.2694 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 391/1000
2023-10-03 23:20:36.846 
Epoch 391/1000 
	 loss: 391.2972, MinusLogProbMetric: 391.2972, val_loss: 392.9293, val_MinusLogProbMetric: 392.9293

Epoch 391: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.2972 - MinusLogProbMetric: 391.2972 - val_loss: 392.9293 - val_MinusLogProbMetric: 392.9293 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 392/1000
2023-10-03 23:20:43.564 
Epoch 392/1000 
	 loss: 391.5231, MinusLogProbMetric: 391.5231, val_loss: 393.4415, val_MinusLogProbMetric: 393.4415

Epoch 392: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.5231 - MinusLogProbMetric: 391.5231 - val_loss: 393.4415 - val_MinusLogProbMetric: 393.4415 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 393/1000
2023-10-03 23:20:50.225 
Epoch 393/1000 
	 loss: 391.3976, MinusLogProbMetric: 391.3976, val_loss: 393.4373, val_MinusLogProbMetric: 393.4373

Epoch 393: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.3976 - MinusLogProbMetric: 391.3976 - val_loss: 393.4373 - val_MinusLogProbMetric: 393.4373 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 394/1000
2023-10-03 23:20:57.080 
Epoch 394/1000 
	 loss: 391.5265, MinusLogProbMetric: 391.5265, val_loss: 394.0351, val_MinusLogProbMetric: 394.0351

Epoch 394: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.5265 - MinusLogProbMetric: 391.5265 - val_loss: 394.0351 - val_MinusLogProbMetric: 394.0351 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 395/1000
2023-10-03 23:21:03.932 
Epoch 395/1000 
	 loss: 391.5314, MinusLogProbMetric: 391.5314, val_loss: 392.7391, val_MinusLogProbMetric: 392.7391

Epoch 395: val_loss did not improve from 392.69757
196/196 - 7s - loss: 391.5314 - MinusLogProbMetric: 391.5314 - val_loss: 392.7391 - val_MinusLogProbMetric: 392.7391 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 396/1000
2023-10-03 23:21:10.558 
Epoch 396/1000 
	 loss: 391.1137, MinusLogProbMetric: 391.1137, val_loss: 392.6971, val_MinusLogProbMetric: 392.6971

Epoch 396: val_loss improved from 392.69757 to 392.69705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 391.1137 - MinusLogProbMetric: 391.1137 - val_loss: 392.6971 - val_MinusLogProbMetric: 392.6971 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 397/1000
2023-10-03 23:21:17.466 
Epoch 397/1000 
	 loss: 391.3085, MinusLogProbMetric: 391.3085, val_loss: 393.0980, val_MinusLogProbMetric: 393.0980

Epoch 397: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.3085 - MinusLogProbMetric: 391.3085 - val_loss: 393.0980 - val_MinusLogProbMetric: 393.0980 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 398/1000
2023-10-03 23:21:24.282 
Epoch 398/1000 
	 loss: 391.6386, MinusLogProbMetric: 391.6386, val_loss: 392.8216, val_MinusLogProbMetric: 392.8216

Epoch 398: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.6386 - MinusLogProbMetric: 391.6386 - val_loss: 392.8216 - val_MinusLogProbMetric: 392.8216 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 399/1000
2023-10-03 23:21:31.054 
Epoch 399/1000 
	 loss: 391.0744, MinusLogProbMetric: 391.0744, val_loss: 392.9598, val_MinusLogProbMetric: 392.9598

Epoch 399: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.0744 - MinusLogProbMetric: 391.0744 - val_loss: 392.9598 - val_MinusLogProbMetric: 392.9598 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 400/1000
2023-10-03 23:21:37.936 
Epoch 400/1000 
	 loss: 391.4733, MinusLogProbMetric: 391.4733, val_loss: 393.4456, val_MinusLogProbMetric: 393.4456

Epoch 400: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.4733 - MinusLogProbMetric: 391.4733 - val_loss: 393.4456 - val_MinusLogProbMetric: 393.4456 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 401/1000
2023-10-03 23:21:44.662 
Epoch 401/1000 
	 loss: 391.3221, MinusLogProbMetric: 391.3221, val_loss: 394.1083, val_MinusLogProbMetric: 394.1083

Epoch 401: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.3221 - MinusLogProbMetric: 391.3221 - val_loss: 394.1083 - val_MinusLogProbMetric: 394.1083 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 402/1000
2023-10-03 23:21:51.496 
Epoch 402/1000 
	 loss: 391.2491, MinusLogProbMetric: 391.2491, val_loss: 393.0334, val_MinusLogProbMetric: 393.0334

Epoch 402: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.2491 - MinusLogProbMetric: 391.2491 - val_loss: 393.0334 - val_MinusLogProbMetric: 393.0334 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 403/1000
2023-10-03 23:21:58.197 
Epoch 403/1000 
	 loss: 391.0914, MinusLogProbMetric: 391.0914, val_loss: 393.3357, val_MinusLogProbMetric: 393.3357

Epoch 403: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.0914 - MinusLogProbMetric: 391.0914 - val_loss: 393.3357 - val_MinusLogProbMetric: 393.3357 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 404/1000
2023-10-03 23:22:04.878 
Epoch 404/1000 
	 loss: 391.3666, MinusLogProbMetric: 391.3666, val_loss: 393.5300, val_MinusLogProbMetric: 393.5300

Epoch 404: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.3666 - MinusLogProbMetric: 391.3666 - val_loss: 393.5300 - val_MinusLogProbMetric: 393.5300 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 405/1000
2023-10-03 23:22:11.945 
Epoch 405/1000 
	 loss: 391.3445, MinusLogProbMetric: 391.3445, val_loss: 394.1300, val_MinusLogProbMetric: 394.1300

Epoch 405: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.3445 - MinusLogProbMetric: 391.3445 - val_loss: 394.1300 - val_MinusLogProbMetric: 394.1300 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 406/1000
2023-10-03 23:22:18.605 
Epoch 406/1000 
	 loss: 391.4211, MinusLogProbMetric: 391.4211, val_loss: 393.0881, val_MinusLogProbMetric: 393.0881

Epoch 406: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.4211 - MinusLogProbMetric: 391.4211 - val_loss: 393.0881 - val_MinusLogProbMetric: 393.0881 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 407/1000
2023-10-03 23:22:25.201 
Epoch 407/1000 
	 loss: 391.4324, MinusLogProbMetric: 391.4324, val_loss: 393.4525, val_MinusLogProbMetric: 393.4525

Epoch 407: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.4324 - MinusLogProbMetric: 391.4324 - val_loss: 393.4525 - val_MinusLogProbMetric: 393.4525 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 408/1000
2023-10-03 23:22:31.926 
Epoch 408/1000 
	 loss: 391.1965, MinusLogProbMetric: 391.1965, val_loss: 393.7232, val_MinusLogProbMetric: 393.7232

Epoch 408: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.1965 - MinusLogProbMetric: 391.1965 - val_loss: 393.7232 - val_MinusLogProbMetric: 393.7232 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 409/1000
2023-10-03 23:22:38.681 
Epoch 409/1000 
	 loss: 391.4332, MinusLogProbMetric: 391.4332, val_loss: 392.9335, val_MinusLogProbMetric: 392.9335

Epoch 409: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.4332 - MinusLogProbMetric: 391.4332 - val_loss: 392.9335 - val_MinusLogProbMetric: 392.9335 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 410/1000
2023-10-03 23:22:45.314 
Epoch 410/1000 
	 loss: 391.3674, MinusLogProbMetric: 391.3674, val_loss: 395.5502, val_MinusLogProbMetric: 395.5502

Epoch 410: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.3674 - MinusLogProbMetric: 391.3674 - val_loss: 395.5502 - val_MinusLogProbMetric: 395.5502 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 411/1000
2023-10-03 23:22:52.462 
Epoch 411/1000 
	 loss: 391.0067, MinusLogProbMetric: 391.0067, val_loss: 393.0187, val_MinusLogProbMetric: 393.0187

Epoch 411: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.0067 - MinusLogProbMetric: 391.0067 - val_loss: 393.0187 - val_MinusLogProbMetric: 393.0187 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 412/1000
2023-10-03 23:22:59.252 
Epoch 412/1000 
	 loss: 391.3996, MinusLogProbMetric: 391.3996, val_loss: 393.2001, val_MinusLogProbMetric: 393.2001

Epoch 412: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.3996 - MinusLogProbMetric: 391.3996 - val_loss: 393.2001 - val_MinusLogProbMetric: 393.2001 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 413/1000
2023-10-03 23:23:05.777 
Epoch 413/1000 
	 loss: 391.0936, MinusLogProbMetric: 391.0936, val_loss: 405.3553, val_MinusLogProbMetric: 405.3553

Epoch 413: val_loss did not improve from 392.69705
196/196 - 7s - loss: 391.0936 - MinusLogProbMetric: 391.0936 - val_loss: 405.3553 - val_MinusLogProbMetric: 405.3553 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 414/1000
2023-10-03 23:23:12.454 
Epoch 414/1000 
	 loss: 391.3238, MinusLogProbMetric: 391.3238, val_loss: 392.6604, val_MinusLogProbMetric: 392.6604

Epoch 414: val_loss improved from 392.69705 to 392.66037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 391.3238 - MinusLogProbMetric: 391.3238 - val_loss: 392.6604 - val_MinusLogProbMetric: 392.6604 - lr: 1.6667e-04 - 8s/epoch - 39ms/step
Epoch 415/1000
2023-10-03 23:23:19.844 
Epoch 415/1000 
	 loss: 391.1626, MinusLogProbMetric: 391.1626, val_loss: 394.3414, val_MinusLogProbMetric: 394.3414

Epoch 415: val_loss did not improve from 392.66037
196/196 - 6s - loss: 391.1626 - MinusLogProbMetric: 391.1626 - val_loss: 394.3414 - val_MinusLogProbMetric: 394.3414 - lr: 1.6667e-04 - 6s/epoch - 33ms/step
Epoch 416/1000
2023-10-03 23:23:26.624 
Epoch 416/1000 
	 loss: 391.2486, MinusLogProbMetric: 391.2486, val_loss: 393.6394, val_MinusLogProbMetric: 393.6394

Epoch 416: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.2486 - MinusLogProbMetric: 391.2486 - val_loss: 393.6394 - val_MinusLogProbMetric: 393.6394 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 417/1000
2023-10-03 23:23:33.221 
Epoch 417/1000 
	 loss: 391.1250, MinusLogProbMetric: 391.1250, val_loss: 394.4747, val_MinusLogProbMetric: 394.4747

Epoch 417: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.1250 - MinusLogProbMetric: 391.1250 - val_loss: 394.4747 - val_MinusLogProbMetric: 394.4747 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 418/1000
2023-10-03 23:23:39.839 
Epoch 418/1000 
	 loss: 391.3465, MinusLogProbMetric: 391.3465, val_loss: 395.3897, val_MinusLogProbMetric: 395.3897

Epoch 418: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.3465 - MinusLogProbMetric: 391.3465 - val_loss: 395.3897 - val_MinusLogProbMetric: 395.3897 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 419/1000
2023-10-03 23:23:46.611 
Epoch 419/1000 
	 loss: 391.4560, MinusLogProbMetric: 391.4560, val_loss: 393.1619, val_MinusLogProbMetric: 393.1619

Epoch 419: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.4560 - MinusLogProbMetric: 391.4560 - val_loss: 393.1619 - val_MinusLogProbMetric: 393.1619 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 420/1000
2023-10-03 23:23:53.235 
Epoch 420/1000 
	 loss: 391.0334, MinusLogProbMetric: 391.0334, val_loss: 392.9300, val_MinusLogProbMetric: 392.9300

Epoch 420: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0334 - MinusLogProbMetric: 391.0334 - val_loss: 392.9300 - val_MinusLogProbMetric: 392.9300 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 421/1000
2023-10-03 23:23:59.948 
Epoch 421/1000 
	 loss: 391.1020, MinusLogProbMetric: 391.1020, val_loss: 393.6803, val_MinusLogProbMetric: 393.6803

Epoch 421: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.1020 - MinusLogProbMetric: 391.1020 - val_loss: 393.6803 - val_MinusLogProbMetric: 393.6803 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 422/1000
2023-10-03 23:24:06.729 
Epoch 422/1000 
	 loss: 391.2447, MinusLogProbMetric: 391.2447, val_loss: 393.9302, val_MinusLogProbMetric: 393.9302

Epoch 422: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.2447 - MinusLogProbMetric: 391.2447 - val_loss: 393.9302 - val_MinusLogProbMetric: 393.9302 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 423/1000
2023-10-03 23:24:13.364 
Epoch 423/1000 
	 loss: 391.2242, MinusLogProbMetric: 391.2242, val_loss: 392.6746, val_MinusLogProbMetric: 392.6746

Epoch 423: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.2242 - MinusLogProbMetric: 391.2242 - val_loss: 392.6746 - val_MinusLogProbMetric: 392.6746 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 424/1000
2023-10-03 23:24:19.993 
Epoch 424/1000 
	 loss: 390.9763, MinusLogProbMetric: 390.9763, val_loss: 393.4683, val_MinusLogProbMetric: 393.4683

Epoch 424: val_loss did not improve from 392.66037
196/196 - 7s - loss: 390.9763 - MinusLogProbMetric: 390.9763 - val_loss: 393.4683 - val_MinusLogProbMetric: 393.4683 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 425/1000
2023-10-03 23:24:26.705 
Epoch 425/1000 
	 loss: 391.5020, MinusLogProbMetric: 391.5020, val_loss: 393.0319, val_MinusLogProbMetric: 393.0319

Epoch 425: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.5020 - MinusLogProbMetric: 391.5020 - val_loss: 393.0319 - val_MinusLogProbMetric: 393.0319 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 426/1000
2023-10-03 23:24:33.321 
Epoch 426/1000 
	 loss: 390.9862, MinusLogProbMetric: 390.9862, val_loss: 392.8641, val_MinusLogProbMetric: 392.8641

Epoch 426: val_loss did not improve from 392.66037
196/196 - 7s - loss: 390.9862 - MinusLogProbMetric: 390.9862 - val_loss: 392.8641 - val_MinusLogProbMetric: 392.8641 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 427/1000
2023-10-03 23:24:39.973 
Epoch 427/1000 
	 loss: 391.0793, MinusLogProbMetric: 391.0793, val_loss: 395.0086, val_MinusLogProbMetric: 395.0086

Epoch 427: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0793 - MinusLogProbMetric: 391.0793 - val_loss: 395.0086 - val_MinusLogProbMetric: 395.0086 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 428/1000
2023-10-03 23:24:46.863 
Epoch 428/1000 
	 loss: 392.3992, MinusLogProbMetric: 392.3992, val_loss: 393.1899, val_MinusLogProbMetric: 393.1899

Epoch 428: val_loss did not improve from 392.66037
196/196 - 7s - loss: 392.3992 - MinusLogProbMetric: 392.3992 - val_loss: 393.1899 - val_MinusLogProbMetric: 393.1899 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 429/1000
2023-10-03 23:24:53.455 
Epoch 429/1000 
	 loss: 391.0217, MinusLogProbMetric: 391.0217, val_loss: 393.5608, val_MinusLogProbMetric: 393.5608

Epoch 429: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0217 - MinusLogProbMetric: 391.0217 - val_loss: 393.5608 - val_MinusLogProbMetric: 393.5608 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 430/1000
2023-10-03 23:24:59.971 
Epoch 430/1000 
	 loss: 391.0336, MinusLogProbMetric: 391.0336, val_loss: 392.7466, val_MinusLogProbMetric: 392.7466

Epoch 430: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0336 - MinusLogProbMetric: 391.0336 - val_loss: 392.7466 - val_MinusLogProbMetric: 392.7466 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 431/1000
2023-10-03 23:25:06.584 
Epoch 431/1000 
	 loss: 390.6790, MinusLogProbMetric: 390.6790, val_loss: 394.2172, val_MinusLogProbMetric: 394.2172

Epoch 431: val_loss did not improve from 392.66037
196/196 - 7s - loss: 390.6790 - MinusLogProbMetric: 390.6790 - val_loss: 394.2172 - val_MinusLogProbMetric: 394.2172 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 432/1000
2023-10-03 23:25:13.621 
Epoch 432/1000 
	 loss: 391.2070, MinusLogProbMetric: 391.2070, val_loss: 393.2033, val_MinusLogProbMetric: 393.2033

Epoch 432: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.2070 - MinusLogProbMetric: 391.2070 - val_loss: 393.2033 - val_MinusLogProbMetric: 393.2033 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 433/1000
2023-10-03 23:25:20.330 
Epoch 433/1000 
	 loss: 390.8677, MinusLogProbMetric: 390.8677, val_loss: 392.8603, val_MinusLogProbMetric: 392.8603

Epoch 433: val_loss did not improve from 392.66037
196/196 - 7s - loss: 390.8677 - MinusLogProbMetric: 390.8677 - val_loss: 392.8603 - val_MinusLogProbMetric: 392.8603 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 434/1000
2023-10-03 23:25:27.392 
Epoch 434/1000 
	 loss: 391.2061, MinusLogProbMetric: 391.2061, val_loss: 393.5644, val_MinusLogProbMetric: 393.5644

Epoch 434: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.2061 - MinusLogProbMetric: 391.2061 - val_loss: 393.5644 - val_MinusLogProbMetric: 393.5644 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 435/1000
2023-10-03 23:25:34.076 
Epoch 435/1000 
	 loss: 391.3061, MinusLogProbMetric: 391.3061, val_loss: 393.1668, val_MinusLogProbMetric: 393.1668

Epoch 435: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.3061 - MinusLogProbMetric: 391.3061 - val_loss: 393.1668 - val_MinusLogProbMetric: 393.1668 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 436/1000
2023-10-03 23:25:40.757 
Epoch 436/1000 
	 loss: 390.8843, MinusLogProbMetric: 390.8843, val_loss: 392.9010, val_MinusLogProbMetric: 392.9010

Epoch 436: val_loss did not improve from 392.66037
196/196 - 7s - loss: 390.8843 - MinusLogProbMetric: 390.8843 - val_loss: 392.9010 - val_MinusLogProbMetric: 392.9010 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 437/1000
2023-10-03 23:25:47.176 
Epoch 437/1000 
	 loss: 391.1018, MinusLogProbMetric: 391.1018, val_loss: 394.5648, val_MinusLogProbMetric: 394.5648

Epoch 437: val_loss did not improve from 392.66037
196/196 - 6s - loss: 391.1018 - MinusLogProbMetric: 391.1018 - val_loss: 394.5648 - val_MinusLogProbMetric: 394.5648 - lr: 1.6667e-04 - 6s/epoch - 33ms/step
Epoch 438/1000
2023-10-03 23:25:53.952 
Epoch 438/1000 
	 loss: 391.0322, MinusLogProbMetric: 391.0322, val_loss: 394.2980, val_MinusLogProbMetric: 394.2980

Epoch 438: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0322 - MinusLogProbMetric: 391.0322 - val_loss: 394.2980 - val_MinusLogProbMetric: 394.2980 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 439/1000
2023-10-03 23:26:00.943 
Epoch 439/1000 
	 loss: 391.0276, MinusLogProbMetric: 391.0276, val_loss: 392.7203, val_MinusLogProbMetric: 392.7203

Epoch 439: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0276 - MinusLogProbMetric: 391.0276 - val_loss: 392.7203 - val_MinusLogProbMetric: 392.7203 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 440/1000
2023-10-03 23:26:07.853 
Epoch 440/1000 
	 loss: 391.2674, MinusLogProbMetric: 391.2674, val_loss: 393.8783, val_MinusLogProbMetric: 393.8783

Epoch 440: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.2674 - MinusLogProbMetric: 391.2674 - val_loss: 393.8783 - val_MinusLogProbMetric: 393.8783 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 441/1000
2023-10-03 23:26:14.599 
Epoch 441/1000 
	 loss: 391.0895, MinusLogProbMetric: 391.0895, val_loss: 392.9846, val_MinusLogProbMetric: 392.9846

Epoch 441: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0895 - MinusLogProbMetric: 391.0895 - val_loss: 392.9846 - val_MinusLogProbMetric: 392.9846 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 442/1000
2023-10-03 23:26:21.195 
Epoch 442/1000 
	 loss: 391.0662, MinusLogProbMetric: 391.0662, val_loss: 393.8459, val_MinusLogProbMetric: 393.8459

Epoch 442: val_loss did not improve from 392.66037
196/196 - 7s - loss: 391.0662 - MinusLogProbMetric: 391.0662 - val_loss: 393.8459 - val_MinusLogProbMetric: 393.8459 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 443/1000
2023-10-03 23:26:27.899 
Epoch 443/1000 
	 loss: 390.8311, MinusLogProbMetric: 390.8311, val_loss: 392.5186, val_MinusLogProbMetric: 392.5186

Epoch 443: val_loss improved from 392.66037 to 392.51859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 390.8311 - MinusLogProbMetric: 390.8311 - val_loss: 392.5186 - val_MinusLogProbMetric: 392.5186 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 444/1000
2023-10-03 23:26:35.045 
Epoch 444/1000 
	 loss: 391.0783, MinusLogProbMetric: 391.0783, val_loss: 392.4740, val_MinusLogProbMetric: 392.4740

Epoch 444: val_loss improved from 392.51859 to 392.47403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 391.0783 - MinusLogProbMetric: 391.0783 - val_loss: 392.4740 - val_MinusLogProbMetric: 392.4740 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 445/1000
2023-10-03 23:26:42.115 
Epoch 445/1000 
	 loss: 390.8150, MinusLogProbMetric: 390.8150, val_loss: 394.7840, val_MinusLogProbMetric: 394.7840

Epoch 445: val_loss did not improve from 392.47403
196/196 - 7s - loss: 390.8150 - MinusLogProbMetric: 390.8150 - val_loss: 394.7840 - val_MinusLogProbMetric: 394.7840 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 446/1000
2023-10-03 23:26:48.791 
Epoch 446/1000 
	 loss: 390.9935, MinusLogProbMetric: 390.9935, val_loss: 393.3597, val_MinusLogProbMetric: 393.3597

Epoch 446: val_loss did not improve from 392.47403
196/196 - 7s - loss: 390.9935 - MinusLogProbMetric: 390.9935 - val_loss: 393.3597 - val_MinusLogProbMetric: 393.3597 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 447/1000
2023-10-03 23:26:55.436 
Epoch 447/1000 
	 loss: 390.7287, MinusLogProbMetric: 390.7287, val_loss: 392.6603, val_MinusLogProbMetric: 392.6603

Epoch 447: val_loss did not improve from 392.47403
196/196 - 7s - loss: 390.7287 - MinusLogProbMetric: 390.7287 - val_loss: 392.6603 - val_MinusLogProbMetric: 392.6603 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 448/1000
2023-10-03 23:27:01.996 
Epoch 448/1000 
	 loss: 390.9602, MinusLogProbMetric: 390.9602, val_loss: 392.2609, val_MinusLogProbMetric: 392.2609

Epoch 448: val_loss improved from 392.47403 to 392.26086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 390.9602 - MinusLogProbMetric: 390.9602 - val_loss: 392.2609 - val_MinusLogProbMetric: 392.2609 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 449/1000
2023-10-03 23:27:08.811 
Epoch 449/1000 
	 loss: 391.5168, MinusLogProbMetric: 391.5168, val_loss: 392.5253, val_MinusLogProbMetric: 392.5253

Epoch 449: val_loss did not improve from 392.26086
196/196 - 7s - loss: 391.5168 - MinusLogProbMetric: 391.5168 - val_loss: 392.5253 - val_MinusLogProbMetric: 392.5253 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 450/1000
2023-10-03 23:27:15.431 
Epoch 450/1000 
	 loss: 391.0962, MinusLogProbMetric: 391.0962, val_loss: 393.1661, val_MinusLogProbMetric: 393.1661

Epoch 450: val_loss did not improve from 392.26086
196/196 - 7s - loss: 391.0962 - MinusLogProbMetric: 391.0962 - val_loss: 393.1661 - val_MinusLogProbMetric: 393.1661 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 451/1000
2023-10-03 23:27:22.315 
Epoch 451/1000 
	 loss: 390.7815, MinusLogProbMetric: 390.7815, val_loss: 393.0205, val_MinusLogProbMetric: 393.0205

Epoch 451: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7815 - MinusLogProbMetric: 390.7815 - val_loss: 393.0205 - val_MinusLogProbMetric: 393.0205 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 452/1000
2023-10-03 23:27:28.359 
Epoch 452/1000 
	 loss: 390.8920, MinusLogProbMetric: 390.8920, val_loss: 393.3016, val_MinusLogProbMetric: 393.3016

Epoch 452: val_loss did not improve from 392.26086
196/196 - 6s - loss: 390.8920 - MinusLogProbMetric: 390.8920 - val_loss: 393.3016 - val_MinusLogProbMetric: 393.3016 - lr: 1.6667e-04 - 6s/epoch - 31ms/step
Epoch 453/1000
2023-10-03 23:27:35.112 
Epoch 453/1000 
	 loss: 390.8510, MinusLogProbMetric: 390.8510, val_loss: 393.1127, val_MinusLogProbMetric: 393.1127

Epoch 453: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8510 - MinusLogProbMetric: 390.8510 - val_loss: 393.1127 - val_MinusLogProbMetric: 393.1127 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 454/1000
2023-10-03 23:27:41.599 
Epoch 454/1000 
	 loss: 390.7826, MinusLogProbMetric: 390.7826, val_loss: 393.3235, val_MinusLogProbMetric: 393.3235

Epoch 454: val_loss did not improve from 392.26086
196/196 - 6s - loss: 390.7826 - MinusLogProbMetric: 390.7826 - val_loss: 393.3235 - val_MinusLogProbMetric: 393.3235 - lr: 1.6667e-04 - 6s/epoch - 33ms/step
Epoch 455/1000
2023-10-03 23:27:48.292 
Epoch 455/1000 
	 loss: 390.9650, MinusLogProbMetric: 390.9650, val_loss: 395.0297, val_MinusLogProbMetric: 395.0297

Epoch 455: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.9650 - MinusLogProbMetric: 390.9650 - val_loss: 395.0297 - val_MinusLogProbMetric: 395.0297 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 456/1000
2023-10-03 23:27:54.794 
Epoch 456/1000 
	 loss: 391.0479, MinusLogProbMetric: 391.0479, val_loss: 393.0122, val_MinusLogProbMetric: 393.0122

Epoch 456: val_loss did not improve from 392.26086
196/196 - 6s - loss: 391.0479 - MinusLogProbMetric: 391.0479 - val_loss: 393.0122 - val_MinusLogProbMetric: 393.0122 - lr: 1.6667e-04 - 6s/epoch - 33ms/step
Epoch 457/1000
2023-10-03 23:28:01.508 
Epoch 457/1000 
	 loss: 390.9229, MinusLogProbMetric: 390.9229, val_loss: 392.5089, val_MinusLogProbMetric: 392.5089

Epoch 457: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.9229 - MinusLogProbMetric: 390.9229 - val_loss: 392.5089 - val_MinusLogProbMetric: 392.5089 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 458/1000
2023-10-03 23:28:08.066 
Epoch 458/1000 
	 loss: 390.7171, MinusLogProbMetric: 390.7171, val_loss: 394.0199, val_MinusLogProbMetric: 394.0199

Epoch 458: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7171 - MinusLogProbMetric: 390.7171 - val_loss: 394.0199 - val_MinusLogProbMetric: 394.0199 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 459/1000
2023-10-03 23:28:14.678 
Epoch 459/1000 
	 loss: 390.7694, MinusLogProbMetric: 390.7694, val_loss: 392.8567, val_MinusLogProbMetric: 392.8567

Epoch 459: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7694 - MinusLogProbMetric: 390.7694 - val_loss: 392.8567 - val_MinusLogProbMetric: 392.8567 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 460/1000
2023-10-03 23:28:21.419 
Epoch 460/1000 
	 loss: 391.0354, MinusLogProbMetric: 391.0354, val_loss: 392.7320, val_MinusLogProbMetric: 392.7320

Epoch 460: val_loss did not improve from 392.26086
196/196 - 7s - loss: 391.0354 - MinusLogProbMetric: 391.0354 - val_loss: 392.7320 - val_MinusLogProbMetric: 392.7320 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 461/1000
2023-10-03 23:28:28.078 
Epoch 461/1000 
	 loss: 390.7682, MinusLogProbMetric: 390.7682, val_loss: 392.8589, val_MinusLogProbMetric: 392.8589

Epoch 461: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7682 - MinusLogProbMetric: 390.7682 - val_loss: 392.8589 - val_MinusLogProbMetric: 392.8589 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 462/1000
2023-10-03 23:28:34.766 
Epoch 462/1000 
	 loss: 390.7335, MinusLogProbMetric: 390.7335, val_loss: 395.5797, val_MinusLogProbMetric: 395.5797

Epoch 462: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7335 - MinusLogProbMetric: 390.7335 - val_loss: 395.5797 - val_MinusLogProbMetric: 395.5797 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 463/1000
2023-10-03 23:28:41.474 
Epoch 463/1000 
	 loss: 390.7559, MinusLogProbMetric: 390.7559, val_loss: 394.8871, val_MinusLogProbMetric: 394.8871

Epoch 463: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7559 - MinusLogProbMetric: 390.7559 - val_loss: 394.8871 - val_MinusLogProbMetric: 394.8871 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 464/1000
2023-10-03 23:28:48.194 
Epoch 464/1000 
	 loss: 390.8827, MinusLogProbMetric: 390.8827, val_loss: 393.3049, val_MinusLogProbMetric: 393.3049

Epoch 464: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8827 - MinusLogProbMetric: 390.8827 - val_loss: 393.3049 - val_MinusLogProbMetric: 393.3049 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 465/1000
2023-10-03 23:28:54.848 
Epoch 465/1000 
	 loss: 391.0241, MinusLogProbMetric: 391.0241, val_loss: 395.4880, val_MinusLogProbMetric: 395.4880

Epoch 465: val_loss did not improve from 392.26086
196/196 - 7s - loss: 391.0241 - MinusLogProbMetric: 391.0241 - val_loss: 395.4880 - val_MinusLogProbMetric: 395.4880 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 466/1000
2023-10-03 23:29:01.556 
Epoch 466/1000 
	 loss: 390.7258, MinusLogProbMetric: 390.7258, val_loss: 394.2681, val_MinusLogProbMetric: 394.2681

Epoch 466: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7258 - MinusLogProbMetric: 390.7258 - val_loss: 394.2681 - val_MinusLogProbMetric: 394.2681 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 467/1000
2023-10-03 23:29:08.159 
Epoch 467/1000 
	 loss: 390.9589, MinusLogProbMetric: 390.9589, val_loss: 393.1779, val_MinusLogProbMetric: 393.1779

Epoch 467: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.9589 - MinusLogProbMetric: 390.9589 - val_loss: 393.1779 - val_MinusLogProbMetric: 393.1779 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 468/1000
2023-10-03 23:29:15.230 
Epoch 468/1000 
	 loss: 390.7254, MinusLogProbMetric: 390.7254, val_loss: 392.6788, val_MinusLogProbMetric: 392.6788

Epoch 468: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7254 - MinusLogProbMetric: 390.7254 - val_loss: 392.6788 - val_MinusLogProbMetric: 392.6788 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 469/1000
2023-10-03 23:29:22.037 
Epoch 469/1000 
	 loss: 390.7400, MinusLogProbMetric: 390.7400, val_loss: 393.1169, val_MinusLogProbMetric: 393.1169

Epoch 469: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7400 - MinusLogProbMetric: 390.7400 - val_loss: 393.1169 - val_MinusLogProbMetric: 393.1169 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 470/1000
2023-10-03 23:29:28.712 
Epoch 470/1000 
	 loss: 391.0618, MinusLogProbMetric: 391.0618, val_loss: 392.3911, val_MinusLogProbMetric: 392.3911

Epoch 470: val_loss did not improve from 392.26086
196/196 - 7s - loss: 391.0618 - MinusLogProbMetric: 391.0618 - val_loss: 392.3911 - val_MinusLogProbMetric: 392.3911 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 471/1000
2023-10-03 23:29:35.539 
Epoch 471/1000 
	 loss: 390.6234, MinusLogProbMetric: 390.6234, val_loss: 395.0165, val_MinusLogProbMetric: 395.0165

Epoch 471: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.6234 - MinusLogProbMetric: 390.6234 - val_loss: 395.0165 - val_MinusLogProbMetric: 395.0165 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 472/1000
2023-10-03 23:29:42.151 
Epoch 472/1000 
	 loss: 390.8749, MinusLogProbMetric: 390.8749, val_loss: 392.3620, val_MinusLogProbMetric: 392.3620

Epoch 472: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8749 - MinusLogProbMetric: 390.8749 - val_loss: 392.3620 - val_MinusLogProbMetric: 392.3620 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 473/1000
2023-10-03 23:29:48.774 
Epoch 473/1000 
	 loss: 390.7312, MinusLogProbMetric: 390.7312, val_loss: 393.3730, val_MinusLogProbMetric: 393.3730

Epoch 473: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7312 - MinusLogProbMetric: 390.7312 - val_loss: 393.3730 - val_MinusLogProbMetric: 393.3730 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 474/1000
2023-10-03 23:29:55.575 
Epoch 474/1000 
	 loss: 390.6508, MinusLogProbMetric: 390.6508, val_loss: 393.3729, val_MinusLogProbMetric: 393.3729

Epoch 474: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.6508 - MinusLogProbMetric: 390.6508 - val_loss: 393.3729 - val_MinusLogProbMetric: 393.3729 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 475/1000
2023-10-03 23:30:02.377 
Epoch 475/1000 
	 loss: 390.7000, MinusLogProbMetric: 390.7000, val_loss: 393.1595, val_MinusLogProbMetric: 393.1595

Epoch 475: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7000 - MinusLogProbMetric: 390.7000 - val_loss: 393.1595 - val_MinusLogProbMetric: 393.1595 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 476/1000
2023-10-03 23:30:09.003 
Epoch 476/1000 
	 loss: 390.7038, MinusLogProbMetric: 390.7038, val_loss: 393.1233, val_MinusLogProbMetric: 393.1233

Epoch 476: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.7038 - MinusLogProbMetric: 390.7038 - val_loss: 393.1233 - val_MinusLogProbMetric: 393.1233 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 477/1000
2023-10-03 23:30:15.728 
Epoch 477/1000 
	 loss: 390.9002, MinusLogProbMetric: 390.9002, val_loss: 393.0650, val_MinusLogProbMetric: 393.0650

Epoch 477: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.9002 - MinusLogProbMetric: 390.9002 - val_loss: 393.0650 - val_MinusLogProbMetric: 393.0650 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 478/1000
2023-10-03 23:30:22.329 
Epoch 478/1000 
	 loss: 390.6125, MinusLogProbMetric: 390.6125, val_loss: 393.7194, val_MinusLogProbMetric: 393.7194

Epoch 478: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.6125 - MinusLogProbMetric: 390.6125 - val_loss: 393.7194 - val_MinusLogProbMetric: 393.7194 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 479/1000
2023-10-03 23:30:29.140 
Epoch 479/1000 
	 loss: 390.8147, MinusLogProbMetric: 390.8147, val_loss: 393.2054, val_MinusLogProbMetric: 393.2054

Epoch 479: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8147 - MinusLogProbMetric: 390.8147 - val_loss: 393.2054 - val_MinusLogProbMetric: 393.2054 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 480/1000
2023-10-03 23:30:35.879 
Epoch 480/1000 
	 loss: 390.8945, MinusLogProbMetric: 390.8945, val_loss: 393.0787, val_MinusLogProbMetric: 393.0787

Epoch 480: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8945 - MinusLogProbMetric: 390.8945 - val_loss: 393.0787 - val_MinusLogProbMetric: 393.0787 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 481/1000
2023-10-03 23:30:42.476 
Epoch 481/1000 
	 loss: 390.6507, MinusLogProbMetric: 390.6507, val_loss: 393.2490, val_MinusLogProbMetric: 393.2490

Epoch 481: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.6507 - MinusLogProbMetric: 390.6507 - val_loss: 393.2490 - val_MinusLogProbMetric: 393.2490 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 482/1000
2023-10-03 23:30:49.100 
Epoch 482/1000 
	 loss: 390.8856, MinusLogProbMetric: 390.8856, val_loss: 394.3027, val_MinusLogProbMetric: 394.3027

Epoch 482: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8856 - MinusLogProbMetric: 390.8856 - val_loss: 394.3027 - val_MinusLogProbMetric: 394.3027 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 483/1000
2023-10-03 23:30:55.735 
Epoch 483/1000 
	 loss: 390.8246, MinusLogProbMetric: 390.8246, val_loss: 393.5248, val_MinusLogProbMetric: 393.5248

Epoch 483: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8246 - MinusLogProbMetric: 390.8246 - val_loss: 393.5248 - val_MinusLogProbMetric: 393.5248 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 484/1000
2023-10-03 23:31:02.294 
Epoch 484/1000 
	 loss: 390.8576, MinusLogProbMetric: 390.8576, val_loss: 392.7378, val_MinusLogProbMetric: 392.7378

Epoch 484: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.8576 - MinusLogProbMetric: 390.8576 - val_loss: 392.7378 - val_MinusLogProbMetric: 392.7378 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 485/1000
2023-10-03 23:31:09.277 
Epoch 485/1000 
	 loss: 390.6630, MinusLogProbMetric: 390.6630, val_loss: 392.9060, val_MinusLogProbMetric: 392.9060

Epoch 485: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.6630 - MinusLogProbMetric: 390.6630 - val_loss: 392.9060 - val_MinusLogProbMetric: 392.9060 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 486/1000
2023-10-03 23:31:15.959 
Epoch 486/1000 
	 loss: 390.5258, MinusLogProbMetric: 390.5258, val_loss: 392.6973, val_MinusLogProbMetric: 392.6973

Epoch 486: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.5258 - MinusLogProbMetric: 390.5258 - val_loss: 392.6973 - val_MinusLogProbMetric: 392.6973 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 487/1000
2023-10-03 23:31:22.579 
Epoch 487/1000 
	 loss: 390.6835, MinusLogProbMetric: 390.6835, val_loss: 393.9591, val_MinusLogProbMetric: 393.9591

Epoch 487: val_loss did not improve from 392.26086
196/196 - 7s - loss: 390.6835 - MinusLogProbMetric: 390.6835 - val_loss: 393.9591 - val_MinusLogProbMetric: 393.9591 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 488/1000
2023-10-03 23:31:29.390 
Epoch 488/1000 
	 loss: 390.7281, MinusLogProbMetric: 390.7281, val_loss: 392.1070, val_MinusLogProbMetric: 392.1070

Epoch 488: val_loss improved from 392.26086 to 392.10703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 390.7281 - MinusLogProbMetric: 390.7281 - val_loss: 392.1070 - val_MinusLogProbMetric: 392.1070 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 489/1000
2023-10-03 23:31:36.178 
Epoch 489/1000 
	 loss: 390.7170, MinusLogProbMetric: 390.7170, val_loss: 393.8610, val_MinusLogProbMetric: 393.8610

Epoch 489: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.7170 - MinusLogProbMetric: 390.7170 - val_loss: 393.8610 - val_MinusLogProbMetric: 393.8610 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 490/1000
2023-10-03 23:31:42.848 
Epoch 490/1000 
	 loss: 390.5361, MinusLogProbMetric: 390.5361, val_loss: 394.2136, val_MinusLogProbMetric: 394.2136

Epoch 490: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5361 - MinusLogProbMetric: 390.5361 - val_loss: 394.2136 - val_MinusLogProbMetric: 394.2136 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 491/1000
2023-10-03 23:31:49.800 
Epoch 491/1000 
	 loss: 390.8658, MinusLogProbMetric: 390.8658, val_loss: 392.7531, val_MinusLogProbMetric: 392.7531

Epoch 491: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.8658 - MinusLogProbMetric: 390.8658 - val_loss: 392.7531 - val_MinusLogProbMetric: 392.7531 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 492/1000
2023-10-03 23:31:56.562 
Epoch 492/1000 
	 loss: 390.6828, MinusLogProbMetric: 390.6828, val_loss: 393.4338, val_MinusLogProbMetric: 393.4338

Epoch 492: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.6828 - MinusLogProbMetric: 390.6828 - val_loss: 393.4338 - val_MinusLogProbMetric: 393.4338 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 493/1000
2023-10-03 23:32:03.210 
Epoch 493/1000 
	 loss: 390.9984, MinusLogProbMetric: 390.9984, val_loss: 392.7972, val_MinusLogProbMetric: 392.7972

Epoch 493: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.9984 - MinusLogProbMetric: 390.9984 - val_loss: 392.7972 - val_MinusLogProbMetric: 392.7972 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 494/1000
2023-10-03 23:32:09.845 
Epoch 494/1000 
	 loss: 390.7271, MinusLogProbMetric: 390.7271, val_loss: 392.7657, val_MinusLogProbMetric: 392.7657

Epoch 494: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.7271 - MinusLogProbMetric: 390.7271 - val_loss: 392.7657 - val_MinusLogProbMetric: 392.7657 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 495/1000
2023-10-03 23:32:16.433 
Epoch 495/1000 
	 loss: 390.1949, MinusLogProbMetric: 390.1949, val_loss: 393.7868, val_MinusLogProbMetric: 393.7868

Epoch 495: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.1949 - MinusLogProbMetric: 390.1949 - val_loss: 393.7868 - val_MinusLogProbMetric: 393.7868 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 496/1000
2023-10-03 23:32:23.314 
Epoch 496/1000 
	 loss: 390.7386, MinusLogProbMetric: 390.7386, val_loss: 394.3758, val_MinusLogProbMetric: 394.3758

Epoch 496: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.7386 - MinusLogProbMetric: 390.7386 - val_loss: 394.3758 - val_MinusLogProbMetric: 394.3758 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 497/1000
2023-10-03 23:32:30.248 
Epoch 497/1000 
	 loss: 390.8344, MinusLogProbMetric: 390.8344, val_loss: 394.2329, val_MinusLogProbMetric: 394.2329

Epoch 497: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.8344 - MinusLogProbMetric: 390.8344 - val_loss: 394.2329 - val_MinusLogProbMetric: 394.2329 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 498/1000
2023-10-03 23:32:36.891 
Epoch 498/1000 
	 loss: 390.6057, MinusLogProbMetric: 390.6057, val_loss: 393.4139, val_MinusLogProbMetric: 393.4139

Epoch 498: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.6057 - MinusLogProbMetric: 390.6057 - val_loss: 393.4139 - val_MinusLogProbMetric: 393.4139 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 499/1000
2023-10-03 23:32:43.441 
Epoch 499/1000 
	 loss: 390.5197, MinusLogProbMetric: 390.5197, val_loss: 395.6679, val_MinusLogProbMetric: 395.6679

Epoch 499: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5197 - MinusLogProbMetric: 390.5197 - val_loss: 395.6679 - val_MinusLogProbMetric: 395.6679 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 500/1000
2023-10-03 23:32:50.067 
Epoch 500/1000 
	 loss: 390.6620, MinusLogProbMetric: 390.6620, val_loss: 393.4297, val_MinusLogProbMetric: 393.4297

Epoch 500: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.6620 - MinusLogProbMetric: 390.6620 - val_loss: 393.4297 - val_MinusLogProbMetric: 393.4297 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 501/1000
2023-10-03 23:32:56.843 
Epoch 501/1000 
	 loss: 390.5833, MinusLogProbMetric: 390.5833, val_loss: 392.7014, val_MinusLogProbMetric: 392.7014

Epoch 501: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5833 - MinusLogProbMetric: 390.5833 - val_loss: 392.7014 - val_MinusLogProbMetric: 392.7014 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 502/1000
2023-10-03 23:33:03.722 
Epoch 502/1000 
	 loss: 390.7637, MinusLogProbMetric: 390.7637, val_loss: 393.1842, val_MinusLogProbMetric: 393.1842

Epoch 502: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.7637 - MinusLogProbMetric: 390.7637 - val_loss: 393.1842 - val_MinusLogProbMetric: 393.1842 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 503/1000
2023-10-03 23:33:10.389 
Epoch 503/1000 
	 loss: 390.8640, MinusLogProbMetric: 390.8640, val_loss: 393.0630, val_MinusLogProbMetric: 393.0630

Epoch 503: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.8640 - MinusLogProbMetric: 390.8640 - val_loss: 393.0630 - val_MinusLogProbMetric: 393.0630 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 504/1000
2023-10-03 23:33:17.014 
Epoch 504/1000 
	 loss: 390.6972, MinusLogProbMetric: 390.6972, val_loss: 393.0843, val_MinusLogProbMetric: 393.0843

Epoch 504: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.6972 - MinusLogProbMetric: 390.6972 - val_loss: 393.0843 - val_MinusLogProbMetric: 393.0843 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 505/1000
2023-10-03 23:33:23.818 
Epoch 505/1000 
	 loss: 390.5304, MinusLogProbMetric: 390.5304, val_loss: 392.7249, val_MinusLogProbMetric: 392.7249

Epoch 505: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5304 - MinusLogProbMetric: 390.5304 - val_loss: 392.7249 - val_MinusLogProbMetric: 392.7249 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 506/1000
2023-10-03 23:33:30.447 
Epoch 506/1000 
	 loss: 390.4258, MinusLogProbMetric: 390.4258, val_loss: 392.7500, val_MinusLogProbMetric: 392.7500

Epoch 506: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.4258 - MinusLogProbMetric: 390.4258 - val_loss: 392.7500 - val_MinusLogProbMetric: 392.7500 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 507/1000
2023-10-03 23:33:37.060 
Epoch 507/1000 
	 loss: 390.4193, MinusLogProbMetric: 390.4193, val_loss: 392.8740, val_MinusLogProbMetric: 392.8740

Epoch 507: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.4193 - MinusLogProbMetric: 390.4193 - val_loss: 392.8740 - val_MinusLogProbMetric: 392.8740 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 508/1000
2023-10-03 23:33:43.807 
Epoch 508/1000 
	 loss: 390.7354, MinusLogProbMetric: 390.7354, val_loss: 392.7945, val_MinusLogProbMetric: 392.7945

Epoch 508: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.7354 - MinusLogProbMetric: 390.7354 - val_loss: 392.7945 - val_MinusLogProbMetric: 392.7945 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 509/1000
2023-10-03 23:33:50.436 
Epoch 509/1000 
	 loss: 390.4914, MinusLogProbMetric: 390.4914, val_loss: 393.6567, val_MinusLogProbMetric: 393.6567

Epoch 509: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.4914 - MinusLogProbMetric: 390.4914 - val_loss: 393.6567 - val_MinusLogProbMetric: 393.6567 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 510/1000
2023-10-03 23:33:57.057 
Epoch 510/1000 
	 loss: 390.6778, MinusLogProbMetric: 390.6778, val_loss: 393.3384, val_MinusLogProbMetric: 393.3384

Epoch 510: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.6778 - MinusLogProbMetric: 390.6778 - val_loss: 393.3384 - val_MinusLogProbMetric: 393.3384 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 511/1000
2023-10-03 23:34:03.659 
Epoch 511/1000 
	 loss: 390.5600, MinusLogProbMetric: 390.5600, val_loss: 393.1616, val_MinusLogProbMetric: 393.1616

Epoch 511: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5600 - MinusLogProbMetric: 390.5600 - val_loss: 393.1616 - val_MinusLogProbMetric: 393.1616 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 512/1000
2023-10-03 23:34:10.395 
Epoch 512/1000 
	 loss: 390.5310, MinusLogProbMetric: 390.5310, val_loss: 393.0392, val_MinusLogProbMetric: 393.0392

Epoch 512: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5310 - MinusLogProbMetric: 390.5310 - val_loss: 393.0392 - val_MinusLogProbMetric: 393.0392 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 513/1000
2023-10-03 23:34:17.574 
Epoch 513/1000 
	 loss: 390.5657, MinusLogProbMetric: 390.5657, val_loss: 392.9000, val_MinusLogProbMetric: 392.9000

Epoch 513: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5657 - MinusLogProbMetric: 390.5657 - val_loss: 392.9000 - val_MinusLogProbMetric: 392.9000 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 514/1000
2023-10-03 23:34:24.544 
Epoch 514/1000 
	 loss: 390.5895, MinusLogProbMetric: 390.5895, val_loss: 393.6338, val_MinusLogProbMetric: 393.6338

Epoch 514: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5895 - MinusLogProbMetric: 390.5895 - val_loss: 393.6338 - val_MinusLogProbMetric: 393.6338 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 515/1000
2023-10-03 23:34:31.162 
Epoch 515/1000 
	 loss: 390.6680, MinusLogProbMetric: 390.6680, val_loss: 392.8975, val_MinusLogProbMetric: 392.8975

Epoch 515: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.6680 - MinusLogProbMetric: 390.6680 - val_loss: 392.8975 - val_MinusLogProbMetric: 392.8975 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 516/1000
2023-10-03 23:34:37.810 
Epoch 516/1000 
	 loss: 390.5517, MinusLogProbMetric: 390.5517, val_loss: 393.2413, val_MinusLogProbMetric: 393.2413

Epoch 516: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5517 - MinusLogProbMetric: 390.5517 - val_loss: 393.2413 - val_MinusLogProbMetric: 393.2413 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 517/1000
2023-10-03 23:34:44.371 
Epoch 517/1000 
	 loss: 390.4842, MinusLogProbMetric: 390.4842, val_loss: 392.3238, val_MinusLogProbMetric: 392.3238

Epoch 517: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.4842 - MinusLogProbMetric: 390.4842 - val_loss: 392.3238 - val_MinusLogProbMetric: 392.3238 - lr: 1.6667e-04 - 7s/epoch - 33ms/step
Epoch 518/1000
2023-10-03 23:34:51.085 
Epoch 518/1000 
	 loss: 390.6470, MinusLogProbMetric: 390.6470, val_loss: 392.6578, val_MinusLogProbMetric: 392.6578

Epoch 518: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.6470 - MinusLogProbMetric: 390.6470 - val_loss: 392.6578 - val_MinusLogProbMetric: 392.6578 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 519/1000
2023-10-03 23:34:57.861 
Epoch 519/1000 
	 loss: 390.3714, MinusLogProbMetric: 390.3714, val_loss: 395.3927, val_MinusLogProbMetric: 395.3927

Epoch 519: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.3714 - MinusLogProbMetric: 390.3714 - val_loss: 395.3927 - val_MinusLogProbMetric: 395.3927 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 520/1000
2023-10-03 23:35:04.692 
Epoch 520/1000 
	 loss: 390.5405, MinusLogProbMetric: 390.5405, val_loss: 394.7376, val_MinusLogProbMetric: 394.7376

Epoch 520: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5405 - MinusLogProbMetric: 390.5405 - val_loss: 394.7376 - val_MinusLogProbMetric: 394.7376 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 521/1000
2023-10-03 23:35:11.349 
Epoch 521/1000 
	 loss: 390.5787, MinusLogProbMetric: 390.5787, val_loss: 393.0683, val_MinusLogProbMetric: 393.0683

Epoch 521: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5787 - MinusLogProbMetric: 390.5787 - val_loss: 393.0683 - val_MinusLogProbMetric: 393.0683 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 522/1000
2023-10-03 23:35:17.991 
Epoch 522/1000 
	 loss: 390.4697, MinusLogProbMetric: 390.4697, val_loss: 393.0873, val_MinusLogProbMetric: 393.0873

Epoch 522: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.4697 - MinusLogProbMetric: 390.4697 - val_loss: 393.0873 - val_MinusLogProbMetric: 393.0873 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 523/1000
2023-10-03 23:35:24.722 
Epoch 523/1000 
	 loss: 390.5938, MinusLogProbMetric: 390.5938, val_loss: 392.6276, val_MinusLogProbMetric: 392.6276

Epoch 523: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5938 - MinusLogProbMetric: 390.5938 - val_loss: 392.6276 - val_MinusLogProbMetric: 392.6276 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 524/1000
2023-10-03 23:35:31.352 
Epoch 524/1000 
	 loss: 390.5224, MinusLogProbMetric: 390.5224, val_loss: 392.4991, val_MinusLogProbMetric: 392.4991

Epoch 524: val_loss did not improve from 392.10703
196/196 - 7s - loss: 390.5224 - MinusLogProbMetric: 390.5224 - val_loss: 392.4991 - val_MinusLogProbMetric: 392.4991 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 525/1000
2023-10-03 23:35:38.299 
Epoch 525/1000 
	 loss: 391.2573, MinusLogProbMetric: 391.2573, val_loss: 392.0290, val_MinusLogProbMetric: 392.0290

Epoch 525: val_loss improved from 392.10703 to 392.02899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 391.2573 - MinusLogProbMetric: 391.2573 - val_loss: 392.0290 - val_MinusLogProbMetric: 392.0290 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 526/1000
2023-10-03 23:35:45.323 
Epoch 526/1000 
	 loss: 390.4935, MinusLogProbMetric: 390.4935, val_loss: 392.7057, val_MinusLogProbMetric: 392.7057

Epoch 526: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4935 - MinusLogProbMetric: 390.4935 - val_loss: 392.7057 - val_MinusLogProbMetric: 392.7057 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 527/1000
2023-10-03 23:35:52.141 
Epoch 527/1000 
	 loss: 390.2229, MinusLogProbMetric: 390.2229, val_loss: 392.8193, val_MinusLogProbMetric: 392.8193

Epoch 527: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.2229 - MinusLogProbMetric: 390.2229 - val_loss: 392.8193 - val_MinusLogProbMetric: 392.8193 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 528/1000
2023-10-03 23:35:58.737 
Epoch 528/1000 
	 loss: 390.5137, MinusLogProbMetric: 390.5137, val_loss: 392.6547, val_MinusLogProbMetric: 392.6547

Epoch 528: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.5137 - MinusLogProbMetric: 390.5137 - val_loss: 392.6547 - val_MinusLogProbMetric: 392.6547 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 529/1000
2023-10-03 23:36:05.453 
Epoch 529/1000 
	 loss: 390.4714, MinusLogProbMetric: 390.4714, val_loss: 393.2379, val_MinusLogProbMetric: 393.2379

Epoch 529: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4714 - MinusLogProbMetric: 390.4714 - val_loss: 393.2379 - val_MinusLogProbMetric: 393.2379 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 530/1000
2023-10-03 23:36:12.133 
Epoch 530/1000 
	 loss: 390.4730, MinusLogProbMetric: 390.4730, val_loss: 392.9649, val_MinusLogProbMetric: 392.9649

Epoch 530: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4730 - MinusLogProbMetric: 390.4730 - val_loss: 392.9649 - val_MinusLogProbMetric: 392.9649 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 531/1000
2023-10-03 23:36:19.160 
Epoch 531/1000 
	 loss: 390.5380, MinusLogProbMetric: 390.5380, val_loss: 393.3007, val_MinusLogProbMetric: 393.3007

Epoch 531: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.5380 - MinusLogProbMetric: 390.5380 - val_loss: 393.3007 - val_MinusLogProbMetric: 393.3007 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 532/1000
2023-10-03 23:36:25.838 
Epoch 532/1000 
	 loss: 390.5291, MinusLogProbMetric: 390.5291, val_loss: 392.6266, val_MinusLogProbMetric: 392.6266

Epoch 532: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.5291 - MinusLogProbMetric: 390.5291 - val_loss: 392.6266 - val_MinusLogProbMetric: 392.6266 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 533/1000
2023-10-03 23:36:32.520 
Epoch 533/1000 
	 loss: 390.4548, MinusLogProbMetric: 390.4548, val_loss: 393.8608, val_MinusLogProbMetric: 393.8608

Epoch 533: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4548 - MinusLogProbMetric: 390.4548 - val_loss: 393.8608 - val_MinusLogProbMetric: 393.8608 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 534/1000
2023-10-03 23:36:39.251 
Epoch 534/1000 
	 loss: 390.3472, MinusLogProbMetric: 390.3472, val_loss: 392.7087, val_MinusLogProbMetric: 392.7087

Epoch 534: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.3472 - MinusLogProbMetric: 390.3472 - val_loss: 392.7087 - val_MinusLogProbMetric: 392.7087 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 535/1000
2023-10-03 23:36:45.903 
Epoch 535/1000 
	 loss: 390.4946, MinusLogProbMetric: 390.4946, val_loss: 393.6367, val_MinusLogProbMetric: 393.6367

Epoch 535: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4946 - MinusLogProbMetric: 390.4946 - val_loss: 393.6367 - val_MinusLogProbMetric: 393.6367 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 536/1000
2023-10-03 23:36:52.799 
Epoch 536/1000 
	 loss: 390.5246, MinusLogProbMetric: 390.5246, val_loss: 394.3027, val_MinusLogProbMetric: 394.3027

Epoch 536: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.5246 - MinusLogProbMetric: 390.5246 - val_loss: 394.3027 - val_MinusLogProbMetric: 394.3027 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 537/1000
2023-10-03 23:36:59.462 
Epoch 537/1000 
	 loss: 390.1317, MinusLogProbMetric: 390.1317, val_loss: 393.3175, val_MinusLogProbMetric: 393.3175

Epoch 537: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.1317 - MinusLogProbMetric: 390.1317 - val_loss: 393.3175 - val_MinusLogProbMetric: 393.3175 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 538/1000
2023-10-03 23:37:06.035 
Epoch 538/1000 
	 loss: 390.7434, MinusLogProbMetric: 390.7434, val_loss: 394.3230, val_MinusLogProbMetric: 394.3230

Epoch 538: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.7434 - MinusLogProbMetric: 390.7434 - val_loss: 394.3230 - val_MinusLogProbMetric: 394.3230 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 539/1000
2023-10-03 23:37:12.764 
Epoch 539/1000 
	 loss: 390.7447, MinusLogProbMetric: 390.7447, val_loss: 392.0807, val_MinusLogProbMetric: 392.0807

Epoch 539: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.7447 - MinusLogProbMetric: 390.7447 - val_loss: 392.0807 - val_MinusLogProbMetric: 392.0807 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 540/1000
2023-10-03 23:37:19.346 
Epoch 540/1000 
	 loss: 390.4882, MinusLogProbMetric: 390.4882, val_loss: 392.7196, val_MinusLogProbMetric: 392.7196

Epoch 540: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4882 - MinusLogProbMetric: 390.4882 - val_loss: 392.7196 - val_MinusLogProbMetric: 392.7196 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 541/1000
2023-10-03 23:37:26.094 
Epoch 541/1000 
	 loss: 390.4274, MinusLogProbMetric: 390.4274, val_loss: 393.3024, val_MinusLogProbMetric: 393.3024

Epoch 541: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4274 - MinusLogProbMetric: 390.4274 - val_loss: 393.3024 - val_MinusLogProbMetric: 393.3024 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 542/1000
2023-10-03 23:37:32.995 
Epoch 542/1000 
	 loss: 390.0340, MinusLogProbMetric: 390.0340, val_loss: 392.2567, val_MinusLogProbMetric: 392.2567

Epoch 542: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.0340 - MinusLogProbMetric: 390.0340 - val_loss: 392.2567 - val_MinusLogProbMetric: 392.2567 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 543/1000
2023-10-03 23:37:39.603 
Epoch 543/1000 
	 loss: 390.6605, MinusLogProbMetric: 390.6605, val_loss: 392.3322, val_MinusLogProbMetric: 392.3322

Epoch 543: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.6605 - MinusLogProbMetric: 390.6605 - val_loss: 392.3322 - val_MinusLogProbMetric: 392.3322 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 544/1000
2023-10-03 23:37:46.372 
Epoch 544/1000 
	 loss: 390.3999, MinusLogProbMetric: 390.3999, val_loss: 392.1716, val_MinusLogProbMetric: 392.1716

Epoch 544: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.3999 - MinusLogProbMetric: 390.3999 - val_loss: 392.1716 - val_MinusLogProbMetric: 392.1716 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 545/1000
2023-10-03 23:37:53.038 
Epoch 545/1000 
	 loss: 390.1091, MinusLogProbMetric: 390.1091, val_loss: 395.0133, val_MinusLogProbMetric: 395.0133

Epoch 545: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.1091 - MinusLogProbMetric: 390.1091 - val_loss: 395.0133 - val_MinusLogProbMetric: 395.0133 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 546/1000
2023-10-03 23:37:59.792 
Epoch 546/1000 
	 loss: 390.4269, MinusLogProbMetric: 390.4269, val_loss: 392.0432, val_MinusLogProbMetric: 392.0432

Epoch 546: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.4269 - MinusLogProbMetric: 390.4269 - val_loss: 392.0432 - val_MinusLogProbMetric: 392.0432 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 547/1000
2023-10-03 23:38:06.634 
Epoch 547/1000 
	 loss: 390.5152, MinusLogProbMetric: 390.5152, val_loss: 394.8279, val_MinusLogProbMetric: 394.8279

Epoch 547: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.5152 - MinusLogProbMetric: 390.5152 - val_loss: 394.8279 - val_MinusLogProbMetric: 394.8279 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 548/1000
2023-10-03 23:38:13.352 
Epoch 548/1000 
	 loss: 390.3528, MinusLogProbMetric: 390.3528, val_loss: 392.1907, val_MinusLogProbMetric: 392.1907

Epoch 548: val_loss did not improve from 392.02899
196/196 - 7s - loss: 390.3528 - MinusLogProbMetric: 390.3528 - val_loss: 392.1907 - val_MinusLogProbMetric: 392.1907 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 549/1000
2023-10-03 23:38:20.019 
Epoch 549/1000 
	 loss: 390.4707, MinusLogProbMetric: 390.4707, val_loss: 392.0252, val_MinusLogProbMetric: 392.0252

Epoch 549: val_loss improved from 392.02899 to 392.02521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 7s - loss: 390.4707 - MinusLogProbMetric: 390.4707 - val_loss: 392.0252 - val_MinusLogProbMetric: 392.0252 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 550/1000
2023-10-03 23:38:26.919 
Epoch 550/1000 
	 loss: 390.2568, MinusLogProbMetric: 390.2568, val_loss: 392.5583, val_MinusLogProbMetric: 392.5583

Epoch 550: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2568 - MinusLogProbMetric: 390.2568 - val_loss: 392.5583 - val_MinusLogProbMetric: 392.5583 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 551/1000
2023-10-03 23:38:33.640 
Epoch 551/1000 
	 loss: 390.2676, MinusLogProbMetric: 390.2676, val_loss: 392.5544, val_MinusLogProbMetric: 392.5544

Epoch 551: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2676 - MinusLogProbMetric: 390.2676 - val_loss: 392.5544 - val_MinusLogProbMetric: 392.5544 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 552/1000
2023-10-03 23:38:40.323 
Epoch 552/1000 
	 loss: 390.2048, MinusLogProbMetric: 390.2048, val_loss: 392.4627, val_MinusLogProbMetric: 392.4627

Epoch 552: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2048 - MinusLogProbMetric: 390.2048 - val_loss: 392.4627 - val_MinusLogProbMetric: 392.4627 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 553/1000
2023-10-03 23:38:46.954 
Epoch 553/1000 
	 loss: 390.5979, MinusLogProbMetric: 390.5979, val_loss: 393.3215, val_MinusLogProbMetric: 393.3215

Epoch 553: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.5979 - MinusLogProbMetric: 390.5979 - val_loss: 393.3215 - val_MinusLogProbMetric: 393.3215 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 554/1000
2023-10-03 23:38:53.623 
Epoch 554/1000 
	 loss: 390.1008, MinusLogProbMetric: 390.1008, val_loss: 392.9541, val_MinusLogProbMetric: 392.9541

Epoch 554: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.1008 - MinusLogProbMetric: 390.1008 - val_loss: 392.9541 - val_MinusLogProbMetric: 392.9541 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 555/1000
2023-10-03 23:39:00.292 
Epoch 555/1000 
	 loss: 390.3013, MinusLogProbMetric: 390.3013, val_loss: 392.3822, val_MinusLogProbMetric: 392.3822

Epoch 555: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.3013 - MinusLogProbMetric: 390.3013 - val_loss: 392.3822 - val_MinusLogProbMetric: 392.3822 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 556/1000
2023-10-03 23:39:07.220 
Epoch 556/1000 
	 loss: 390.3083, MinusLogProbMetric: 390.3083, val_loss: 392.9441, val_MinusLogProbMetric: 392.9441

Epoch 556: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.3083 - MinusLogProbMetric: 390.3083 - val_loss: 392.9441 - val_MinusLogProbMetric: 392.9441 - lr: 1.6667e-04 - 7s/epoch - 35ms/step
Epoch 557/1000
2023-10-03 23:39:14.497 
Epoch 557/1000 
	 loss: 390.4320, MinusLogProbMetric: 390.4320, val_loss: 393.4369, val_MinusLogProbMetric: 393.4369

Epoch 557: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.4320 - MinusLogProbMetric: 390.4320 - val_loss: 393.4369 - val_MinusLogProbMetric: 393.4369 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 558/1000
2023-10-03 23:39:21.892 
Epoch 558/1000 
	 loss: 390.1083, MinusLogProbMetric: 390.1083, val_loss: 392.2412, val_MinusLogProbMetric: 392.2412

Epoch 558: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.1083 - MinusLogProbMetric: 390.1083 - val_loss: 392.2412 - val_MinusLogProbMetric: 392.2412 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 559/1000
2023-10-03 23:39:28.615 
Epoch 559/1000 
	 loss: 390.6209, MinusLogProbMetric: 390.6209, val_loss: 393.0170, val_MinusLogProbMetric: 393.0170

Epoch 559: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.6209 - MinusLogProbMetric: 390.6209 - val_loss: 393.0170 - val_MinusLogProbMetric: 393.0170 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 560/1000
2023-10-03 23:39:35.921 
Epoch 560/1000 
	 loss: 390.3334, MinusLogProbMetric: 390.3334, val_loss: 392.8527, val_MinusLogProbMetric: 392.8527

Epoch 560: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.3334 - MinusLogProbMetric: 390.3334 - val_loss: 392.8527 - val_MinusLogProbMetric: 392.8527 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 561/1000
2023-10-03 23:39:43.311 
Epoch 561/1000 
	 loss: 390.2085, MinusLogProbMetric: 390.2085, val_loss: 393.3369, val_MinusLogProbMetric: 393.3369

Epoch 561: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2085 - MinusLogProbMetric: 390.2085 - val_loss: 393.3369 - val_MinusLogProbMetric: 393.3369 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 562/1000
2023-10-03 23:39:50.418 
Epoch 562/1000 
	 loss: 390.4385, MinusLogProbMetric: 390.4385, val_loss: 392.1712, val_MinusLogProbMetric: 392.1712

Epoch 562: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.4385 - MinusLogProbMetric: 390.4385 - val_loss: 392.1712 - val_MinusLogProbMetric: 392.1712 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 563/1000
2023-10-03 23:39:57.644 
Epoch 563/1000 
	 loss: 390.3651, MinusLogProbMetric: 390.3651, val_loss: 393.9286, val_MinusLogProbMetric: 393.9286

Epoch 563: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.3651 - MinusLogProbMetric: 390.3651 - val_loss: 393.9286 - val_MinusLogProbMetric: 393.9286 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 564/1000
2023-10-03 23:40:04.656 
Epoch 564/1000 
	 loss: 390.2928, MinusLogProbMetric: 390.2928, val_loss: 396.6132, val_MinusLogProbMetric: 396.6132

Epoch 564: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2928 - MinusLogProbMetric: 390.2928 - val_loss: 396.6132 - val_MinusLogProbMetric: 396.6132 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 565/1000
2023-10-03 23:40:11.782 
Epoch 565/1000 
	 loss: 390.3444, MinusLogProbMetric: 390.3444, val_loss: 392.5225, val_MinusLogProbMetric: 392.5225

Epoch 565: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.3444 - MinusLogProbMetric: 390.3444 - val_loss: 392.5225 - val_MinusLogProbMetric: 392.5225 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 566/1000
2023-10-03 23:40:19.219 
Epoch 566/1000 
	 loss: 390.1794, MinusLogProbMetric: 390.1794, val_loss: 393.3681, val_MinusLogProbMetric: 393.3681

Epoch 566: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.1794 - MinusLogProbMetric: 390.1794 - val_loss: 393.3681 - val_MinusLogProbMetric: 393.3681 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 567/1000
2023-10-03 23:40:26.572 
Epoch 567/1000 
	 loss: 390.3528, MinusLogProbMetric: 390.3528, val_loss: 392.6946, val_MinusLogProbMetric: 392.6946

Epoch 567: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.3528 - MinusLogProbMetric: 390.3528 - val_loss: 392.6946 - val_MinusLogProbMetric: 392.6946 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 568/1000
2023-10-03 23:40:33.903 
Epoch 568/1000 
	 loss: 390.2075, MinusLogProbMetric: 390.2075, val_loss: 393.0086, val_MinusLogProbMetric: 393.0086

Epoch 568: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2075 - MinusLogProbMetric: 390.2075 - val_loss: 393.0086 - val_MinusLogProbMetric: 393.0086 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 569/1000
2023-10-03 23:40:41.293 
Epoch 569/1000 
	 loss: 390.2618, MinusLogProbMetric: 390.2618, val_loss: 393.5187, val_MinusLogProbMetric: 393.5187

Epoch 569: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2618 - MinusLogProbMetric: 390.2618 - val_loss: 393.5187 - val_MinusLogProbMetric: 393.5187 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 570/1000
2023-10-03 23:40:48.593 
Epoch 570/1000 
	 loss: 390.2364, MinusLogProbMetric: 390.2364, val_loss: 392.4401, val_MinusLogProbMetric: 392.4401

Epoch 570: val_loss did not improve from 392.02521
196/196 - 7s - loss: 390.2364 - MinusLogProbMetric: 390.2364 - val_loss: 392.4401 - val_MinusLogProbMetric: 392.4401 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 571/1000
2023-10-03 23:40:56.027 
Epoch 571/1000 
	 loss: 390.6050, MinusLogProbMetric: 390.6050, val_loss: 391.9101, val_MinusLogProbMetric: 391.9101

Epoch 571: val_loss improved from 392.02521 to 391.91013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 390.6050 - MinusLogProbMetric: 390.6050 - val_loss: 391.9101 - val_MinusLogProbMetric: 391.9101 - lr: 1.6667e-04 - 8s/epoch - 39ms/step
Epoch 572/1000
2023-10-03 23:41:03.529 
Epoch 572/1000 
	 loss: 390.0692, MinusLogProbMetric: 390.0692, val_loss: 391.7186, val_MinusLogProbMetric: 391.7186

Epoch 572: val_loss improved from 391.91013 to 391.71857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 390.0692 - MinusLogProbMetric: 390.0692 - val_loss: 391.7186 - val_MinusLogProbMetric: 391.7186 - lr: 1.6667e-04 - 8s/epoch - 40ms/step
Epoch 573/1000
2023-10-03 23:41:11.502 
Epoch 573/1000 
	 loss: 390.2672, MinusLogProbMetric: 390.2672, val_loss: 392.6737, val_MinusLogProbMetric: 392.6737

Epoch 573: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.2672 - MinusLogProbMetric: 390.2672 - val_loss: 392.6737 - val_MinusLogProbMetric: 392.6737 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 574/1000
2023-10-03 23:41:18.661 
Epoch 574/1000 
	 loss: 390.0395, MinusLogProbMetric: 390.0395, val_loss: 392.1424, val_MinusLogProbMetric: 392.1424

Epoch 574: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0395 - MinusLogProbMetric: 390.0395 - val_loss: 392.1424 - val_MinusLogProbMetric: 392.1424 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 575/1000
2023-10-03 23:41:25.667 
Epoch 575/1000 
	 loss: 390.4398, MinusLogProbMetric: 390.4398, val_loss: 392.3576, val_MinusLogProbMetric: 392.3576

Epoch 575: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.4398 - MinusLogProbMetric: 390.4398 - val_loss: 392.3576 - val_MinusLogProbMetric: 392.3576 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 576/1000
2023-10-03 23:41:33.083 
Epoch 576/1000 
	 loss: 390.0950, MinusLogProbMetric: 390.0950, val_loss: 396.0014, val_MinusLogProbMetric: 396.0014

Epoch 576: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0950 - MinusLogProbMetric: 390.0950 - val_loss: 396.0014 - val_MinusLogProbMetric: 396.0014 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 577/1000
2023-10-03 23:41:40.421 
Epoch 577/1000 
	 loss: 390.5280, MinusLogProbMetric: 390.5280, val_loss: 392.9227, val_MinusLogProbMetric: 392.9227

Epoch 577: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.5280 - MinusLogProbMetric: 390.5280 - val_loss: 392.9227 - val_MinusLogProbMetric: 392.9227 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 578/1000
2023-10-03 23:41:47.703 
Epoch 578/1000 
	 loss: 390.2348, MinusLogProbMetric: 390.2348, val_loss: 392.5389, val_MinusLogProbMetric: 392.5389

Epoch 578: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.2348 - MinusLogProbMetric: 390.2348 - val_loss: 392.5389 - val_MinusLogProbMetric: 392.5389 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 579/1000
2023-10-03 23:41:55.038 
Epoch 579/1000 
	 loss: 389.8651, MinusLogProbMetric: 389.8651, val_loss: 398.4765, val_MinusLogProbMetric: 398.4765

Epoch 579: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.8651 - MinusLogProbMetric: 389.8651 - val_loss: 398.4765 - val_MinusLogProbMetric: 398.4765 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 580/1000
2023-10-03 23:42:02.305 
Epoch 580/1000 
	 loss: 393.3962, MinusLogProbMetric: 393.3962, val_loss: 392.3972, val_MinusLogProbMetric: 392.3972

Epoch 580: val_loss did not improve from 391.71857
196/196 - 7s - loss: 393.3962 - MinusLogProbMetric: 393.3962 - val_loss: 392.3972 - val_MinusLogProbMetric: 392.3972 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 581/1000
2023-10-03 23:42:09.698 
Epoch 581/1000 
	 loss: 389.8214, MinusLogProbMetric: 389.8214, val_loss: 393.4196, val_MinusLogProbMetric: 393.4196

Epoch 581: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.8214 - MinusLogProbMetric: 389.8214 - val_loss: 393.4196 - val_MinusLogProbMetric: 393.4196 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 582/1000
2023-10-03 23:42:17.070 
Epoch 582/1000 
	 loss: 389.8579, MinusLogProbMetric: 389.8579, val_loss: 392.5887, val_MinusLogProbMetric: 392.5887

Epoch 582: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.8579 - MinusLogProbMetric: 389.8579 - val_loss: 392.5887 - val_MinusLogProbMetric: 392.5887 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 583/1000
2023-10-03 23:42:24.066 
Epoch 583/1000 
	 loss: 390.0160, MinusLogProbMetric: 390.0160, val_loss: 393.1387, val_MinusLogProbMetric: 393.1387

Epoch 583: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0160 - MinusLogProbMetric: 390.0160 - val_loss: 393.1387 - val_MinusLogProbMetric: 393.1387 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 584/1000
2023-10-03 23:42:30.663 
Epoch 584/1000 
	 loss: 389.9231, MinusLogProbMetric: 389.9231, val_loss: 394.8944, val_MinusLogProbMetric: 394.8944

Epoch 584: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9231 - MinusLogProbMetric: 389.9231 - val_loss: 394.8944 - val_MinusLogProbMetric: 394.8944 - lr: 1.6667e-04 - 7s/epoch - 34ms/step
Epoch 585/1000
2023-10-03 23:42:37.735 
Epoch 585/1000 
	 loss: 389.9706, MinusLogProbMetric: 389.9706, val_loss: 392.6325, val_MinusLogProbMetric: 392.6325

Epoch 585: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9706 - MinusLogProbMetric: 389.9706 - val_loss: 392.6325 - val_MinusLogProbMetric: 392.6325 - lr: 1.6667e-04 - 7s/epoch - 36ms/step
Epoch 586/1000
2023-10-03 23:42:44.917 
Epoch 586/1000 
	 loss: 390.0772, MinusLogProbMetric: 390.0772, val_loss: 392.3351, val_MinusLogProbMetric: 392.3351

Epoch 586: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0772 - MinusLogProbMetric: 390.0772 - val_loss: 392.3351 - val_MinusLogProbMetric: 392.3351 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 587/1000
2023-10-03 23:42:52.151 
Epoch 587/1000 
	 loss: 390.2441, MinusLogProbMetric: 390.2441, val_loss: 392.5003, val_MinusLogProbMetric: 392.5003

Epoch 587: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.2441 - MinusLogProbMetric: 390.2441 - val_loss: 392.5003 - val_MinusLogProbMetric: 392.5003 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 588/1000
2023-10-03 23:42:59.512 
Epoch 588/1000 
	 loss: 389.7419, MinusLogProbMetric: 389.7419, val_loss: 392.1160, val_MinusLogProbMetric: 392.1160

Epoch 588: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7419 - MinusLogProbMetric: 389.7419 - val_loss: 392.1160 - val_MinusLogProbMetric: 392.1160 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 589/1000
2023-10-03 23:43:06.812 
Epoch 589/1000 
	 loss: 390.1165, MinusLogProbMetric: 390.1165, val_loss: 392.6516, val_MinusLogProbMetric: 392.6516

Epoch 589: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.1165 - MinusLogProbMetric: 390.1165 - val_loss: 392.6516 - val_MinusLogProbMetric: 392.6516 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 590/1000
2023-10-03 23:43:14.222 
Epoch 590/1000 
	 loss: 390.2185, MinusLogProbMetric: 390.2185, val_loss: 393.5008, val_MinusLogProbMetric: 393.5008

Epoch 590: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.2185 - MinusLogProbMetric: 390.2185 - val_loss: 393.5008 - val_MinusLogProbMetric: 393.5008 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 591/1000
2023-10-03 23:43:21.610 
Epoch 591/1000 
	 loss: 389.9141, MinusLogProbMetric: 389.9141, val_loss: 393.5345, val_MinusLogProbMetric: 393.5345

Epoch 591: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9141 - MinusLogProbMetric: 389.9141 - val_loss: 393.5345 - val_MinusLogProbMetric: 393.5345 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 592/1000
2023-10-03 23:43:28.855 
Epoch 592/1000 
	 loss: 389.9330, MinusLogProbMetric: 389.9330, val_loss: 392.5777, val_MinusLogProbMetric: 392.5777

Epoch 592: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9330 - MinusLogProbMetric: 389.9330 - val_loss: 392.5777 - val_MinusLogProbMetric: 392.5777 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 593/1000
2023-10-03 23:43:36.248 
Epoch 593/1000 
	 loss: 389.9440, MinusLogProbMetric: 389.9440, val_loss: 392.5244, val_MinusLogProbMetric: 392.5244

Epoch 593: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9440 - MinusLogProbMetric: 389.9440 - val_loss: 392.5244 - val_MinusLogProbMetric: 392.5244 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 594/1000
2023-10-03 23:43:43.617 
Epoch 594/1000 
	 loss: 390.7104, MinusLogProbMetric: 390.7104, val_loss: 393.1767, val_MinusLogProbMetric: 393.1767

Epoch 594: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.7104 - MinusLogProbMetric: 390.7104 - val_loss: 393.1767 - val_MinusLogProbMetric: 393.1767 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 595/1000
2023-10-03 23:43:50.900 
Epoch 595/1000 
	 loss: 389.8770, MinusLogProbMetric: 389.8770, val_loss: 392.4426, val_MinusLogProbMetric: 392.4426

Epoch 595: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.8770 - MinusLogProbMetric: 389.8770 - val_loss: 392.4426 - val_MinusLogProbMetric: 392.4426 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 596/1000
2023-10-03 23:43:58.388 
Epoch 596/1000 
	 loss: 389.9198, MinusLogProbMetric: 389.9198, val_loss: 393.1710, val_MinusLogProbMetric: 393.1710

Epoch 596: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9198 - MinusLogProbMetric: 389.9198 - val_loss: 393.1710 - val_MinusLogProbMetric: 393.1710 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 597/1000
2023-10-03 23:44:05.604 
Epoch 597/1000 
	 loss: 389.9225, MinusLogProbMetric: 389.9225, val_loss: 393.1083, val_MinusLogProbMetric: 393.1083

Epoch 597: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9225 - MinusLogProbMetric: 389.9225 - val_loss: 393.1083 - val_MinusLogProbMetric: 393.1083 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 598/1000
2023-10-03 23:44:12.927 
Epoch 598/1000 
	 loss: 390.0934, MinusLogProbMetric: 390.0934, val_loss: 393.5252, val_MinusLogProbMetric: 393.5252

Epoch 598: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0934 - MinusLogProbMetric: 390.0934 - val_loss: 393.5252 - val_MinusLogProbMetric: 393.5252 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 599/1000
2023-10-03 23:44:20.316 
Epoch 599/1000 
	 loss: 390.0978, MinusLogProbMetric: 390.0978, val_loss: 392.3524, val_MinusLogProbMetric: 392.3524

Epoch 599: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0978 - MinusLogProbMetric: 390.0978 - val_loss: 392.3524 - val_MinusLogProbMetric: 392.3524 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 600/1000
2023-10-03 23:44:27.604 
Epoch 600/1000 
	 loss: 390.3454, MinusLogProbMetric: 390.3454, val_loss: 394.7361, val_MinusLogProbMetric: 394.7361

Epoch 600: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.3454 - MinusLogProbMetric: 390.3454 - val_loss: 394.7361 - val_MinusLogProbMetric: 394.7361 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 601/1000
2023-10-03 23:44:34.936 
Epoch 601/1000 
	 loss: 389.9554, MinusLogProbMetric: 389.9554, val_loss: 394.4967, val_MinusLogProbMetric: 394.4967

Epoch 601: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9554 - MinusLogProbMetric: 389.9554 - val_loss: 394.4967 - val_MinusLogProbMetric: 394.4967 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 602/1000
2023-10-03 23:44:42.287 
Epoch 602/1000 
	 loss: 390.1273, MinusLogProbMetric: 390.1273, val_loss: 393.2077, val_MinusLogProbMetric: 393.2077

Epoch 602: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.1273 - MinusLogProbMetric: 390.1273 - val_loss: 393.2077 - val_MinusLogProbMetric: 393.2077 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 603/1000
2023-10-03 23:44:49.662 
Epoch 603/1000 
	 loss: 390.0168, MinusLogProbMetric: 390.0168, val_loss: 392.1786, val_MinusLogProbMetric: 392.1786

Epoch 603: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0168 - MinusLogProbMetric: 390.0168 - val_loss: 392.1786 - val_MinusLogProbMetric: 392.1786 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 604/1000
2023-10-03 23:44:57.060 
Epoch 604/1000 
	 loss: 389.9856, MinusLogProbMetric: 389.9856, val_loss: 392.3821, val_MinusLogProbMetric: 392.3821

Epoch 604: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9856 - MinusLogProbMetric: 389.9856 - val_loss: 392.3821 - val_MinusLogProbMetric: 392.3821 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 605/1000
2023-10-03 23:45:04.359 
Epoch 605/1000 
	 loss: 389.7356, MinusLogProbMetric: 389.7356, val_loss: 395.5190, val_MinusLogProbMetric: 395.5190

Epoch 605: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7356 - MinusLogProbMetric: 389.7356 - val_loss: 395.5190 - val_MinusLogProbMetric: 395.5190 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 606/1000
2023-10-03 23:45:11.647 
Epoch 606/1000 
	 loss: 390.1992, MinusLogProbMetric: 390.1992, val_loss: 392.6994, val_MinusLogProbMetric: 392.6994

Epoch 606: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.1992 - MinusLogProbMetric: 390.1992 - val_loss: 392.6994 - val_MinusLogProbMetric: 392.6994 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 607/1000
2023-10-03 23:45:18.835 
Epoch 607/1000 
	 loss: 389.8678, MinusLogProbMetric: 389.8678, val_loss: 392.3165, val_MinusLogProbMetric: 392.3165

Epoch 607: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.8678 - MinusLogProbMetric: 389.8678 - val_loss: 392.3165 - val_MinusLogProbMetric: 392.3165 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 608/1000
2023-10-03 23:45:26.182 
Epoch 608/1000 
	 loss: 390.1176, MinusLogProbMetric: 390.1176, val_loss: 392.9545, val_MinusLogProbMetric: 392.9545

Epoch 608: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.1176 - MinusLogProbMetric: 390.1176 - val_loss: 392.9545 - val_MinusLogProbMetric: 392.9545 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 609/1000
2023-10-03 23:45:33.544 
Epoch 609/1000 
	 loss: 389.8982, MinusLogProbMetric: 389.8982, val_loss: 392.7570, val_MinusLogProbMetric: 392.7570

Epoch 609: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.8982 - MinusLogProbMetric: 389.8982 - val_loss: 392.7570 - val_MinusLogProbMetric: 392.7570 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 610/1000
2023-10-03 23:45:40.934 
Epoch 610/1000 
	 loss: 389.7110, MinusLogProbMetric: 389.7110, val_loss: 394.4189, val_MinusLogProbMetric: 394.4189

Epoch 610: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7110 - MinusLogProbMetric: 389.7110 - val_loss: 394.4189 - val_MinusLogProbMetric: 394.4189 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 611/1000
2023-10-03 23:45:48.396 
Epoch 611/1000 
	 loss: 392.3176, MinusLogProbMetric: 392.3176, val_loss: 393.1367, val_MinusLogProbMetric: 393.1367

Epoch 611: val_loss did not improve from 391.71857
196/196 - 7s - loss: 392.3176 - MinusLogProbMetric: 392.3176 - val_loss: 393.1367 - val_MinusLogProbMetric: 393.1367 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 612/1000
2023-10-03 23:45:55.826 
Epoch 612/1000 
	 loss: 389.7257, MinusLogProbMetric: 389.7257, val_loss: 393.0807, val_MinusLogProbMetric: 393.0807

Epoch 612: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7257 - MinusLogProbMetric: 389.7257 - val_loss: 393.0807 - val_MinusLogProbMetric: 393.0807 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 613/1000
2023-10-03 23:46:03.131 
Epoch 613/1000 
	 loss: 389.7051, MinusLogProbMetric: 389.7051, val_loss: 393.0318, val_MinusLogProbMetric: 393.0318

Epoch 613: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7051 - MinusLogProbMetric: 389.7051 - val_loss: 393.0318 - val_MinusLogProbMetric: 393.0318 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 614/1000
2023-10-03 23:46:10.454 
Epoch 614/1000 
	 loss: 389.7773, MinusLogProbMetric: 389.7773, val_loss: 392.8158, val_MinusLogProbMetric: 392.8158

Epoch 614: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7773 - MinusLogProbMetric: 389.7773 - val_loss: 392.8158 - val_MinusLogProbMetric: 392.8158 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 615/1000
2023-10-03 23:46:17.832 
Epoch 615/1000 
	 loss: 390.0450, MinusLogProbMetric: 390.0450, val_loss: 392.6315, val_MinusLogProbMetric: 392.6315

Epoch 615: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0450 - MinusLogProbMetric: 390.0450 - val_loss: 392.6315 - val_MinusLogProbMetric: 392.6315 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 616/1000
2023-10-03 23:46:25.162 
Epoch 616/1000 
	 loss: 389.7697, MinusLogProbMetric: 389.7697, val_loss: 392.3888, val_MinusLogProbMetric: 392.3888

Epoch 616: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7697 - MinusLogProbMetric: 389.7697 - val_loss: 392.3888 - val_MinusLogProbMetric: 392.3888 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 617/1000
2023-10-03 23:46:32.353 
Epoch 617/1000 
	 loss: 389.9557, MinusLogProbMetric: 389.9557, val_loss: 393.3018, val_MinusLogProbMetric: 393.3018

Epoch 617: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.9557 - MinusLogProbMetric: 389.9557 - val_loss: 393.3018 - val_MinusLogProbMetric: 393.3018 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 618/1000
2023-10-03 23:46:39.778 
Epoch 618/1000 
	 loss: 390.1272, MinusLogProbMetric: 390.1272, val_loss: 394.6855, val_MinusLogProbMetric: 394.6855

Epoch 618: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.1272 - MinusLogProbMetric: 390.1272 - val_loss: 394.6855 - val_MinusLogProbMetric: 394.6855 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 619/1000
2023-10-03 23:46:47.095 
Epoch 619/1000 
	 loss: 390.0428, MinusLogProbMetric: 390.0428, val_loss: 393.1225, val_MinusLogProbMetric: 393.1225

Epoch 619: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0428 - MinusLogProbMetric: 390.0428 - val_loss: 393.1225 - val_MinusLogProbMetric: 393.1225 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 620/1000
2023-10-03 23:46:54.492 
Epoch 620/1000 
	 loss: 389.7484, MinusLogProbMetric: 389.7484, val_loss: 392.2849, val_MinusLogProbMetric: 392.2849

Epoch 620: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7484 - MinusLogProbMetric: 389.7484 - val_loss: 392.2849 - val_MinusLogProbMetric: 392.2849 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 621/1000
2023-10-03 23:47:01.897 
Epoch 621/1000 
	 loss: 390.0925, MinusLogProbMetric: 390.0925, val_loss: 393.0536, val_MinusLogProbMetric: 393.0536

Epoch 621: val_loss did not improve from 391.71857
196/196 - 7s - loss: 390.0925 - MinusLogProbMetric: 390.0925 - val_loss: 393.0536 - val_MinusLogProbMetric: 393.0536 - lr: 1.6667e-04 - 7s/epoch - 38ms/step
Epoch 622/1000
2023-10-03 23:47:09.234 
Epoch 622/1000 
	 loss: 389.7811, MinusLogProbMetric: 389.7811, val_loss: 394.2547, val_MinusLogProbMetric: 394.2547

Epoch 622: val_loss did not improve from 391.71857
196/196 - 7s - loss: 389.7811 - MinusLogProbMetric: 389.7811 - val_loss: 394.2547 - val_MinusLogProbMetric: 394.2547 - lr: 1.6667e-04 - 7s/epoch - 37ms/step
Epoch 623/1000
2023-10-03 23:47:16.586 
Epoch 623/1000 
	 loss: 388.3221, MinusLogProbMetric: 388.3221, val_loss: 391.1569, val_MinusLogProbMetric: 391.1569

Epoch 623: val_loss improved from 391.71857 to 391.15692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 388.3221 - MinusLogProbMetric: 388.3221 - val_loss: 391.1569 - val_MinusLogProbMetric: 391.1569 - lr: 8.3333e-05 - 8s/epoch - 39ms/step
Epoch 624/1000
2023-10-03 23:47:24.229 
Epoch 624/1000 
	 loss: 388.1362, MinusLogProbMetric: 388.1362, val_loss: 391.1644, val_MinusLogProbMetric: 391.1644

Epoch 624: val_loss did not improve from 391.15692
196/196 - 7s - loss: 388.1362 - MinusLogProbMetric: 388.1362 - val_loss: 391.1644 - val_MinusLogProbMetric: 391.1644 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 625/1000
2023-10-03 23:47:31.543 
Epoch 625/1000 
	 loss: 388.1275, MinusLogProbMetric: 388.1275, val_loss: 390.9158, val_MinusLogProbMetric: 390.9158

Epoch 625: val_loss improved from 391.15692 to 390.91577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 388.1275 - MinusLogProbMetric: 388.1275 - val_loss: 390.9158 - val_MinusLogProbMetric: 390.9158 - lr: 8.3333e-05 - 8s/epoch - 39ms/step
Epoch 626/1000
2023-10-03 23:47:39.121 
Epoch 626/1000 
	 loss: 388.1403, MinusLogProbMetric: 388.1403, val_loss: 390.9345, val_MinusLogProbMetric: 390.9345

Epoch 626: val_loss did not improve from 390.91577
196/196 - 7s - loss: 388.1403 - MinusLogProbMetric: 388.1403 - val_loss: 390.9345 - val_MinusLogProbMetric: 390.9345 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 627/1000
2023-10-03 23:47:46.379 
Epoch 627/1000 
	 loss: 388.2130, MinusLogProbMetric: 388.2130, val_loss: 390.9156, val_MinusLogProbMetric: 390.9156

Epoch 627: val_loss improved from 390.91577 to 390.91559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 388.2130 - MinusLogProbMetric: 388.2130 - val_loss: 390.9156 - val_MinusLogProbMetric: 390.9156 - lr: 8.3333e-05 - 8s/epoch - 38ms/step
Epoch 628/1000
2023-10-03 23:47:53.950 
Epoch 628/1000 
	 loss: 388.4502, MinusLogProbMetric: 388.4502, val_loss: 391.8582, val_MinusLogProbMetric: 391.8582

Epoch 628: val_loss did not improve from 390.91559
196/196 - 7s - loss: 388.4502 - MinusLogProbMetric: 388.4502 - val_loss: 391.8582 - val_MinusLogProbMetric: 391.8582 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 629/1000
2023-10-03 23:48:01.290 
Epoch 629/1000 
	 loss: 388.3333, MinusLogProbMetric: 388.3333, val_loss: 391.5851, val_MinusLogProbMetric: 391.5851

Epoch 629: val_loss did not improve from 390.91559
196/196 - 7s - loss: 388.3333 - MinusLogProbMetric: 388.3333 - val_loss: 391.5851 - val_MinusLogProbMetric: 391.5851 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 630/1000
2023-10-03 23:48:08.668 
Epoch 630/1000 
	 loss: 388.2261, MinusLogProbMetric: 388.2261, val_loss: 391.2209, val_MinusLogProbMetric: 391.2209

Epoch 630: val_loss did not improve from 390.91559
196/196 - 7s - loss: 388.2261 - MinusLogProbMetric: 388.2261 - val_loss: 391.2209 - val_MinusLogProbMetric: 391.2209 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 631/1000
2023-10-03 23:48:16.128 
Epoch 631/1000 
	 loss: 388.2460, MinusLogProbMetric: 388.2460, val_loss: 390.8604, val_MinusLogProbMetric: 390.8604

Epoch 631: val_loss improved from 390.91559 to 390.86044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 388.2460 - MinusLogProbMetric: 388.2460 - val_loss: 390.8604 - val_MinusLogProbMetric: 390.8604 - lr: 8.3333e-05 - 8s/epoch - 39ms/step
Epoch 632/1000
2023-10-03 23:48:23.740 
Epoch 632/1000 
	 loss: 388.1145, MinusLogProbMetric: 388.1145, val_loss: 391.3044, val_MinusLogProbMetric: 391.3044

Epoch 632: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1145 - MinusLogProbMetric: 388.1145 - val_loss: 391.3044 - val_MinusLogProbMetric: 391.3044 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 633/1000
2023-10-03 23:48:31.082 
Epoch 633/1000 
	 loss: 388.0748, MinusLogProbMetric: 388.0748, val_loss: 390.8659, val_MinusLogProbMetric: 390.8659

Epoch 633: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.0748 - MinusLogProbMetric: 388.0748 - val_loss: 390.8659 - val_MinusLogProbMetric: 390.8659 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 634/1000
2023-10-03 23:48:38.433 
Epoch 634/1000 
	 loss: 388.1451, MinusLogProbMetric: 388.1451, val_loss: 391.2179, val_MinusLogProbMetric: 391.2179

Epoch 634: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1451 - MinusLogProbMetric: 388.1451 - val_loss: 391.2179 - val_MinusLogProbMetric: 391.2179 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 635/1000
2023-10-03 23:48:45.774 
Epoch 635/1000 
	 loss: 388.2878, MinusLogProbMetric: 388.2878, val_loss: 391.2020, val_MinusLogProbMetric: 391.2020

Epoch 635: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.2878 - MinusLogProbMetric: 388.2878 - val_loss: 391.2020 - val_MinusLogProbMetric: 391.2020 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 636/1000
2023-10-03 23:48:53.151 
Epoch 636/1000 
	 loss: 388.1575, MinusLogProbMetric: 388.1575, val_loss: 391.8414, val_MinusLogProbMetric: 391.8414

Epoch 636: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1575 - MinusLogProbMetric: 388.1575 - val_loss: 391.8414 - val_MinusLogProbMetric: 391.8414 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 637/1000
2023-10-03 23:49:00.611 
Epoch 637/1000 
	 loss: 388.0794, MinusLogProbMetric: 388.0794, val_loss: 391.2693, val_MinusLogProbMetric: 391.2693

Epoch 637: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.0794 - MinusLogProbMetric: 388.0794 - val_loss: 391.2693 - val_MinusLogProbMetric: 391.2693 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 638/1000
2023-10-03 23:49:07.915 
Epoch 638/1000 
	 loss: 388.3669, MinusLogProbMetric: 388.3669, val_loss: 391.1135, val_MinusLogProbMetric: 391.1135

Epoch 638: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.3669 - MinusLogProbMetric: 388.3669 - val_loss: 391.1135 - val_MinusLogProbMetric: 391.1135 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 639/1000
2023-10-03 23:49:15.257 
Epoch 639/1000 
	 loss: 388.2522, MinusLogProbMetric: 388.2522, val_loss: 391.0871, val_MinusLogProbMetric: 391.0871

Epoch 639: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.2522 - MinusLogProbMetric: 388.2522 - val_loss: 391.0871 - val_MinusLogProbMetric: 391.0871 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 640/1000
2023-10-03 23:49:22.596 
Epoch 640/1000 
	 loss: 388.1404, MinusLogProbMetric: 388.1404, val_loss: 391.3266, val_MinusLogProbMetric: 391.3266

Epoch 640: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1404 - MinusLogProbMetric: 388.1404 - val_loss: 391.3266 - val_MinusLogProbMetric: 391.3266 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 641/1000
2023-10-03 23:49:30.071 
Epoch 641/1000 
	 loss: 388.3419, MinusLogProbMetric: 388.3419, val_loss: 391.5142, val_MinusLogProbMetric: 391.5142

Epoch 641: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.3419 - MinusLogProbMetric: 388.3419 - val_loss: 391.5142 - val_MinusLogProbMetric: 391.5142 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 642/1000
2023-10-03 23:49:37.513 
Epoch 642/1000 
	 loss: 388.3746, MinusLogProbMetric: 388.3746, val_loss: 393.4300, val_MinusLogProbMetric: 393.4300

Epoch 642: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.3746 - MinusLogProbMetric: 388.3746 - val_loss: 393.4300 - val_MinusLogProbMetric: 393.4300 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 643/1000
2023-10-03 23:49:44.982 
Epoch 643/1000 
	 loss: 388.2532, MinusLogProbMetric: 388.2532, val_loss: 391.6056, val_MinusLogProbMetric: 391.6056

Epoch 643: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.2532 - MinusLogProbMetric: 388.2532 - val_loss: 391.6056 - val_MinusLogProbMetric: 391.6056 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 644/1000
2023-10-03 23:49:52.471 
Epoch 644/1000 
	 loss: 388.1635, MinusLogProbMetric: 388.1635, val_loss: 391.4804, val_MinusLogProbMetric: 391.4804

Epoch 644: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1635 - MinusLogProbMetric: 388.1635 - val_loss: 391.4804 - val_MinusLogProbMetric: 391.4804 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 645/1000
2023-10-03 23:49:59.949 
Epoch 645/1000 
	 loss: 388.1735, MinusLogProbMetric: 388.1735, val_loss: 391.0714, val_MinusLogProbMetric: 391.0714

Epoch 645: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1735 - MinusLogProbMetric: 388.1735 - val_loss: 391.0714 - val_MinusLogProbMetric: 391.0714 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 646/1000
2023-10-03 23:50:07.380 
Epoch 646/1000 
	 loss: 388.3732, MinusLogProbMetric: 388.3732, val_loss: 392.2455, val_MinusLogProbMetric: 392.2455

Epoch 646: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.3732 - MinusLogProbMetric: 388.3732 - val_loss: 392.2455 - val_MinusLogProbMetric: 392.2455 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 647/1000
2023-10-03 23:50:14.737 
Epoch 647/1000 
	 loss: 388.2684, MinusLogProbMetric: 388.2684, val_loss: 395.1425, val_MinusLogProbMetric: 395.1425

Epoch 647: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.2684 - MinusLogProbMetric: 388.2684 - val_loss: 395.1425 - val_MinusLogProbMetric: 395.1425 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 648/1000
2023-10-03 23:50:22.140 
Epoch 648/1000 
	 loss: 388.5310, MinusLogProbMetric: 388.5310, val_loss: 393.0768, val_MinusLogProbMetric: 393.0768

Epoch 648: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.5310 - MinusLogProbMetric: 388.5310 - val_loss: 393.0768 - val_MinusLogProbMetric: 393.0768 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 649/1000
2023-10-03 23:50:29.357 
Epoch 649/1000 
	 loss: 388.5772, MinusLogProbMetric: 388.5772, val_loss: 391.4623, val_MinusLogProbMetric: 391.4623

Epoch 649: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.5772 - MinusLogProbMetric: 388.5772 - val_loss: 391.4623 - val_MinusLogProbMetric: 391.4623 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 650/1000
2023-10-03 23:50:36.682 
Epoch 650/1000 
	 loss: 388.3232, MinusLogProbMetric: 388.3232, val_loss: 391.0545, val_MinusLogProbMetric: 391.0545

Epoch 650: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.3232 - MinusLogProbMetric: 388.3232 - val_loss: 391.0545 - val_MinusLogProbMetric: 391.0545 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 651/1000
2023-10-03 23:50:44.151 
Epoch 651/1000 
	 loss: 388.1104, MinusLogProbMetric: 388.1104, val_loss: 391.0662, val_MinusLogProbMetric: 391.0662

Epoch 651: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1104 - MinusLogProbMetric: 388.1104 - val_loss: 391.0662 - val_MinusLogProbMetric: 391.0662 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 652/1000
2023-10-03 23:50:51.426 
Epoch 652/1000 
	 loss: 388.1240, MinusLogProbMetric: 388.1240, val_loss: 391.7421, val_MinusLogProbMetric: 391.7421

Epoch 652: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1240 - MinusLogProbMetric: 388.1240 - val_loss: 391.7421 - val_MinusLogProbMetric: 391.7421 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 653/1000
2023-10-03 23:50:58.853 
Epoch 653/1000 
	 loss: 388.1819, MinusLogProbMetric: 388.1819, val_loss: 391.6101, val_MinusLogProbMetric: 391.6101

Epoch 653: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.1819 - MinusLogProbMetric: 388.1819 - val_loss: 391.6101 - val_MinusLogProbMetric: 391.6101 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 654/1000
2023-10-03 23:51:06.211 
Epoch 654/1000 
	 loss: 388.0595, MinusLogProbMetric: 388.0595, val_loss: 391.8895, val_MinusLogProbMetric: 391.8895

Epoch 654: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.0595 - MinusLogProbMetric: 388.0595 - val_loss: 391.8895 - val_MinusLogProbMetric: 391.8895 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 655/1000
2023-10-03 23:51:13.469 
Epoch 655/1000 
	 loss: 388.4317, MinusLogProbMetric: 388.4317, val_loss: 391.4040, val_MinusLogProbMetric: 391.4040

Epoch 655: val_loss did not improve from 390.86044
196/196 - 7s - loss: 388.4317 - MinusLogProbMetric: 388.4317 - val_loss: 391.4040 - val_MinusLogProbMetric: 391.4040 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 656/1000
2023-10-03 23:51:20.744 
Epoch 656/1000 
	 loss: 388.2393, MinusLogProbMetric: 388.2393, val_loss: 390.7465, val_MinusLogProbMetric: 390.7465

Epoch 656: val_loss improved from 390.86044 to 390.74652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 388.2393 - MinusLogProbMetric: 388.2393 - val_loss: 390.7465 - val_MinusLogProbMetric: 390.7465 - lr: 8.3333e-05 - 8s/epoch - 38ms/step
Epoch 657/1000
2023-10-03 23:51:28.330 
Epoch 657/1000 
	 loss: 388.2094, MinusLogProbMetric: 388.2094, val_loss: 391.2238, val_MinusLogProbMetric: 391.2238

Epoch 657: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.2094 - MinusLogProbMetric: 388.2094 - val_loss: 391.2238 - val_MinusLogProbMetric: 391.2238 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 658/1000
2023-10-03 23:51:35.599 
Epoch 658/1000 
	 loss: 388.1076, MinusLogProbMetric: 388.1076, val_loss: 391.0185, val_MinusLogProbMetric: 391.0185

Epoch 658: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.1076 - MinusLogProbMetric: 388.1076 - val_loss: 391.0185 - val_MinusLogProbMetric: 391.0185 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 659/1000
2023-10-03 23:51:43.101 
Epoch 659/1000 
	 loss: 388.1690, MinusLogProbMetric: 388.1690, val_loss: 391.2846, val_MinusLogProbMetric: 391.2846

Epoch 659: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.1690 - MinusLogProbMetric: 388.1690 - val_loss: 391.2846 - val_MinusLogProbMetric: 391.2846 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 660/1000
2023-10-03 23:51:50.480 
Epoch 660/1000 
	 loss: 388.3214, MinusLogProbMetric: 388.3214, val_loss: 391.4490, val_MinusLogProbMetric: 391.4490

Epoch 660: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.3214 - MinusLogProbMetric: 388.3214 - val_loss: 391.4490 - val_MinusLogProbMetric: 391.4490 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 661/1000
2023-10-03 23:51:57.811 
Epoch 661/1000 
	 loss: 388.1798, MinusLogProbMetric: 388.1798, val_loss: 391.1152, val_MinusLogProbMetric: 391.1152

Epoch 661: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.1798 - MinusLogProbMetric: 388.1798 - val_loss: 391.1152 - val_MinusLogProbMetric: 391.1152 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 662/1000
2023-10-03 23:52:05.055 
Epoch 662/1000 
	 loss: 388.0612, MinusLogProbMetric: 388.0612, val_loss: 391.4892, val_MinusLogProbMetric: 391.4892

Epoch 662: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.0612 - MinusLogProbMetric: 388.0612 - val_loss: 391.4892 - val_MinusLogProbMetric: 391.4892 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 663/1000
2023-10-03 23:52:12.444 
Epoch 663/1000 
	 loss: 388.2595, MinusLogProbMetric: 388.2595, val_loss: 391.1045, val_MinusLogProbMetric: 391.1045

Epoch 663: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.2595 - MinusLogProbMetric: 388.2595 - val_loss: 391.1045 - val_MinusLogProbMetric: 391.1045 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 664/1000
2023-10-03 23:52:19.747 
Epoch 664/1000 
	 loss: 388.4294, MinusLogProbMetric: 388.4294, val_loss: 391.2455, val_MinusLogProbMetric: 391.2455

Epoch 664: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.4294 - MinusLogProbMetric: 388.4294 - val_loss: 391.2455 - val_MinusLogProbMetric: 391.2455 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 665/1000
2023-10-03 23:52:27.144 
Epoch 665/1000 
	 loss: 388.3069, MinusLogProbMetric: 388.3069, val_loss: 392.6336, val_MinusLogProbMetric: 392.6336

Epoch 665: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.3069 - MinusLogProbMetric: 388.3069 - val_loss: 392.6336 - val_MinusLogProbMetric: 392.6336 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 666/1000
2023-10-03 23:52:34.544 
Epoch 666/1000 
	 loss: 388.0945, MinusLogProbMetric: 388.0945, val_loss: 391.0866, val_MinusLogProbMetric: 391.0866

Epoch 666: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.0945 - MinusLogProbMetric: 388.0945 - val_loss: 391.0866 - val_MinusLogProbMetric: 391.0866 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 667/1000
2023-10-03 23:52:41.824 
Epoch 667/1000 
	 loss: 388.0750, MinusLogProbMetric: 388.0750, val_loss: 390.8422, val_MinusLogProbMetric: 390.8422

Epoch 667: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.0750 - MinusLogProbMetric: 388.0750 - val_loss: 390.8422 - val_MinusLogProbMetric: 390.8422 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 668/1000
2023-10-03 23:52:49.343 
Epoch 668/1000 
	 loss: 388.2578, MinusLogProbMetric: 388.2578, val_loss: 391.9972, val_MinusLogProbMetric: 391.9972

Epoch 668: val_loss did not improve from 390.74652
196/196 - 8s - loss: 388.2578 - MinusLogProbMetric: 388.2578 - val_loss: 391.9972 - val_MinusLogProbMetric: 391.9972 - lr: 8.3333e-05 - 8s/epoch - 38ms/step
Epoch 669/1000
2023-10-03 23:52:56.833 
Epoch 669/1000 
	 loss: 388.2802, MinusLogProbMetric: 388.2802, val_loss: 391.4424, val_MinusLogProbMetric: 391.4424

Epoch 669: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.2802 - MinusLogProbMetric: 388.2802 - val_loss: 391.4424 - val_MinusLogProbMetric: 391.4424 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 670/1000
2023-10-03 23:53:04.220 
Epoch 670/1000 
	 loss: 388.3128, MinusLogProbMetric: 388.3128, val_loss: 392.0718, val_MinusLogProbMetric: 392.0718

Epoch 670: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.3128 - MinusLogProbMetric: 388.3128 - val_loss: 392.0718 - val_MinusLogProbMetric: 392.0718 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 671/1000
2023-10-03 23:53:11.629 
Epoch 671/1000 
	 loss: 388.1125, MinusLogProbMetric: 388.1125, val_loss: 391.3290, val_MinusLogProbMetric: 391.3290

Epoch 671: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.1125 - MinusLogProbMetric: 388.1125 - val_loss: 391.3290 - val_MinusLogProbMetric: 391.3290 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 672/1000
2023-10-03 23:53:18.996 
Epoch 672/1000 
	 loss: 388.1272, MinusLogProbMetric: 388.1272, val_loss: 391.3242, val_MinusLogProbMetric: 391.3242

Epoch 672: val_loss did not improve from 390.74652
196/196 - 7s - loss: 388.1272 - MinusLogProbMetric: 388.1272 - val_loss: 391.3242 - val_MinusLogProbMetric: 391.3242 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 673/1000
2023-10-03 23:53:26.396 
Epoch 673/1000 
	 loss: 387.9711, MinusLogProbMetric: 387.9711, val_loss: 390.6522, val_MinusLogProbMetric: 390.6522

Epoch 673: val_loss improved from 390.74652 to 390.65216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 387.9711 - MinusLogProbMetric: 387.9711 - val_loss: 390.6522 - val_MinusLogProbMetric: 390.6522 - lr: 8.3333e-05 - 8s/epoch - 39ms/step
Epoch 674/1000
2023-10-03 23:53:34.098 
Epoch 674/1000 
	 loss: 388.0928, MinusLogProbMetric: 388.0928, val_loss: 391.1761, val_MinusLogProbMetric: 391.1761

Epoch 674: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0928 - MinusLogProbMetric: 388.0928 - val_loss: 391.1761 - val_MinusLogProbMetric: 391.1761 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 675/1000
2023-10-03 23:53:41.476 
Epoch 675/1000 
	 loss: 387.9241, MinusLogProbMetric: 387.9241, val_loss: 392.9176, val_MinusLogProbMetric: 392.9176

Epoch 675: val_loss did not improve from 390.65216
196/196 - 7s - loss: 387.9241 - MinusLogProbMetric: 387.9241 - val_loss: 392.9176 - val_MinusLogProbMetric: 392.9176 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 676/1000
2023-10-03 23:53:48.779 
Epoch 676/1000 
	 loss: 388.2666, MinusLogProbMetric: 388.2666, val_loss: 391.4408, val_MinusLogProbMetric: 391.4408

Epoch 676: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2666 - MinusLogProbMetric: 388.2666 - val_loss: 391.4408 - val_MinusLogProbMetric: 391.4408 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 677/1000
2023-10-03 23:53:56.119 
Epoch 677/1000 
	 loss: 388.4243, MinusLogProbMetric: 388.4243, val_loss: 391.0574, val_MinusLogProbMetric: 391.0574

Epoch 677: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.4243 - MinusLogProbMetric: 388.4243 - val_loss: 391.0574 - val_MinusLogProbMetric: 391.0574 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 678/1000
2023-10-03 23:54:03.445 
Epoch 678/1000 
	 loss: 388.0314, MinusLogProbMetric: 388.0314, val_loss: 391.4829, val_MinusLogProbMetric: 391.4829

Epoch 678: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0314 - MinusLogProbMetric: 388.0314 - val_loss: 391.4829 - val_MinusLogProbMetric: 391.4829 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 679/1000
2023-10-03 23:54:10.816 
Epoch 679/1000 
	 loss: 388.0413, MinusLogProbMetric: 388.0413, val_loss: 391.1745, val_MinusLogProbMetric: 391.1745

Epoch 679: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0413 - MinusLogProbMetric: 388.0413 - val_loss: 391.1745 - val_MinusLogProbMetric: 391.1745 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 680/1000
2023-10-03 23:54:18.236 
Epoch 680/1000 
	 loss: 388.1700, MinusLogProbMetric: 388.1700, val_loss: 391.3339, val_MinusLogProbMetric: 391.3339

Epoch 680: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1700 - MinusLogProbMetric: 388.1700 - val_loss: 391.3339 - val_MinusLogProbMetric: 391.3339 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 681/1000
2023-10-03 23:54:25.694 
Epoch 681/1000 
	 loss: 388.5124, MinusLogProbMetric: 388.5124, val_loss: 392.0314, val_MinusLogProbMetric: 392.0314

Epoch 681: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.5124 - MinusLogProbMetric: 388.5124 - val_loss: 392.0314 - val_MinusLogProbMetric: 392.0314 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 682/1000
2023-10-03 23:54:32.923 
Epoch 682/1000 
	 loss: 388.1064, MinusLogProbMetric: 388.1064, val_loss: 391.4702, val_MinusLogProbMetric: 391.4702

Epoch 682: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1064 - MinusLogProbMetric: 388.1064 - val_loss: 391.4702 - val_MinusLogProbMetric: 391.4702 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 683/1000
2023-10-03 23:54:40.266 
Epoch 683/1000 
	 loss: 388.1846, MinusLogProbMetric: 388.1846, val_loss: 390.9193, val_MinusLogProbMetric: 390.9193

Epoch 683: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1846 - MinusLogProbMetric: 388.1846 - val_loss: 390.9193 - val_MinusLogProbMetric: 390.9193 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 684/1000
2023-10-03 23:54:47.661 
Epoch 684/1000 
	 loss: 388.3346, MinusLogProbMetric: 388.3346, val_loss: 390.8165, val_MinusLogProbMetric: 390.8165

Epoch 684: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.3346 - MinusLogProbMetric: 388.3346 - val_loss: 390.8165 - val_MinusLogProbMetric: 390.8165 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 685/1000
2023-10-03 23:54:55.018 
Epoch 685/1000 
	 loss: 388.2155, MinusLogProbMetric: 388.2155, val_loss: 390.9990, val_MinusLogProbMetric: 390.9990

Epoch 685: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2155 - MinusLogProbMetric: 388.2155 - val_loss: 390.9990 - val_MinusLogProbMetric: 390.9990 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 686/1000
2023-10-03 23:55:02.343 
Epoch 686/1000 
	 loss: 388.1343, MinusLogProbMetric: 388.1343, val_loss: 391.0028, val_MinusLogProbMetric: 391.0028

Epoch 686: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1343 - MinusLogProbMetric: 388.1343 - val_loss: 391.0028 - val_MinusLogProbMetric: 391.0028 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 687/1000
2023-10-03 23:55:09.561 
Epoch 687/1000 
	 loss: 388.0145, MinusLogProbMetric: 388.0145, val_loss: 391.2603, val_MinusLogProbMetric: 391.2603

Epoch 687: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0145 - MinusLogProbMetric: 388.0145 - val_loss: 391.2603 - val_MinusLogProbMetric: 391.2603 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 688/1000
2023-10-03 23:55:16.378 
Epoch 688/1000 
	 loss: 388.0189, MinusLogProbMetric: 388.0189, val_loss: 391.6771, val_MinusLogProbMetric: 391.6771

Epoch 688: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0189 - MinusLogProbMetric: 388.0189 - val_loss: 391.6771 - val_MinusLogProbMetric: 391.6771 - lr: 8.3333e-05 - 7s/epoch - 35ms/step
Epoch 689/1000
2023-10-03 23:55:23.661 
Epoch 689/1000 
	 loss: 388.1532, MinusLogProbMetric: 388.1532, val_loss: 391.1804, val_MinusLogProbMetric: 391.1804

Epoch 689: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1532 - MinusLogProbMetric: 388.1532 - val_loss: 391.1804 - val_MinusLogProbMetric: 391.1804 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 690/1000
2023-10-03 23:55:31.014 
Epoch 690/1000 
	 loss: 388.1788, MinusLogProbMetric: 388.1788, val_loss: 391.5415, val_MinusLogProbMetric: 391.5415

Epoch 690: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1788 - MinusLogProbMetric: 388.1788 - val_loss: 391.5415 - val_MinusLogProbMetric: 391.5415 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 691/1000
2023-10-03 23:55:38.610 
Epoch 691/1000 
	 loss: 388.0865, MinusLogProbMetric: 388.0865, val_loss: 391.2058, val_MinusLogProbMetric: 391.2058

Epoch 691: val_loss did not improve from 390.65216
196/196 - 8s - loss: 388.0865 - MinusLogProbMetric: 388.0865 - val_loss: 391.2058 - val_MinusLogProbMetric: 391.2058 - lr: 8.3333e-05 - 8s/epoch - 39ms/step
Epoch 692/1000
2023-10-03 23:55:46.028 
Epoch 692/1000 
	 loss: 388.4241, MinusLogProbMetric: 388.4241, val_loss: 391.3217, val_MinusLogProbMetric: 391.3217

Epoch 692: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.4241 - MinusLogProbMetric: 388.4241 - val_loss: 391.3217 - val_MinusLogProbMetric: 391.3217 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 693/1000
2023-10-03 23:55:52.851 
Epoch 693/1000 
	 loss: 388.3060, MinusLogProbMetric: 388.3060, val_loss: 391.2254, val_MinusLogProbMetric: 391.2254

Epoch 693: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.3060 - MinusLogProbMetric: 388.3060 - val_loss: 391.2254 - val_MinusLogProbMetric: 391.2254 - lr: 8.3333e-05 - 7s/epoch - 35ms/step
Epoch 694/1000
2023-10-03 23:55:59.884 
Epoch 694/1000 
	 loss: 388.1060, MinusLogProbMetric: 388.1060, val_loss: 391.0830, val_MinusLogProbMetric: 391.0830

Epoch 694: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1060 - MinusLogProbMetric: 388.1060 - val_loss: 391.0830 - val_MinusLogProbMetric: 391.0830 - lr: 8.3333e-05 - 7s/epoch - 36ms/step
Epoch 695/1000
2023-10-03 23:56:07.080 
Epoch 695/1000 
	 loss: 388.0261, MinusLogProbMetric: 388.0261, val_loss: 391.3244, val_MinusLogProbMetric: 391.3244

Epoch 695: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0261 - MinusLogProbMetric: 388.0261 - val_loss: 391.3244 - val_MinusLogProbMetric: 391.3244 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 696/1000
2023-10-03 23:56:14.541 
Epoch 696/1000 
	 loss: 387.9715, MinusLogProbMetric: 387.9715, val_loss: 391.0155, val_MinusLogProbMetric: 391.0155

Epoch 696: val_loss did not improve from 390.65216
196/196 - 7s - loss: 387.9715 - MinusLogProbMetric: 387.9715 - val_loss: 391.0155 - val_MinusLogProbMetric: 391.0155 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 697/1000
2023-10-03 23:56:21.980 
Epoch 697/1000 
	 loss: 388.1234, MinusLogProbMetric: 388.1234, val_loss: 392.5953, val_MinusLogProbMetric: 392.5953

Epoch 697: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1234 - MinusLogProbMetric: 388.1234 - val_loss: 392.5953 - val_MinusLogProbMetric: 392.5953 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 698/1000
2023-10-03 23:56:29.381 
Epoch 698/1000 
	 loss: 388.4084, MinusLogProbMetric: 388.4084, val_loss: 391.2238, val_MinusLogProbMetric: 391.2238

Epoch 698: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.4084 - MinusLogProbMetric: 388.4084 - val_loss: 391.2238 - val_MinusLogProbMetric: 391.2238 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 699/1000
2023-10-03 23:56:36.806 
Epoch 699/1000 
	 loss: 388.1318, MinusLogProbMetric: 388.1318, val_loss: 391.4245, val_MinusLogProbMetric: 391.4245

Epoch 699: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1318 - MinusLogProbMetric: 388.1318 - val_loss: 391.4245 - val_MinusLogProbMetric: 391.4245 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 700/1000
2023-10-03 23:56:44.132 
Epoch 700/1000 
	 loss: 388.2164, MinusLogProbMetric: 388.2164, val_loss: 391.4103, val_MinusLogProbMetric: 391.4103

Epoch 700: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2164 - MinusLogProbMetric: 388.2164 - val_loss: 391.4103 - val_MinusLogProbMetric: 391.4103 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 701/1000
2023-10-03 23:56:51.478 
Epoch 701/1000 
	 loss: 388.2217, MinusLogProbMetric: 388.2217, val_loss: 391.0794, val_MinusLogProbMetric: 391.0794

Epoch 701: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2217 - MinusLogProbMetric: 388.2217 - val_loss: 391.0794 - val_MinusLogProbMetric: 391.0794 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 702/1000
2023-10-03 23:56:58.847 
Epoch 702/1000 
	 loss: 388.1650, MinusLogProbMetric: 388.1650, val_loss: 391.2942, val_MinusLogProbMetric: 391.2942

Epoch 702: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1650 - MinusLogProbMetric: 388.1650 - val_loss: 391.2942 - val_MinusLogProbMetric: 391.2942 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 703/1000
2023-10-03 23:57:06.232 
Epoch 703/1000 
	 loss: 388.0748, MinusLogProbMetric: 388.0748, val_loss: 391.0214, val_MinusLogProbMetric: 391.0214

Epoch 703: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0748 - MinusLogProbMetric: 388.0748 - val_loss: 391.0214 - val_MinusLogProbMetric: 391.0214 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 704/1000
2023-10-03 23:57:13.612 
Epoch 704/1000 
	 loss: 387.9738, MinusLogProbMetric: 387.9738, val_loss: 391.8629, val_MinusLogProbMetric: 391.8629

Epoch 704: val_loss did not improve from 390.65216
196/196 - 7s - loss: 387.9738 - MinusLogProbMetric: 387.9738 - val_loss: 391.8629 - val_MinusLogProbMetric: 391.8629 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 705/1000
2023-10-03 23:57:21.053 
Epoch 705/1000 
	 loss: 388.0433, MinusLogProbMetric: 388.0433, val_loss: 391.1789, val_MinusLogProbMetric: 391.1789

Epoch 705: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0433 - MinusLogProbMetric: 388.0433 - val_loss: 391.1789 - val_MinusLogProbMetric: 391.1789 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 706/1000
2023-10-03 23:57:28.590 
Epoch 706/1000 
	 loss: 387.9764, MinusLogProbMetric: 387.9764, val_loss: 391.1668, val_MinusLogProbMetric: 391.1668

Epoch 706: val_loss did not improve from 390.65216
196/196 - 8s - loss: 387.9764 - MinusLogProbMetric: 387.9764 - val_loss: 391.1668 - val_MinusLogProbMetric: 391.1668 - lr: 8.3333e-05 - 8s/epoch - 38ms/step
Epoch 707/1000
2023-10-03 23:57:36.277 
Epoch 707/1000 
	 loss: 388.0827, MinusLogProbMetric: 388.0827, val_loss: 391.6941, val_MinusLogProbMetric: 391.6941

Epoch 707: val_loss did not improve from 390.65216
196/196 - 8s - loss: 388.0827 - MinusLogProbMetric: 388.0827 - val_loss: 391.6941 - val_MinusLogProbMetric: 391.6941 - lr: 8.3333e-05 - 8s/epoch - 39ms/step
Epoch 708/1000
2023-10-03 23:57:43.566 
Epoch 708/1000 
	 loss: 388.2154, MinusLogProbMetric: 388.2154, val_loss: 391.8418, val_MinusLogProbMetric: 391.8418

Epoch 708: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2154 - MinusLogProbMetric: 388.2154 - val_loss: 391.8418 - val_MinusLogProbMetric: 391.8418 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 709/1000
2023-10-03 23:57:50.939 
Epoch 709/1000 
	 loss: 388.2086, MinusLogProbMetric: 388.2086, val_loss: 393.9254, val_MinusLogProbMetric: 393.9254

Epoch 709: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2086 - MinusLogProbMetric: 388.2086 - val_loss: 393.9254 - val_MinusLogProbMetric: 393.9254 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 710/1000
2023-10-03 23:57:57.864 
Epoch 710/1000 
	 loss: 388.0568, MinusLogProbMetric: 388.0568, val_loss: 391.2374, val_MinusLogProbMetric: 391.2374

Epoch 710: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0568 - MinusLogProbMetric: 388.0568 - val_loss: 391.2374 - val_MinusLogProbMetric: 391.2374 - lr: 8.3333e-05 - 7s/epoch - 35ms/step
Epoch 711/1000
2023-10-03 23:58:05.042 
Epoch 711/1000 
	 loss: 388.0781, MinusLogProbMetric: 388.0781, val_loss: 391.1854, val_MinusLogProbMetric: 391.1854

Epoch 711: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0781 - MinusLogProbMetric: 388.0781 - val_loss: 391.1854 - val_MinusLogProbMetric: 391.1854 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 712/1000
2023-10-03 23:58:12.500 
Epoch 712/1000 
	 loss: 388.2396, MinusLogProbMetric: 388.2396, val_loss: 391.0165, val_MinusLogProbMetric: 391.0165

Epoch 712: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2396 - MinusLogProbMetric: 388.2396 - val_loss: 391.0165 - val_MinusLogProbMetric: 391.0165 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 713/1000
2023-10-03 23:58:19.848 
Epoch 713/1000 
	 loss: 388.2772, MinusLogProbMetric: 388.2772, val_loss: 391.4940, val_MinusLogProbMetric: 391.4940

Epoch 713: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2772 - MinusLogProbMetric: 388.2772 - val_loss: 391.4940 - val_MinusLogProbMetric: 391.4940 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 714/1000
2023-10-03 23:58:27.158 
Epoch 714/1000 
	 loss: 388.1450, MinusLogProbMetric: 388.1450, val_loss: 391.0401, val_MinusLogProbMetric: 391.0401

Epoch 714: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1450 - MinusLogProbMetric: 388.1450 - val_loss: 391.0401 - val_MinusLogProbMetric: 391.0401 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 715/1000
2023-10-03 23:58:34.587 
Epoch 715/1000 
	 loss: 388.3617, MinusLogProbMetric: 388.3617, val_loss: 391.2184, val_MinusLogProbMetric: 391.2184

Epoch 715: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.3617 - MinusLogProbMetric: 388.3617 - val_loss: 391.2184 - val_MinusLogProbMetric: 391.2184 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 716/1000
2023-10-03 23:58:41.967 
Epoch 716/1000 
	 loss: 388.4529, MinusLogProbMetric: 388.4529, val_loss: 391.0356, val_MinusLogProbMetric: 391.0356

Epoch 716: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.4529 - MinusLogProbMetric: 388.4529 - val_loss: 391.0356 - val_MinusLogProbMetric: 391.0356 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 717/1000
2023-10-03 23:58:49.381 
Epoch 717/1000 
	 loss: 388.2892, MinusLogProbMetric: 388.2892, val_loss: 391.5822, val_MinusLogProbMetric: 391.5822

Epoch 717: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.2892 - MinusLogProbMetric: 388.2892 - val_loss: 391.5822 - val_MinusLogProbMetric: 391.5822 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 718/1000
2023-10-03 23:58:56.644 
Epoch 718/1000 
	 loss: 388.0710, MinusLogProbMetric: 388.0710, val_loss: 391.2933, val_MinusLogProbMetric: 391.2933

Epoch 718: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0710 - MinusLogProbMetric: 388.0710 - val_loss: 391.2933 - val_MinusLogProbMetric: 391.2933 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 719/1000
2023-10-03 23:59:04.013 
Epoch 719/1000 
	 loss: 387.9613, MinusLogProbMetric: 387.9613, val_loss: 390.7287, val_MinusLogProbMetric: 390.7287

Epoch 719: val_loss did not improve from 390.65216
196/196 - 7s - loss: 387.9613 - MinusLogProbMetric: 387.9613 - val_loss: 390.7287 - val_MinusLogProbMetric: 390.7287 - lr: 8.3333e-05 - 7s/epoch - 38ms/step
Epoch 720/1000
2023-10-03 23:59:11.346 
Epoch 720/1000 
	 loss: 388.1003, MinusLogProbMetric: 388.1003, val_loss: 390.9574, val_MinusLogProbMetric: 390.9574

Epoch 720: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.1003 - MinusLogProbMetric: 388.1003 - val_loss: 390.9574 - val_MinusLogProbMetric: 390.9574 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 721/1000
2023-10-03 23:59:18.664 
Epoch 721/1000 
	 loss: 388.0709, MinusLogProbMetric: 388.0709, val_loss: 391.3047, val_MinusLogProbMetric: 391.3047

Epoch 721: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0709 - MinusLogProbMetric: 388.0709 - val_loss: 391.3047 - val_MinusLogProbMetric: 391.3047 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 722/1000
2023-10-03 23:59:25.947 
Epoch 722/1000 
	 loss: 387.9633, MinusLogProbMetric: 387.9633, val_loss: 391.9756, val_MinusLogProbMetric: 391.9756

Epoch 722: val_loss did not improve from 390.65216
196/196 - 7s - loss: 387.9633 - MinusLogProbMetric: 387.9633 - val_loss: 391.9756 - val_MinusLogProbMetric: 391.9756 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 723/1000
2023-10-03 23:59:33.240 
Epoch 723/1000 
	 loss: 388.0975, MinusLogProbMetric: 388.0975, val_loss: 391.1475, val_MinusLogProbMetric: 391.1475

Epoch 723: val_loss did not improve from 390.65216
196/196 - 7s - loss: 388.0975 - MinusLogProbMetric: 388.0975 - val_loss: 391.1475 - val_MinusLogProbMetric: 391.1475 - lr: 8.3333e-05 - 7s/epoch - 37ms/step
Epoch 724/1000
2023-10-03 23:59:40.654 
Epoch 724/1000 
	 loss: 387.3226, MinusLogProbMetric: 387.3226, val_loss: 390.8120, val_MinusLogProbMetric: 390.8120

Epoch 724: val_loss did not improve from 390.65216
196/196 - 7s - loss: 387.3226 - MinusLogProbMetric: 387.3226 - val_loss: 390.8120 - val_MinusLogProbMetric: 390.8120 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 725/1000
2023-10-03 23:59:48.014 
Epoch 725/1000 
	 loss: 387.2881, MinusLogProbMetric: 387.2881, val_loss: 390.5887, val_MinusLogProbMetric: 390.5887

Epoch 725: val_loss improved from 390.65216 to 390.58865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 387.2881 - MinusLogProbMetric: 387.2881 - val_loss: 390.5887 - val_MinusLogProbMetric: 390.5887 - lr: 4.1667e-05 - 8s/epoch - 40ms/step
Epoch 726/1000
2023-10-03 23:59:55.786 
Epoch 726/1000 
	 loss: 387.3019, MinusLogProbMetric: 387.3019, val_loss: 390.3497, val_MinusLogProbMetric: 390.3497

Epoch 726: val_loss improved from 390.58865 to 390.34973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 387.3019 - MinusLogProbMetric: 387.3019 - val_loss: 390.3497 - val_MinusLogProbMetric: 390.3497 - lr: 4.1667e-05 - 8s/epoch - 39ms/step
Epoch 727/1000
2023-10-04 00:00:03.369 
Epoch 727/1000 
	 loss: 387.2648, MinusLogProbMetric: 387.2648, val_loss: 390.6790, val_MinusLogProbMetric: 390.6790

Epoch 727: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2648 - MinusLogProbMetric: 387.2648 - val_loss: 390.6790 - val_MinusLogProbMetric: 390.6790 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 728/1000
2023-10-04 00:00:10.778 
Epoch 728/1000 
	 loss: 387.2758, MinusLogProbMetric: 387.2758, val_loss: 390.5130, val_MinusLogProbMetric: 390.5130

Epoch 728: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2758 - MinusLogProbMetric: 387.2758 - val_loss: 390.5130 - val_MinusLogProbMetric: 390.5130 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 729/1000
2023-10-04 00:00:18.138 
Epoch 729/1000 
	 loss: 387.2359, MinusLogProbMetric: 387.2359, val_loss: 390.6432, val_MinusLogProbMetric: 390.6432

Epoch 729: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2359 - MinusLogProbMetric: 387.2359 - val_loss: 390.6432 - val_MinusLogProbMetric: 390.6432 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 730/1000
2023-10-04 00:00:25.592 
Epoch 730/1000 
	 loss: 387.2370, MinusLogProbMetric: 387.2370, val_loss: 390.5871, val_MinusLogProbMetric: 390.5871

Epoch 730: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2370 - MinusLogProbMetric: 387.2370 - val_loss: 390.5871 - val_MinusLogProbMetric: 390.5871 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 731/1000
2023-10-04 00:00:33.029 
Epoch 731/1000 
	 loss: 387.2551, MinusLogProbMetric: 387.2551, val_loss: 390.4319, val_MinusLogProbMetric: 390.4319

Epoch 731: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2551 - MinusLogProbMetric: 387.2551 - val_loss: 390.4319 - val_MinusLogProbMetric: 390.4319 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 732/1000
2023-10-04 00:00:40.446 
Epoch 732/1000 
	 loss: 387.2874, MinusLogProbMetric: 387.2874, val_loss: 391.0561, val_MinusLogProbMetric: 391.0561

Epoch 732: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2874 - MinusLogProbMetric: 387.2874 - val_loss: 391.0561 - val_MinusLogProbMetric: 391.0561 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 733/1000
2023-10-04 00:00:47.779 
Epoch 733/1000 
	 loss: 387.2872, MinusLogProbMetric: 387.2872, val_loss: 390.7588, val_MinusLogProbMetric: 390.7588

Epoch 733: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2872 - MinusLogProbMetric: 387.2872 - val_loss: 390.7588 - val_MinusLogProbMetric: 390.7588 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 734/1000
2023-10-04 00:00:55.107 
Epoch 734/1000 
	 loss: 387.3283, MinusLogProbMetric: 387.3283, val_loss: 390.3506, val_MinusLogProbMetric: 390.3506

Epoch 734: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3283 - MinusLogProbMetric: 387.3283 - val_loss: 390.3506 - val_MinusLogProbMetric: 390.3506 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 735/1000
2023-10-04 00:01:02.530 
Epoch 735/1000 
	 loss: 387.2513, MinusLogProbMetric: 387.2513, val_loss: 390.7425, val_MinusLogProbMetric: 390.7425

Epoch 735: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2513 - MinusLogProbMetric: 387.2513 - val_loss: 390.7425 - val_MinusLogProbMetric: 390.7425 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 736/1000
2023-10-04 00:01:09.826 
Epoch 736/1000 
	 loss: 387.2682, MinusLogProbMetric: 387.2682, val_loss: 390.9363, val_MinusLogProbMetric: 390.9363

Epoch 736: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2682 - MinusLogProbMetric: 387.2682 - val_loss: 390.9363 - val_MinusLogProbMetric: 390.9363 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 737/1000
2023-10-04 00:01:17.174 
Epoch 737/1000 
	 loss: 387.2695, MinusLogProbMetric: 387.2695, val_loss: 390.6983, val_MinusLogProbMetric: 390.6983

Epoch 737: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2695 - MinusLogProbMetric: 387.2695 - val_loss: 390.6983 - val_MinusLogProbMetric: 390.6983 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 738/1000
2023-10-04 00:01:24.536 
Epoch 738/1000 
	 loss: 387.2253, MinusLogProbMetric: 387.2253, val_loss: 390.5787, val_MinusLogProbMetric: 390.5787

Epoch 738: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2253 - MinusLogProbMetric: 387.2253 - val_loss: 390.5787 - val_MinusLogProbMetric: 390.5787 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 739/1000
2023-10-04 00:01:31.843 
Epoch 739/1000 
	 loss: 387.2239, MinusLogProbMetric: 387.2239, val_loss: 390.4886, val_MinusLogProbMetric: 390.4886

Epoch 739: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2239 - MinusLogProbMetric: 387.2239 - val_loss: 390.4886 - val_MinusLogProbMetric: 390.4886 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 740/1000
2023-10-04 00:01:39.145 
Epoch 740/1000 
	 loss: 387.2558, MinusLogProbMetric: 387.2558, val_loss: 390.4644, val_MinusLogProbMetric: 390.4644

Epoch 740: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2558 - MinusLogProbMetric: 387.2558 - val_loss: 390.4644 - val_MinusLogProbMetric: 390.4644 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 741/1000
2023-10-04 00:01:46.306 
Epoch 741/1000 
	 loss: 387.2512, MinusLogProbMetric: 387.2512, val_loss: 390.9369, val_MinusLogProbMetric: 390.9369

Epoch 741: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2512 - MinusLogProbMetric: 387.2512 - val_loss: 390.9369 - val_MinusLogProbMetric: 390.9369 - lr: 4.1667e-05 - 7s/epoch - 36ms/step
Epoch 742/1000
2023-10-04 00:01:53.522 
Epoch 742/1000 
	 loss: 387.2216, MinusLogProbMetric: 387.2216, val_loss: 390.5058, val_MinusLogProbMetric: 390.5058

Epoch 742: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2216 - MinusLogProbMetric: 387.2216 - val_loss: 390.5058 - val_MinusLogProbMetric: 390.5058 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 743/1000
2023-10-04 00:02:00.608 
Epoch 743/1000 
	 loss: 387.3135, MinusLogProbMetric: 387.3135, val_loss: 390.4908, val_MinusLogProbMetric: 390.4908

Epoch 743: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3135 - MinusLogProbMetric: 387.3135 - val_loss: 390.4908 - val_MinusLogProbMetric: 390.4908 - lr: 4.1667e-05 - 7s/epoch - 36ms/step
Epoch 744/1000
2023-10-04 00:02:07.646 
Epoch 744/1000 
	 loss: 387.2927, MinusLogProbMetric: 387.2927, val_loss: 390.7520, val_MinusLogProbMetric: 390.7520

Epoch 744: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2927 - MinusLogProbMetric: 387.2927 - val_loss: 390.7520 - val_MinusLogProbMetric: 390.7520 - lr: 4.1667e-05 - 7s/epoch - 36ms/step
Epoch 745/1000
2023-10-04 00:02:14.652 
Epoch 745/1000 
	 loss: 387.3425, MinusLogProbMetric: 387.3425, val_loss: 391.0022, val_MinusLogProbMetric: 391.0022

Epoch 745: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3425 - MinusLogProbMetric: 387.3425 - val_loss: 391.0022 - val_MinusLogProbMetric: 391.0022 - lr: 4.1667e-05 - 7s/epoch - 36ms/step
Epoch 746/1000
2023-10-04 00:02:21.734 
Epoch 746/1000 
	 loss: 387.2869, MinusLogProbMetric: 387.2869, val_loss: 390.7965, val_MinusLogProbMetric: 390.7965

Epoch 746: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2869 - MinusLogProbMetric: 387.2869 - val_loss: 390.7965 - val_MinusLogProbMetric: 390.7965 - lr: 4.1667e-05 - 7s/epoch - 36ms/step
Epoch 747/1000
2023-10-04 00:02:28.978 
Epoch 747/1000 
	 loss: 387.2493, MinusLogProbMetric: 387.2493, val_loss: 390.8442, val_MinusLogProbMetric: 390.8442

Epoch 747: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2493 - MinusLogProbMetric: 387.2493 - val_loss: 390.8442 - val_MinusLogProbMetric: 390.8442 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 748/1000
2023-10-04 00:02:36.278 
Epoch 748/1000 
	 loss: 387.2234, MinusLogProbMetric: 387.2234, val_loss: 390.3521, val_MinusLogProbMetric: 390.3521

Epoch 748: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2234 - MinusLogProbMetric: 387.2234 - val_loss: 390.3521 - val_MinusLogProbMetric: 390.3521 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 749/1000
2023-10-04 00:02:43.526 
Epoch 749/1000 
	 loss: 387.1805, MinusLogProbMetric: 387.1805, val_loss: 391.4242, val_MinusLogProbMetric: 391.4242

Epoch 749: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.1805 - MinusLogProbMetric: 387.1805 - val_loss: 391.4242 - val_MinusLogProbMetric: 391.4242 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 750/1000
2023-10-04 00:02:50.854 
Epoch 750/1000 
	 loss: 387.2213, MinusLogProbMetric: 387.2213, val_loss: 390.7724, val_MinusLogProbMetric: 390.7724

Epoch 750: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2213 - MinusLogProbMetric: 387.2213 - val_loss: 390.7724 - val_MinusLogProbMetric: 390.7724 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 751/1000
2023-10-04 00:02:58.235 
Epoch 751/1000 
	 loss: 387.1826, MinusLogProbMetric: 387.1826, val_loss: 390.8084, val_MinusLogProbMetric: 390.8084

Epoch 751: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.1826 - MinusLogProbMetric: 387.1826 - val_loss: 390.8084 - val_MinusLogProbMetric: 390.8084 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 752/1000
2023-10-04 00:03:05.500 
Epoch 752/1000 
	 loss: 387.2185, MinusLogProbMetric: 387.2185, val_loss: 390.5374, val_MinusLogProbMetric: 390.5374

Epoch 752: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2185 - MinusLogProbMetric: 387.2185 - val_loss: 390.5374 - val_MinusLogProbMetric: 390.5374 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 753/1000
2023-10-04 00:03:12.826 
Epoch 753/1000 
	 loss: 387.1775, MinusLogProbMetric: 387.1775, val_loss: 390.4770, val_MinusLogProbMetric: 390.4770

Epoch 753: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.1775 - MinusLogProbMetric: 387.1775 - val_loss: 390.4770 - val_MinusLogProbMetric: 390.4770 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 754/1000
2023-10-04 00:03:20.043 
Epoch 754/1000 
	 loss: 387.1919, MinusLogProbMetric: 387.1919, val_loss: 391.0619, val_MinusLogProbMetric: 391.0619

Epoch 754: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.1919 - MinusLogProbMetric: 387.1919 - val_loss: 391.0619 - val_MinusLogProbMetric: 391.0619 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 755/1000
2023-10-04 00:03:27.372 
Epoch 755/1000 
	 loss: 387.2621, MinusLogProbMetric: 387.2621, val_loss: 390.8765, val_MinusLogProbMetric: 390.8765

Epoch 755: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2621 - MinusLogProbMetric: 387.2621 - val_loss: 390.8765 - val_MinusLogProbMetric: 390.8765 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 756/1000
2023-10-04 00:03:34.717 
Epoch 756/1000 
	 loss: 387.2014, MinusLogProbMetric: 387.2014, val_loss: 390.6282, val_MinusLogProbMetric: 390.6282

Epoch 756: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2014 - MinusLogProbMetric: 387.2014 - val_loss: 390.6282 - val_MinusLogProbMetric: 390.6282 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 757/1000
2023-10-04 00:03:42.023 
Epoch 757/1000 
	 loss: 387.3040, MinusLogProbMetric: 387.3040, val_loss: 390.5229, val_MinusLogProbMetric: 390.5229

Epoch 757: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3040 - MinusLogProbMetric: 387.3040 - val_loss: 390.5229 - val_MinusLogProbMetric: 390.5229 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 758/1000
2023-10-04 00:03:49.227 
Epoch 758/1000 
	 loss: 387.3499, MinusLogProbMetric: 387.3499, val_loss: 391.1421, val_MinusLogProbMetric: 391.1421

Epoch 758: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3499 - MinusLogProbMetric: 387.3499 - val_loss: 391.1421 - val_MinusLogProbMetric: 391.1421 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 759/1000
2023-10-04 00:03:56.521 
Epoch 759/1000 
	 loss: 387.3525, MinusLogProbMetric: 387.3525, val_loss: 391.0922, val_MinusLogProbMetric: 391.0922

Epoch 759: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3525 - MinusLogProbMetric: 387.3525 - val_loss: 391.0922 - val_MinusLogProbMetric: 391.0922 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 760/1000
2023-10-04 00:04:03.471 
Epoch 760/1000 
	 loss: 387.2262, MinusLogProbMetric: 387.2262, val_loss: 390.9315, val_MinusLogProbMetric: 390.9315

Epoch 760: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2262 - MinusLogProbMetric: 387.2262 - val_loss: 390.9315 - val_MinusLogProbMetric: 390.9315 - lr: 4.1667e-05 - 7s/epoch - 35ms/step
Epoch 761/1000
2023-10-04 00:04:10.291 
Epoch 761/1000 
	 loss: 387.2708, MinusLogProbMetric: 387.2708, val_loss: 390.8009, val_MinusLogProbMetric: 390.8009

Epoch 761: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2708 - MinusLogProbMetric: 387.2708 - val_loss: 390.8009 - val_MinusLogProbMetric: 390.8009 - lr: 4.1667e-05 - 7s/epoch - 35ms/step
Epoch 762/1000
2023-10-04 00:04:17.642 
Epoch 762/1000 
	 loss: 387.2452, MinusLogProbMetric: 387.2452, val_loss: 390.6002, val_MinusLogProbMetric: 390.6002

Epoch 762: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2452 - MinusLogProbMetric: 387.2452 - val_loss: 390.6002 - val_MinusLogProbMetric: 390.6002 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 763/1000
2023-10-04 00:04:25.002 
Epoch 763/1000 
	 loss: 387.2483, MinusLogProbMetric: 387.2483, val_loss: 390.4485, val_MinusLogProbMetric: 390.4485

Epoch 763: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2483 - MinusLogProbMetric: 387.2483 - val_loss: 390.4485 - val_MinusLogProbMetric: 390.4485 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 764/1000
2023-10-04 00:04:32.345 
Epoch 764/1000 
	 loss: 387.2845, MinusLogProbMetric: 387.2845, val_loss: 391.0781, val_MinusLogProbMetric: 391.0781

Epoch 764: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2845 - MinusLogProbMetric: 387.2845 - val_loss: 391.0781 - val_MinusLogProbMetric: 391.0781 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 765/1000
2023-10-04 00:04:39.624 
Epoch 765/1000 
	 loss: 387.2085, MinusLogProbMetric: 387.2085, val_loss: 390.4007, val_MinusLogProbMetric: 390.4007

Epoch 765: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2085 - MinusLogProbMetric: 387.2085 - val_loss: 390.4007 - val_MinusLogProbMetric: 390.4007 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 766/1000
2023-10-04 00:04:47.064 
Epoch 766/1000 
	 loss: 387.3154, MinusLogProbMetric: 387.3154, val_loss: 390.9496, val_MinusLogProbMetric: 390.9496

Epoch 766: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3154 - MinusLogProbMetric: 387.3154 - val_loss: 390.9496 - val_MinusLogProbMetric: 390.9496 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 767/1000
2023-10-04 00:04:54.373 
Epoch 767/1000 
	 loss: 387.2561, MinusLogProbMetric: 387.2561, val_loss: 390.6495, val_MinusLogProbMetric: 390.6495

Epoch 767: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2561 - MinusLogProbMetric: 387.2561 - val_loss: 390.6495 - val_MinusLogProbMetric: 390.6495 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 768/1000
2023-10-04 00:05:01.703 
Epoch 768/1000 
	 loss: 387.1716, MinusLogProbMetric: 387.1716, val_loss: 390.6246, val_MinusLogProbMetric: 390.6246

Epoch 768: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.1716 - MinusLogProbMetric: 387.1716 - val_loss: 390.6246 - val_MinusLogProbMetric: 390.6246 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 769/1000
2023-10-04 00:05:09.016 
Epoch 769/1000 
	 loss: 387.1794, MinusLogProbMetric: 387.1794, val_loss: 390.7345, val_MinusLogProbMetric: 390.7345

Epoch 769: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.1794 - MinusLogProbMetric: 387.1794 - val_loss: 390.7345 - val_MinusLogProbMetric: 390.7345 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 770/1000
2023-10-04 00:05:16.416 
Epoch 770/1000 
	 loss: 387.2646, MinusLogProbMetric: 387.2646, val_loss: 391.4910, val_MinusLogProbMetric: 391.4910

Epoch 770: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2646 - MinusLogProbMetric: 387.2646 - val_loss: 391.4910 - val_MinusLogProbMetric: 391.4910 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 771/1000
2023-10-04 00:05:23.790 
Epoch 771/1000 
	 loss: 387.2887, MinusLogProbMetric: 387.2887, val_loss: 391.0430, val_MinusLogProbMetric: 391.0430

Epoch 771: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2887 - MinusLogProbMetric: 387.2887 - val_loss: 391.0430 - val_MinusLogProbMetric: 391.0430 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 772/1000
2023-10-04 00:05:31.127 
Epoch 772/1000 
	 loss: 387.2999, MinusLogProbMetric: 387.2999, val_loss: 390.9554, val_MinusLogProbMetric: 390.9554

Epoch 772: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.2999 - MinusLogProbMetric: 387.2999 - val_loss: 390.9554 - val_MinusLogProbMetric: 390.9554 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 773/1000
2023-10-04 00:05:38.556 
Epoch 773/1000 
	 loss: 387.3459, MinusLogProbMetric: 387.3459, val_loss: 391.3683, val_MinusLogProbMetric: 391.3683

Epoch 773: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3459 - MinusLogProbMetric: 387.3459 - val_loss: 391.3683 - val_MinusLogProbMetric: 391.3683 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 774/1000
2023-10-04 00:05:45.894 
Epoch 774/1000 
	 loss: 387.4341, MinusLogProbMetric: 387.4341, val_loss: 390.8293, val_MinusLogProbMetric: 390.8293

Epoch 774: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.4341 - MinusLogProbMetric: 387.4341 - val_loss: 390.8293 - val_MinusLogProbMetric: 390.8293 - lr: 4.1667e-05 - 7s/epoch - 37ms/step
Epoch 775/1000
2023-10-04 00:05:53.257 
Epoch 775/1000 
	 loss: 387.3217, MinusLogProbMetric: 387.3217, val_loss: 390.8543, val_MinusLogProbMetric: 390.8543

Epoch 775: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3217 - MinusLogProbMetric: 387.3217 - val_loss: 390.8543 - val_MinusLogProbMetric: 390.8543 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 776/1000
2023-10-04 00:06:00.686 
Epoch 776/1000 
	 loss: 387.3564, MinusLogProbMetric: 387.3564, val_loss: 391.4443, val_MinusLogProbMetric: 391.4443

Epoch 776: val_loss did not improve from 390.34973
196/196 - 7s - loss: 387.3564 - MinusLogProbMetric: 387.3564 - val_loss: 391.4443 - val_MinusLogProbMetric: 391.4443 - lr: 4.1667e-05 - 7s/epoch - 38ms/step
Epoch 777/1000
2023-10-04 00:06:08.001 
Epoch 777/1000 
	 loss: 386.9370, MinusLogProbMetric: 386.9370, val_loss: 390.4413, val_MinusLogProbMetric: 390.4413

Epoch 777: val_loss did not improve from 390.34973
196/196 - 7s - loss: 386.9370 - MinusLogProbMetric: 386.9370 - val_loss: 390.4413 - val_MinusLogProbMetric: 390.4413 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 778/1000
2023-10-04 00:06:15.388 
Epoch 778/1000 
	 loss: 386.9189, MinusLogProbMetric: 386.9189, val_loss: 390.5852, val_MinusLogProbMetric: 390.5852

Epoch 778: val_loss did not improve from 390.34973
196/196 - 7s - loss: 386.9189 - MinusLogProbMetric: 386.9189 - val_loss: 390.5852 - val_MinusLogProbMetric: 390.5852 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 779/1000
2023-10-04 00:06:22.752 
Epoch 779/1000 
	 loss: 386.9265, MinusLogProbMetric: 386.9265, val_loss: 390.5501, val_MinusLogProbMetric: 390.5501

Epoch 779: val_loss did not improve from 390.34973
196/196 - 7s - loss: 386.9265 - MinusLogProbMetric: 386.9265 - val_loss: 390.5501 - val_MinusLogProbMetric: 390.5501 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 780/1000
2023-10-04 00:06:30.121 
Epoch 780/1000 
	 loss: 386.9176, MinusLogProbMetric: 386.9176, val_loss: 390.4391, val_MinusLogProbMetric: 390.4391

Epoch 780: val_loss did not improve from 390.34973
196/196 - 7s - loss: 386.9176 - MinusLogProbMetric: 386.9176 - val_loss: 390.4391 - val_MinusLogProbMetric: 390.4391 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 781/1000
2023-10-04 00:06:37.411 
Epoch 781/1000 
	 loss: 386.9140, MinusLogProbMetric: 386.9140, val_loss: 390.3845, val_MinusLogProbMetric: 390.3845

Epoch 781: val_loss did not improve from 390.34973
196/196 - 7s - loss: 386.9140 - MinusLogProbMetric: 386.9140 - val_loss: 390.3845 - val_MinusLogProbMetric: 390.3845 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 782/1000
2023-10-04 00:06:44.721 
Epoch 782/1000 
	 loss: 386.9086, MinusLogProbMetric: 386.9086, val_loss: 390.5281, val_MinusLogProbMetric: 390.5281

Epoch 782: val_loss did not improve from 390.34973
196/196 - 7s - loss: 386.9086 - MinusLogProbMetric: 386.9086 - val_loss: 390.5281 - val_MinusLogProbMetric: 390.5281 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 783/1000
2023-10-04 00:06:52.005 
Epoch 783/1000 
	 loss: 386.9102, MinusLogProbMetric: 386.9102, val_loss: 390.2450, val_MinusLogProbMetric: 390.2450

Epoch 783: val_loss improved from 390.34973 to 390.24496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_354/weights/best_weights.h5
196/196 - 8s - loss: 386.9102 - MinusLogProbMetric: 386.9102 - val_loss: 390.2450 - val_MinusLogProbMetric: 390.2450 - lr: 2.0833e-05 - 8s/epoch - 40ms/step
Epoch 784/1000
2023-10-04 00:06:59.879 
Epoch 784/1000 
	 loss: 386.9324, MinusLogProbMetric: 386.9324, val_loss: 390.5732, val_MinusLogProbMetric: 390.5732

Epoch 784: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9324 - MinusLogProbMetric: 386.9324 - val_loss: 390.5732 - val_MinusLogProbMetric: 390.5732 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 785/1000
2023-10-04 00:07:07.158 
Epoch 785/1000 
	 loss: 386.9160, MinusLogProbMetric: 386.9160, val_loss: 390.6441, val_MinusLogProbMetric: 390.6441

Epoch 785: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9160 - MinusLogProbMetric: 386.9160 - val_loss: 390.6441 - val_MinusLogProbMetric: 390.6441 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 786/1000
2023-10-04 00:07:14.472 
Epoch 786/1000 
	 loss: 386.9145, MinusLogProbMetric: 386.9145, val_loss: 390.4899, val_MinusLogProbMetric: 390.4899

Epoch 786: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9145 - MinusLogProbMetric: 386.9145 - val_loss: 390.4899 - val_MinusLogProbMetric: 390.4899 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 787/1000
2023-10-04 00:07:21.766 
Epoch 787/1000 
	 loss: 386.9253, MinusLogProbMetric: 386.9253, val_loss: 390.6982, val_MinusLogProbMetric: 390.6982

Epoch 787: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9253 - MinusLogProbMetric: 386.9253 - val_loss: 390.6982 - val_MinusLogProbMetric: 390.6982 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 788/1000
2023-10-04 00:07:29.102 
Epoch 788/1000 
	 loss: 386.9244, MinusLogProbMetric: 386.9244, val_loss: 390.5620, val_MinusLogProbMetric: 390.5620

Epoch 788: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9244 - MinusLogProbMetric: 386.9244 - val_loss: 390.5620 - val_MinusLogProbMetric: 390.5620 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 789/1000
2023-10-04 00:07:36.301 
Epoch 789/1000 
	 loss: 386.9149, MinusLogProbMetric: 386.9149, val_loss: 390.5878, val_MinusLogProbMetric: 390.5878

Epoch 789: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9149 - MinusLogProbMetric: 386.9149 - val_loss: 390.5878 - val_MinusLogProbMetric: 390.5878 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 790/1000
2023-10-04 00:07:43.599 
Epoch 790/1000 
	 loss: 386.9081, MinusLogProbMetric: 386.9081, val_loss: 390.4829, val_MinusLogProbMetric: 390.4829

Epoch 790: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9081 - MinusLogProbMetric: 386.9081 - val_loss: 390.4829 - val_MinusLogProbMetric: 390.4829 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 791/1000
2023-10-04 00:07:50.906 
Epoch 791/1000 
	 loss: 386.9086, MinusLogProbMetric: 386.9086, val_loss: 390.3426, val_MinusLogProbMetric: 390.3426

Epoch 791: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9086 - MinusLogProbMetric: 386.9086 - val_loss: 390.3426 - val_MinusLogProbMetric: 390.3426 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 792/1000
2023-10-04 00:07:58.135 
Epoch 792/1000 
	 loss: 386.8958, MinusLogProbMetric: 386.8958, val_loss: 390.5148, val_MinusLogProbMetric: 390.5148

Epoch 792: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8958 - MinusLogProbMetric: 386.8958 - val_loss: 390.5148 - val_MinusLogProbMetric: 390.5148 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 793/1000
2023-10-04 00:08:05.343 
Epoch 793/1000 
	 loss: 386.8812, MinusLogProbMetric: 386.8812, val_loss: 390.5856, val_MinusLogProbMetric: 390.5856

Epoch 793: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8812 - MinusLogProbMetric: 386.8812 - val_loss: 390.5856 - val_MinusLogProbMetric: 390.5856 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 794/1000
2023-10-04 00:08:12.725 
Epoch 794/1000 
	 loss: 386.8862, MinusLogProbMetric: 386.8862, val_loss: 390.4156, val_MinusLogProbMetric: 390.4156

Epoch 794: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8862 - MinusLogProbMetric: 386.8862 - val_loss: 390.4156 - val_MinusLogProbMetric: 390.4156 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 795/1000
2023-10-04 00:08:20.199 
Epoch 795/1000 
	 loss: 386.8813, MinusLogProbMetric: 386.8813, val_loss: 390.5761, val_MinusLogProbMetric: 390.5761

Epoch 795: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8813 - MinusLogProbMetric: 386.8813 - val_loss: 390.5761 - val_MinusLogProbMetric: 390.5761 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 796/1000
2023-10-04 00:08:27.679 
Epoch 796/1000 
	 loss: 386.8812, MinusLogProbMetric: 386.8812, val_loss: 390.5246, val_MinusLogProbMetric: 390.5246

Epoch 796: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8812 - MinusLogProbMetric: 386.8812 - val_loss: 390.5246 - val_MinusLogProbMetric: 390.5246 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 797/1000
2023-10-04 00:08:35.210 
Epoch 797/1000 
	 loss: 386.9168, MinusLogProbMetric: 386.9168, val_loss: 390.6922, val_MinusLogProbMetric: 390.6922

Epoch 797: val_loss did not improve from 390.24496
196/196 - 8s - loss: 386.9168 - MinusLogProbMetric: 386.9168 - val_loss: 390.6922 - val_MinusLogProbMetric: 390.6922 - lr: 2.0833e-05 - 8s/epoch - 38ms/step
Epoch 798/1000
2023-10-04 00:08:42.642 
Epoch 798/1000 
	 loss: 386.9163, MinusLogProbMetric: 386.9163, val_loss: 390.7492, val_MinusLogProbMetric: 390.7492

Epoch 798: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9163 - MinusLogProbMetric: 386.9163 - val_loss: 390.7492 - val_MinusLogProbMetric: 390.7492 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 799/1000
2023-10-04 00:08:49.930 
Epoch 799/1000 
	 loss: 386.9465, MinusLogProbMetric: 386.9465, val_loss: 390.5310, val_MinusLogProbMetric: 390.5310

Epoch 799: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9465 - MinusLogProbMetric: 386.9465 - val_loss: 390.5310 - val_MinusLogProbMetric: 390.5310 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 800/1000
2023-10-04 00:08:57.350 
Epoch 800/1000 
	 loss: 386.8771, MinusLogProbMetric: 386.8771, val_loss: 390.4391, val_MinusLogProbMetric: 390.4391

Epoch 800: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8771 - MinusLogProbMetric: 386.8771 - val_loss: 390.4391 - val_MinusLogProbMetric: 390.4391 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 801/1000
2023-10-04 00:09:04.679 
Epoch 801/1000 
	 loss: 386.9146, MinusLogProbMetric: 386.9146, val_loss: 390.4223, val_MinusLogProbMetric: 390.4223

Epoch 801: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9146 - MinusLogProbMetric: 386.9146 - val_loss: 390.4223 - val_MinusLogProbMetric: 390.4223 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 802/1000
2023-10-04 00:09:12.019 
Epoch 802/1000 
	 loss: 386.8963, MinusLogProbMetric: 386.8963, val_loss: 390.4638, val_MinusLogProbMetric: 390.4638

Epoch 802: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8963 - MinusLogProbMetric: 386.8963 - val_loss: 390.4638 - val_MinusLogProbMetric: 390.4638 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 803/1000
2023-10-04 00:09:19.424 
Epoch 803/1000 
	 loss: 386.9215, MinusLogProbMetric: 386.9215, val_loss: 390.3529, val_MinusLogProbMetric: 390.3529

Epoch 803: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9215 - MinusLogProbMetric: 386.9215 - val_loss: 390.3529 - val_MinusLogProbMetric: 390.3529 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 804/1000
2023-10-04 00:09:26.833 
Epoch 804/1000 
	 loss: 386.9076, MinusLogProbMetric: 386.9076, val_loss: 390.4559, val_MinusLogProbMetric: 390.4559

Epoch 804: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9076 - MinusLogProbMetric: 386.9076 - val_loss: 390.4559 - val_MinusLogProbMetric: 390.4559 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 805/1000
2023-10-04 00:09:34.235 
Epoch 805/1000 
	 loss: 386.9053, MinusLogProbMetric: 386.9053, val_loss: 390.5676, val_MinusLogProbMetric: 390.5676

Epoch 805: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9053 - MinusLogProbMetric: 386.9053 - val_loss: 390.5676 - val_MinusLogProbMetric: 390.5676 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 806/1000
2023-10-04 00:09:41.700 
Epoch 806/1000 
	 loss: 386.8997, MinusLogProbMetric: 386.8997, val_loss: 390.5995, val_MinusLogProbMetric: 390.5995

Epoch 806: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8997 - MinusLogProbMetric: 386.8997 - val_loss: 390.5995 - val_MinusLogProbMetric: 390.5995 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 807/1000
2023-10-04 00:09:49.128 
Epoch 807/1000 
	 loss: 386.8789, MinusLogProbMetric: 386.8789, val_loss: 390.3821, val_MinusLogProbMetric: 390.3821

Epoch 807: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8789 - MinusLogProbMetric: 386.8789 - val_loss: 390.3821 - val_MinusLogProbMetric: 390.3821 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 808/1000
2023-10-04 00:09:56.509 
Epoch 808/1000 
	 loss: 386.8755, MinusLogProbMetric: 386.8755, val_loss: 390.6631, val_MinusLogProbMetric: 390.6631

Epoch 808: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8755 - MinusLogProbMetric: 386.8755 - val_loss: 390.6631 - val_MinusLogProbMetric: 390.6631 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 809/1000
2023-10-04 00:10:03.812 
Epoch 809/1000 
	 loss: 386.8672, MinusLogProbMetric: 386.8672, val_loss: 390.6578, val_MinusLogProbMetric: 390.6578

Epoch 809: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8672 - MinusLogProbMetric: 386.8672 - val_loss: 390.6578 - val_MinusLogProbMetric: 390.6578 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 810/1000
2023-10-04 00:10:11.018 
Epoch 810/1000 
	 loss: 386.8695, MinusLogProbMetric: 386.8695, val_loss: 390.5083, val_MinusLogProbMetric: 390.5083

Epoch 810: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8695 - MinusLogProbMetric: 386.8695 - val_loss: 390.5083 - val_MinusLogProbMetric: 390.5083 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 811/1000
2023-10-04 00:10:18.282 
Epoch 811/1000 
	 loss: 386.8759, MinusLogProbMetric: 386.8759, val_loss: 390.3604, val_MinusLogProbMetric: 390.3604

Epoch 811: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8759 - MinusLogProbMetric: 386.8759 - val_loss: 390.3604 - val_MinusLogProbMetric: 390.3604 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 812/1000
2023-10-04 00:10:25.681 
Epoch 812/1000 
	 loss: 386.8895, MinusLogProbMetric: 386.8895, val_loss: 390.3498, val_MinusLogProbMetric: 390.3498

Epoch 812: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8895 - MinusLogProbMetric: 386.8895 - val_loss: 390.3498 - val_MinusLogProbMetric: 390.3498 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 813/1000
2023-10-04 00:10:33.134 
Epoch 813/1000 
	 loss: 386.8810, MinusLogProbMetric: 386.8810, val_loss: 390.5662, val_MinusLogProbMetric: 390.5662

Epoch 813: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8810 - MinusLogProbMetric: 386.8810 - val_loss: 390.5662 - val_MinusLogProbMetric: 390.5662 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 814/1000
2023-10-04 00:10:40.524 
Epoch 814/1000 
	 loss: 386.8853, MinusLogProbMetric: 386.8853, val_loss: 390.4103, val_MinusLogProbMetric: 390.4103

Epoch 814: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8853 - MinusLogProbMetric: 386.8853 - val_loss: 390.4103 - val_MinusLogProbMetric: 390.4103 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 815/1000
2023-10-04 00:10:47.832 
Epoch 815/1000 
	 loss: 386.8642, MinusLogProbMetric: 386.8642, val_loss: 390.4871, val_MinusLogProbMetric: 390.4871

Epoch 815: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8642 - MinusLogProbMetric: 386.8642 - val_loss: 390.4871 - val_MinusLogProbMetric: 390.4871 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 816/1000
2023-10-04 00:10:55.137 
Epoch 816/1000 
	 loss: 386.8837, MinusLogProbMetric: 386.8837, val_loss: 390.5927, val_MinusLogProbMetric: 390.5927

Epoch 816: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8837 - MinusLogProbMetric: 386.8837 - val_loss: 390.5927 - val_MinusLogProbMetric: 390.5927 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 817/1000
2023-10-04 00:11:02.482 
Epoch 817/1000 
	 loss: 386.9063, MinusLogProbMetric: 386.9063, val_loss: 390.6213, val_MinusLogProbMetric: 390.6213

Epoch 817: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9063 - MinusLogProbMetric: 386.9063 - val_loss: 390.6213 - val_MinusLogProbMetric: 390.6213 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 818/1000
2023-10-04 00:11:09.941 
Epoch 818/1000 
	 loss: 386.9268, MinusLogProbMetric: 386.9268, val_loss: 390.2519, val_MinusLogProbMetric: 390.2519

Epoch 818: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9268 - MinusLogProbMetric: 386.9268 - val_loss: 390.2519 - val_MinusLogProbMetric: 390.2519 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 819/1000
2023-10-04 00:11:17.297 
Epoch 819/1000 
	 loss: 386.9075, MinusLogProbMetric: 386.9075, val_loss: 390.4133, val_MinusLogProbMetric: 390.4133

Epoch 819: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9075 - MinusLogProbMetric: 386.9075 - val_loss: 390.4133 - val_MinusLogProbMetric: 390.4133 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 820/1000
2023-10-04 00:11:24.733 
Epoch 820/1000 
	 loss: 386.8988, MinusLogProbMetric: 386.8988, val_loss: 390.5556, val_MinusLogProbMetric: 390.5556

Epoch 820: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8988 - MinusLogProbMetric: 386.8988 - val_loss: 390.5556 - val_MinusLogProbMetric: 390.5556 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 821/1000
2023-10-04 00:11:32.066 
Epoch 821/1000 
	 loss: 386.8631, MinusLogProbMetric: 386.8631, val_loss: 390.4528, val_MinusLogProbMetric: 390.4528

Epoch 821: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8631 - MinusLogProbMetric: 386.8631 - val_loss: 390.4528 - val_MinusLogProbMetric: 390.4528 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 822/1000
2023-10-04 00:11:39.085 
Epoch 822/1000 
	 loss: 386.8969, MinusLogProbMetric: 386.8969, val_loss: 390.7011, val_MinusLogProbMetric: 390.7011

Epoch 822: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8969 - MinusLogProbMetric: 386.8969 - val_loss: 390.7011 - val_MinusLogProbMetric: 390.7011 - lr: 2.0833e-05 - 7s/epoch - 36ms/step
Epoch 823/1000
2023-10-04 00:11:46.396 
Epoch 823/1000 
	 loss: 386.9642, MinusLogProbMetric: 386.9642, val_loss: 390.4060, val_MinusLogProbMetric: 390.4060

Epoch 823: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9642 - MinusLogProbMetric: 386.9642 - val_loss: 390.4060 - val_MinusLogProbMetric: 390.4060 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 824/1000
2023-10-04 00:11:53.742 
Epoch 824/1000 
	 loss: 386.8894, MinusLogProbMetric: 386.8894, val_loss: 390.5239, val_MinusLogProbMetric: 390.5239

Epoch 824: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8894 - MinusLogProbMetric: 386.8894 - val_loss: 390.5239 - val_MinusLogProbMetric: 390.5239 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 825/1000
2023-10-04 00:12:01.074 
Epoch 825/1000 
	 loss: 386.8844, MinusLogProbMetric: 386.8844, val_loss: 390.4312, val_MinusLogProbMetric: 390.4312

Epoch 825: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.8844 - MinusLogProbMetric: 386.8844 - val_loss: 390.4312 - val_MinusLogProbMetric: 390.4312 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 826/1000
2023-10-04 00:12:08.318 
Epoch 826/1000 
	 loss: 386.9217, MinusLogProbMetric: 386.9217, val_loss: 390.3268, val_MinusLogProbMetric: 390.3268

Epoch 826: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9217 - MinusLogProbMetric: 386.9217 - val_loss: 390.3268 - val_MinusLogProbMetric: 390.3268 - lr: 2.0833e-05 - 7s/epoch - 37ms/step
Epoch 827/1000
2023-10-04 00:12:15.260 
Epoch 827/1000 
	 loss: 386.9417, MinusLogProbMetric: 386.9417, val_loss: 390.9781, val_MinusLogProbMetric: 390.9781

Epoch 827: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9417 - MinusLogProbMetric: 386.9417 - val_loss: 390.9781 - val_MinusLogProbMetric: 390.9781 - lr: 2.0833e-05 - 7s/epoch - 35ms/step
Epoch 828/1000
2023-10-04 00:12:22.339 
Epoch 828/1000 
	 loss: 386.9251, MinusLogProbMetric: 386.9251, val_loss: 390.7176, val_MinusLogProbMetric: 390.7176

Epoch 828: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9251 - MinusLogProbMetric: 386.9251 - val_loss: 390.7176 - val_MinusLogProbMetric: 390.7176 - lr: 2.0833e-05 - 7s/epoch - 36ms/step
Epoch 829/1000
2023-10-04 00:12:29.700 
Epoch 829/1000 
	 loss: 386.9522, MinusLogProbMetric: 386.9522, val_loss: 390.2866, val_MinusLogProbMetric: 390.2866

Epoch 829: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9522 - MinusLogProbMetric: 386.9522 - val_loss: 390.2866 - val_MinusLogProbMetric: 390.2866 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 830/1000
2023-10-04 00:12:37.115 
Epoch 830/1000 
	 loss: 386.9537, MinusLogProbMetric: 386.9537, val_loss: 390.3638, val_MinusLogProbMetric: 390.3638

Epoch 830: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9537 - MinusLogProbMetric: 386.9537 - val_loss: 390.3638 - val_MinusLogProbMetric: 390.3638 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 831/1000
2023-10-04 00:12:44.566 
Epoch 831/1000 
	 loss: 386.9987, MinusLogProbMetric: 386.9987, val_loss: 390.4654, val_MinusLogProbMetric: 390.4654

Epoch 831: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9987 - MinusLogProbMetric: 386.9987 - val_loss: 390.4654 - val_MinusLogProbMetric: 390.4654 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 832/1000
2023-10-04 00:12:51.946 
Epoch 832/1000 
	 loss: 386.9507, MinusLogProbMetric: 386.9507, val_loss: 390.3972, val_MinusLogProbMetric: 390.3972

Epoch 832: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9507 - MinusLogProbMetric: 386.9507 - val_loss: 390.3972 - val_MinusLogProbMetric: 390.3972 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 833/1000
2023-10-04 00:12:59.363 
Epoch 833/1000 
	 loss: 386.9348, MinusLogProbMetric: 386.9348, val_loss: 390.5587, val_MinusLogProbMetric: 390.5587

Epoch 833: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.9348 - MinusLogProbMetric: 386.9348 - val_loss: 390.5587 - val_MinusLogProbMetric: 390.5587 - lr: 2.0833e-05 - 7s/epoch - 38ms/step
Epoch 834/1000
2023-10-04 00:13:06.615 
Epoch 834/1000 
	 loss: 386.7756, MinusLogProbMetric: 386.7756, val_loss: 390.2838, val_MinusLogProbMetric: 390.2838

Epoch 834: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7756 - MinusLogProbMetric: 386.7756 - val_loss: 390.2838 - val_MinusLogProbMetric: 390.2838 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 835/1000
2023-10-04 00:13:13.983 
Epoch 835/1000 
	 loss: 386.7774, MinusLogProbMetric: 386.7774, val_loss: 390.4627, val_MinusLogProbMetric: 390.4627

Epoch 835: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7774 - MinusLogProbMetric: 386.7774 - val_loss: 390.4627 - val_MinusLogProbMetric: 390.4627 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 836/1000
2023-10-04 00:13:21.305 
Epoch 836/1000 
	 loss: 386.7675, MinusLogProbMetric: 386.7675, val_loss: 390.4966, val_MinusLogProbMetric: 390.4966

Epoch 836: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7675 - MinusLogProbMetric: 386.7675 - val_loss: 390.4966 - val_MinusLogProbMetric: 390.4966 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 837/1000
2023-10-04 00:13:28.581 
Epoch 837/1000 
	 loss: 386.7701, MinusLogProbMetric: 386.7701, val_loss: 390.5123, val_MinusLogProbMetric: 390.5123

Epoch 837: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7701 - MinusLogProbMetric: 386.7701 - val_loss: 390.5123 - val_MinusLogProbMetric: 390.5123 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 838/1000
2023-10-04 00:13:35.900 
Epoch 838/1000 
	 loss: 386.7598, MinusLogProbMetric: 386.7598, val_loss: 390.4759, val_MinusLogProbMetric: 390.4759

Epoch 838: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7598 - MinusLogProbMetric: 386.7598 - val_loss: 390.4759 - val_MinusLogProbMetric: 390.4759 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 839/1000
2023-10-04 00:13:43.239 
Epoch 839/1000 
	 loss: 386.7574, MinusLogProbMetric: 386.7574, val_loss: 390.3243, val_MinusLogProbMetric: 390.3243

Epoch 839: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7574 - MinusLogProbMetric: 386.7574 - val_loss: 390.3243 - val_MinusLogProbMetric: 390.3243 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 840/1000
2023-10-04 00:13:50.601 
Epoch 840/1000 
	 loss: 386.7712, MinusLogProbMetric: 386.7712, val_loss: 390.5255, val_MinusLogProbMetric: 390.5255

Epoch 840: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7712 - MinusLogProbMetric: 386.7712 - val_loss: 390.5255 - val_MinusLogProbMetric: 390.5255 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 841/1000
2023-10-04 00:13:57.949 
Epoch 841/1000 
	 loss: 386.7569, MinusLogProbMetric: 386.7569, val_loss: 390.3188, val_MinusLogProbMetric: 390.3188

Epoch 841: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7569 - MinusLogProbMetric: 386.7569 - val_loss: 390.3188 - val_MinusLogProbMetric: 390.3188 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 842/1000
2023-10-04 00:14:05.274 
Epoch 842/1000 
	 loss: 386.7593, MinusLogProbMetric: 386.7593, val_loss: 390.4207, val_MinusLogProbMetric: 390.4207

Epoch 842: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7593 - MinusLogProbMetric: 386.7593 - val_loss: 390.4207 - val_MinusLogProbMetric: 390.4207 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 843/1000
2023-10-04 00:14:12.657 
Epoch 843/1000 
	 loss: 386.7670, MinusLogProbMetric: 386.7670, val_loss: 390.4234, val_MinusLogProbMetric: 390.4234

Epoch 843: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7670 - MinusLogProbMetric: 386.7670 - val_loss: 390.4234 - val_MinusLogProbMetric: 390.4234 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 844/1000
2023-10-04 00:14:19.956 
Epoch 844/1000 
	 loss: 386.7450, MinusLogProbMetric: 386.7450, val_loss: 390.3054, val_MinusLogProbMetric: 390.3054

Epoch 844: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7450 - MinusLogProbMetric: 386.7450 - val_loss: 390.3054 - val_MinusLogProbMetric: 390.3054 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 845/1000
2023-10-04 00:14:27.314 
Epoch 845/1000 
	 loss: 386.7515, MinusLogProbMetric: 386.7515, val_loss: 390.3547, val_MinusLogProbMetric: 390.3547

Epoch 845: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7515 - MinusLogProbMetric: 386.7515 - val_loss: 390.3547 - val_MinusLogProbMetric: 390.3547 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 846/1000
2023-10-04 00:14:34.569 
Epoch 846/1000 
	 loss: 386.7408, MinusLogProbMetric: 386.7408, val_loss: 390.3922, val_MinusLogProbMetric: 390.3922

Epoch 846: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7408 - MinusLogProbMetric: 386.7408 - val_loss: 390.3922 - val_MinusLogProbMetric: 390.3922 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 847/1000
2023-10-04 00:14:41.847 
Epoch 847/1000 
	 loss: 386.7464, MinusLogProbMetric: 386.7464, val_loss: 390.4217, val_MinusLogProbMetric: 390.4217

Epoch 847: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7464 - MinusLogProbMetric: 386.7464 - val_loss: 390.4217 - val_MinusLogProbMetric: 390.4217 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 848/1000
2023-10-04 00:14:49.071 
Epoch 848/1000 
	 loss: 386.7478, MinusLogProbMetric: 386.7478, val_loss: 390.5841, val_MinusLogProbMetric: 390.5841

Epoch 848: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7478 - MinusLogProbMetric: 386.7478 - val_loss: 390.5841 - val_MinusLogProbMetric: 390.5841 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 849/1000
2023-10-04 00:14:56.401 
Epoch 849/1000 
	 loss: 386.7396, MinusLogProbMetric: 386.7396, val_loss: 390.4161, val_MinusLogProbMetric: 390.4161

Epoch 849: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7396 - MinusLogProbMetric: 386.7396 - val_loss: 390.4161 - val_MinusLogProbMetric: 390.4161 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 850/1000
2023-10-04 00:15:03.669 
Epoch 850/1000 
	 loss: 386.7370, MinusLogProbMetric: 386.7370, val_loss: 390.4659, val_MinusLogProbMetric: 390.4659

Epoch 850: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7370 - MinusLogProbMetric: 386.7370 - val_loss: 390.4659 - val_MinusLogProbMetric: 390.4659 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 851/1000
2023-10-04 00:15:10.966 
Epoch 851/1000 
	 loss: 386.7420, MinusLogProbMetric: 386.7420, val_loss: 390.3972, val_MinusLogProbMetric: 390.3972

Epoch 851: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7420 - MinusLogProbMetric: 386.7420 - val_loss: 390.3972 - val_MinusLogProbMetric: 390.3972 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 852/1000
2023-10-04 00:15:18.243 
Epoch 852/1000 
	 loss: 386.7309, MinusLogProbMetric: 386.7309, val_loss: 390.3544, val_MinusLogProbMetric: 390.3544

Epoch 852: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7309 - MinusLogProbMetric: 386.7309 - val_loss: 390.3544 - val_MinusLogProbMetric: 390.3544 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 853/1000
2023-10-04 00:15:25.602 
Epoch 853/1000 
	 loss: 386.7261, MinusLogProbMetric: 386.7261, val_loss: 390.3506, val_MinusLogProbMetric: 390.3506

Epoch 853: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7261 - MinusLogProbMetric: 386.7261 - val_loss: 390.3506 - val_MinusLogProbMetric: 390.3506 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 854/1000
2023-10-04 00:15:32.849 
Epoch 854/1000 
	 loss: 386.7266, MinusLogProbMetric: 386.7266, val_loss: 390.4324, val_MinusLogProbMetric: 390.4324

Epoch 854: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7266 - MinusLogProbMetric: 386.7266 - val_loss: 390.4324 - val_MinusLogProbMetric: 390.4324 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 855/1000
2023-10-04 00:15:40.225 
Epoch 855/1000 
	 loss: 386.7272, MinusLogProbMetric: 386.7272, val_loss: 390.4078, val_MinusLogProbMetric: 390.4078

Epoch 855: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7272 - MinusLogProbMetric: 386.7272 - val_loss: 390.4078 - val_MinusLogProbMetric: 390.4078 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 856/1000
2023-10-04 00:15:47.502 
Epoch 856/1000 
	 loss: 386.7299, MinusLogProbMetric: 386.7299, val_loss: 390.5004, val_MinusLogProbMetric: 390.5004

Epoch 856: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7299 - MinusLogProbMetric: 386.7299 - val_loss: 390.5004 - val_MinusLogProbMetric: 390.5004 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 857/1000
2023-10-04 00:15:54.433 
Epoch 857/1000 
	 loss: 386.7272, MinusLogProbMetric: 386.7272, val_loss: 390.3768, val_MinusLogProbMetric: 390.3768

Epoch 857: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7272 - MinusLogProbMetric: 386.7272 - val_loss: 390.3768 - val_MinusLogProbMetric: 390.3768 - lr: 1.0417e-05 - 7s/epoch - 35ms/step
Epoch 858/1000
2023-10-04 00:16:01.502 
Epoch 858/1000 
	 loss: 386.7295, MinusLogProbMetric: 386.7295, val_loss: 390.5236, val_MinusLogProbMetric: 390.5236

Epoch 858: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7295 - MinusLogProbMetric: 386.7295 - val_loss: 390.5236 - val_MinusLogProbMetric: 390.5236 - lr: 1.0417e-05 - 7s/epoch - 36ms/step
Epoch 859/1000
2023-10-04 00:16:08.252 
Epoch 859/1000 
	 loss: 386.7428, MinusLogProbMetric: 386.7428, val_loss: 390.4723, val_MinusLogProbMetric: 390.4723

Epoch 859: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7428 - MinusLogProbMetric: 386.7428 - val_loss: 390.4723 - val_MinusLogProbMetric: 390.4723 - lr: 1.0417e-05 - 7s/epoch - 34ms/step
Epoch 860/1000
2023-10-04 00:16:14.934 
Epoch 860/1000 
	 loss: 386.7229, MinusLogProbMetric: 386.7229, val_loss: 390.4534, val_MinusLogProbMetric: 390.4534

Epoch 860: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7229 - MinusLogProbMetric: 386.7229 - val_loss: 390.4534 - val_MinusLogProbMetric: 390.4534 - lr: 1.0417e-05 - 7s/epoch - 34ms/step
Epoch 861/1000
2023-10-04 00:16:21.936 
Epoch 861/1000 
	 loss: 386.7313, MinusLogProbMetric: 386.7313, val_loss: 390.3702, val_MinusLogProbMetric: 390.3702

Epoch 861: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7313 - MinusLogProbMetric: 386.7313 - val_loss: 390.3702 - val_MinusLogProbMetric: 390.3702 - lr: 1.0417e-05 - 7s/epoch - 36ms/step
Epoch 862/1000
2023-10-04 00:16:29.380 
Epoch 862/1000 
	 loss: 386.7286, MinusLogProbMetric: 386.7286, val_loss: 390.4357, val_MinusLogProbMetric: 390.4357

Epoch 862: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7286 - MinusLogProbMetric: 386.7286 - val_loss: 390.4357 - val_MinusLogProbMetric: 390.4357 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 863/1000
2023-10-04 00:16:36.761 
Epoch 863/1000 
	 loss: 386.7416, MinusLogProbMetric: 386.7416, val_loss: 390.3993, val_MinusLogProbMetric: 390.3993

Epoch 863: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7416 - MinusLogProbMetric: 386.7416 - val_loss: 390.3993 - val_MinusLogProbMetric: 390.3993 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 864/1000
2023-10-04 00:16:44.044 
Epoch 864/1000 
	 loss: 386.7339, MinusLogProbMetric: 386.7339, val_loss: 390.4446, val_MinusLogProbMetric: 390.4446

Epoch 864: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7339 - MinusLogProbMetric: 386.7339 - val_loss: 390.4446 - val_MinusLogProbMetric: 390.4446 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 865/1000
2023-10-04 00:16:51.452 
Epoch 865/1000 
	 loss: 386.7268, MinusLogProbMetric: 386.7268, val_loss: 390.4319, val_MinusLogProbMetric: 390.4319

Epoch 865: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7268 - MinusLogProbMetric: 386.7268 - val_loss: 390.4319 - val_MinusLogProbMetric: 390.4319 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 866/1000
2023-10-04 00:16:58.806 
Epoch 866/1000 
	 loss: 386.7242, MinusLogProbMetric: 386.7242, val_loss: 390.4665, val_MinusLogProbMetric: 390.4665

Epoch 866: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7242 - MinusLogProbMetric: 386.7242 - val_loss: 390.4665 - val_MinusLogProbMetric: 390.4665 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 867/1000
2023-10-04 00:17:06.140 
Epoch 867/1000 
	 loss: 386.7276, MinusLogProbMetric: 386.7276, val_loss: 390.3815, val_MinusLogProbMetric: 390.3815

Epoch 867: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7276 - MinusLogProbMetric: 386.7276 - val_loss: 390.3815 - val_MinusLogProbMetric: 390.3815 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 868/1000
2023-10-04 00:17:13.486 
Epoch 868/1000 
	 loss: 386.7440, MinusLogProbMetric: 386.7440, val_loss: 390.5352, val_MinusLogProbMetric: 390.5352

Epoch 868: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7440 - MinusLogProbMetric: 386.7440 - val_loss: 390.5352 - val_MinusLogProbMetric: 390.5352 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 869/1000
2023-10-04 00:17:20.820 
Epoch 869/1000 
	 loss: 386.7360, MinusLogProbMetric: 386.7360, val_loss: 390.4664, val_MinusLogProbMetric: 390.4664

Epoch 869: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7360 - MinusLogProbMetric: 386.7360 - val_loss: 390.4664 - val_MinusLogProbMetric: 390.4664 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 870/1000
2023-10-04 00:17:28.198 
Epoch 870/1000 
	 loss: 386.7284, MinusLogProbMetric: 386.7284, val_loss: 390.4868, val_MinusLogProbMetric: 390.4868

Epoch 870: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7284 - MinusLogProbMetric: 386.7284 - val_loss: 390.4868 - val_MinusLogProbMetric: 390.4868 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 871/1000
2023-10-04 00:17:35.573 
Epoch 871/1000 
	 loss: 386.7306, MinusLogProbMetric: 386.7306, val_loss: 390.3719, val_MinusLogProbMetric: 390.3719

Epoch 871: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7306 - MinusLogProbMetric: 386.7306 - val_loss: 390.3719 - val_MinusLogProbMetric: 390.3719 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 872/1000
2023-10-04 00:17:42.875 
Epoch 872/1000 
	 loss: 386.7184, MinusLogProbMetric: 386.7184, val_loss: 390.3536, val_MinusLogProbMetric: 390.3536

Epoch 872: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7184 - MinusLogProbMetric: 386.7184 - val_loss: 390.3536 - val_MinusLogProbMetric: 390.3536 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 873/1000
2023-10-04 00:17:50.281 
Epoch 873/1000 
	 loss: 386.7227, MinusLogProbMetric: 386.7227, val_loss: 390.3557, val_MinusLogProbMetric: 390.3557

Epoch 873: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7227 - MinusLogProbMetric: 386.7227 - val_loss: 390.3557 - val_MinusLogProbMetric: 390.3557 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 874/1000
2023-10-04 00:17:57.687 
Epoch 874/1000 
	 loss: 386.7196, MinusLogProbMetric: 386.7196, val_loss: 390.4573, val_MinusLogProbMetric: 390.4573

Epoch 874: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7196 - MinusLogProbMetric: 386.7196 - val_loss: 390.4573 - val_MinusLogProbMetric: 390.4573 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 875/1000
2023-10-04 00:18:05.085 
Epoch 875/1000 
	 loss: 386.7215, MinusLogProbMetric: 386.7215, val_loss: 390.4656, val_MinusLogProbMetric: 390.4656

Epoch 875: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7215 - MinusLogProbMetric: 386.7215 - val_loss: 390.4656 - val_MinusLogProbMetric: 390.4656 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 876/1000
2023-10-04 00:18:12.365 
Epoch 876/1000 
	 loss: 386.7192, MinusLogProbMetric: 386.7192, val_loss: 390.4631, val_MinusLogProbMetric: 390.4631

Epoch 876: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7192 - MinusLogProbMetric: 386.7192 - val_loss: 390.4631 - val_MinusLogProbMetric: 390.4631 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 877/1000
2023-10-04 00:18:19.639 
Epoch 877/1000 
	 loss: 386.7146, MinusLogProbMetric: 386.7146, val_loss: 390.2658, val_MinusLogProbMetric: 390.2658

Epoch 877: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7146 - MinusLogProbMetric: 386.7146 - val_loss: 390.2658 - val_MinusLogProbMetric: 390.2658 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 878/1000
2023-10-04 00:18:27.001 
Epoch 878/1000 
	 loss: 386.7213, MinusLogProbMetric: 386.7213, val_loss: 390.4011, val_MinusLogProbMetric: 390.4011

Epoch 878: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7213 - MinusLogProbMetric: 386.7213 - val_loss: 390.4011 - val_MinusLogProbMetric: 390.4011 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 879/1000
2023-10-04 00:18:34.322 
Epoch 879/1000 
	 loss: 386.7191, MinusLogProbMetric: 386.7191, val_loss: 390.3805, val_MinusLogProbMetric: 390.3805

Epoch 879: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7191 - MinusLogProbMetric: 386.7191 - val_loss: 390.3805 - val_MinusLogProbMetric: 390.3805 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 880/1000
2023-10-04 00:18:41.644 
Epoch 880/1000 
	 loss: 386.7196, MinusLogProbMetric: 386.7196, val_loss: 390.4131, val_MinusLogProbMetric: 390.4131

Epoch 880: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7196 - MinusLogProbMetric: 386.7196 - val_loss: 390.4131 - val_MinusLogProbMetric: 390.4131 - lr: 1.0417e-05 - 7s/epoch - 37ms/step
Epoch 881/1000
2023-10-04 00:18:49.011 
Epoch 881/1000 
	 loss: 386.7122, MinusLogProbMetric: 386.7122, val_loss: 390.4094, val_MinusLogProbMetric: 390.4094

Epoch 881: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7122 - MinusLogProbMetric: 386.7122 - val_loss: 390.4094 - val_MinusLogProbMetric: 390.4094 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 882/1000
2023-10-04 00:18:56.403 
Epoch 882/1000 
	 loss: 386.7118, MinusLogProbMetric: 386.7118, val_loss: 390.3605, val_MinusLogProbMetric: 390.3605

Epoch 882: val_loss did not improve from 390.24496
196/196 - 7s - loss: 386.7118 - MinusLogProbMetric: 386.7118 - val_loss: 390.3605 - val_MinusLogProbMetric: 390.3605 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 883/1000
2023-10-04 00:19:03.682 
Epoch 883/1000 
	 loss: 386.7104, MinusLogProbMetric: 386.7104, val_loss: 390.3430, val_MinusLogProbMetric: 390.3430

Epoch 883: val_loss did not improve from 390.24496
Restoring model weights from the end of the best epoch: 783.
196/196 - 7s - loss: 386.7104 - MinusLogProbMetric: 386.7104 - val_loss: 390.3430 - val_MinusLogProbMetric: 390.3430 - lr: 1.0417e-05 - 7s/epoch - 38ms/step
Epoch 883: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 2582.1913698089775 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 2624.4038334429497 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 2719.1617827729788 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 2716.687597020995 seconds.
Training succeeded with seed 926.
Model trained in 6191.86 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 10670.86 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 10671.12 s.
===========
Run 354/360 done in 17042.56 s.
===========

Directory ../../results/MAFN_new/run_355/ already exists.
Skipping it.
===========
Run 355/360 already exists. Skipping it.
===========

===========
Generating train data for run 356.
===========
Train data generated in 0.73 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_356/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_356/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_356
self.data_kwargs: {'seed': 926}
self.x_data: [[7.8542657  4.4182515  5.3175983  ... 3.8539157  8.104709   6.7665896 ]
 [8.06671    5.087758   5.108798   ... 2.7856822  7.9267573  7.1754713 ]
 [5.9009757  0.26195854 4.63319    ... 5.096487   6.4324465  2.782943  ]
 ...
 [5.7334824  0.57263243 4.688254   ... 4.731608   6.577419   3.9972575 ]
 [5.8251185  0.91118604 4.8986015  ... 4.9331145  6.9940166  4.170423  ]
 [5.469847   7.5352907  5.8636546  ... 8.857828   2.8293529  6.7579055 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_15"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_146 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  9018400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7fb6ec5fe8f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb10e06ad0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb10e06ad0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb08799870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb10db2e60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb10db3340>, <keras.callbacks.ModelCheckpoint object at 0x7fbb10db3400>, <keras.callbacks.EarlyStopping object at 0x7fbb10db3670>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbb10db36a0>, <keras.callbacks.TerminateOnNaN object at 0x7fbb10db32e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_356/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 356/360 with hyperparameters:
timestamp = 2023-10-04 03:16:57.632877
ndims = 1000
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.85426569e+00  4.41825151e+00  5.31759834e+00  2.60887671e+00
  6.41178656e+00  2.13207030e+00  5.74208021e+00  1.84043932e+00
  1.69763756e+00  4.54294538e+00  3.11069012e+00  2.37418079e+00
  2.54046345e+00  1.62041679e-01  9.34055448e-02  2.05885458e+00
  5.24263716e+00  7.68947506e+00  4.56616545e+00  9.68113995e+00
  2.26570651e-01  1.77447128e+00  3.29641533e+00  6.17277622e+00
  8.20737648e+00  3.43033528e+00  2.55136061e+00  5.97014785e-01
  8.28674316e+00  3.98282480e+00  5.93197346e+00  1.06245613e+01
  9.49379206e-01  1.63838947e+00  7.06365585e+00  7.94115591e+00
  7.53974438e+00  5.61496639e+00  9.80136871e+00  3.15971422e+00
  6.21875811e+00 -9.54353809e-03  4.26025915e+00  7.39404678e+00
  2.21483946e+00  9.84185696e+00  7.24155045e+00  3.30283046e+00
  7.74830770e+00  9.08916414e-01  9.71360779e+00  6.58768594e-01
  8.35239983e+00  2.32051826e+00  7.30240631e+00 -8.83946955e-01
  3.03193045e+00  7.32728100e+00  3.74522591e+00  6.38918638e-01
  3.79207659e+00  4.00098944e+00  5.66429996e+00  5.85767746e+00
  8.47701550e+00  5.04558372e+00  3.84120035e+00  1.24103403e+00
  7.94665098e+00  7.07438660e+00  3.71784091e+00  6.12205219e+00
  6.66771460e+00  2.12367511e+00 -9.60745335e-01  3.74312615e+00
  5.94952774e+00  4.78941059e+00  6.61200619e+00  8.09659863e+00
  4.87202358e+00  2.70784044e+00  9.03696346e+00  5.63844013e+00
  7.53536034e+00  7.15711594e+00  9.41573143e+00  2.42348766e+00
  7.42718220e+00  3.36646557e+00  3.73524284e+00  1.32205629e+00
  3.59988236e+00  9.48313904e+00  1.93991113e+00  2.75920010e+00
  5.01511669e+00  3.30604935e+00  5.06413269e+00  7.73147202e+00
  2.30829501e+00  4.39429140e+00  6.82420444e+00  6.79297352e+00
  9.61048603e+00  6.80823135e+00  7.26028967e+00  9.56877518e+00
  3.79452395e+00  4.15086555e+00 -2.60416776e-01  1.66208482e+00
  3.06919241e+00  9.93813455e-01  7.33946943e+00  5.69343281e+00
  5.63299084e+00  7.84667826e+00  2.26034689e+00  3.43404484e+00
  7.00420618e+00  3.05020475e+00 -7.25471079e-02  3.36938071e+00
  9.09626770e+00  4.11609411e+00  2.85297346e+00  9.84171486e+00
  6.31026506e+00  4.01078224e+00  1.21615183e+00  7.14538717e+00
  6.33219719e+00  9.81515121e+00  2.49839687e+00  6.05599999e-01
  7.24084187e+00  2.75359464e+00  2.50772285e+00  4.09680367e+00
  8.30523300e+00  6.01748800e+00  4.66583490e-01  4.52141905e+00
  5.89312696e+00  5.51575184e+00  5.69468737e+00  1.91655934e+00
  8.48398876e+00  5.63048315e+00  7.34936619e+00  9.74814224e+00
  3.48965788e+00  8.88919735e+00  9.60582942e-02  1.66400623e+00
  5.99077606e+00  3.11164737e+00  2.75282717e+00  2.60087824e+00
  2.64426851e+00  7.44998503e+00  5.85604477e+00  7.84842968e+00
  8.28961468e+00  8.40444946e+00  4.12819386e+00  4.58752203e+00
  9.98953819e+00  6.43158293e+00  5.55915296e-01  5.72953415e+00
  4.68271637e+00  9.90587902e+00 -8.83717835e-01  5.94018364e+00
  7.98547173e+00  2.84753513e+00  9.44544220e+00  9.53206062e+00
  9.07542324e+00  6.41640186e-01  6.17845011e+00  3.03050613e+00
  8.23334789e+00  6.30862665e+00  5.43327713e+00  1.29359305e+00
  3.25533032e+00  6.31515408e+00  1.03074417e+01  5.75671196e+00
  9.49255085e+00  9.50892544e+00  6.79545212e+00  4.71281195e+00
  3.50111294e+00  9.18763638e+00  4.49652076e-01  1.14515615e+00
  2.40005207e+00  7.48550272e+00  3.02852464e+00  6.13900280e+00
  7.07990456e+00  4.02328634e+00  5.32941818e+00  5.56215000e+00
  9.59676170e+00  6.84960365e+00  6.14611101e+00  7.01535404e-01
  9.08370018e+00  5.82812595e+00  7.74594402e+00  6.01755202e-01
  3.96469641e+00  2.35852933e+00  2.45156860e+00  8.29704666e+00
  6.53238392e+00  4.18872261e+00 -6.58399761e-01  9.65485573e+00
  9.80379200e+00  1.98804641e+00 -2.41938114e-01  5.28435040e+00
  9.89102745e+00  8.85664177e+00  4.39452076e+00  3.62702298e+00
  2.62890053e+00  8.97751904e+00  6.36042309e+00  1.33491313e+00
  2.77312636e+00  2.77005053e+00  1.88997674e+00  5.14272547e+00
  8.35700703e+00  6.68333673e+00  1.91080523e+00 -7.04806745e-01
  1.96866465e+00  3.76307774e+00  7.95638275e+00  9.92790127e+00
  3.73964596e+00  7.23839140e+00  9.96518135e+00  7.50328255e+00
  6.60035801e+00  8.07895303e-01  3.62572813e+00  9.14682579e+00
  2.86480689e+00  8.59156322e+00  7.98565483e+00  4.01899433e+00
  4.55118990e+00  5.28791046e+00  7.33071327e+00  5.27818727e+00
  8.85012031e-01  7.58879042e+00  3.19855785e+00  9.85190296e+00
  2.27252841e+00  2.54628229e+00  2.68598413e+00  5.91966152e+00
  8.05706692e+00  1.65711355e+00  1.67708433e+00  6.08699369e+00
  4.28031015e+00  5.60096979e+00  6.40451479e+00  1.06371555e+01
  5.36539984e+00  6.85709620e+00  8.92924786e+00  3.32924414e+00
  2.92118073e+00  6.10644913e+00  5.32210112e+00  9.56133556e+00
  4.79739636e-01  9.31261921e+00  5.15149164e+00  1.02548325e+00
  2.37651587e+00  6.48900795e+00  4.59978104e+00  9.12724590e+00
  4.42423201e+00  4.07489395e+00  8.31999683e+00  5.35771799e+00
  6.72994852e+00  7.68310690e+00  6.69467068e+00  4.67777395e+00
  6.99015021e-01  4.14359522e+00  8.36619568e+00  3.67691922e+00
  9.04692936e+00  4.49189425e+00  6.32296133e+00  8.04836750e+00
  4.41118336e+00  8.30536187e-01  9.79877853e+00  2.71371508e+00
  5.12246799e+00  4.35286856e+00  5.49793339e+00  3.22211933e+00
  7.62125874e+00  1.31961989e+00  1.10699387e+01  8.59894085e+00
  5.24368238e+00  4.61356401e+00  3.45124292e+00  5.36580896e+00
  8.41000843e+00  2.77613783e+00  4.29664612e+00  1.90059173e+00
  3.43093586e+00  2.01634741e+00  1.07569265e+01  5.71786928e+00
  7.56941891e+00  1.09229112e+00  2.52849770e+00  8.91465664e+00
  9.52245593e-01  1.17926216e+00  6.83719635e+00  6.82608938e+00
  7.53233051e+00  1.01758318e+01  1.57473803e+00  4.45485210e+00
  4.28795290e+00  1.64892748e-01  1.81490600e+00  2.76026893e+00
  4.74909878e+00  3.11362815e+00  8.00123119e+00  3.19517183e+00
  9.50560093e+00  1.03588438e+01  4.30221748e+00  6.96166873e-01
  3.03467369e+00  7.79732800e+00  2.67352748e+00  3.50514221e+00
  5.08668947e+00  7.38722420e+00  9.63334274e+00  2.62670517e+00
  4.05484772e+00  2.31513214e+00  2.64107323e+00  2.18680382e-01
  4.29812479e+00  8.41921329e+00  4.19078636e+00  3.98049355e+00
  8.93853092e+00  7.08515024e+00  5.22109699e+00  7.64430141e+00
  2.03226995e+00  3.94330883e+00  1.36629438e+00  2.33601141e+00
  3.89655709e-01  3.61157084e+00  4.44274712e+00  9.43988323e+00
  2.50003672e+00  5.12204170e+00  8.95978451e+00  7.98188019e+00
  7.24716902e+00  2.88545752e+00  4.57083321e+00  2.54753900e+00
  1.84977734e+00  4.41161156e+00  7.37936544e+00  5.14107323e+00
  7.78119135e+00  8.40401840e+00  1.04103088e-02  5.93060875e+00
  5.76526022e+00  9.53269196e+00  7.67608523e-01  6.57992554e+00
  1.95678222e+00  5.49429357e-01  5.23544788e+00  7.59472132e+00
  7.29799318e+00 -1.20772636e+00  4.30894661e+00  4.65965390e-01
  2.60425591e+00  1.97165930e+00  1.86237991e+00  2.43441749e+00
  1.47088110e+00  9.78774834e+00  3.94406772e+00  8.90942097e+00
  1.22915721e+00  7.60721827e+00  8.06412756e-01  4.82014179e+00
  5.46107650e-01  7.98665380e+00  9.84689426e+00  5.38293898e-01
  8.54449463e+00  8.00841713e+00  3.39391780e+00  9.08403206e+00
  2.01289654e+00  4.27781343e+00  9.96862292e-01  6.87161112e+00
 -9.90051091e-01  4.79714537e+00  6.87879229e+00  8.84089375e+00
  2.07931614e+00  8.38031006e+00  5.88058186e+00  5.93655205e+00
  5.64989281e+00  6.21854544e-01  6.68466759e+00  4.26185799e+00
  8.59449577e+00  3.42455196e+00  5.77914000e+00  4.20325327e+00
  8.32858276e+00  2.54622722e+00  3.04043859e-01  8.33305359e+00
  4.29302406e+00  5.85027218e+00  3.44100654e-01  1.02574406e+01
  9.13591194e+00  6.44509363e+00  2.23200083e+00  3.65651631e+00
  9.79177284e+00  6.80626631e+00  1.36182153e+00  4.29894447e+00
  9.91862965e+00  6.15662336e+00  8.39294195e-02  5.73490715e+00
  3.36321139e+00  9.89277542e-01  4.06990290e+00  4.61008358e+00
 -7.16332495e-01  6.55716562e+00  5.19925833e+00  7.17309570e+00
  4.03727007e+00  5.57426548e+00  6.92208004e+00  8.72025681e+00
  3.84482765e+00  6.02073288e+00  7.86660480e+00 -5.73046148e-01
  7.67636967e+00  3.58273864e+00  7.47981644e+00  5.36535072e+00
  7.16784859e+00  7.85914564e+00  1.76731563e+00  6.72368050e+00
  8.51148510e+00  1.61090291e+00  6.08754253e+00  6.35778236e+00
  9.91369438e+00  2.55167031e+00  1.66099370e+00  3.64644337e+00
  2.42382288e+00  7.45638466e+00  7.33168173e+00  6.45663881e+00
  2.01684594e+00  8.60406494e+00  1.10975132e+01  5.38561678e+00
  5.44827414e+00  4.94278336e+00  1.86474085e+00  1.97823441e+00
  4.30962515e+00  1.54604924e+00  9.07867241e+00  7.96630621e+00
  5.26732063e+00  3.58175659e+00  9.51251125e+00  1.04863870e+00
  5.07879162e+00  1.05271053e+00  1.87499964e+00  6.65355206e+00
  6.66385412e+00  6.33094311e+00  8.71027946e+00  9.82982922e+00
  6.55875683e+00  5.20563507e+00  8.00854397e+00  2.41466331e+00
  7.00420952e+00  4.88513613e+00  1.61634290e+00  9.31071377e+00
  3.35792899e+00  5.86867237e+00  8.60531521e+00  5.60204506e+00
  1.54713285e+00  7.74277306e+00  3.66394973e+00  9.42229176e+00
  8.48596764e+00  6.48666763e+00  3.23076034e+00  7.68814230e+00
  9.35188389e+00  4.08446217e+00  5.02816010e+00  3.38081270e-01
  6.36648226e+00  8.51817703e+00  2.80446982e+00  3.20807362e+00
  4.51574850e+00  5.93633711e-01  9.46782529e-01  2.16139364e+00
  2.60937262e+00  5.28547049e+00  4.84176254e+00  9.81929684e+00
  7.88902140e+00  1.66553545e+00  8.82484341e+00  7.09167838e-01
  5.03990793e+00  2.42374206e+00  2.91808867e+00  7.50122595e+00
  4.18691397e+00  9.02093601e+00  7.10776806e+00  9.57430172e+00
  5.59825611e+00  8.02043629e+00  3.25235939e+00  4.47115898e+00
  4.30808973e+00  3.80606079e+00  1.12467217e+00  5.29313755e+00
  1.50810623e+00  8.85228539e+00 -4.06661093e-01  5.10796404e+00
  3.78489470e+00  7.05098629e-01  7.91552544e+00  6.63939571e+00
  1.46795654e+00  6.65862942e+00  8.33073235e+00  8.50840759e+00
  1.30628014e+00  6.92401600e+00  7.72493744e+00  8.51644993e+00
  5.43707657e+00  4.83005619e+00  8.90700626e+00  8.11882019e+00
  5.73599482e+00  2.80633116e+00  9.65988350e+00  4.51777983e+00
  9.21329618e-01  4.88313866e+00  9.77910519e+00  4.40435410e+00
 -3.27194422e-01  6.12769270e+00  9.89487934e+00  7.31696033e+00
  6.35276508e+00  9.55050945e+00  8.07680893e+00  6.34605026e+00
  4.77183819e+00  7.15873861e+00  8.06319714e+00  5.61736882e-01
  7.67201567e+00  3.85464573e+00  6.69436502e+00  7.86791897e+00
  4.83945560e+00  1.62425935e+00  4.19299459e+00  9.26880896e-01
  6.88019085e+00  9.74193478e+00  8.84968662e+00  3.65187788e+00
  6.33452368e+00  3.75078416e+00  4.67366219e+00  3.54604197e+00
  8.16938019e+00  2.72105742e+00  9.75844955e+00  6.57792234e+00
  2.28564978e-01  4.99803901e-01  1.02100105e+01  8.88365149e-01
  3.53124762e+00  1.72824395e+00  9.85693932e+00  5.20375729e+00
  5.34282923e+00  6.05618429e+00  2.67098927e+00  5.57222414e+00
  4.29868984e+00  1.93069673e+00  6.95643902e+00  3.56284547e+00
  7.73937511e+00  4.35557556e+00  5.20385551e+00  7.79240131e+00
  4.75181484e+00  9.50519943e+00  8.01542187e+00  8.55453110e+00
  9.82915688e+00  3.75745511e+00  5.48529482e+00  7.18655014e+00
  6.00739861e+00  1.01745186e+01  2.86505723e+00  3.40535784e+00
  8.88479424e+00  7.66841078e+00  2.33434486e+00  6.26787615e+00
  7.08852863e+00  2.91594267e+00  5.48495722e+00  5.74615335e+00
  4.40000677e+00  7.55737638e+00 -8.34494978e-02  4.98475075e+00
  3.07788324e+00  5.51033068e+00  7.23143101e+00  7.69217253e-01
  1.29760706e+00  6.85467720e+00  7.40828228e+00  5.57670832e+00
  9.81540394e+00  5.34229517e+00  6.78031504e-01  6.85619688e+00
  5.78618479e+00  5.76328468e+00  5.48964214e+00  8.51025105e+00
  1.36436164e+00  1.25749195e+00  1.53151178e+00  2.31167674e+00
  5.93736410e+00  6.02586079e+00  2.10990143e+00  3.74104166e+00
  4.49994564e+00  4.30085993e+00  8.55859375e+00  4.00177526e+00
  6.96290779e+00  1.65811288e+00  5.44697285e+00  8.24122715e+00
  1.78438163e+00  4.33162642e+00  5.78468227e+00  2.05096912e+00
  7.39810944e+00  1.59008896e+00  8.76768684e+00  4.69711876e+00
  4.04790258e+00  1.24405193e+00  2.83710051e+00  6.33698320e+00
  1.33927023e+00  3.15551066e+00  9.67255783e+00  6.40811014e+00
  2.06277108e+00  2.24199677e+00  8.81563950e+00  3.05780077e+00
  1.87078035e+00  9.16118813e+00  3.53771424e+00  1.65168977e+00
  5.19690895e+00  4.59403133e+00  9.05195808e+00  1.09238386e+00
  1.04556847e+00  8.68545175e-01  9.68726254e+00  5.04982138e+00
  2.69507694e+00  5.98866844e+00  4.93495703e+00  2.08913898e+00
  6.94658279e+00  2.16514921e+00  2.17784786e+00  5.89404345e+00
  2.47186089e+00  3.07393289e+00  9.32638359e+00  1.50767267e+00
  3.44327784e+00  3.86774182e+00  2.78355002e+00  2.11643291e+00
  8.07786083e+00  3.65396214e+00 -2.35649347e-01  5.44787359e+00
  8.69826198e-01  2.77893639e+00  9.38866138e+00  3.60090685e+00
  5.78489542e+00  5.85346889e+00  2.58838773e-01  6.52211618e+00
  9.64319706e+00  8.25448573e-01  5.58079672e+00  7.69353580e+00
  8.50068855e+00  1.93690157e+00  2.22951698e+00  6.91337490e+00
  3.25202256e-01  7.09594727e+00  8.97951889e+00  4.25646448e+00
  4.87320185e+00  3.06530523e+00  6.00203633e-01  1.48462224e+00
  8.84535408e+00  8.94914532e+00  1.44960713e+00  9.55700684e+00
  3.84997725e+00  2.66661191e+00  9.85832453e-01  9.06340885e+00
  9.97899628e+00  3.32584810e+00  6.95174551e+00  3.19924808e+00
 -1.51607394e-01  4.97948170e+00  5.40087318e+00  1.11839056e+00
  8.18654537e+00  5.18570328e+00  2.13246346e+00  6.35160780e+00
  8.03942680e+00  3.31687546e+00  8.01695156e+00  4.56763649e+00
  9.25021172e+00  7.31373024e+00  3.17991471e+00  1.24928892e-01
  5.86790705e+00  2.48283291e+00  1.13595665e+00  9.80613613e+00
  1.05967150e+01  2.69507051e+00  3.01809096e+00  1.64316809e+00
  4.14577913e+00  8.58723545e+00  6.37656307e+00  8.65282822e+00
  1.23624945e+00  6.83144760e+00  9.12307358e+00  2.77780533e+00
  6.90577936e+00  3.74633145e+00  2.03699780e+00  9.43050480e+00
  3.88212705e+00  1.03552914e+00  3.94930816e+00  3.95189595e+00
  7.43439579e+00  8.88795757e+00  7.81051493e+00  9.53783226e+00
  4.64317465e+00  7.03867435e+00  3.24562931e+00  9.52776432e-01
  3.66223240e+00  7.14639282e+00  3.33262825e+00  7.86202145e+00
  7.74663401e+00  1.78269148e+00  2.58091092e+00  8.37615490e+00
  3.35916471e+00  3.20344758e+00  3.59131765e+00  8.01384544e+00
 -2.43629217e-02  7.42378175e-01  5.56940269e+00  4.90014553e+00
  6.48054647e+00  3.32681608e+00  9.08345699e+00  7.95236826e+00
  6.34802222e-01  6.87756348e+00  9.49590397e+00  1.01780138e+01
  7.24633646e+00  1.21293211e+00  6.68064976e+00  3.20778751e+00
  1.00395985e+01  7.21336079e+00  6.02273321e+00  6.93427706e+00
  4.42568588e+00  4.00593805e+00  9.61300564e+00  3.34010863e+00
  5.83148003e-01  4.11973238e+00  8.45203018e+00  6.38991165e+00
  1.04141502e+01  7.16550827e-01  8.68817711e+00  4.16127014e+00
  3.57046032e+00  2.75189900e+00  1.17473054e+00  3.15659904e+00
  7.35806704e+00  4.37037677e-01  1.24299979e+00 -1.54236555e-02
  8.57396030e+00  5.91643810e+00  1.83133376e+00  7.77049971e+00
  1.06167221e+01  9.25513077e+00  9.58188915e+00 -5.68707407e-01
  5.92560768e+00  2.55895495e+00  5.55885506e+00  2.23880720e+00
  1.15490997e+00  5.06085348e+00  5.79838181e+00  1.02773275e+01
 -8.31137300e-02  3.83537984e+00  8.54463768e+00  5.31149960e+00
  8.12730598e+00  6.55586195e+00  7.92252541e+00  5.34070778e+00
  2.95670152e+00  8.34237576e+00  5.68467236e+00  1.28557813e+00
  8.03099155e+00  9.61913395e+00  4.18909931e+00  3.39216042e+00
  6.58131719e-01  6.45680571e+00  8.95029449e+00  1.41747653e+00
  4.75601006e+00  2.90228367e-01  5.12265801e-01  6.72031784e+00
  8.77206898e+00  3.71332788e+00  8.48167229e+00  1.21904695e+00
  1.87795568e+00  9.97635460e+00  4.99329329e+00  6.38564539e+00
  2.54908800e-01  1.29785419e+00  5.73886812e-01  8.37132168e+00
  1.09254289e+00  8.28661823e+00  3.38485265e+00  7.73170853e+00
  4.09941435e+00  2.49423814e+00  1.25597119e+00  2.96604967e+00
  3.78301477e+00  8.97054768e+00  4.46885729e+00  9.85267830e+00
  5.88528252e+00 -5.40345371e-01  4.29398537e+00  7.70487690e+00
  1.43975925e+00  8.53642845e+00  5.70689344e+00 -4.84625697e-02
  7.89377451e+00  5.98359823e+00  8.12033236e-01  4.98253584e+00
  3.64299822e+00  9.16150284e+00  2.32410598e+00  9.28726101e+00
  5.53273964e+00  3.85391569e+00  8.10470867e+00  6.76658964e+00]
Epoch 1/1000
2023-10-04 03:17:32.278 
Epoch 1/1000 
	 loss: 1571.4916, MinusLogProbMetric: 1571.4916, val_loss: 650.2950, val_MinusLogProbMetric: 650.2950

Epoch 1: val_loss improved from inf to 650.29498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 35s - loss: 1571.4916 - MinusLogProbMetric: 1571.4916 - val_loss: 650.2950 - val_MinusLogProbMetric: 650.2950 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 2/1000
2023-10-04 03:17:42.846 
Epoch 2/1000 
	 loss: 573.3627, MinusLogProbMetric: 573.3627, val_loss: 559.0053, val_MinusLogProbMetric: 559.0053

Epoch 2: val_loss improved from 650.29498 to 559.00531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 10s - loss: 573.3627 - MinusLogProbMetric: 573.3627 - val_loss: 559.0053 - val_MinusLogProbMetric: 559.0053 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 3/1000
2023-10-04 03:17:53.274 
Epoch 3/1000 
	 loss: 539.2190, MinusLogProbMetric: 539.2190, val_loss: 498.6347, val_MinusLogProbMetric: 498.6347

Epoch 3: val_loss improved from 559.00531 to 498.63470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 539.2190 - MinusLogProbMetric: 539.2190 - val_loss: 498.6347 - val_MinusLogProbMetric: 498.6347 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 4/1000
2023-10-04 03:18:03.897 
Epoch 4/1000 
	 loss: 512.2459, MinusLogProbMetric: 512.2459, val_loss: 511.7671, val_MinusLogProbMetric: 511.7671

Epoch 4: val_loss did not improve from 498.63470
196/196 - 10s - loss: 512.2459 - MinusLogProbMetric: 512.2459 - val_loss: 511.7671 - val_MinusLogProbMetric: 511.7671 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 5/1000
2023-10-04 03:18:14.221 
Epoch 5/1000 
	 loss: 492.5705, MinusLogProbMetric: 492.5705, val_loss: 486.2845, val_MinusLogProbMetric: 486.2845

Epoch 5: val_loss improved from 498.63470 to 486.28445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 492.5705 - MinusLogProbMetric: 492.5705 - val_loss: 486.2845 - val_MinusLogProbMetric: 486.2845 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 6/1000
2023-10-04 03:18:25.125 
Epoch 6/1000 
	 loss: 480.9624, MinusLogProbMetric: 480.9624, val_loss: 474.3613, val_MinusLogProbMetric: 474.3613

Epoch 6: val_loss improved from 486.28445 to 474.36127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 480.9624 - MinusLogProbMetric: 480.9624 - val_loss: 474.3613 - val_MinusLogProbMetric: 474.3613 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 7/1000
2023-10-04 03:18:35.870 
Epoch 7/1000 
	 loss: 479.2541, MinusLogProbMetric: 479.2541, val_loss: 469.7798, val_MinusLogProbMetric: 469.7798

Epoch 7: val_loss improved from 474.36127 to 469.77985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 479.2541 - MinusLogProbMetric: 479.2541 - val_loss: 469.7798 - val_MinusLogProbMetric: 469.7798 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 8/1000
2023-10-04 03:18:46.591 
Epoch 8/1000 
	 loss: 463.8084, MinusLogProbMetric: 463.8084, val_loss: 458.7510, val_MinusLogProbMetric: 458.7510

Epoch 8: val_loss improved from 469.77985 to 458.75098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 463.8084 - MinusLogProbMetric: 463.8084 - val_loss: 458.7510 - val_MinusLogProbMetric: 458.7510 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 116: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-04 03:18:53.747 
Epoch 9/1000 
	 loss: nan, MinusLogProbMetric: 34676712407040.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 9: val_loss did not improve from 458.75098
196/196 - 7s - loss: nan - MinusLogProbMetric: 34676712407040.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 7s/epoch - 35ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 356.
===========
Train data generated in 0.63 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_356/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_356/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_356
self.data_kwargs: {'seed': 926}
self.x_data: [[7.8542657  4.4182515  5.3175983  ... 3.8539157  8.104709   6.7665896 ]
 [8.06671    5.087758   5.108798   ... 2.7856822  7.9267573  7.1754713 ]
 [5.9009757  0.26195854 4.63319    ... 5.096487   6.4324465  2.782943  ]
 ...
 [5.7334824  0.57263243 4.688254   ... 4.731608   6.577419   3.9972575 ]
 [5.8251185  0.91118604 4.8986015  ... 4.9331145  6.9940166  4.170423  ]
 [5.469847   7.5352907  5.8636546  ... 8.857828   2.8293529  6.7579055 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_157 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  9018400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7fb6cc62a2f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fb6cc62b8b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fb6cc62b8b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fb6ac7c0d00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fb6ec1b17e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fb6ac7c09d0>, <keras.callbacks.ModelCheckpoint object at 0x7fb6cc7b0850>, <keras.callbacks.EarlyStopping object at 0x7fbb08805840>, <keras.callbacks.ReduceLROnPlateau object at 0x7fb6cc7f7010>, <keras.callbacks.TerminateOnNaN object at 0x7fb6cc67fdc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 356/360 with hyperparameters:
timestamp = 2023-10-04 03:18:56.703181
ndims = 1000
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.85426569e+00  4.41825151e+00  5.31759834e+00  2.60887671e+00
  6.41178656e+00  2.13207030e+00  5.74208021e+00  1.84043932e+00
  1.69763756e+00  4.54294538e+00  3.11069012e+00  2.37418079e+00
  2.54046345e+00  1.62041679e-01  9.34055448e-02  2.05885458e+00
  5.24263716e+00  7.68947506e+00  4.56616545e+00  9.68113995e+00
  2.26570651e-01  1.77447128e+00  3.29641533e+00  6.17277622e+00
  8.20737648e+00  3.43033528e+00  2.55136061e+00  5.97014785e-01
  8.28674316e+00  3.98282480e+00  5.93197346e+00  1.06245613e+01
  9.49379206e-01  1.63838947e+00  7.06365585e+00  7.94115591e+00
  7.53974438e+00  5.61496639e+00  9.80136871e+00  3.15971422e+00
  6.21875811e+00 -9.54353809e-03  4.26025915e+00  7.39404678e+00
  2.21483946e+00  9.84185696e+00  7.24155045e+00  3.30283046e+00
  7.74830770e+00  9.08916414e-01  9.71360779e+00  6.58768594e-01
  8.35239983e+00  2.32051826e+00  7.30240631e+00 -8.83946955e-01
  3.03193045e+00  7.32728100e+00  3.74522591e+00  6.38918638e-01
  3.79207659e+00  4.00098944e+00  5.66429996e+00  5.85767746e+00
  8.47701550e+00  5.04558372e+00  3.84120035e+00  1.24103403e+00
  7.94665098e+00  7.07438660e+00  3.71784091e+00  6.12205219e+00
  6.66771460e+00  2.12367511e+00 -9.60745335e-01  3.74312615e+00
  5.94952774e+00  4.78941059e+00  6.61200619e+00  8.09659863e+00
  4.87202358e+00  2.70784044e+00  9.03696346e+00  5.63844013e+00
  7.53536034e+00  7.15711594e+00  9.41573143e+00  2.42348766e+00
  7.42718220e+00  3.36646557e+00  3.73524284e+00  1.32205629e+00
  3.59988236e+00  9.48313904e+00  1.93991113e+00  2.75920010e+00
  5.01511669e+00  3.30604935e+00  5.06413269e+00  7.73147202e+00
  2.30829501e+00  4.39429140e+00  6.82420444e+00  6.79297352e+00
  9.61048603e+00  6.80823135e+00  7.26028967e+00  9.56877518e+00
  3.79452395e+00  4.15086555e+00 -2.60416776e-01  1.66208482e+00
  3.06919241e+00  9.93813455e-01  7.33946943e+00  5.69343281e+00
  5.63299084e+00  7.84667826e+00  2.26034689e+00  3.43404484e+00
  7.00420618e+00  3.05020475e+00 -7.25471079e-02  3.36938071e+00
  9.09626770e+00  4.11609411e+00  2.85297346e+00  9.84171486e+00
  6.31026506e+00  4.01078224e+00  1.21615183e+00  7.14538717e+00
  6.33219719e+00  9.81515121e+00  2.49839687e+00  6.05599999e-01
  7.24084187e+00  2.75359464e+00  2.50772285e+00  4.09680367e+00
  8.30523300e+00  6.01748800e+00  4.66583490e-01  4.52141905e+00
  5.89312696e+00  5.51575184e+00  5.69468737e+00  1.91655934e+00
  8.48398876e+00  5.63048315e+00  7.34936619e+00  9.74814224e+00
  3.48965788e+00  8.88919735e+00  9.60582942e-02  1.66400623e+00
  5.99077606e+00  3.11164737e+00  2.75282717e+00  2.60087824e+00
  2.64426851e+00  7.44998503e+00  5.85604477e+00  7.84842968e+00
  8.28961468e+00  8.40444946e+00  4.12819386e+00  4.58752203e+00
  9.98953819e+00  6.43158293e+00  5.55915296e-01  5.72953415e+00
  4.68271637e+00  9.90587902e+00 -8.83717835e-01  5.94018364e+00
  7.98547173e+00  2.84753513e+00  9.44544220e+00  9.53206062e+00
  9.07542324e+00  6.41640186e-01  6.17845011e+00  3.03050613e+00
  8.23334789e+00  6.30862665e+00  5.43327713e+00  1.29359305e+00
  3.25533032e+00  6.31515408e+00  1.03074417e+01  5.75671196e+00
  9.49255085e+00  9.50892544e+00  6.79545212e+00  4.71281195e+00
  3.50111294e+00  9.18763638e+00  4.49652076e-01  1.14515615e+00
  2.40005207e+00  7.48550272e+00  3.02852464e+00  6.13900280e+00
  7.07990456e+00  4.02328634e+00  5.32941818e+00  5.56215000e+00
  9.59676170e+00  6.84960365e+00  6.14611101e+00  7.01535404e-01
  9.08370018e+00  5.82812595e+00  7.74594402e+00  6.01755202e-01
  3.96469641e+00  2.35852933e+00  2.45156860e+00  8.29704666e+00
  6.53238392e+00  4.18872261e+00 -6.58399761e-01  9.65485573e+00
  9.80379200e+00  1.98804641e+00 -2.41938114e-01  5.28435040e+00
  9.89102745e+00  8.85664177e+00  4.39452076e+00  3.62702298e+00
  2.62890053e+00  8.97751904e+00  6.36042309e+00  1.33491313e+00
  2.77312636e+00  2.77005053e+00  1.88997674e+00  5.14272547e+00
  8.35700703e+00  6.68333673e+00  1.91080523e+00 -7.04806745e-01
  1.96866465e+00  3.76307774e+00  7.95638275e+00  9.92790127e+00
  3.73964596e+00  7.23839140e+00  9.96518135e+00  7.50328255e+00
  6.60035801e+00  8.07895303e-01  3.62572813e+00  9.14682579e+00
  2.86480689e+00  8.59156322e+00  7.98565483e+00  4.01899433e+00
  4.55118990e+00  5.28791046e+00  7.33071327e+00  5.27818727e+00
  8.85012031e-01  7.58879042e+00  3.19855785e+00  9.85190296e+00
  2.27252841e+00  2.54628229e+00  2.68598413e+00  5.91966152e+00
  8.05706692e+00  1.65711355e+00  1.67708433e+00  6.08699369e+00
  4.28031015e+00  5.60096979e+00  6.40451479e+00  1.06371555e+01
  5.36539984e+00  6.85709620e+00  8.92924786e+00  3.32924414e+00
  2.92118073e+00  6.10644913e+00  5.32210112e+00  9.56133556e+00
  4.79739636e-01  9.31261921e+00  5.15149164e+00  1.02548325e+00
  2.37651587e+00  6.48900795e+00  4.59978104e+00  9.12724590e+00
  4.42423201e+00  4.07489395e+00  8.31999683e+00  5.35771799e+00
  6.72994852e+00  7.68310690e+00  6.69467068e+00  4.67777395e+00
  6.99015021e-01  4.14359522e+00  8.36619568e+00  3.67691922e+00
  9.04692936e+00  4.49189425e+00  6.32296133e+00  8.04836750e+00
  4.41118336e+00  8.30536187e-01  9.79877853e+00  2.71371508e+00
  5.12246799e+00  4.35286856e+00  5.49793339e+00  3.22211933e+00
  7.62125874e+00  1.31961989e+00  1.10699387e+01  8.59894085e+00
  5.24368238e+00  4.61356401e+00  3.45124292e+00  5.36580896e+00
  8.41000843e+00  2.77613783e+00  4.29664612e+00  1.90059173e+00
  3.43093586e+00  2.01634741e+00  1.07569265e+01  5.71786928e+00
  7.56941891e+00  1.09229112e+00  2.52849770e+00  8.91465664e+00
  9.52245593e-01  1.17926216e+00  6.83719635e+00  6.82608938e+00
  7.53233051e+00  1.01758318e+01  1.57473803e+00  4.45485210e+00
  4.28795290e+00  1.64892748e-01  1.81490600e+00  2.76026893e+00
  4.74909878e+00  3.11362815e+00  8.00123119e+00  3.19517183e+00
  9.50560093e+00  1.03588438e+01  4.30221748e+00  6.96166873e-01
  3.03467369e+00  7.79732800e+00  2.67352748e+00  3.50514221e+00
  5.08668947e+00  7.38722420e+00  9.63334274e+00  2.62670517e+00
  4.05484772e+00  2.31513214e+00  2.64107323e+00  2.18680382e-01
  4.29812479e+00  8.41921329e+00  4.19078636e+00  3.98049355e+00
  8.93853092e+00  7.08515024e+00  5.22109699e+00  7.64430141e+00
  2.03226995e+00  3.94330883e+00  1.36629438e+00  2.33601141e+00
  3.89655709e-01  3.61157084e+00  4.44274712e+00  9.43988323e+00
  2.50003672e+00  5.12204170e+00  8.95978451e+00  7.98188019e+00
  7.24716902e+00  2.88545752e+00  4.57083321e+00  2.54753900e+00
  1.84977734e+00  4.41161156e+00  7.37936544e+00  5.14107323e+00
  7.78119135e+00  8.40401840e+00  1.04103088e-02  5.93060875e+00
  5.76526022e+00  9.53269196e+00  7.67608523e-01  6.57992554e+00
  1.95678222e+00  5.49429357e-01  5.23544788e+00  7.59472132e+00
  7.29799318e+00 -1.20772636e+00  4.30894661e+00  4.65965390e-01
  2.60425591e+00  1.97165930e+00  1.86237991e+00  2.43441749e+00
  1.47088110e+00  9.78774834e+00  3.94406772e+00  8.90942097e+00
  1.22915721e+00  7.60721827e+00  8.06412756e-01  4.82014179e+00
  5.46107650e-01  7.98665380e+00  9.84689426e+00  5.38293898e-01
  8.54449463e+00  8.00841713e+00  3.39391780e+00  9.08403206e+00
  2.01289654e+00  4.27781343e+00  9.96862292e-01  6.87161112e+00
 -9.90051091e-01  4.79714537e+00  6.87879229e+00  8.84089375e+00
  2.07931614e+00  8.38031006e+00  5.88058186e+00  5.93655205e+00
  5.64989281e+00  6.21854544e-01  6.68466759e+00  4.26185799e+00
  8.59449577e+00  3.42455196e+00  5.77914000e+00  4.20325327e+00
  8.32858276e+00  2.54622722e+00  3.04043859e-01  8.33305359e+00
  4.29302406e+00  5.85027218e+00  3.44100654e-01  1.02574406e+01
  9.13591194e+00  6.44509363e+00  2.23200083e+00  3.65651631e+00
  9.79177284e+00  6.80626631e+00  1.36182153e+00  4.29894447e+00
  9.91862965e+00  6.15662336e+00  8.39294195e-02  5.73490715e+00
  3.36321139e+00  9.89277542e-01  4.06990290e+00  4.61008358e+00
 -7.16332495e-01  6.55716562e+00  5.19925833e+00  7.17309570e+00
  4.03727007e+00  5.57426548e+00  6.92208004e+00  8.72025681e+00
  3.84482765e+00  6.02073288e+00  7.86660480e+00 -5.73046148e-01
  7.67636967e+00  3.58273864e+00  7.47981644e+00  5.36535072e+00
  7.16784859e+00  7.85914564e+00  1.76731563e+00  6.72368050e+00
  8.51148510e+00  1.61090291e+00  6.08754253e+00  6.35778236e+00
  9.91369438e+00  2.55167031e+00  1.66099370e+00  3.64644337e+00
  2.42382288e+00  7.45638466e+00  7.33168173e+00  6.45663881e+00
  2.01684594e+00  8.60406494e+00  1.10975132e+01  5.38561678e+00
  5.44827414e+00  4.94278336e+00  1.86474085e+00  1.97823441e+00
  4.30962515e+00  1.54604924e+00  9.07867241e+00  7.96630621e+00
  5.26732063e+00  3.58175659e+00  9.51251125e+00  1.04863870e+00
  5.07879162e+00  1.05271053e+00  1.87499964e+00  6.65355206e+00
  6.66385412e+00  6.33094311e+00  8.71027946e+00  9.82982922e+00
  6.55875683e+00  5.20563507e+00  8.00854397e+00  2.41466331e+00
  7.00420952e+00  4.88513613e+00  1.61634290e+00  9.31071377e+00
  3.35792899e+00  5.86867237e+00  8.60531521e+00  5.60204506e+00
  1.54713285e+00  7.74277306e+00  3.66394973e+00  9.42229176e+00
  8.48596764e+00  6.48666763e+00  3.23076034e+00  7.68814230e+00
  9.35188389e+00  4.08446217e+00  5.02816010e+00  3.38081270e-01
  6.36648226e+00  8.51817703e+00  2.80446982e+00  3.20807362e+00
  4.51574850e+00  5.93633711e-01  9.46782529e-01  2.16139364e+00
  2.60937262e+00  5.28547049e+00  4.84176254e+00  9.81929684e+00
  7.88902140e+00  1.66553545e+00  8.82484341e+00  7.09167838e-01
  5.03990793e+00  2.42374206e+00  2.91808867e+00  7.50122595e+00
  4.18691397e+00  9.02093601e+00  7.10776806e+00  9.57430172e+00
  5.59825611e+00  8.02043629e+00  3.25235939e+00  4.47115898e+00
  4.30808973e+00  3.80606079e+00  1.12467217e+00  5.29313755e+00
  1.50810623e+00  8.85228539e+00 -4.06661093e-01  5.10796404e+00
  3.78489470e+00  7.05098629e-01  7.91552544e+00  6.63939571e+00
  1.46795654e+00  6.65862942e+00  8.33073235e+00  8.50840759e+00
  1.30628014e+00  6.92401600e+00  7.72493744e+00  8.51644993e+00
  5.43707657e+00  4.83005619e+00  8.90700626e+00  8.11882019e+00
  5.73599482e+00  2.80633116e+00  9.65988350e+00  4.51777983e+00
  9.21329618e-01  4.88313866e+00  9.77910519e+00  4.40435410e+00
 -3.27194422e-01  6.12769270e+00  9.89487934e+00  7.31696033e+00
  6.35276508e+00  9.55050945e+00  8.07680893e+00  6.34605026e+00
  4.77183819e+00  7.15873861e+00  8.06319714e+00  5.61736882e-01
  7.67201567e+00  3.85464573e+00  6.69436502e+00  7.86791897e+00
  4.83945560e+00  1.62425935e+00  4.19299459e+00  9.26880896e-01
  6.88019085e+00  9.74193478e+00  8.84968662e+00  3.65187788e+00
  6.33452368e+00  3.75078416e+00  4.67366219e+00  3.54604197e+00
  8.16938019e+00  2.72105742e+00  9.75844955e+00  6.57792234e+00
  2.28564978e-01  4.99803901e-01  1.02100105e+01  8.88365149e-01
  3.53124762e+00  1.72824395e+00  9.85693932e+00  5.20375729e+00
  5.34282923e+00  6.05618429e+00  2.67098927e+00  5.57222414e+00
  4.29868984e+00  1.93069673e+00  6.95643902e+00  3.56284547e+00
  7.73937511e+00  4.35557556e+00  5.20385551e+00  7.79240131e+00
  4.75181484e+00  9.50519943e+00  8.01542187e+00  8.55453110e+00
  9.82915688e+00  3.75745511e+00  5.48529482e+00  7.18655014e+00
  6.00739861e+00  1.01745186e+01  2.86505723e+00  3.40535784e+00
  8.88479424e+00  7.66841078e+00  2.33434486e+00  6.26787615e+00
  7.08852863e+00  2.91594267e+00  5.48495722e+00  5.74615335e+00
  4.40000677e+00  7.55737638e+00 -8.34494978e-02  4.98475075e+00
  3.07788324e+00  5.51033068e+00  7.23143101e+00  7.69217253e-01
  1.29760706e+00  6.85467720e+00  7.40828228e+00  5.57670832e+00
  9.81540394e+00  5.34229517e+00  6.78031504e-01  6.85619688e+00
  5.78618479e+00  5.76328468e+00  5.48964214e+00  8.51025105e+00
  1.36436164e+00  1.25749195e+00  1.53151178e+00  2.31167674e+00
  5.93736410e+00  6.02586079e+00  2.10990143e+00  3.74104166e+00
  4.49994564e+00  4.30085993e+00  8.55859375e+00  4.00177526e+00
  6.96290779e+00  1.65811288e+00  5.44697285e+00  8.24122715e+00
  1.78438163e+00  4.33162642e+00  5.78468227e+00  2.05096912e+00
  7.39810944e+00  1.59008896e+00  8.76768684e+00  4.69711876e+00
  4.04790258e+00  1.24405193e+00  2.83710051e+00  6.33698320e+00
  1.33927023e+00  3.15551066e+00  9.67255783e+00  6.40811014e+00
  2.06277108e+00  2.24199677e+00  8.81563950e+00  3.05780077e+00
  1.87078035e+00  9.16118813e+00  3.53771424e+00  1.65168977e+00
  5.19690895e+00  4.59403133e+00  9.05195808e+00  1.09238386e+00
  1.04556847e+00  8.68545175e-01  9.68726254e+00  5.04982138e+00
  2.69507694e+00  5.98866844e+00  4.93495703e+00  2.08913898e+00
  6.94658279e+00  2.16514921e+00  2.17784786e+00  5.89404345e+00
  2.47186089e+00  3.07393289e+00  9.32638359e+00  1.50767267e+00
  3.44327784e+00  3.86774182e+00  2.78355002e+00  2.11643291e+00
  8.07786083e+00  3.65396214e+00 -2.35649347e-01  5.44787359e+00
  8.69826198e-01  2.77893639e+00  9.38866138e+00  3.60090685e+00
  5.78489542e+00  5.85346889e+00  2.58838773e-01  6.52211618e+00
  9.64319706e+00  8.25448573e-01  5.58079672e+00  7.69353580e+00
  8.50068855e+00  1.93690157e+00  2.22951698e+00  6.91337490e+00
  3.25202256e-01  7.09594727e+00  8.97951889e+00  4.25646448e+00
  4.87320185e+00  3.06530523e+00  6.00203633e-01  1.48462224e+00
  8.84535408e+00  8.94914532e+00  1.44960713e+00  9.55700684e+00
  3.84997725e+00  2.66661191e+00  9.85832453e-01  9.06340885e+00
  9.97899628e+00  3.32584810e+00  6.95174551e+00  3.19924808e+00
 -1.51607394e-01  4.97948170e+00  5.40087318e+00  1.11839056e+00
  8.18654537e+00  5.18570328e+00  2.13246346e+00  6.35160780e+00
  8.03942680e+00  3.31687546e+00  8.01695156e+00  4.56763649e+00
  9.25021172e+00  7.31373024e+00  3.17991471e+00  1.24928892e-01
  5.86790705e+00  2.48283291e+00  1.13595665e+00  9.80613613e+00
  1.05967150e+01  2.69507051e+00  3.01809096e+00  1.64316809e+00
  4.14577913e+00  8.58723545e+00  6.37656307e+00  8.65282822e+00
  1.23624945e+00  6.83144760e+00  9.12307358e+00  2.77780533e+00
  6.90577936e+00  3.74633145e+00  2.03699780e+00  9.43050480e+00
  3.88212705e+00  1.03552914e+00  3.94930816e+00  3.95189595e+00
  7.43439579e+00  8.88795757e+00  7.81051493e+00  9.53783226e+00
  4.64317465e+00  7.03867435e+00  3.24562931e+00  9.52776432e-01
  3.66223240e+00  7.14639282e+00  3.33262825e+00  7.86202145e+00
  7.74663401e+00  1.78269148e+00  2.58091092e+00  8.37615490e+00
  3.35916471e+00  3.20344758e+00  3.59131765e+00  8.01384544e+00
 -2.43629217e-02  7.42378175e-01  5.56940269e+00  4.90014553e+00
  6.48054647e+00  3.32681608e+00  9.08345699e+00  7.95236826e+00
  6.34802222e-01  6.87756348e+00  9.49590397e+00  1.01780138e+01
  7.24633646e+00  1.21293211e+00  6.68064976e+00  3.20778751e+00
  1.00395985e+01  7.21336079e+00  6.02273321e+00  6.93427706e+00
  4.42568588e+00  4.00593805e+00  9.61300564e+00  3.34010863e+00
  5.83148003e-01  4.11973238e+00  8.45203018e+00  6.38991165e+00
  1.04141502e+01  7.16550827e-01  8.68817711e+00  4.16127014e+00
  3.57046032e+00  2.75189900e+00  1.17473054e+00  3.15659904e+00
  7.35806704e+00  4.37037677e-01  1.24299979e+00 -1.54236555e-02
  8.57396030e+00  5.91643810e+00  1.83133376e+00  7.77049971e+00
  1.06167221e+01  9.25513077e+00  9.58188915e+00 -5.68707407e-01
  5.92560768e+00  2.55895495e+00  5.55885506e+00  2.23880720e+00
  1.15490997e+00  5.06085348e+00  5.79838181e+00  1.02773275e+01
 -8.31137300e-02  3.83537984e+00  8.54463768e+00  5.31149960e+00
  8.12730598e+00  6.55586195e+00  7.92252541e+00  5.34070778e+00
  2.95670152e+00  8.34237576e+00  5.68467236e+00  1.28557813e+00
  8.03099155e+00  9.61913395e+00  4.18909931e+00  3.39216042e+00
  6.58131719e-01  6.45680571e+00  8.95029449e+00  1.41747653e+00
  4.75601006e+00  2.90228367e-01  5.12265801e-01  6.72031784e+00
  8.77206898e+00  3.71332788e+00  8.48167229e+00  1.21904695e+00
  1.87795568e+00  9.97635460e+00  4.99329329e+00  6.38564539e+00
  2.54908800e-01  1.29785419e+00  5.73886812e-01  8.37132168e+00
  1.09254289e+00  8.28661823e+00  3.38485265e+00  7.73170853e+00
  4.09941435e+00  2.49423814e+00  1.25597119e+00  2.96604967e+00
  3.78301477e+00  8.97054768e+00  4.46885729e+00  9.85267830e+00
  5.88528252e+00 -5.40345371e-01  4.29398537e+00  7.70487690e+00
  1.43975925e+00  8.53642845e+00  5.70689344e+00 -4.84625697e-02
  7.89377451e+00  5.98359823e+00  8.12033236e-01  4.98253584e+00
  3.64299822e+00  9.16150284e+00  2.32410598e+00  9.28726101e+00
  5.53273964e+00  3.85391569e+00  8.10470867e+00  6.76658964e+00]
Epoch 1/1000
2023-10-04 03:19:32.258 
Epoch 1/1000 
	 loss: 489.1826, MinusLogProbMetric: 489.1826, val_loss: 433.7405, val_MinusLogProbMetric: 433.7405

Epoch 1: val_loss improved from inf to 433.74051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 36s - loss: 489.1826 - MinusLogProbMetric: 489.1826 - val_loss: 433.7405 - val_MinusLogProbMetric: 433.7405 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 2/1000
2023-10-04 03:19:43.123 
Epoch 2/1000 
	 loss: 431.1182, MinusLogProbMetric: 431.1182, val_loss: 431.1370, val_MinusLogProbMetric: 431.1370

Epoch 2: val_loss improved from 433.74051 to 431.13696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 431.1182 - MinusLogProbMetric: 431.1182 - val_loss: 431.1370 - val_MinusLogProbMetric: 431.1370 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 3/1000
2023-10-04 03:19:53.816 
Epoch 3/1000 
	 loss: 428.9178, MinusLogProbMetric: 428.9178, val_loss: 427.6104, val_MinusLogProbMetric: 427.6104

Epoch 3: val_loss improved from 431.13696 to 427.61035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 428.9178 - MinusLogProbMetric: 428.9178 - val_loss: 427.6104 - val_MinusLogProbMetric: 427.6104 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 4/1000
2023-10-04 03:20:04.709 
Epoch 4/1000 
	 loss: 427.3380, MinusLogProbMetric: 427.3380, val_loss: 426.4059, val_MinusLogProbMetric: 426.4059

Epoch 4: val_loss improved from 427.61035 to 426.40585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 427.3380 - MinusLogProbMetric: 427.3380 - val_loss: 426.4059 - val_MinusLogProbMetric: 426.4059 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 5/1000
2023-10-04 03:20:15.351 
Epoch 5/1000 
	 loss: 427.6201, MinusLogProbMetric: 427.6201, val_loss: 432.4516, val_MinusLogProbMetric: 432.4516

Epoch 5: val_loss did not improve from 426.40585
196/196 - 10s - loss: 427.6201 - MinusLogProbMetric: 427.6201 - val_loss: 432.4516 - val_MinusLogProbMetric: 432.4516 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 6/1000
2023-10-04 03:20:25.686 
Epoch 6/1000 
	 loss: 424.4587, MinusLogProbMetric: 424.4587, val_loss: 420.4666, val_MinusLogProbMetric: 420.4666

Epoch 6: val_loss improved from 426.40585 to 420.46664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 424.4587 - MinusLogProbMetric: 424.4587 - val_loss: 420.4666 - val_MinusLogProbMetric: 420.4666 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 7/1000
2023-10-04 03:20:36.290 
Epoch 7/1000 
	 loss: 422.8427, MinusLogProbMetric: 422.8427, val_loss: 420.8189, val_MinusLogProbMetric: 420.8189

Epoch 7: val_loss did not improve from 420.46664
196/196 - 10s - loss: 422.8427 - MinusLogProbMetric: 422.8427 - val_loss: 420.8189 - val_MinusLogProbMetric: 420.8189 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 8/1000
2023-10-04 03:20:46.430 
Epoch 8/1000 
	 loss: 422.7000, MinusLogProbMetric: 422.7000, val_loss: 419.8746, val_MinusLogProbMetric: 419.8746

Epoch 8: val_loss improved from 420.46664 to 419.87463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 10s - loss: 422.7000 - MinusLogProbMetric: 422.7000 - val_loss: 419.8746 - val_MinusLogProbMetric: 419.8746 - lr: 3.3333e-04 - 10s/epoch - 54ms/step
Epoch 9/1000
2023-10-04 03:20:56.692 
Epoch 9/1000 
	 loss: 420.9901, MinusLogProbMetric: 420.9901, val_loss: 420.8471, val_MinusLogProbMetric: 420.8471

Epoch 9: val_loss did not improve from 419.87463
196/196 - 10s - loss: 420.9901 - MinusLogProbMetric: 420.9901 - val_loss: 420.8471 - val_MinusLogProbMetric: 420.8471 - lr: 3.3333e-04 - 10s/epoch - 50ms/step
Epoch 10/1000
2023-10-04 03:21:06.860 
Epoch 10/1000 
	 loss: 419.9428, MinusLogProbMetric: 419.9428, val_loss: 424.7213, val_MinusLogProbMetric: 424.7213

Epoch 10: val_loss did not improve from 419.87463
196/196 - 10s - loss: 419.9428 - MinusLogProbMetric: 419.9428 - val_loss: 424.7213 - val_MinusLogProbMetric: 424.7213 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 11/1000
2023-10-04 03:21:17.077 
Epoch 11/1000 
	 loss: 419.5390, MinusLogProbMetric: 419.5390, val_loss: 418.1972, val_MinusLogProbMetric: 418.1972

Epoch 11: val_loss improved from 419.87463 to 418.19724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 419.5390 - MinusLogProbMetric: 419.5390 - val_loss: 418.1972 - val_MinusLogProbMetric: 418.1972 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 12/1000
2023-10-04 03:21:28.132 
Epoch 12/1000 
	 loss: 417.2964, MinusLogProbMetric: 417.2964, val_loss: 416.5073, val_MinusLogProbMetric: 416.5073

Epoch 12: val_loss improved from 418.19724 to 416.50726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 417.2964 - MinusLogProbMetric: 417.2964 - val_loss: 416.5073 - val_MinusLogProbMetric: 416.5073 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 13/1000
2023-10-04 03:21:39.576 
Epoch 13/1000 
	 loss: 417.6037, MinusLogProbMetric: 417.6037, val_loss: 417.0802, val_MinusLogProbMetric: 417.0802

Epoch 13: val_loss did not improve from 416.50726
196/196 - 11s - loss: 417.6037 - MinusLogProbMetric: 417.6037 - val_loss: 417.0802 - val_MinusLogProbMetric: 417.0802 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 14/1000
2023-10-04 03:21:50.208 
Epoch 14/1000 
	 loss: 417.0273, MinusLogProbMetric: 417.0273, val_loss: 416.6273, val_MinusLogProbMetric: 416.6273

Epoch 14: val_loss did not improve from 416.50726
196/196 - 11s - loss: 417.0273 - MinusLogProbMetric: 417.0273 - val_loss: 416.6273 - val_MinusLogProbMetric: 416.6273 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 15/1000
2023-10-04 03:22:00.443 
Epoch 15/1000 
	 loss: 417.4920, MinusLogProbMetric: 417.4920, val_loss: 418.0482, val_MinusLogProbMetric: 418.0482

Epoch 15: val_loss did not improve from 416.50726
196/196 - 10s - loss: 417.4920 - MinusLogProbMetric: 417.4920 - val_loss: 418.0482 - val_MinusLogProbMetric: 418.0482 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 16/1000
2023-10-04 03:22:10.793 
Epoch 16/1000 
	 loss: 415.5543, MinusLogProbMetric: 415.5543, val_loss: 423.3953, val_MinusLogProbMetric: 423.3953

Epoch 16: val_loss did not improve from 416.50726
196/196 - 10s - loss: 415.5543 - MinusLogProbMetric: 415.5543 - val_loss: 423.3953 - val_MinusLogProbMetric: 423.3953 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 17/1000
2023-10-04 03:22:21.149 
Epoch 17/1000 
	 loss: 414.3109, MinusLogProbMetric: 414.3109, val_loss: 412.9349, val_MinusLogProbMetric: 412.9349

Epoch 17: val_loss improved from 416.50726 to 412.93491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 414.3109 - MinusLogProbMetric: 414.3109 - val_loss: 412.9349 - val_MinusLogProbMetric: 412.9349 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 18/1000
2023-10-04 03:22:31.749 
Epoch 18/1000 
	 loss: 416.6551, MinusLogProbMetric: 416.6551, val_loss: 413.2290, val_MinusLogProbMetric: 413.2290

Epoch 18: val_loss did not improve from 412.93491
196/196 - 10s - loss: 416.6551 - MinusLogProbMetric: 416.6551 - val_loss: 413.2290 - val_MinusLogProbMetric: 413.2290 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 19/1000
2023-10-04 03:22:42.217 
Epoch 19/1000 
	 loss: 412.8347, MinusLogProbMetric: 412.8347, val_loss: 416.2751, val_MinusLogProbMetric: 416.2751

Epoch 19: val_loss did not improve from 412.93491
196/196 - 10s - loss: 412.8347 - MinusLogProbMetric: 412.8347 - val_loss: 416.2751 - val_MinusLogProbMetric: 416.2751 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 20/1000
2023-10-04 03:22:52.627 
Epoch 20/1000 
	 loss: 412.4254, MinusLogProbMetric: 412.4254, val_loss: 413.8096, val_MinusLogProbMetric: 413.8096

Epoch 20: val_loss did not improve from 412.93491
196/196 - 10s - loss: 412.4254 - MinusLogProbMetric: 412.4254 - val_loss: 413.8096 - val_MinusLogProbMetric: 413.8096 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 21/1000
2023-10-04 03:23:03.029 
Epoch 21/1000 
	 loss: 413.3126, MinusLogProbMetric: 413.3126, val_loss: 416.8848, val_MinusLogProbMetric: 416.8848

Epoch 21: val_loss did not improve from 412.93491
196/196 - 10s - loss: 413.3126 - MinusLogProbMetric: 413.3126 - val_loss: 416.8848 - val_MinusLogProbMetric: 416.8848 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 22/1000
2023-10-04 03:23:13.653 
Epoch 22/1000 
	 loss: 412.6605, MinusLogProbMetric: 412.6605, val_loss: 411.4403, val_MinusLogProbMetric: 411.4403

Epoch 22: val_loss improved from 412.93491 to 411.44034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 412.6605 - MinusLogProbMetric: 412.6605 - val_loss: 411.4403 - val_MinusLogProbMetric: 411.4403 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 23/1000
2023-10-04 03:23:24.217 
Epoch 23/1000 
	 loss: 412.9638, MinusLogProbMetric: 412.9638, val_loss: 414.8378, val_MinusLogProbMetric: 414.8378

Epoch 23: val_loss did not improve from 411.44034
196/196 - 10s - loss: 412.9638 - MinusLogProbMetric: 412.9638 - val_loss: 414.8378 - val_MinusLogProbMetric: 414.8378 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 24/1000
2023-10-04 03:23:34.491 
Epoch 24/1000 
	 loss: 411.6394, MinusLogProbMetric: 411.6394, val_loss: 419.4908, val_MinusLogProbMetric: 419.4908

Epoch 24: val_loss did not improve from 411.44034
196/196 - 10s - loss: 411.6394 - MinusLogProbMetric: 411.6394 - val_loss: 419.4908 - val_MinusLogProbMetric: 419.4908 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 25/1000
2023-10-04 03:23:44.687 
Epoch 25/1000 
	 loss: 410.8781, MinusLogProbMetric: 410.8781, val_loss: 411.6317, val_MinusLogProbMetric: 411.6317

Epoch 25: val_loss did not improve from 411.44034
196/196 - 10s - loss: 410.8781 - MinusLogProbMetric: 410.8781 - val_loss: 411.6317 - val_MinusLogProbMetric: 411.6317 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 26/1000
2023-10-04 03:23:55.062 
Epoch 26/1000 
	 loss: 411.1734, MinusLogProbMetric: 411.1734, val_loss: 408.6508, val_MinusLogProbMetric: 408.6508

Epoch 26: val_loss improved from 411.44034 to 408.65085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 411.1734 - MinusLogProbMetric: 411.1734 - val_loss: 408.6508 - val_MinusLogProbMetric: 408.6508 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 27/1000
2023-10-04 03:24:05.660 
Epoch 27/1000 
	 loss: 411.7070, MinusLogProbMetric: 411.7070, val_loss: 413.4511, val_MinusLogProbMetric: 413.4511

Epoch 27: val_loss did not improve from 408.65085
196/196 - 10s - loss: 411.7070 - MinusLogProbMetric: 411.7070 - val_loss: 413.4511 - val_MinusLogProbMetric: 413.4511 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 28/1000
2023-10-04 03:24:15.910 
Epoch 28/1000 
	 loss: 410.0497, MinusLogProbMetric: 410.0497, val_loss: 412.7890, val_MinusLogProbMetric: 412.7890

Epoch 28: val_loss did not improve from 408.65085
196/196 - 10s - loss: 410.0497 - MinusLogProbMetric: 410.0497 - val_loss: 412.7890 - val_MinusLogProbMetric: 412.7890 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 29/1000
2023-10-04 03:24:26.039 
Epoch 29/1000 
	 loss: 409.7158, MinusLogProbMetric: 409.7158, val_loss: 410.1411, val_MinusLogProbMetric: 410.1411

Epoch 29: val_loss did not improve from 408.65085
196/196 - 10s - loss: 409.7158 - MinusLogProbMetric: 409.7158 - val_loss: 410.1411 - val_MinusLogProbMetric: 410.1411 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 30/1000
2023-10-04 03:24:36.328 
Epoch 30/1000 
	 loss: 410.3771, MinusLogProbMetric: 410.3771, val_loss: 412.4925, val_MinusLogProbMetric: 412.4925

Epoch 30: val_loss did not improve from 408.65085
196/196 - 10s - loss: 410.3771 - MinusLogProbMetric: 410.3771 - val_loss: 412.4925 - val_MinusLogProbMetric: 412.4925 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 31/1000
2023-10-04 03:24:46.527 
Epoch 31/1000 
	 loss: 408.5032, MinusLogProbMetric: 408.5032, val_loss: 414.3549, val_MinusLogProbMetric: 414.3549

Epoch 31: val_loss did not improve from 408.65085
196/196 - 10s - loss: 408.5032 - MinusLogProbMetric: 408.5032 - val_loss: 414.3549 - val_MinusLogProbMetric: 414.3549 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 32/1000
2023-10-04 03:24:56.761 
Epoch 32/1000 
	 loss: 409.7178, MinusLogProbMetric: 409.7178, val_loss: 411.5794, val_MinusLogProbMetric: 411.5794

Epoch 32: val_loss did not improve from 408.65085
196/196 - 10s - loss: 409.7178 - MinusLogProbMetric: 409.7178 - val_loss: 411.5794 - val_MinusLogProbMetric: 411.5794 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 33/1000
2023-10-04 03:25:07.019 
Epoch 33/1000 
	 loss: 407.9400, MinusLogProbMetric: 407.9400, val_loss: 408.9373, val_MinusLogProbMetric: 408.9373

Epoch 33: val_loss did not improve from 408.65085
196/196 - 10s - loss: 407.9400 - MinusLogProbMetric: 407.9400 - val_loss: 408.9373 - val_MinusLogProbMetric: 408.9373 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 34/1000
2023-10-04 03:25:17.208 
Epoch 34/1000 
	 loss: 409.2809, MinusLogProbMetric: 409.2809, val_loss: 411.7385, val_MinusLogProbMetric: 411.7385

Epoch 34: val_loss did not improve from 408.65085
196/196 - 10s - loss: 409.2809 - MinusLogProbMetric: 409.2809 - val_loss: 411.7385 - val_MinusLogProbMetric: 411.7385 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 35/1000
2023-10-04 03:25:27.442 
Epoch 35/1000 
	 loss: 408.3564, MinusLogProbMetric: 408.3564, val_loss: 411.0766, val_MinusLogProbMetric: 411.0766

Epoch 35: val_loss did not improve from 408.65085
196/196 - 10s - loss: 408.3564 - MinusLogProbMetric: 408.3564 - val_loss: 411.0766 - val_MinusLogProbMetric: 411.0766 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 36/1000
2023-10-04 03:25:37.694 
Epoch 36/1000 
	 loss: 407.5317, MinusLogProbMetric: 407.5317, val_loss: 408.7216, val_MinusLogProbMetric: 408.7216

Epoch 36: val_loss did not improve from 408.65085
196/196 - 10s - loss: 407.5317 - MinusLogProbMetric: 407.5317 - val_loss: 408.7216 - val_MinusLogProbMetric: 408.7216 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 37/1000
2023-10-04 03:25:48.071 
Epoch 37/1000 
	 loss: 409.1330, MinusLogProbMetric: 409.1330, val_loss: 412.1453, val_MinusLogProbMetric: 412.1453

Epoch 37: val_loss did not improve from 408.65085
196/196 - 10s - loss: 409.1330 - MinusLogProbMetric: 409.1330 - val_loss: 412.1453 - val_MinusLogProbMetric: 412.1453 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 38/1000
2023-10-04 03:25:58.354 
Epoch 38/1000 
	 loss: 406.8120, MinusLogProbMetric: 406.8120, val_loss: 408.8559, val_MinusLogProbMetric: 408.8559

Epoch 38: val_loss did not improve from 408.65085
196/196 - 10s - loss: 406.8120 - MinusLogProbMetric: 406.8120 - val_loss: 408.8559 - val_MinusLogProbMetric: 408.8559 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 39/1000
2023-10-04 03:26:08.762 
Epoch 39/1000 
	 loss: 406.8979, MinusLogProbMetric: 406.8979, val_loss: 408.9318, val_MinusLogProbMetric: 408.9318

Epoch 39: val_loss did not improve from 408.65085
196/196 - 10s - loss: 406.8979 - MinusLogProbMetric: 406.8979 - val_loss: 408.9318 - val_MinusLogProbMetric: 408.9318 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 40/1000
2023-10-04 03:26:18.834 
Epoch 40/1000 
	 loss: 407.0096, MinusLogProbMetric: 407.0096, val_loss: 409.2260, val_MinusLogProbMetric: 409.2260

Epoch 40: val_loss did not improve from 408.65085
196/196 - 10s - loss: 407.0096 - MinusLogProbMetric: 407.0096 - val_loss: 409.2260 - val_MinusLogProbMetric: 409.2260 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 41/1000
2023-10-04 03:26:28.989 
Epoch 41/1000 
	 loss: 407.3275, MinusLogProbMetric: 407.3275, val_loss: 409.1468, val_MinusLogProbMetric: 409.1468

Epoch 41: val_loss did not improve from 408.65085
196/196 - 10s - loss: 407.3275 - MinusLogProbMetric: 407.3275 - val_loss: 409.1468 - val_MinusLogProbMetric: 409.1468 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 42/1000
2023-10-04 03:26:39.207 
Epoch 42/1000 
	 loss: 405.9319, MinusLogProbMetric: 405.9319, val_loss: 409.4312, val_MinusLogProbMetric: 409.4312

Epoch 42: val_loss did not improve from 408.65085
196/196 - 10s - loss: 405.9319 - MinusLogProbMetric: 405.9319 - val_loss: 409.4312 - val_MinusLogProbMetric: 409.4312 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 43/1000
2023-10-04 03:26:49.660 
Epoch 43/1000 
	 loss: 406.4380, MinusLogProbMetric: 406.4380, val_loss: 407.1472, val_MinusLogProbMetric: 407.1472

Epoch 43: val_loss improved from 408.65085 to 407.14725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 406.4380 - MinusLogProbMetric: 406.4380 - val_loss: 407.1472 - val_MinusLogProbMetric: 407.1472 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 44/1000
2023-10-04 03:27:00.164 
Epoch 44/1000 
	 loss: 406.3517, MinusLogProbMetric: 406.3517, val_loss: 410.8805, val_MinusLogProbMetric: 410.8805

Epoch 44: val_loss did not improve from 407.14725
196/196 - 10s - loss: 406.3517 - MinusLogProbMetric: 406.3517 - val_loss: 410.8805 - val_MinusLogProbMetric: 410.8805 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 45/1000
2023-10-04 03:27:10.437 
Epoch 45/1000 
	 loss: 405.8358, MinusLogProbMetric: 405.8358, val_loss: 408.1385, val_MinusLogProbMetric: 408.1385

Epoch 45: val_loss did not improve from 407.14725
196/196 - 10s - loss: 405.8358 - MinusLogProbMetric: 405.8358 - val_loss: 408.1385 - val_MinusLogProbMetric: 408.1385 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 46/1000
2023-10-04 03:27:20.680 
Epoch 46/1000 
	 loss: 405.1526, MinusLogProbMetric: 405.1526, val_loss: 409.9567, val_MinusLogProbMetric: 409.9567

Epoch 46: val_loss did not improve from 407.14725
196/196 - 10s - loss: 405.1526 - MinusLogProbMetric: 405.1526 - val_loss: 409.9567 - val_MinusLogProbMetric: 409.9567 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 47/1000
2023-10-04 03:27:30.872 
Epoch 47/1000 
	 loss: 406.0422, MinusLogProbMetric: 406.0422, val_loss: 407.2776, val_MinusLogProbMetric: 407.2776

Epoch 47: val_loss did not improve from 407.14725
196/196 - 10s - loss: 406.0422 - MinusLogProbMetric: 406.0422 - val_loss: 407.2776 - val_MinusLogProbMetric: 407.2776 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 48/1000
2023-10-04 03:27:41.106 
Epoch 48/1000 
	 loss: 405.2789, MinusLogProbMetric: 405.2789, val_loss: 417.8204, val_MinusLogProbMetric: 417.8204

Epoch 48: val_loss did not improve from 407.14725
196/196 - 10s - loss: 405.2789 - MinusLogProbMetric: 405.2789 - val_loss: 417.8204 - val_MinusLogProbMetric: 417.8204 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 49/1000
2023-10-04 03:27:51.544 
Epoch 49/1000 
	 loss: 406.7284, MinusLogProbMetric: 406.7284, val_loss: 405.0957, val_MinusLogProbMetric: 405.0957

Epoch 49: val_loss improved from 407.14725 to 405.09570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 406.7284 - MinusLogProbMetric: 406.7284 - val_loss: 405.0957 - val_MinusLogProbMetric: 405.0957 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 50/1000
2023-10-04 03:28:03.270 
Epoch 50/1000 
	 loss: 404.3228, MinusLogProbMetric: 404.3228, val_loss: 411.1792, val_MinusLogProbMetric: 411.1792

Epoch 50: val_loss did not improve from 405.09570
196/196 - 11s - loss: 404.3228 - MinusLogProbMetric: 404.3228 - val_loss: 411.1792 - val_MinusLogProbMetric: 411.1792 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 51/1000
2023-10-04 03:28:13.958 
Epoch 51/1000 
	 loss: 404.4206, MinusLogProbMetric: 404.4206, val_loss: 407.2200, val_MinusLogProbMetric: 407.2200

Epoch 51: val_loss did not improve from 405.09570
196/196 - 11s - loss: 404.4206 - MinusLogProbMetric: 404.4206 - val_loss: 407.2200 - val_MinusLogProbMetric: 407.2200 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 52/1000
2023-10-04 03:28:23.923 
Epoch 52/1000 
	 loss: 403.8294, MinusLogProbMetric: 403.8294, val_loss: 407.3420, val_MinusLogProbMetric: 407.3420

Epoch 52: val_loss did not improve from 405.09570
196/196 - 10s - loss: 403.8294 - MinusLogProbMetric: 403.8294 - val_loss: 407.3420 - val_MinusLogProbMetric: 407.3420 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 53/1000
2023-10-04 03:28:34.316 
Epoch 53/1000 
	 loss: 404.4698, MinusLogProbMetric: 404.4698, val_loss: 451.2485, val_MinusLogProbMetric: 451.2485

Epoch 53: val_loss did not improve from 405.09570
196/196 - 10s - loss: 404.4698 - MinusLogProbMetric: 404.4698 - val_loss: 451.2485 - val_MinusLogProbMetric: 451.2485 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 54/1000
2023-10-04 03:28:44.709 
Epoch 54/1000 
	 loss: 406.2184, MinusLogProbMetric: 406.2184, val_loss: 408.5384, val_MinusLogProbMetric: 408.5384

Epoch 54: val_loss did not improve from 405.09570
196/196 - 10s - loss: 406.2184 - MinusLogProbMetric: 406.2184 - val_loss: 408.5384 - val_MinusLogProbMetric: 408.5384 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 55/1000
2023-10-04 03:28:55.055 
Epoch 55/1000 
	 loss: 403.5439, MinusLogProbMetric: 403.5439, val_loss: 406.6794, val_MinusLogProbMetric: 406.6794

Epoch 55: val_loss did not improve from 405.09570
196/196 - 10s - loss: 403.5439 - MinusLogProbMetric: 403.5439 - val_loss: 406.6794 - val_MinusLogProbMetric: 406.6794 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 56/1000
2023-10-04 03:29:05.283 
Epoch 56/1000 
	 loss: 404.6430, MinusLogProbMetric: 404.6430, val_loss: 405.8282, val_MinusLogProbMetric: 405.8282

Epoch 56: val_loss did not improve from 405.09570
196/196 - 10s - loss: 404.6430 - MinusLogProbMetric: 404.6430 - val_loss: 405.8282 - val_MinusLogProbMetric: 405.8282 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 57/1000
2023-10-04 03:29:15.665 
Epoch 57/1000 
	 loss: 404.0668, MinusLogProbMetric: 404.0668, val_loss: 406.8620, val_MinusLogProbMetric: 406.8620

Epoch 57: val_loss did not improve from 405.09570
196/196 - 10s - loss: 404.0668 - MinusLogProbMetric: 404.0668 - val_loss: 406.8620 - val_MinusLogProbMetric: 406.8620 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 58/1000
2023-10-04 03:29:26.002 
Epoch 58/1000 
	 loss: 404.1814, MinusLogProbMetric: 404.1814, val_loss: 407.4071, val_MinusLogProbMetric: 407.4071

Epoch 58: val_loss did not improve from 405.09570
196/196 - 10s - loss: 404.1814 - MinusLogProbMetric: 404.1814 - val_loss: 407.4071 - val_MinusLogProbMetric: 407.4071 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 59/1000
2023-10-04 03:29:36.321 
Epoch 59/1000 
	 loss: 403.0346, MinusLogProbMetric: 403.0346, val_loss: 406.9882, val_MinusLogProbMetric: 406.9882

Epoch 59: val_loss did not improve from 405.09570
196/196 - 10s - loss: 403.0346 - MinusLogProbMetric: 403.0346 - val_loss: 406.9882 - val_MinusLogProbMetric: 406.9882 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 60/1000
2023-10-04 03:29:46.740 
Epoch 60/1000 
	 loss: 403.2938, MinusLogProbMetric: 403.2938, val_loss: 406.8063, val_MinusLogProbMetric: 406.8063

Epoch 60: val_loss did not improve from 405.09570
196/196 - 10s - loss: 403.2938 - MinusLogProbMetric: 403.2938 - val_loss: 406.8063 - val_MinusLogProbMetric: 406.8063 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 61/1000
2023-10-04 03:29:57.052 
Epoch 61/1000 
	 loss: 402.5502, MinusLogProbMetric: 402.5502, val_loss: 407.5069, val_MinusLogProbMetric: 407.5069

Epoch 61: val_loss did not improve from 405.09570
196/196 - 10s - loss: 402.5502 - MinusLogProbMetric: 402.5502 - val_loss: 407.5069 - val_MinusLogProbMetric: 407.5069 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 62/1000
2023-10-04 03:30:07.381 
Epoch 62/1000 
	 loss: 403.2575, MinusLogProbMetric: 403.2575, val_loss: 404.0682, val_MinusLogProbMetric: 404.0682

Epoch 62: val_loss improved from 405.09570 to 404.06818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 403.2575 - MinusLogProbMetric: 403.2575 - val_loss: 404.0682 - val_MinusLogProbMetric: 404.0682 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 63/1000
2023-10-04 03:30:18.156 
Epoch 63/1000 
	 loss: 402.5690, MinusLogProbMetric: 402.5690, val_loss: 405.3759, val_MinusLogProbMetric: 405.3759

Epoch 63: val_loss did not improve from 404.06818
196/196 - 10s - loss: 402.5690 - MinusLogProbMetric: 402.5690 - val_loss: 405.3759 - val_MinusLogProbMetric: 405.3759 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 64/1000
2023-10-04 03:30:28.298 
Epoch 64/1000 
	 loss: 402.5158, MinusLogProbMetric: 402.5158, val_loss: 406.8549, val_MinusLogProbMetric: 406.8549

Epoch 64: val_loss did not improve from 404.06818
196/196 - 10s - loss: 402.5158 - MinusLogProbMetric: 402.5158 - val_loss: 406.8549 - val_MinusLogProbMetric: 406.8549 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 65/1000
2023-10-04 03:30:38.608 
Epoch 65/1000 
	 loss: 403.2002, MinusLogProbMetric: 403.2002, val_loss: 404.5350, val_MinusLogProbMetric: 404.5350

Epoch 65: val_loss did not improve from 404.06818
196/196 - 10s - loss: 403.2002 - MinusLogProbMetric: 403.2002 - val_loss: 404.5350 - val_MinusLogProbMetric: 404.5350 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 66/1000
2023-10-04 03:30:48.970 
Epoch 66/1000 
	 loss: 403.0827, MinusLogProbMetric: 403.0827, val_loss: 404.0106, val_MinusLogProbMetric: 404.0106

Epoch 66: val_loss improved from 404.06818 to 404.01062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 403.0827 - MinusLogProbMetric: 403.0827 - val_loss: 404.0106 - val_MinusLogProbMetric: 404.0106 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 67/1000
2023-10-04 03:30:59.370 
Epoch 67/1000 
	 loss: 402.9631, MinusLogProbMetric: 402.9631, val_loss: 408.1637, val_MinusLogProbMetric: 408.1637

Epoch 67: val_loss did not improve from 404.01062
196/196 - 10s - loss: 402.9631 - MinusLogProbMetric: 402.9631 - val_loss: 408.1637 - val_MinusLogProbMetric: 408.1637 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 68/1000
2023-10-04 03:31:09.581 
Epoch 68/1000 
	 loss: 402.1625, MinusLogProbMetric: 402.1625, val_loss: 406.1357, val_MinusLogProbMetric: 406.1357

Epoch 68: val_loss did not improve from 404.01062
196/196 - 10s - loss: 402.1625 - MinusLogProbMetric: 402.1625 - val_loss: 406.1357 - val_MinusLogProbMetric: 406.1357 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 69/1000
2023-10-04 03:31:19.902 
Epoch 69/1000 
	 loss: 402.8696, MinusLogProbMetric: 402.8696, val_loss: 408.4498, val_MinusLogProbMetric: 408.4498

Epoch 69: val_loss did not improve from 404.01062
196/196 - 10s - loss: 402.8696 - MinusLogProbMetric: 402.8696 - val_loss: 408.4498 - val_MinusLogProbMetric: 408.4498 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 70/1000
2023-10-04 03:31:30.201 
Epoch 70/1000 
	 loss: 402.5795, MinusLogProbMetric: 402.5795, val_loss: 406.8857, val_MinusLogProbMetric: 406.8857

Epoch 70: val_loss did not improve from 404.01062
196/196 - 10s - loss: 402.5795 - MinusLogProbMetric: 402.5795 - val_loss: 406.8857 - val_MinusLogProbMetric: 406.8857 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 71/1000
2023-10-04 03:31:40.485 
Epoch 71/1000 
	 loss: 401.6017, MinusLogProbMetric: 401.6017, val_loss: 403.7315, val_MinusLogProbMetric: 403.7315

Epoch 71: val_loss improved from 404.01062 to 403.73154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 401.6017 - MinusLogProbMetric: 401.6017 - val_loss: 403.7315 - val_MinusLogProbMetric: 403.7315 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 72/1000
2023-10-04 03:31:51.125 
Epoch 72/1000 
	 loss: 401.7182, MinusLogProbMetric: 401.7182, val_loss: 406.8478, val_MinusLogProbMetric: 406.8478

Epoch 72: val_loss did not improve from 403.73154
196/196 - 10s - loss: 401.7182 - MinusLogProbMetric: 401.7182 - val_loss: 406.8478 - val_MinusLogProbMetric: 406.8478 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 73/1000
2023-10-04 03:32:01.362 
Epoch 73/1000 
	 loss: 401.5802, MinusLogProbMetric: 401.5802, val_loss: 417.7951, val_MinusLogProbMetric: 417.7951

Epoch 73: val_loss did not improve from 403.73154
196/196 - 10s - loss: 401.5802 - MinusLogProbMetric: 401.5802 - val_loss: 417.7951 - val_MinusLogProbMetric: 417.7951 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 74/1000
2023-10-04 03:32:11.673 
Epoch 74/1000 
	 loss: 401.5440, MinusLogProbMetric: 401.5440, val_loss: 409.8945, val_MinusLogProbMetric: 409.8945

Epoch 74: val_loss did not improve from 403.73154
196/196 - 10s - loss: 401.5440 - MinusLogProbMetric: 401.5440 - val_loss: 409.8945 - val_MinusLogProbMetric: 409.8945 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 75/1000
2023-10-04 03:32:21.986 
Epoch 75/1000 
	 loss: 401.0517, MinusLogProbMetric: 401.0517, val_loss: 405.5916, val_MinusLogProbMetric: 405.5916

Epoch 75: val_loss did not improve from 403.73154
196/196 - 10s - loss: 401.0517 - MinusLogProbMetric: 401.0517 - val_loss: 405.5916 - val_MinusLogProbMetric: 405.5916 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 76/1000
2023-10-04 03:32:32.227 
Epoch 76/1000 
	 loss: 403.3313, MinusLogProbMetric: 403.3313, val_loss: 403.7471, val_MinusLogProbMetric: 403.7471

Epoch 76: val_loss did not improve from 403.73154
196/196 - 10s - loss: 403.3313 - MinusLogProbMetric: 403.3313 - val_loss: 403.7471 - val_MinusLogProbMetric: 403.7471 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 77/1000
2023-10-04 03:32:42.522 
Epoch 77/1000 
	 loss: 401.2572, MinusLogProbMetric: 401.2572, val_loss: 403.0779, val_MinusLogProbMetric: 403.0779

Epoch 77: val_loss improved from 403.73154 to 403.07791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 401.2572 - MinusLogProbMetric: 401.2572 - val_loss: 403.0779 - val_MinusLogProbMetric: 403.0779 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 78/1000
2023-10-04 03:32:53.549 
Epoch 78/1000 
	 loss: 401.3356, MinusLogProbMetric: 401.3356, val_loss: 405.8048, val_MinusLogProbMetric: 405.8048

Epoch 78: val_loss did not improve from 403.07791
196/196 - 10s - loss: 401.3356 - MinusLogProbMetric: 401.3356 - val_loss: 405.8048 - val_MinusLogProbMetric: 405.8048 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 79/1000
2023-10-04 03:33:03.807 
Epoch 79/1000 
	 loss: 401.0819, MinusLogProbMetric: 401.0819, val_loss: 403.7499, val_MinusLogProbMetric: 403.7499

Epoch 79: val_loss did not improve from 403.07791
196/196 - 10s - loss: 401.0819 - MinusLogProbMetric: 401.0819 - val_loss: 403.7499 - val_MinusLogProbMetric: 403.7499 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 80/1000
2023-10-04 03:33:14.206 
Epoch 80/1000 
	 loss: 401.6794, MinusLogProbMetric: 401.6794, val_loss: 404.4818, val_MinusLogProbMetric: 404.4818

Epoch 80: val_loss did not improve from 403.07791
196/196 - 10s - loss: 401.6794 - MinusLogProbMetric: 401.6794 - val_loss: 404.4818 - val_MinusLogProbMetric: 404.4818 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 81/1000
2023-10-04 03:33:24.575 
Epoch 81/1000 
	 loss: 400.7061, MinusLogProbMetric: 400.7061, val_loss: 407.9583, val_MinusLogProbMetric: 407.9583

Epoch 81: val_loss did not improve from 403.07791
196/196 - 10s - loss: 400.7061 - MinusLogProbMetric: 400.7061 - val_loss: 407.9583 - val_MinusLogProbMetric: 407.9583 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 82/1000
2023-10-04 03:33:34.927 
Epoch 82/1000 
	 loss: 400.5990, MinusLogProbMetric: 400.5990, val_loss: 406.6513, val_MinusLogProbMetric: 406.6513

Epoch 82: val_loss did not improve from 403.07791
196/196 - 10s - loss: 400.5990 - MinusLogProbMetric: 400.5990 - val_loss: 406.6513 - val_MinusLogProbMetric: 406.6513 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 83/1000
2023-10-04 03:33:45.277 
Epoch 83/1000 
	 loss: 400.4511, MinusLogProbMetric: 400.4511, val_loss: 405.8591, val_MinusLogProbMetric: 405.8591

Epoch 83: val_loss did not improve from 403.07791
196/196 - 10s - loss: 400.4511 - MinusLogProbMetric: 400.4511 - val_loss: 405.8591 - val_MinusLogProbMetric: 405.8591 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 84/1000
2023-10-04 03:33:55.567 
Epoch 84/1000 
	 loss: 400.4060, MinusLogProbMetric: 400.4060, val_loss: 403.7523, val_MinusLogProbMetric: 403.7523

Epoch 84: val_loss did not improve from 403.07791
196/196 - 10s - loss: 400.4060 - MinusLogProbMetric: 400.4060 - val_loss: 403.7523 - val_MinusLogProbMetric: 403.7523 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 85/1000
2023-10-04 03:34:05.826 
Epoch 85/1000 
	 loss: 400.5834, MinusLogProbMetric: 400.5834, val_loss: 409.4714, val_MinusLogProbMetric: 409.4714

Epoch 85: val_loss did not improve from 403.07791
196/196 - 10s - loss: 400.5834 - MinusLogProbMetric: 400.5834 - val_loss: 409.4714 - val_MinusLogProbMetric: 409.4714 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 86/1000
2023-10-04 03:34:16.278 
Epoch 86/1000 
	 loss: 400.9034, MinusLogProbMetric: 400.9034, val_loss: 414.8382, val_MinusLogProbMetric: 414.8382

Epoch 86: val_loss did not improve from 403.07791
196/196 - 10s - loss: 400.9034 - MinusLogProbMetric: 400.9034 - val_loss: 414.8382 - val_MinusLogProbMetric: 414.8382 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 87/1000
2023-10-04 03:34:27.398 
Epoch 87/1000 
	 loss: 400.4064, MinusLogProbMetric: 400.4064, val_loss: 409.5591, val_MinusLogProbMetric: 409.5591

Epoch 87: val_loss did not improve from 403.07791
196/196 - 11s - loss: 400.4064 - MinusLogProbMetric: 400.4064 - val_loss: 409.5591 - val_MinusLogProbMetric: 409.5591 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 88/1000
2023-10-04 03:34:38.068 
Epoch 88/1000 
	 loss: 400.2179, MinusLogProbMetric: 400.2179, val_loss: 403.9682, val_MinusLogProbMetric: 403.9682

Epoch 88: val_loss did not improve from 403.07791
196/196 - 11s - loss: 400.2179 - MinusLogProbMetric: 400.2179 - val_loss: 403.9682 - val_MinusLogProbMetric: 403.9682 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 89/1000
2023-10-04 03:34:48.249 
Epoch 89/1000 
	 loss: 399.8589, MinusLogProbMetric: 399.8589, val_loss: 402.7537, val_MinusLogProbMetric: 402.7537

Epoch 89: val_loss improved from 403.07791 to 402.75369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 399.8589 - MinusLogProbMetric: 399.8589 - val_loss: 402.7537 - val_MinusLogProbMetric: 402.7537 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 90/1000
2023-10-04 03:34:58.838 
Epoch 90/1000 
	 loss: 400.4145, MinusLogProbMetric: 400.4145, val_loss: 402.4770, val_MinusLogProbMetric: 402.4770

Epoch 90: val_loss improved from 402.75369 to 402.47696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 400.4145 - MinusLogProbMetric: 400.4145 - val_loss: 402.4770 - val_MinusLogProbMetric: 402.4770 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 91/1000
2023-10-04 03:35:09.696 
Epoch 91/1000 
	 loss: 399.5117, MinusLogProbMetric: 399.5117, val_loss: 403.7093, val_MinusLogProbMetric: 403.7093

Epoch 91: val_loss did not improve from 402.47696
196/196 - 10s - loss: 399.5117 - MinusLogProbMetric: 399.5117 - val_loss: 403.7093 - val_MinusLogProbMetric: 403.7093 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 92/1000
2023-10-04 03:35:19.987 
Epoch 92/1000 
	 loss: 399.7746, MinusLogProbMetric: 399.7746, val_loss: 404.9727, val_MinusLogProbMetric: 404.9727

Epoch 92: val_loss did not improve from 402.47696
196/196 - 10s - loss: 399.7746 - MinusLogProbMetric: 399.7746 - val_loss: 404.9727 - val_MinusLogProbMetric: 404.9727 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 93/1000
2023-10-04 03:35:30.436 
Epoch 93/1000 
	 loss: 399.8888, MinusLogProbMetric: 399.8888, val_loss: 402.5459, val_MinusLogProbMetric: 402.5459

Epoch 93: val_loss did not improve from 402.47696
196/196 - 10s - loss: 399.8888 - MinusLogProbMetric: 399.8888 - val_loss: 402.5459 - val_MinusLogProbMetric: 402.5459 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-10-04 03:35:40.830 
Epoch 94/1000 
	 loss: 399.3145, MinusLogProbMetric: 399.3145, val_loss: 407.7970, val_MinusLogProbMetric: 407.7970

Epoch 94: val_loss did not improve from 402.47696
196/196 - 10s - loss: 399.3145 - MinusLogProbMetric: 399.3145 - val_loss: 407.7970 - val_MinusLogProbMetric: 407.7970 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 95/1000
2023-10-04 03:35:51.205 
Epoch 95/1000 
	 loss: 401.2951, MinusLogProbMetric: 401.2951, val_loss: 404.6593, val_MinusLogProbMetric: 404.6593

Epoch 95: val_loss did not improve from 402.47696
196/196 - 10s - loss: 401.2951 - MinusLogProbMetric: 401.2951 - val_loss: 404.6593 - val_MinusLogProbMetric: 404.6593 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 96/1000
2023-10-04 03:36:01.496 
Epoch 96/1000 
	 loss: 398.4373, MinusLogProbMetric: 398.4373, val_loss: 410.5663, val_MinusLogProbMetric: 410.5663

Epoch 96: val_loss did not improve from 402.47696
196/196 - 10s - loss: 398.4373 - MinusLogProbMetric: 398.4373 - val_loss: 410.5663 - val_MinusLogProbMetric: 410.5663 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 97/1000
2023-10-04 03:36:11.819 
Epoch 97/1000 
	 loss: 399.5563, MinusLogProbMetric: 399.5563, val_loss: 403.4844, val_MinusLogProbMetric: 403.4844

Epoch 97: val_loss did not improve from 402.47696
196/196 - 10s - loss: 399.5563 - MinusLogProbMetric: 399.5563 - val_loss: 403.4844 - val_MinusLogProbMetric: 403.4844 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 98/1000
2023-10-04 03:36:22.105 
Epoch 98/1000 
	 loss: 399.6273, MinusLogProbMetric: 399.6273, val_loss: 404.8180, val_MinusLogProbMetric: 404.8180

Epoch 98: val_loss did not improve from 402.47696
196/196 - 10s - loss: 399.6273 - MinusLogProbMetric: 399.6273 - val_loss: 404.8180 - val_MinusLogProbMetric: 404.8180 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 99/1000
2023-10-04 03:36:32.328 
Epoch 99/1000 
	 loss: 398.7693, MinusLogProbMetric: 398.7693, val_loss: 403.6352, val_MinusLogProbMetric: 403.6352

Epoch 99: val_loss did not improve from 402.47696
196/196 - 10s - loss: 398.7693 - MinusLogProbMetric: 398.7693 - val_loss: 403.6352 - val_MinusLogProbMetric: 403.6352 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 100/1000
2023-10-04 03:36:42.735 
Epoch 100/1000 
	 loss: 398.8057, MinusLogProbMetric: 398.8057, val_loss: 402.4768, val_MinusLogProbMetric: 402.4768

Epoch 100: val_loss improved from 402.47696 to 402.47678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 398.8057 - MinusLogProbMetric: 398.8057 - val_loss: 402.4768 - val_MinusLogProbMetric: 402.4768 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 101/1000
2023-10-04 03:36:53.052 
Epoch 101/1000 
	 loss: 398.9426, MinusLogProbMetric: 398.9426, val_loss: 406.1495, val_MinusLogProbMetric: 406.1495

Epoch 101: val_loss did not improve from 402.47678
196/196 - 10s - loss: 398.9426 - MinusLogProbMetric: 398.9426 - val_loss: 406.1495 - val_MinusLogProbMetric: 406.1495 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 102/1000
2023-10-04 03:37:03.124 
Epoch 102/1000 
	 loss: 399.5786, MinusLogProbMetric: 399.5786, val_loss: 402.8763, val_MinusLogProbMetric: 402.8763

Epoch 102: val_loss did not improve from 402.47678
196/196 - 10s - loss: 399.5786 - MinusLogProbMetric: 399.5786 - val_loss: 402.8763 - val_MinusLogProbMetric: 402.8763 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 103/1000
2023-10-04 03:37:13.098 
Epoch 103/1000 
	 loss: 398.8528, MinusLogProbMetric: 398.8528, val_loss: 407.7132, val_MinusLogProbMetric: 407.7132

Epoch 103: val_loss did not improve from 402.47678
196/196 - 10s - loss: 398.8528 - MinusLogProbMetric: 398.8528 - val_loss: 407.7132 - val_MinusLogProbMetric: 407.7132 - lr: 3.3333e-04 - 10s/epoch - 51ms/step
Epoch 104/1000
2023-10-04 03:37:23.395 
Epoch 104/1000 
	 loss: 398.5558, MinusLogProbMetric: 398.5558, val_loss: 402.3646, val_MinusLogProbMetric: 402.3646

Epoch 104: val_loss improved from 402.47678 to 402.36459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 398.5558 - MinusLogProbMetric: 398.5558 - val_loss: 402.3646 - val_MinusLogProbMetric: 402.3646 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 105/1000
2023-10-04 03:37:34.274 
Epoch 105/1000 
	 loss: 398.4817, MinusLogProbMetric: 398.4817, val_loss: 403.2250, val_MinusLogProbMetric: 403.2250

Epoch 105: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.4817 - MinusLogProbMetric: 398.4817 - val_loss: 403.2250 - val_MinusLogProbMetric: 403.2250 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 106/1000
2023-10-04 03:37:44.450 
Epoch 106/1000 
	 loss: 398.8924, MinusLogProbMetric: 398.8924, val_loss: 402.7017, val_MinusLogProbMetric: 402.7017

Epoch 106: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.8924 - MinusLogProbMetric: 398.8924 - val_loss: 402.7017 - val_MinusLogProbMetric: 402.7017 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 107/1000
2023-10-04 03:37:54.609 
Epoch 107/1000 
	 loss: 398.5572, MinusLogProbMetric: 398.5572, val_loss: 407.0644, val_MinusLogProbMetric: 407.0644

Epoch 107: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.5572 - MinusLogProbMetric: 398.5572 - val_loss: 407.0644 - val_MinusLogProbMetric: 407.0644 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 108/1000
2023-10-04 03:38:04.953 
Epoch 108/1000 
	 loss: 398.9099, MinusLogProbMetric: 398.9099, val_loss: 403.3965, val_MinusLogProbMetric: 403.3965

Epoch 108: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.9099 - MinusLogProbMetric: 398.9099 - val_loss: 403.3965 - val_MinusLogProbMetric: 403.3965 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 109/1000
2023-10-04 03:38:15.087 
Epoch 109/1000 
	 loss: 399.0078, MinusLogProbMetric: 399.0078, val_loss: 403.7225, val_MinusLogProbMetric: 403.7225

Epoch 109: val_loss did not improve from 402.36459
196/196 - 10s - loss: 399.0078 - MinusLogProbMetric: 399.0078 - val_loss: 403.7225 - val_MinusLogProbMetric: 403.7225 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 110/1000
2023-10-04 03:38:25.390 
Epoch 110/1000 
	 loss: 397.8246, MinusLogProbMetric: 397.8246, val_loss: 409.9761, val_MinusLogProbMetric: 409.9761

Epoch 110: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.8246 - MinusLogProbMetric: 397.8246 - val_loss: 409.9761 - val_MinusLogProbMetric: 409.9761 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 111/1000
2023-10-04 03:38:35.807 
Epoch 111/1000 
	 loss: 398.7032, MinusLogProbMetric: 398.7032, val_loss: 405.6698, val_MinusLogProbMetric: 405.6698

Epoch 111: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.7032 - MinusLogProbMetric: 398.7032 - val_loss: 405.6698 - val_MinusLogProbMetric: 405.6698 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 112/1000
2023-10-04 03:38:45.977 
Epoch 112/1000 
	 loss: 398.0657, MinusLogProbMetric: 398.0657, val_loss: 403.8054, val_MinusLogProbMetric: 403.8054

Epoch 112: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.0657 - MinusLogProbMetric: 398.0657 - val_loss: 403.8054 - val_MinusLogProbMetric: 403.8054 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 113/1000
2023-10-04 03:38:56.191 
Epoch 113/1000 
	 loss: 397.7946, MinusLogProbMetric: 397.7946, val_loss: 402.9828, val_MinusLogProbMetric: 402.9828

Epoch 113: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.7946 - MinusLogProbMetric: 397.7946 - val_loss: 402.9828 - val_MinusLogProbMetric: 402.9828 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 114/1000
2023-10-04 03:39:06.405 
Epoch 114/1000 
	 loss: 398.0234, MinusLogProbMetric: 398.0234, val_loss: 404.3034, val_MinusLogProbMetric: 404.3034

Epoch 114: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.0234 - MinusLogProbMetric: 398.0234 - val_loss: 404.3034 - val_MinusLogProbMetric: 404.3034 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 115/1000
2023-10-04 03:39:16.611 
Epoch 115/1000 
	 loss: 398.2325, MinusLogProbMetric: 398.2325, val_loss: 402.8131, val_MinusLogProbMetric: 402.8131

Epoch 115: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.2325 - MinusLogProbMetric: 398.2325 - val_loss: 402.8131 - val_MinusLogProbMetric: 402.8131 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 116/1000
2023-10-04 03:39:26.906 
Epoch 116/1000 
	 loss: 397.4256, MinusLogProbMetric: 397.4256, val_loss: 406.6135, val_MinusLogProbMetric: 406.6135

Epoch 116: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.4256 - MinusLogProbMetric: 397.4256 - val_loss: 406.6135 - val_MinusLogProbMetric: 406.6135 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 117/1000
2023-10-04 03:39:37.175 
Epoch 117/1000 
	 loss: 397.8794, MinusLogProbMetric: 397.8794, val_loss: 403.7305, val_MinusLogProbMetric: 403.7305

Epoch 117: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.8794 - MinusLogProbMetric: 397.8794 - val_loss: 403.7305 - val_MinusLogProbMetric: 403.7305 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 118/1000
2023-10-04 03:39:47.606 
Epoch 118/1000 
	 loss: 397.5581, MinusLogProbMetric: 397.5581, val_loss: 407.3799, val_MinusLogProbMetric: 407.3799

Epoch 118: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.5581 - MinusLogProbMetric: 397.5581 - val_loss: 407.3799 - val_MinusLogProbMetric: 407.3799 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 119/1000
2023-10-04 03:39:58.000 
Epoch 119/1000 
	 loss: 397.3051, MinusLogProbMetric: 397.3051, val_loss: 404.4089, val_MinusLogProbMetric: 404.4089

Epoch 119: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.3051 - MinusLogProbMetric: 397.3051 - val_loss: 404.4089 - val_MinusLogProbMetric: 404.4089 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 120/1000
2023-10-04 03:40:08.313 
Epoch 120/1000 
	 loss: 398.3517, MinusLogProbMetric: 398.3517, val_loss: 404.2077, val_MinusLogProbMetric: 404.2077

Epoch 120: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.3517 - MinusLogProbMetric: 398.3517 - val_loss: 404.2077 - val_MinusLogProbMetric: 404.2077 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 121/1000
2023-10-04 03:40:18.607 
Epoch 121/1000 
	 loss: 397.1805, MinusLogProbMetric: 397.1805, val_loss: 406.7311, val_MinusLogProbMetric: 406.7311

Epoch 121: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.1805 - MinusLogProbMetric: 397.1805 - val_loss: 406.7311 - val_MinusLogProbMetric: 406.7311 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 122/1000
2023-10-04 03:40:29.010 
Epoch 122/1000 
	 loss: 398.5842, MinusLogProbMetric: 398.5842, val_loss: 406.9595, val_MinusLogProbMetric: 406.9595

Epoch 122: val_loss did not improve from 402.36459
196/196 - 10s - loss: 398.5842 - MinusLogProbMetric: 398.5842 - val_loss: 406.9595 - val_MinusLogProbMetric: 406.9595 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 123/1000
2023-10-04 03:40:39.357 
Epoch 123/1000 
	 loss: 397.5494, MinusLogProbMetric: 397.5494, val_loss: 402.6493, val_MinusLogProbMetric: 402.6493

Epoch 123: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.5494 - MinusLogProbMetric: 397.5494 - val_loss: 402.6493 - val_MinusLogProbMetric: 402.6493 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 124/1000
2023-10-04 03:40:50.120 
Epoch 124/1000 
	 loss: 396.6906, MinusLogProbMetric: 396.6906, val_loss: 413.2710, val_MinusLogProbMetric: 413.2710

Epoch 124: val_loss did not improve from 402.36459
196/196 - 11s - loss: 396.6906 - MinusLogProbMetric: 396.6906 - val_loss: 413.2710 - val_MinusLogProbMetric: 413.2710 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 125/1000
2023-10-04 03:41:00.983 
Epoch 125/1000 
	 loss: 398.6508, MinusLogProbMetric: 398.6508, val_loss: 403.0800, val_MinusLogProbMetric: 403.0800

Epoch 125: val_loss did not improve from 402.36459
196/196 - 11s - loss: 398.6508 - MinusLogProbMetric: 398.6508 - val_loss: 403.0800 - val_MinusLogProbMetric: 403.0800 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 126/1000
2023-10-04 03:41:11.217 
Epoch 126/1000 
	 loss: 396.7774, MinusLogProbMetric: 396.7774, val_loss: 404.8671, val_MinusLogProbMetric: 404.8671

Epoch 126: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.7774 - MinusLogProbMetric: 396.7774 - val_loss: 404.8671 - val_MinusLogProbMetric: 404.8671 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 127/1000
2023-10-04 03:41:21.428 
Epoch 127/1000 
	 loss: 397.1659, MinusLogProbMetric: 397.1659, val_loss: 402.6205, val_MinusLogProbMetric: 402.6205

Epoch 127: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.1659 - MinusLogProbMetric: 397.1659 - val_loss: 402.6205 - val_MinusLogProbMetric: 402.6205 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 128/1000
2023-10-04 03:41:31.698 
Epoch 128/1000 
	 loss: 396.9767, MinusLogProbMetric: 396.9767, val_loss: 407.0417, val_MinusLogProbMetric: 407.0417

Epoch 128: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.9767 - MinusLogProbMetric: 396.9767 - val_loss: 407.0417 - val_MinusLogProbMetric: 407.0417 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 129/1000
2023-10-04 03:41:42.095 
Epoch 129/1000 
	 loss: 397.2509, MinusLogProbMetric: 397.2509, val_loss: 402.7168, val_MinusLogProbMetric: 402.7168

Epoch 129: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.2509 - MinusLogProbMetric: 397.2509 - val_loss: 402.7168 - val_MinusLogProbMetric: 402.7168 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 130/1000
2023-10-04 03:41:52.247 
Epoch 130/1000 
	 loss: 396.7245, MinusLogProbMetric: 396.7245, val_loss: 402.6151, val_MinusLogProbMetric: 402.6151

Epoch 130: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.7245 - MinusLogProbMetric: 396.7245 - val_loss: 402.6151 - val_MinusLogProbMetric: 402.6151 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 131/1000
2023-10-04 03:42:02.608 
Epoch 131/1000 
	 loss: 396.9385, MinusLogProbMetric: 396.9385, val_loss: 405.3296, val_MinusLogProbMetric: 405.3296

Epoch 131: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.9385 - MinusLogProbMetric: 396.9385 - val_loss: 405.3296 - val_MinusLogProbMetric: 405.3296 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 132/1000
2023-10-04 03:42:12.845 
Epoch 132/1000 
	 loss: 396.5475, MinusLogProbMetric: 396.5475, val_loss: 404.0607, val_MinusLogProbMetric: 404.0607

Epoch 132: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.5475 - MinusLogProbMetric: 396.5475 - val_loss: 404.0607 - val_MinusLogProbMetric: 404.0607 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 133/1000
2023-10-04 03:42:23.265 
Epoch 133/1000 
	 loss: 396.4276, MinusLogProbMetric: 396.4276, val_loss: 406.5517, val_MinusLogProbMetric: 406.5517

Epoch 133: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.4276 - MinusLogProbMetric: 396.4276 - val_loss: 406.5517 - val_MinusLogProbMetric: 406.5517 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 134/1000
2023-10-04 03:42:33.658 
Epoch 134/1000 
	 loss: 396.9700, MinusLogProbMetric: 396.9700, val_loss: 403.5456, val_MinusLogProbMetric: 403.5456

Epoch 134: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.9700 - MinusLogProbMetric: 396.9700 - val_loss: 403.5456 - val_MinusLogProbMetric: 403.5456 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 135/1000
2023-10-04 03:42:43.849 
Epoch 135/1000 
	 loss: 397.1358, MinusLogProbMetric: 397.1358, val_loss: 403.2662, val_MinusLogProbMetric: 403.2662

Epoch 135: val_loss did not improve from 402.36459
196/196 - 10s - loss: 397.1358 - MinusLogProbMetric: 397.1358 - val_loss: 403.2662 - val_MinusLogProbMetric: 403.2662 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 136/1000
2023-10-04 03:42:54.230 
Epoch 136/1000 
	 loss: 396.5932, MinusLogProbMetric: 396.5932, val_loss: 402.6355, val_MinusLogProbMetric: 402.6355

Epoch 136: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.5932 - MinusLogProbMetric: 396.5932 - val_loss: 402.6355 - val_MinusLogProbMetric: 402.6355 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 137/1000
2023-10-04 03:43:04.591 
Epoch 137/1000 
	 loss: 396.0094, MinusLogProbMetric: 396.0094, val_loss: 402.9199, val_MinusLogProbMetric: 402.9199

Epoch 137: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.0094 - MinusLogProbMetric: 396.0094 - val_loss: 402.9199 - val_MinusLogProbMetric: 402.9199 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 138/1000
2023-10-04 03:43:14.795 
Epoch 138/1000 
	 loss: 396.8611, MinusLogProbMetric: 396.8611, val_loss: 403.7172, val_MinusLogProbMetric: 403.7172

Epoch 138: val_loss did not improve from 402.36459
196/196 - 10s - loss: 396.8611 - MinusLogProbMetric: 396.8611 - val_loss: 403.7172 - val_MinusLogProbMetric: 403.7172 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 139/1000
2023-10-04 03:43:25.109 
Epoch 139/1000 
	 loss: 396.5595, MinusLogProbMetric: 396.5595, val_loss: 401.9965, val_MinusLogProbMetric: 401.9965

Epoch 139: val_loss improved from 402.36459 to 401.99646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 396.5595 - MinusLogProbMetric: 396.5595 - val_loss: 401.9965 - val_MinusLogProbMetric: 401.9965 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 140/1000
2023-10-04 03:43:35.592 
Epoch 140/1000 
	 loss: 396.0143, MinusLogProbMetric: 396.0143, val_loss: 422.2973, val_MinusLogProbMetric: 422.2973

Epoch 140: val_loss did not improve from 401.99646
196/196 - 10s - loss: 396.0143 - MinusLogProbMetric: 396.0143 - val_loss: 422.2973 - val_MinusLogProbMetric: 422.2973 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 141/1000
2023-10-04 03:43:45.910 
Epoch 141/1000 
	 loss: 397.0619, MinusLogProbMetric: 397.0619, val_loss: 403.1450, val_MinusLogProbMetric: 403.1450

Epoch 141: val_loss did not improve from 401.99646
196/196 - 10s - loss: 397.0619 - MinusLogProbMetric: 397.0619 - val_loss: 403.1450 - val_MinusLogProbMetric: 403.1450 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 142/1000
2023-10-04 03:43:56.322 
Epoch 142/1000 
	 loss: 395.5168, MinusLogProbMetric: 395.5168, val_loss: 413.2551, val_MinusLogProbMetric: 413.2551

Epoch 142: val_loss did not improve from 401.99646
196/196 - 10s - loss: 395.5168 - MinusLogProbMetric: 395.5168 - val_loss: 413.2551 - val_MinusLogProbMetric: 413.2551 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 143/1000
2023-10-04 03:44:06.705 
Epoch 143/1000 
	 loss: 395.9682, MinusLogProbMetric: 395.9682, val_loss: 401.8345, val_MinusLogProbMetric: 401.8345

Epoch 143: val_loss improved from 401.99646 to 401.83447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 395.9682 - MinusLogProbMetric: 395.9682 - val_loss: 401.8345 - val_MinusLogProbMetric: 401.8345 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 144/1000
2023-10-04 03:44:17.388 
Epoch 144/1000 
	 loss: 396.1873, MinusLogProbMetric: 396.1873, val_loss: 402.9335, val_MinusLogProbMetric: 402.9335

Epoch 144: val_loss did not improve from 401.83447
196/196 - 10s - loss: 396.1873 - MinusLogProbMetric: 396.1873 - val_loss: 402.9335 - val_MinusLogProbMetric: 402.9335 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 145/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 92: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-04 03:44:22.989 
Epoch 145/1000 
	 loss: nan, MinusLogProbMetric: 1417123922444288.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 145: val_loss did not improve from 401.83447
196/196 - 6s - loss: nan - MinusLogProbMetric: 1417123922444288.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 6s/epoch - 29ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 356.
===========
Train data generated in 0.75 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_356/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_356/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_356
self.data_kwargs: {'seed': 926}
self.x_data: [[7.8542657  4.4182515  5.3175983  ... 3.8539157  8.104709   6.7665896 ]
 [8.06671    5.087758   5.108798   ... 2.7856822  7.9267573  7.1754713 ]
 [5.9009757  0.26195854 4.63319    ... 5.096487   6.4324465  2.782943  ]
 ...
 [5.7334824  0.57263243 4.688254   ... 4.731608   6.577419   3.9972575 ]
 [5.8251185  0.91118604 4.8986015  ... 4.9331145  6.9940166  4.170423  ]
 [5.469847   7.5352907  5.8636546  ... 8.857828   2.8293529  6.7579055 ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_168 (InputLayer)      [(None, 1000)]            0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  9018400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7fbad7288130>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fbb10f00310>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fbb10f00310>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fbb086c5fc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fbb084d3010>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fbb084d3670>, <keras.callbacks.ModelCheckpoint object at 0x7fbb084d3730>, <keras.callbacks.EarlyStopping object at 0x7fbb084d39a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fbb084d39d0>, <keras.callbacks.TerminateOnNaN object at 0x7fbb084d3610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.890381  ,  5.1094627 ,  5.320669  , ...,  2.9114172 ,
         7.98736   ,  6.274715  ],
       [ 8.64889   ,  4.9066677 ,  5.1673865 , ...,  3.4661267 ,
         8.383892  ,  6.9187694 ],
       [ 5.994749  , -0.32748276,  4.8254414 , ...,  4.5099654 ,
         6.14343   ,  4.413803  ],
       ...,
       [ 8.669559  ,  4.9627995 ,  5.2309265 , ...,  2.5445173 ,
         8.967615  ,  7.207417  ],
       [ 7.7727075 ,  4.7435365 ,  5.241729  , ...,  5.3062873 ,
         8.059448  ,  7.321834  ],
       [ 8.550159  ,  4.8899236 ,  5.337554  , ...,  3.817484  ,
         8.130934  ,  6.345315  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 356/360 with hyperparameters:
timestamp = 2023-10-04 03:44:26.058403
ndims = 1000
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.85426569e+00  4.41825151e+00  5.31759834e+00  2.60887671e+00
  6.41178656e+00  2.13207030e+00  5.74208021e+00  1.84043932e+00
  1.69763756e+00  4.54294538e+00  3.11069012e+00  2.37418079e+00
  2.54046345e+00  1.62041679e-01  9.34055448e-02  2.05885458e+00
  5.24263716e+00  7.68947506e+00  4.56616545e+00  9.68113995e+00
  2.26570651e-01  1.77447128e+00  3.29641533e+00  6.17277622e+00
  8.20737648e+00  3.43033528e+00  2.55136061e+00  5.97014785e-01
  8.28674316e+00  3.98282480e+00  5.93197346e+00  1.06245613e+01
  9.49379206e-01  1.63838947e+00  7.06365585e+00  7.94115591e+00
  7.53974438e+00  5.61496639e+00  9.80136871e+00  3.15971422e+00
  6.21875811e+00 -9.54353809e-03  4.26025915e+00  7.39404678e+00
  2.21483946e+00  9.84185696e+00  7.24155045e+00  3.30283046e+00
  7.74830770e+00  9.08916414e-01  9.71360779e+00  6.58768594e-01
  8.35239983e+00  2.32051826e+00  7.30240631e+00 -8.83946955e-01
  3.03193045e+00  7.32728100e+00  3.74522591e+00  6.38918638e-01
  3.79207659e+00  4.00098944e+00  5.66429996e+00  5.85767746e+00
  8.47701550e+00  5.04558372e+00  3.84120035e+00  1.24103403e+00
  7.94665098e+00  7.07438660e+00  3.71784091e+00  6.12205219e+00
  6.66771460e+00  2.12367511e+00 -9.60745335e-01  3.74312615e+00
  5.94952774e+00  4.78941059e+00  6.61200619e+00  8.09659863e+00
  4.87202358e+00  2.70784044e+00  9.03696346e+00  5.63844013e+00
  7.53536034e+00  7.15711594e+00  9.41573143e+00  2.42348766e+00
  7.42718220e+00  3.36646557e+00  3.73524284e+00  1.32205629e+00
  3.59988236e+00  9.48313904e+00  1.93991113e+00  2.75920010e+00
  5.01511669e+00  3.30604935e+00  5.06413269e+00  7.73147202e+00
  2.30829501e+00  4.39429140e+00  6.82420444e+00  6.79297352e+00
  9.61048603e+00  6.80823135e+00  7.26028967e+00  9.56877518e+00
  3.79452395e+00  4.15086555e+00 -2.60416776e-01  1.66208482e+00
  3.06919241e+00  9.93813455e-01  7.33946943e+00  5.69343281e+00
  5.63299084e+00  7.84667826e+00  2.26034689e+00  3.43404484e+00
  7.00420618e+00  3.05020475e+00 -7.25471079e-02  3.36938071e+00
  9.09626770e+00  4.11609411e+00  2.85297346e+00  9.84171486e+00
  6.31026506e+00  4.01078224e+00  1.21615183e+00  7.14538717e+00
  6.33219719e+00  9.81515121e+00  2.49839687e+00  6.05599999e-01
  7.24084187e+00  2.75359464e+00  2.50772285e+00  4.09680367e+00
  8.30523300e+00  6.01748800e+00  4.66583490e-01  4.52141905e+00
  5.89312696e+00  5.51575184e+00  5.69468737e+00  1.91655934e+00
  8.48398876e+00  5.63048315e+00  7.34936619e+00  9.74814224e+00
  3.48965788e+00  8.88919735e+00  9.60582942e-02  1.66400623e+00
  5.99077606e+00  3.11164737e+00  2.75282717e+00  2.60087824e+00
  2.64426851e+00  7.44998503e+00  5.85604477e+00  7.84842968e+00
  8.28961468e+00  8.40444946e+00  4.12819386e+00  4.58752203e+00
  9.98953819e+00  6.43158293e+00  5.55915296e-01  5.72953415e+00
  4.68271637e+00  9.90587902e+00 -8.83717835e-01  5.94018364e+00
  7.98547173e+00  2.84753513e+00  9.44544220e+00  9.53206062e+00
  9.07542324e+00  6.41640186e-01  6.17845011e+00  3.03050613e+00
  8.23334789e+00  6.30862665e+00  5.43327713e+00  1.29359305e+00
  3.25533032e+00  6.31515408e+00  1.03074417e+01  5.75671196e+00
  9.49255085e+00  9.50892544e+00  6.79545212e+00  4.71281195e+00
  3.50111294e+00  9.18763638e+00  4.49652076e-01  1.14515615e+00
  2.40005207e+00  7.48550272e+00  3.02852464e+00  6.13900280e+00
  7.07990456e+00  4.02328634e+00  5.32941818e+00  5.56215000e+00
  9.59676170e+00  6.84960365e+00  6.14611101e+00  7.01535404e-01
  9.08370018e+00  5.82812595e+00  7.74594402e+00  6.01755202e-01
  3.96469641e+00  2.35852933e+00  2.45156860e+00  8.29704666e+00
  6.53238392e+00  4.18872261e+00 -6.58399761e-01  9.65485573e+00
  9.80379200e+00  1.98804641e+00 -2.41938114e-01  5.28435040e+00
  9.89102745e+00  8.85664177e+00  4.39452076e+00  3.62702298e+00
  2.62890053e+00  8.97751904e+00  6.36042309e+00  1.33491313e+00
  2.77312636e+00  2.77005053e+00  1.88997674e+00  5.14272547e+00
  8.35700703e+00  6.68333673e+00  1.91080523e+00 -7.04806745e-01
  1.96866465e+00  3.76307774e+00  7.95638275e+00  9.92790127e+00
  3.73964596e+00  7.23839140e+00  9.96518135e+00  7.50328255e+00
  6.60035801e+00  8.07895303e-01  3.62572813e+00  9.14682579e+00
  2.86480689e+00  8.59156322e+00  7.98565483e+00  4.01899433e+00
  4.55118990e+00  5.28791046e+00  7.33071327e+00  5.27818727e+00
  8.85012031e-01  7.58879042e+00  3.19855785e+00  9.85190296e+00
  2.27252841e+00  2.54628229e+00  2.68598413e+00  5.91966152e+00
  8.05706692e+00  1.65711355e+00  1.67708433e+00  6.08699369e+00
  4.28031015e+00  5.60096979e+00  6.40451479e+00  1.06371555e+01
  5.36539984e+00  6.85709620e+00  8.92924786e+00  3.32924414e+00
  2.92118073e+00  6.10644913e+00  5.32210112e+00  9.56133556e+00
  4.79739636e-01  9.31261921e+00  5.15149164e+00  1.02548325e+00
  2.37651587e+00  6.48900795e+00  4.59978104e+00  9.12724590e+00
  4.42423201e+00  4.07489395e+00  8.31999683e+00  5.35771799e+00
  6.72994852e+00  7.68310690e+00  6.69467068e+00  4.67777395e+00
  6.99015021e-01  4.14359522e+00  8.36619568e+00  3.67691922e+00
  9.04692936e+00  4.49189425e+00  6.32296133e+00  8.04836750e+00
  4.41118336e+00  8.30536187e-01  9.79877853e+00  2.71371508e+00
  5.12246799e+00  4.35286856e+00  5.49793339e+00  3.22211933e+00
  7.62125874e+00  1.31961989e+00  1.10699387e+01  8.59894085e+00
  5.24368238e+00  4.61356401e+00  3.45124292e+00  5.36580896e+00
  8.41000843e+00  2.77613783e+00  4.29664612e+00  1.90059173e+00
  3.43093586e+00  2.01634741e+00  1.07569265e+01  5.71786928e+00
  7.56941891e+00  1.09229112e+00  2.52849770e+00  8.91465664e+00
  9.52245593e-01  1.17926216e+00  6.83719635e+00  6.82608938e+00
  7.53233051e+00  1.01758318e+01  1.57473803e+00  4.45485210e+00
  4.28795290e+00  1.64892748e-01  1.81490600e+00  2.76026893e+00
  4.74909878e+00  3.11362815e+00  8.00123119e+00  3.19517183e+00
  9.50560093e+00  1.03588438e+01  4.30221748e+00  6.96166873e-01
  3.03467369e+00  7.79732800e+00  2.67352748e+00  3.50514221e+00
  5.08668947e+00  7.38722420e+00  9.63334274e+00  2.62670517e+00
  4.05484772e+00  2.31513214e+00  2.64107323e+00  2.18680382e-01
  4.29812479e+00  8.41921329e+00  4.19078636e+00  3.98049355e+00
  8.93853092e+00  7.08515024e+00  5.22109699e+00  7.64430141e+00
  2.03226995e+00  3.94330883e+00  1.36629438e+00  2.33601141e+00
  3.89655709e-01  3.61157084e+00  4.44274712e+00  9.43988323e+00
  2.50003672e+00  5.12204170e+00  8.95978451e+00  7.98188019e+00
  7.24716902e+00  2.88545752e+00  4.57083321e+00  2.54753900e+00
  1.84977734e+00  4.41161156e+00  7.37936544e+00  5.14107323e+00
  7.78119135e+00  8.40401840e+00  1.04103088e-02  5.93060875e+00
  5.76526022e+00  9.53269196e+00  7.67608523e-01  6.57992554e+00
  1.95678222e+00  5.49429357e-01  5.23544788e+00  7.59472132e+00
  7.29799318e+00 -1.20772636e+00  4.30894661e+00  4.65965390e-01
  2.60425591e+00  1.97165930e+00  1.86237991e+00  2.43441749e+00
  1.47088110e+00  9.78774834e+00  3.94406772e+00  8.90942097e+00
  1.22915721e+00  7.60721827e+00  8.06412756e-01  4.82014179e+00
  5.46107650e-01  7.98665380e+00  9.84689426e+00  5.38293898e-01
  8.54449463e+00  8.00841713e+00  3.39391780e+00  9.08403206e+00
  2.01289654e+00  4.27781343e+00  9.96862292e-01  6.87161112e+00
 -9.90051091e-01  4.79714537e+00  6.87879229e+00  8.84089375e+00
  2.07931614e+00  8.38031006e+00  5.88058186e+00  5.93655205e+00
  5.64989281e+00  6.21854544e-01  6.68466759e+00  4.26185799e+00
  8.59449577e+00  3.42455196e+00  5.77914000e+00  4.20325327e+00
  8.32858276e+00  2.54622722e+00  3.04043859e-01  8.33305359e+00
  4.29302406e+00  5.85027218e+00  3.44100654e-01  1.02574406e+01
  9.13591194e+00  6.44509363e+00  2.23200083e+00  3.65651631e+00
  9.79177284e+00  6.80626631e+00  1.36182153e+00  4.29894447e+00
  9.91862965e+00  6.15662336e+00  8.39294195e-02  5.73490715e+00
  3.36321139e+00  9.89277542e-01  4.06990290e+00  4.61008358e+00
 -7.16332495e-01  6.55716562e+00  5.19925833e+00  7.17309570e+00
  4.03727007e+00  5.57426548e+00  6.92208004e+00  8.72025681e+00
  3.84482765e+00  6.02073288e+00  7.86660480e+00 -5.73046148e-01
  7.67636967e+00  3.58273864e+00  7.47981644e+00  5.36535072e+00
  7.16784859e+00  7.85914564e+00  1.76731563e+00  6.72368050e+00
  8.51148510e+00  1.61090291e+00  6.08754253e+00  6.35778236e+00
  9.91369438e+00  2.55167031e+00  1.66099370e+00  3.64644337e+00
  2.42382288e+00  7.45638466e+00  7.33168173e+00  6.45663881e+00
  2.01684594e+00  8.60406494e+00  1.10975132e+01  5.38561678e+00
  5.44827414e+00  4.94278336e+00  1.86474085e+00  1.97823441e+00
  4.30962515e+00  1.54604924e+00  9.07867241e+00  7.96630621e+00
  5.26732063e+00  3.58175659e+00  9.51251125e+00  1.04863870e+00
  5.07879162e+00  1.05271053e+00  1.87499964e+00  6.65355206e+00
  6.66385412e+00  6.33094311e+00  8.71027946e+00  9.82982922e+00
  6.55875683e+00  5.20563507e+00  8.00854397e+00  2.41466331e+00
  7.00420952e+00  4.88513613e+00  1.61634290e+00  9.31071377e+00
  3.35792899e+00  5.86867237e+00  8.60531521e+00  5.60204506e+00
  1.54713285e+00  7.74277306e+00  3.66394973e+00  9.42229176e+00
  8.48596764e+00  6.48666763e+00  3.23076034e+00  7.68814230e+00
  9.35188389e+00  4.08446217e+00  5.02816010e+00  3.38081270e-01
  6.36648226e+00  8.51817703e+00  2.80446982e+00  3.20807362e+00
  4.51574850e+00  5.93633711e-01  9.46782529e-01  2.16139364e+00
  2.60937262e+00  5.28547049e+00  4.84176254e+00  9.81929684e+00
  7.88902140e+00  1.66553545e+00  8.82484341e+00  7.09167838e-01
  5.03990793e+00  2.42374206e+00  2.91808867e+00  7.50122595e+00
  4.18691397e+00  9.02093601e+00  7.10776806e+00  9.57430172e+00
  5.59825611e+00  8.02043629e+00  3.25235939e+00  4.47115898e+00
  4.30808973e+00  3.80606079e+00  1.12467217e+00  5.29313755e+00
  1.50810623e+00  8.85228539e+00 -4.06661093e-01  5.10796404e+00
  3.78489470e+00  7.05098629e-01  7.91552544e+00  6.63939571e+00
  1.46795654e+00  6.65862942e+00  8.33073235e+00  8.50840759e+00
  1.30628014e+00  6.92401600e+00  7.72493744e+00  8.51644993e+00
  5.43707657e+00  4.83005619e+00  8.90700626e+00  8.11882019e+00
  5.73599482e+00  2.80633116e+00  9.65988350e+00  4.51777983e+00
  9.21329618e-01  4.88313866e+00  9.77910519e+00  4.40435410e+00
 -3.27194422e-01  6.12769270e+00  9.89487934e+00  7.31696033e+00
  6.35276508e+00  9.55050945e+00  8.07680893e+00  6.34605026e+00
  4.77183819e+00  7.15873861e+00  8.06319714e+00  5.61736882e-01
  7.67201567e+00  3.85464573e+00  6.69436502e+00  7.86791897e+00
  4.83945560e+00  1.62425935e+00  4.19299459e+00  9.26880896e-01
  6.88019085e+00  9.74193478e+00  8.84968662e+00  3.65187788e+00
  6.33452368e+00  3.75078416e+00  4.67366219e+00  3.54604197e+00
  8.16938019e+00  2.72105742e+00  9.75844955e+00  6.57792234e+00
  2.28564978e-01  4.99803901e-01  1.02100105e+01  8.88365149e-01
  3.53124762e+00  1.72824395e+00  9.85693932e+00  5.20375729e+00
  5.34282923e+00  6.05618429e+00  2.67098927e+00  5.57222414e+00
  4.29868984e+00  1.93069673e+00  6.95643902e+00  3.56284547e+00
  7.73937511e+00  4.35557556e+00  5.20385551e+00  7.79240131e+00
  4.75181484e+00  9.50519943e+00  8.01542187e+00  8.55453110e+00
  9.82915688e+00  3.75745511e+00  5.48529482e+00  7.18655014e+00
  6.00739861e+00  1.01745186e+01  2.86505723e+00  3.40535784e+00
  8.88479424e+00  7.66841078e+00  2.33434486e+00  6.26787615e+00
  7.08852863e+00  2.91594267e+00  5.48495722e+00  5.74615335e+00
  4.40000677e+00  7.55737638e+00 -8.34494978e-02  4.98475075e+00
  3.07788324e+00  5.51033068e+00  7.23143101e+00  7.69217253e-01
  1.29760706e+00  6.85467720e+00  7.40828228e+00  5.57670832e+00
  9.81540394e+00  5.34229517e+00  6.78031504e-01  6.85619688e+00
  5.78618479e+00  5.76328468e+00  5.48964214e+00  8.51025105e+00
  1.36436164e+00  1.25749195e+00  1.53151178e+00  2.31167674e+00
  5.93736410e+00  6.02586079e+00  2.10990143e+00  3.74104166e+00
  4.49994564e+00  4.30085993e+00  8.55859375e+00  4.00177526e+00
  6.96290779e+00  1.65811288e+00  5.44697285e+00  8.24122715e+00
  1.78438163e+00  4.33162642e+00  5.78468227e+00  2.05096912e+00
  7.39810944e+00  1.59008896e+00  8.76768684e+00  4.69711876e+00
  4.04790258e+00  1.24405193e+00  2.83710051e+00  6.33698320e+00
  1.33927023e+00  3.15551066e+00  9.67255783e+00  6.40811014e+00
  2.06277108e+00  2.24199677e+00  8.81563950e+00  3.05780077e+00
  1.87078035e+00  9.16118813e+00  3.53771424e+00  1.65168977e+00
  5.19690895e+00  4.59403133e+00  9.05195808e+00  1.09238386e+00
  1.04556847e+00  8.68545175e-01  9.68726254e+00  5.04982138e+00
  2.69507694e+00  5.98866844e+00  4.93495703e+00  2.08913898e+00
  6.94658279e+00  2.16514921e+00  2.17784786e+00  5.89404345e+00
  2.47186089e+00  3.07393289e+00  9.32638359e+00  1.50767267e+00
  3.44327784e+00  3.86774182e+00  2.78355002e+00  2.11643291e+00
  8.07786083e+00  3.65396214e+00 -2.35649347e-01  5.44787359e+00
  8.69826198e-01  2.77893639e+00  9.38866138e+00  3.60090685e+00
  5.78489542e+00  5.85346889e+00  2.58838773e-01  6.52211618e+00
  9.64319706e+00  8.25448573e-01  5.58079672e+00  7.69353580e+00
  8.50068855e+00  1.93690157e+00  2.22951698e+00  6.91337490e+00
  3.25202256e-01  7.09594727e+00  8.97951889e+00  4.25646448e+00
  4.87320185e+00  3.06530523e+00  6.00203633e-01  1.48462224e+00
  8.84535408e+00  8.94914532e+00  1.44960713e+00  9.55700684e+00
  3.84997725e+00  2.66661191e+00  9.85832453e-01  9.06340885e+00
  9.97899628e+00  3.32584810e+00  6.95174551e+00  3.19924808e+00
 -1.51607394e-01  4.97948170e+00  5.40087318e+00  1.11839056e+00
  8.18654537e+00  5.18570328e+00  2.13246346e+00  6.35160780e+00
  8.03942680e+00  3.31687546e+00  8.01695156e+00  4.56763649e+00
  9.25021172e+00  7.31373024e+00  3.17991471e+00  1.24928892e-01
  5.86790705e+00  2.48283291e+00  1.13595665e+00  9.80613613e+00
  1.05967150e+01  2.69507051e+00  3.01809096e+00  1.64316809e+00
  4.14577913e+00  8.58723545e+00  6.37656307e+00  8.65282822e+00
  1.23624945e+00  6.83144760e+00  9.12307358e+00  2.77780533e+00
  6.90577936e+00  3.74633145e+00  2.03699780e+00  9.43050480e+00
  3.88212705e+00  1.03552914e+00  3.94930816e+00  3.95189595e+00
  7.43439579e+00  8.88795757e+00  7.81051493e+00  9.53783226e+00
  4.64317465e+00  7.03867435e+00  3.24562931e+00  9.52776432e-01
  3.66223240e+00  7.14639282e+00  3.33262825e+00  7.86202145e+00
  7.74663401e+00  1.78269148e+00  2.58091092e+00  8.37615490e+00
  3.35916471e+00  3.20344758e+00  3.59131765e+00  8.01384544e+00
 -2.43629217e-02  7.42378175e-01  5.56940269e+00  4.90014553e+00
  6.48054647e+00  3.32681608e+00  9.08345699e+00  7.95236826e+00
  6.34802222e-01  6.87756348e+00  9.49590397e+00  1.01780138e+01
  7.24633646e+00  1.21293211e+00  6.68064976e+00  3.20778751e+00
  1.00395985e+01  7.21336079e+00  6.02273321e+00  6.93427706e+00
  4.42568588e+00  4.00593805e+00  9.61300564e+00  3.34010863e+00
  5.83148003e-01  4.11973238e+00  8.45203018e+00  6.38991165e+00
  1.04141502e+01  7.16550827e-01  8.68817711e+00  4.16127014e+00
  3.57046032e+00  2.75189900e+00  1.17473054e+00  3.15659904e+00
  7.35806704e+00  4.37037677e-01  1.24299979e+00 -1.54236555e-02
  8.57396030e+00  5.91643810e+00  1.83133376e+00  7.77049971e+00
  1.06167221e+01  9.25513077e+00  9.58188915e+00 -5.68707407e-01
  5.92560768e+00  2.55895495e+00  5.55885506e+00  2.23880720e+00
  1.15490997e+00  5.06085348e+00  5.79838181e+00  1.02773275e+01
 -8.31137300e-02  3.83537984e+00  8.54463768e+00  5.31149960e+00
  8.12730598e+00  6.55586195e+00  7.92252541e+00  5.34070778e+00
  2.95670152e+00  8.34237576e+00  5.68467236e+00  1.28557813e+00
  8.03099155e+00  9.61913395e+00  4.18909931e+00  3.39216042e+00
  6.58131719e-01  6.45680571e+00  8.95029449e+00  1.41747653e+00
  4.75601006e+00  2.90228367e-01  5.12265801e-01  6.72031784e+00
  8.77206898e+00  3.71332788e+00  8.48167229e+00  1.21904695e+00
  1.87795568e+00  9.97635460e+00  4.99329329e+00  6.38564539e+00
  2.54908800e-01  1.29785419e+00  5.73886812e-01  8.37132168e+00
  1.09254289e+00  8.28661823e+00  3.38485265e+00  7.73170853e+00
  4.09941435e+00  2.49423814e+00  1.25597119e+00  2.96604967e+00
  3.78301477e+00  8.97054768e+00  4.46885729e+00  9.85267830e+00
  5.88528252e+00 -5.40345371e-01  4.29398537e+00  7.70487690e+00
  1.43975925e+00  8.53642845e+00  5.70689344e+00 -4.84625697e-02
  7.89377451e+00  5.98359823e+00  8.12033236e-01  4.98253584e+00
  3.64299822e+00  9.16150284e+00  2.32410598e+00  9.28726101e+00
  5.53273964e+00  3.85391569e+00  8.10470867e+00  6.76658964e+00]
Epoch 1/1000
2023-10-04 03:45:01.265 
Epoch 1/1000 
	 loss: 404.6476, MinusLogProbMetric: 404.6476, val_loss: 398.9497, val_MinusLogProbMetric: 398.9497

Epoch 1: val_loss improved from inf to 398.94971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 35s - loss: 404.6476 - MinusLogProbMetric: 404.6476 - val_loss: 398.9497 - val_MinusLogProbMetric: 398.9497 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 2/1000
2023-10-04 03:45:11.790 
Epoch 2/1000 
	 loss: 390.2107, MinusLogProbMetric: 390.2107, val_loss: 399.6396, val_MinusLogProbMetric: 399.6396

Epoch 2: val_loss did not improve from 398.94971
196/196 - 10s - loss: 390.2107 - MinusLogProbMetric: 390.2107 - val_loss: 399.6396 - val_MinusLogProbMetric: 399.6396 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 3/1000
2023-10-04 03:45:22.061 
Epoch 3/1000 
	 loss: 390.4488, MinusLogProbMetric: 390.4488, val_loss: 399.0638, val_MinusLogProbMetric: 399.0638

Epoch 3: val_loss did not improve from 398.94971
196/196 - 10s - loss: 390.4488 - MinusLogProbMetric: 390.4488 - val_loss: 399.0638 - val_MinusLogProbMetric: 399.0638 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 4/1000
2023-10-04 03:45:32.352 
Epoch 4/1000 
	 loss: 390.2328, MinusLogProbMetric: 390.2328, val_loss: 403.5958, val_MinusLogProbMetric: 403.5958

Epoch 4: val_loss did not improve from 398.94971
196/196 - 10s - loss: 390.2328 - MinusLogProbMetric: 390.2328 - val_loss: 403.5958 - val_MinusLogProbMetric: 403.5958 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 5/1000
2023-10-04 03:45:42.473 
Epoch 5/1000 
	 loss: 390.2921, MinusLogProbMetric: 390.2921, val_loss: 398.6877, val_MinusLogProbMetric: 398.6877

Epoch 5: val_loss improved from 398.94971 to 398.68774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 390.2921 - MinusLogProbMetric: 390.2921 - val_loss: 398.6877 - val_MinusLogProbMetric: 398.6877 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 6/1000
2023-10-04 03:45:53.173 
Epoch 6/1000 
	 loss: 390.4521, MinusLogProbMetric: 390.4521, val_loss: 398.7358, val_MinusLogProbMetric: 398.7358

Epoch 6: val_loss did not improve from 398.68774
196/196 - 10s - loss: 390.4521 - MinusLogProbMetric: 390.4521 - val_loss: 398.7358 - val_MinusLogProbMetric: 398.7358 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 7/1000
2023-10-04 03:46:03.523 
Epoch 7/1000 
	 loss: 390.0070, MinusLogProbMetric: 390.0070, val_loss: 398.6107, val_MinusLogProbMetric: 398.6107

Epoch 7: val_loss improved from 398.68774 to 398.61066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 390.0070 - MinusLogProbMetric: 390.0070 - val_loss: 398.6107 - val_MinusLogProbMetric: 398.6107 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 8/1000
2023-10-04 03:46:14.169 
Epoch 8/1000 
	 loss: 390.1225, MinusLogProbMetric: 390.1225, val_loss: 398.2883, val_MinusLogProbMetric: 398.2883

Epoch 8: val_loss improved from 398.61066 to 398.28830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 390.1225 - MinusLogProbMetric: 390.1225 - val_loss: 398.2883 - val_MinusLogProbMetric: 398.2883 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 9/1000
2023-10-04 03:46:24.732 
Epoch 9/1000 
	 loss: 389.8467, MinusLogProbMetric: 389.8467, val_loss: 400.2698, val_MinusLogProbMetric: 400.2698

Epoch 9: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.8467 - MinusLogProbMetric: 389.8467 - val_loss: 400.2698 - val_MinusLogProbMetric: 400.2698 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 10/1000
2023-10-04 03:46:34.863 
Epoch 10/1000 
	 loss: 389.9353, MinusLogProbMetric: 389.9353, val_loss: 398.8051, val_MinusLogProbMetric: 398.8051

Epoch 10: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.9353 - MinusLogProbMetric: 389.9353 - val_loss: 398.8051 - val_MinusLogProbMetric: 398.8051 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 11/1000
2023-10-04 03:46:44.981 
Epoch 11/1000 
	 loss: 389.8371, MinusLogProbMetric: 389.8371, val_loss: 399.1550, val_MinusLogProbMetric: 399.1550

Epoch 11: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.8371 - MinusLogProbMetric: 389.8371 - val_loss: 399.1550 - val_MinusLogProbMetric: 399.1550 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 12/1000
2023-10-04 03:46:55.075 
Epoch 12/1000 
	 loss: 389.8361, MinusLogProbMetric: 389.8361, val_loss: 398.3688, val_MinusLogProbMetric: 398.3688

Epoch 12: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.8361 - MinusLogProbMetric: 389.8361 - val_loss: 398.3688 - val_MinusLogProbMetric: 398.3688 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 13/1000
2023-10-04 03:47:05.722 
Epoch 13/1000 
	 loss: 389.9325, MinusLogProbMetric: 389.9325, val_loss: 398.8789, val_MinusLogProbMetric: 398.8789

Epoch 13: val_loss did not improve from 398.28830
196/196 - 11s - loss: 389.9325 - MinusLogProbMetric: 389.9325 - val_loss: 398.8789 - val_MinusLogProbMetric: 398.8789 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 14/1000
2023-10-04 03:47:16.777 
Epoch 14/1000 
	 loss: 389.4348, MinusLogProbMetric: 389.4348, val_loss: 398.9716, val_MinusLogProbMetric: 398.9716

Epoch 14: val_loss did not improve from 398.28830
196/196 - 11s - loss: 389.4348 - MinusLogProbMetric: 389.4348 - val_loss: 398.9716 - val_MinusLogProbMetric: 398.9716 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 15/1000
2023-10-04 03:47:27.168 
Epoch 15/1000 
	 loss: 389.6870, MinusLogProbMetric: 389.6870, val_loss: 398.3189, val_MinusLogProbMetric: 398.3189

Epoch 15: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.6870 - MinusLogProbMetric: 389.6870 - val_loss: 398.3189 - val_MinusLogProbMetric: 398.3189 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 16/1000
2023-10-04 03:47:37.477 
Epoch 16/1000 
	 loss: 389.3589, MinusLogProbMetric: 389.3589, val_loss: 403.0067, val_MinusLogProbMetric: 403.0067

Epoch 16: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.3589 - MinusLogProbMetric: 389.3589 - val_loss: 403.0067 - val_MinusLogProbMetric: 403.0067 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 17/1000
2023-10-04 03:47:47.646 
Epoch 17/1000 
	 loss: 389.4353, MinusLogProbMetric: 389.4353, val_loss: 398.8957, val_MinusLogProbMetric: 398.8957

Epoch 17: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.4353 - MinusLogProbMetric: 389.4353 - val_loss: 398.8957 - val_MinusLogProbMetric: 398.8957 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 18/1000
2023-10-04 03:47:57.799 
Epoch 18/1000 
	 loss: 389.4445, MinusLogProbMetric: 389.4445, val_loss: 399.1312, val_MinusLogProbMetric: 399.1312

Epoch 18: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.4445 - MinusLogProbMetric: 389.4445 - val_loss: 399.1312 - val_MinusLogProbMetric: 399.1312 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 19/1000
2023-10-04 03:48:07.856 
Epoch 19/1000 
	 loss: 389.8400, MinusLogProbMetric: 389.8400, val_loss: 399.8039, val_MinusLogProbMetric: 399.8039

Epoch 19: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.8400 - MinusLogProbMetric: 389.8400 - val_loss: 399.8039 - val_MinusLogProbMetric: 399.8039 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 20/1000
2023-10-04 03:48:18.115 
Epoch 20/1000 
	 loss: 389.2198, MinusLogProbMetric: 389.2198, val_loss: 398.9620, val_MinusLogProbMetric: 398.9620

Epoch 20: val_loss did not improve from 398.28830
196/196 - 10s - loss: 389.2198 - MinusLogProbMetric: 389.2198 - val_loss: 398.9620 - val_MinusLogProbMetric: 398.9620 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 21/1000
2023-10-04 03:48:28.450 
Epoch 21/1000 
	 loss: 389.3218, MinusLogProbMetric: 389.3218, val_loss: 398.1244, val_MinusLogProbMetric: 398.1244

Epoch 21: val_loss improved from 398.28830 to 398.12442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 389.3218 - MinusLogProbMetric: 389.3218 - val_loss: 398.1244 - val_MinusLogProbMetric: 398.1244 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 22/1000
2023-10-04 03:48:39.019 
Epoch 22/1000 
	 loss: 389.4719, MinusLogProbMetric: 389.4719, val_loss: 399.5021, val_MinusLogProbMetric: 399.5021

Epoch 22: val_loss did not improve from 398.12442
196/196 - 10s - loss: 389.4719 - MinusLogProbMetric: 389.4719 - val_loss: 399.5021 - val_MinusLogProbMetric: 399.5021 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 23/1000
2023-10-04 03:48:49.211 
Epoch 23/1000 
	 loss: 389.1799, MinusLogProbMetric: 389.1799, val_loss: 400.2141, val_MinusLogProbMetric: 400.2141

Epoch 23: val_loss did not improve from 398.12442
196/196 - 10s - loss: 389.1799 - MinusLogProbMetric: 389.1799 - val_loss: 400.2141 - val_MinusLogProbMetric: 400.2141 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 24/1000
2023-10-04 03:48:59.415 
Epoch 24/1000 
	 loss: 388.9995, MinusLogProbMetric: 388.9995, val_loss: 398.0927, val_MinusLogProbMetric: 398.0927

Epoch 24: val_loss improved from 398.12442 to 398.09271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 388.9995 - MinusLogProbMetric: 388.9995 - val_loss: 398.0927 - val_MinusLogProbMetric: 398.0927 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 25/1000
2023-10-04 03:49:10.076 
Epoch 25/1000 
	 loss: 389.0344, MinusLogProbMetric: 389.0344, val_loss: 398.4747, val_MinusLogProbMetric: 398.4747

Epoch 25: val_loss did not improve from 398.09271
196/196 - 10s - loss: 389.0344 - MinusLogProbMetric: 389.0344 - val_loss: 398.4747 - val_MinusLogProbMetric: 398.4747 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 26/1000
2023-10-04 03:49:20.168 
Epoch 26/1000 
	 loss: 388.9701, MinusLogProbMetric: 388.9701, val_loss: 398.2919, val_MinusLogProbMetric: 398.2919

Epoch 26: val_loss did not improve from 398.09271
196/196 - 10s - loss: 388.9701 - MinusLogProbMetric: 388.9701 - val_loss: 398.2919 - val_MinusLogProbMetric: 398.2919 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 27/1000
2023-10-04 03:49:30.388 
Epoch 27/1000 
	 loss: 389.0080, MinusLogProbMetric: 389.0080, val_loss: 400.7425, val_MinusLogProbMetric: 400.7425

Epoch 27: val_loss did not improve from 398.09271
196/196 - 10s - loss: 389.0080 - MinusLogProbMetric: 389.0080 - val_loss: 400.7425 - val_MinusLogProbMetric: 400.7425 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 28/1000
2023-10-04 03:49:40.748 
Epoch 28/1000 
	 loss: 389.3356, MinusLogProbMetric: 389.3356, val_loss: 397.9076, val_MinusLogProbMetric: 397.9076

Epoch 28: val_loss improved from 398.09271 to 397.90759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 389.3356 - MinusLogProbMetric: 389.3356 - val_loss: 397.9076 - val_MinusLogProbMetric: 397.9076 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 29/1000
2023-10-04 03:49:51.409 
Epoch 29/1000 
	 loss: 388.9291, MinusLogProbMetric: 388.9291, val_loss: 399.1560, val_MinusLogProbMetric: 399.1560

Epoch 29: val_loss did not improve from 397.90759
196/196 - 10s - loss: 388.9291 - MinusLogProbMetric: 388.9291 - val_loss: 399.1560 - val_MinusLogProbMetric: 399.1560 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 30/1000
2023-10-04 03:50:01.341 
Epoch 30/1000 
	 loss: 388.7566, MinusLogProbMetric: 388.7566, val_loss: 398.9616, val_MinusLogProbMetric: 398.9616

Epoch 30: val_loss did not improve from 397.90759
196/196 - 10s - loss: 388.7566 - MinusLogProbMetric: 388.7566 - val_loss: 398.9616 - val_MinusLogProbMetric: 398.9616 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 31/1000
2023-10-04 03:50:11.514 
Epoch 31/1000 
	 loss: 388.5659, MinusLogProbMetric: 388.5659, val_loss: 398.9788, val_MinusLogProbMetric: 398.9788

Epoch 31: val_loss did not improve from 397.90759
196/196 - 10s - loss: 388.5659 - MinusLogProbMetric: 388.5659 - val_loss: 398.9788 - val_MinusLogProbMetric: 398.9788 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 32/1000
2023-10-04 03:50:21.840 
Epoch 32/1000 
	 loss: 388.8842, MinusLogProbMetric: 388.8842, val_loss: 400.6947, val_MinusLogProbMetric: 400.6947

Epoch 32: val_loss did not improve from 397.90759
196/196 - 10s - loss: 388.8842 - MinusLogProbMetric: 388.8842 - val_loss: 400.6947 - val_MinusLogProbMetric: 400.6947 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 33/1000
2023-10-04 03:50:32.041 
Epoch 33/1000 
	 loss: 388.6801, MinusLogProbMetric: 388.6801, val_loss: 399.6839, val_MinusLogProbMetric: 399.6839

Epoch 33: val_loss did not improve from 397.90759
196/196 - 10s - loss: 388.6801 - MinusLogProbMetric: 388.6801 - val_loss: 399.6839 - val_MinusLogProbMetric: 399.6839 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 34/1000
2023-10-04 03:50:42.338 
Epoch 34/1000 
	 loss: 388.7290, MinusLogProbMetric: 388.7290, val_loss: 399.8273, val_MinusLogProbMetric: 399.8273

Epoch 34: val_loss did not improve from 397.90759
196/196 - 10s - loss: 388.7290 - MinusLogProbMetric: 388.7290 - val_loss: 399.8273 - val_MinusLogProbMetric: 399.8273 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 35/1000
2023-10-04 03:50:52.521 
Epoch 35/1000 
	 loss: 388.6374, MinusLogProbMetric: 388.6374, val_loss: 400.2126, val_MinusLogProbMetric: 400.2126

Epoch 35: val_loss did not improve from 397.90759
196/196 - 10s - loss: 388.6374 - MinusLogProbMetric: 388.6374 - val_loss: 400.2126 - val_MinusLogProbMetric: 400.2126 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 36/1000
2023-10-04 03:51:02.713 
Epoch 36/1000 
	 loss: 388.5656, MinusLogProbMetric: 388.5656, val_loss: 397.7102, val_MinusLogProbMetric: 397.7102

Epoch 36: val_loss improved from 397.90759 to 397.71024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 388.5656 - MinusLogProbMetric: 388.5656 - val_loss: 397.7102 - val_MinusLogProbMetric: 397.7102 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 37/1000
2023-10-04 03:51:13.130 
Epoch 37/1000 
	 loss: 388.4533, MinusLogProbMetric: 388.4533, val_loss: 398.1025, val_MinusLogProbMetric: 398.1025

Epoch 37: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.4533 - MinusLogProbMetric: 388.4533 - val_loss: 398.1025 - val_MinusLogProbMetric: 398.1025 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 38/1000
2023-10-04 03:51:23.129 
Epoch 38/1000 
	 loss: 388.6111, MinusLogProbMetric: 388.6111, val_loss: 398.2546, val_MinusLogProbMetric: 398.2546

Epoch 38: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.6111 - MinusLogProbMetric: 388.6111 - val_loss: 398.2546 - val_MinusLogProbMetric: 398.2546 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 39/1000
2023-10-04 03:51:33.399 
Epoch 39/1000 
	 loss: 388.4999, MinusLogProbMetric: 388.4999, val_loss: 400.5183, val_MinusLogProbMetric: 400.5183

Epoch 39: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.4999 - MinusLogProbMetric: 388.4999 - val_loss: 400.5183 - val_MinusLogProbMetric: 400.5183 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 40/1000
2023-10-04 03:51:43.675 
Epoch 40/1000 
	 loss: 388.4574, MinusLogProbMetric: 388.4574, val_loss: 398.3476, val_MinusLogProbMetric: 398.3476

Epoch 40: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.4574 - MinusLogProbMetric: 388.4574 - val_loss: 398.3476 - val_MinusLogProbMetric: 398.3476 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 41/1000
2023-10-04 03:51:53.948 
Epoch 41/1000 
	 loss: 388.5842, MinusLogProbMetric: 388.5842, val_loss: 399.2890, val_MinusLogProbMetric: 399.2890

Epoch 41: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.5842 - MinusLogProbMetric: 388.5842 - val_loss: 399.2890 - val_MinusLogProbMetric: 399.2890 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 42/1000
2023-10-04 03:52:04.118 
Epoch 42/1000 
	 loss: 388.6053, MinusLogProbMetric: 388.6053, val_loss: 398.2672, val_MinusLogProbMetric: 398.2672

Epoch 42: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.6053 - MinusLogProbMetric: 388.6053 - val_loss: 398.2672 - val_MinusLogProbMetric: 398.2672 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 43/1000
2023-10-04 03:52:14.404 
Epoch 43/1000 
	 loss: 388.2167, MinusLogProbMetric: 388.2167, val_loss: 398.7138, val_MinusLogProbMetric: 398.7138

Epoch 43: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.2167 - MinusLogProbMetric: 388.2167 - val_loss: 398.7138 - val_MinusLogProbMetric: 398.7138 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 44/1000
2023-10-04 03:52:24.627 
Epoch 44/1000 
	 loss: 388.1338, MinusLogProbMetric: 388.1338, val_loss: 399.2183, val_MinusLogProbMetric: 399.2183

Epoch 44: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.1338 - MinusLogProbMetric: 388.1338 - val_loss: 399.2183 - val_MinusLogProbMetric: 399.2183 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 45/1000
2023-10-04 03:52:34.905 
Epoch 45/1000 
	 loss: 388.4932, MinusLogProbMetric: 388.4932, val_loss: 398.2226, val_MinusLogProbMetric: 398.2226

Epoch 45: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.4932 - MinusLogProbMetric: 388.4932 - val_loss: 398.2226 - val_MinusLogProbMetric: 398.2226 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 46/1000
2023-10-04 03:52:45.078 
Epoch 46/1000 
	 loss: 388.3034, MinusLogProbMetric: 388.3034, val_loss: 398.8723, val_MinusLogProbMetric: 398.8723

Epoch 46: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.3034 - MinusLogProbMetric: 388.3034 - val_loss: 398.8723 - val_MinusLogProbMetric: 398.8723 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 47/1000
2023-10-04 03:52:55.098 
Epoch 47/1000 
	 loss: 388.0575, MinusLogProbMetric: 388.0575, val_loss: 399.3981, val_MinusLogProbMetric: 399.3981

Epoch 47: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.0575 - MinusLogProbMetric: 388.0575 - val_loss: 399.3981 - val_MinusLogProbMetric: 399.3981 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 48/1000
2023-10-04 03:53:05.269 
Epoch 48/1000 
	 loss: 388.3000, MinusLogProbMetric: 388.3000, val_loss: 398.8287, val_MinusLogProbMetric: 398.8287

Epoch 48: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.3000 - MinusLogProbMetric: 388.3000 - val_loss: 398.8287 - val_MinusLogProbMetric: 398.8287 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 49/1000
2023-10-04 03:53:15.452 
Epoch 49/1000 
	 loss: 388.0134, MinusLogProbMetric: 388.0134, val_loss: 400.0417, val_MinusLogProbMetric: 400.0417

Epoch 49: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.0134 - MinusLogProbMetric: 388.0134 - val_loss: 400.0417 - val_MinusLogProbMetric: 400.0417 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 50/1000
2023-10-04 03:53:25.702 
Epoch 50/1000 
	 loss: 387.9926, MinusLogProbMetric: 387.9926, val_loss: 398.6150, val_MinusLogProbMetric: 398.6150

Epoch 50: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.9926 - MinusLogProbMetric: 387.9926 - val_loss: 398.6150 - val_MinusLogProbMetric: 398.6150 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 51/1000
2023-10-04 03:53:36.918 
Epoch 51/1000 
	 loss: 388.2532, MinusLogProbMetric: 388.2532, val_loss: 399.1512, val_MinusLogProbMetric: 399.1512

Epoch 51: val_loss did not improve from 397.71024
196/196 - 11s - loss: 388.2532 - MinusLogProbMetric: 388.2532 - val_loss: 399.1512 - val_MinusLogProbMetric: 399.1512 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 52/1000
2023-10-04 03:53:47.703 
Epoch 52/1000 
	 loss: 388.2854, MinusLogProbMetric: 388.2854, val_loss: 398.0917, val_MinusLogProbMetric: 398.0917

Epoch 52: val_loss did not improve from 397.71024
196/196 - 11s - loss: 388.2854 - MinusLogProbMetric: 388.2854 - val_loss: 398.0917 - val_MinusLogProbMetric: 398.0917 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 53/1000
2023-10-04 03:53:58.091 
Epoch 53/1000 
	 loss: 388.3114, MinusLogProbMetric: 388.3114, val_loss: 399.2909, val_MinusLogProbMetric: 399.2909

Epoch 53: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.3114 - MinusLogProbMetric: 388.3114 - val_loss: 399.2909 - val_MinusLogProbMetric: 399.2909 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 54/1000
2023-10-04 03:54:08.329 
Epoch 54/1000 
	 loss: 387.7438, MinusLogProbMetric: 387.7438, val_loss: 398.8482, val_MinusLogProbMetric: 398.8482

Epoch 54: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.7438 - MinusLogProbMetric: 387.7438 - val_loss: 398.8482 - val_MinusLogProbMetric: 398.8482 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 55/1000
2023-10-04 03:54:18.513 
Epoch 55/1000 
	 loss: 387.7720, MinusLogProbMetric: 387.7720, val_loss: 398.5483, val_MinusLogProbMetric: 398.5483

Epoch 55: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.7720 - MinusLogProbMetric: 387.7720 - val_loss: 398.5483 - val_MinusLogProbMetric: 398.5483 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 56/1000
2023-10-04 03:54:28.680 
Epoch 56/1000 
	 loss: 388.0134, MinusLogProbMetric: 388.0134, val_loss: 398.7155, val_MinusLogProbMetric: 398.7155

Epoch 56: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.0134 - MinusLogProbMetric: 388.0134 - val_loss: 398.7155 - val_MinusLogProbMetric: 398.7155 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 57/1000
2023-10-04 03:54:38.900 
Epoch 57/1000 
	 loss: 388.0085, MinusLogProbMetric: 388.0085, val_loss: 400.5635, val_MinusLogProbMetric: 400.5635

Epoch 57: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.0085 - MinusLogProbMetric: 388.0085 - val_loss: 400.5635 - val_MinusLogProbMetric: 400.5635 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 58/1000
2023-10-04 03:54:49.185 
Epoch 58/1000 
	 loss: 387.8212, MinusLogProbMetric: 387.8212, val_loss: 399.5750, val_MinusLogProbMetric: 399.5750

Epoch 58: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.8212 - MinusLogProbMetric: 387.8212 - val_loss: 399.5750 - val_MinusLogProbMetric: 399.5750 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 59/1000
2023-10-04 03:54:59.406 
Epoch 59/1000 
	 loss: 387.7568, MinusLogProbMetric: 387.7568, val_loss: 401.2630, val_MinusLogProbMetric: 401.2630

Epoch 59: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.7568 - MinusLogProbMetric: 387.7568 - val_loss: 401.2630 - val_MinusLogProbMetric: 401.2630 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 60/1000
2023-10-04 03:55:09.798 
Epoch 60/1000 
	 loss: 387.9528, MinusLogProbMetric: 387.9528, val_loss: 398.2217, val_MinusLogProbMetric: 398.2217

Epoch 60: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.9528 - MinusLogProbMetric: 387.9528 - val_loss: 398.2217 - val_MinusLogProbMetric: 398.2217 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 61/1000
2023-10-04 03:55:20.015 
Epoch 61/1000 
	 loss: 387.7519, MinusLogProbMetric: 387.7519, val_loss: 398.7013, val_MinusLogProbMetric: 398.7013

Epoch 61: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.7519 - MinusLogProbMetric: 387.7519 - val_loss: 398.7013 - val_MinusLogProbMetric: 398.7013 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 62/1000
2023-10-04 03:55:30.365 
Epoch 62/1000 
	 loss: 387.7933, MinusLogProbMetric: 387.7933, val_loss: 398.4926, val_MinusLogProbMetric: 398.4926

Epoch 62: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.7933 - MinusLogProbMetric: 387.7933 - val_loss: 398.4926 - val_MinusLogProbMetric: 398.4926 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 63/1000
2023-10-04 03:55:40.769 
Epoch 63/1000 
	 loss: 387.9572, MinusLogProbMetric: 387.9572, val_loss: 398.3515, val_MinusLogProbMetric: 398.3515

Epoch 63: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.9572 - MinusLogProbMetric: 387.9572 - val_loss: 398.3515 - val_MinusLogProbMetric: 398.3515 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 64/1000
2023-10-04 03:55:50.965 
Epoch 64/1000 
	 loss: 387.6257, MinusLogProbMetric: 387.6257, val_loss: 398.4389, val_MinusLogProbMetric: 398.4389

Epoch 64: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.6257 - MinusLogProbMetric: 387.6257 - val_loss: 398.4389 - val_MinusLogProbMetric: 398.4389 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 65/1000
2023-10-04 03:56:01.253 
Epoch 65/1000 
	 loss: 387.6314, MinusLogProbMetric: 387.6314, val_loss: 402.6957, val_MinusLogProbMetric: 402.6957

Epoch 65: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.6314 - MinusLogProbMetric: 387.6314 - val_loss: 402.6957 - val_MinusLogProbMetric: 402.6957 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 66/1000
2023-10-04 03:56:11.537 
Epoch 66/1000 
	 loss: 387.4850, MinusLogProbMetric: 387.4850, val_loss: 399.2808, val_MinusLogProbMetric: 399.2808

Epoch 66: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.4850 - MinusLogProbMetric: 387.4850 - val_loss: 399.2808 - val_MinusLogProbMetric: 399.2808 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 67/1000
2023-10-04 03:56:21.702 
Epoch 67/1000 
	 loss: 387.4927, MinusLogProbMetric: 387.4927, val_loss: 398.9856, val_MinusLogProbMetric: 398.9856

Epoch 67: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.4927 - MinusLogProbMetric: 387.4927 - val_loss: 398.9856 - val_MinusLogProbMetric: 398.9856 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 68/1000
2023-10-04 03:56:31.954 
Epoch 68/1000 
	 loss: 387.6351, MinusLogProbMetric: 387.6351, val_loss: 401.5292, val_MinusLogProbMetric: 401.5292

Epoch 68: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.6351 - MinusLogProbMetric: 387.6351 - val_loss: 401.5292 - val_MinusLogProbMetric: 401.5292 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 69/1000
2023-10-04 03:56:42.259 
Epoch 69/1000 
	 loss: 387.5373, MinusLogProbMetric: 387.5373, val_loss: 398.6918, val_MinusLogProbMetric: 398.6918

Epoch 69: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.5373 - MinusLogProbMetric: 387.5373 - val_loss: 398.6918 - val_MinusLogProbMetric: 398.6918 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 70/1000
2023-10-04 03:56:52.442 
Epoch 70/1000 
	 loss: 387.6121, MinusLogProbMetric: 387.6121, val_loss: 398.7706, val_MinusLogProbMetric: 398.7706

Epoch 70: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.6121 - MinusLogProbMetric: 387.6121 - val_loss: 398.7706 - val_MinusLogProbMetric: 398.7706 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 71/1000
2023-10-04 03:57:02.804 
Epoch 71/1000 
	 loss: 388.0650, MinusLogProbMetric: 388.0650, val_loss: 398.3323, val_MinusLogProbMetric: 398.3323

Epoch 71: val_loss did not improve from 397.71024
196/196 - 10s - loss: 388.0650 - MinusLogProbMetric: 388.0650 - val_loss: 398.3323 - val_MinusLogProbMetric: 398.3323 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 72/1000
2023-10-04 03:57:13.070 
Epoch 72/1000 
	 loss: 387.1226, MinusLogProbMetric: 387.1226, val_loss: 398.6662, val_MinusLogProbMetric: 398.6662

Epoch 72: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.1226 - MinusLogProbMetric: 387.1226 - val_loss: 398.6662 - val_MinusLogProbMetric: 398.6662 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 73/1000
2023-10-04 03:57:23.127 
Epoch 73/1000 
	 loss: 387.6495, MinusLogProbMetric: 387.6495, val_loss: 399.8155, val_MinusLogProbMetric: 399.8155

Epoch 73: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.6495 - MinusLogProbMetric: 387.6495 - val_loss: 399.8155 - val_MinusLogProbMetric: 399.8155 - lr: 1.1111e-04 - 10s/epoch - 51ms/step
Epoch 74/1000
2023-10-04 03:57:33.414 
Epoch 74/1000 
	 loss: 387.4081, MinusLogProbMetric: 387.4081, val_loss: 399.4025, val_MinusLogProbMetric: 399.4025

Epoch 74: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.4081 - MinusLogProbMetric: 387.4081 - val_loss: 399.4025 - val_MinusLogProbMetric: 399.4025 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 75/1000
2023-10-04 03:57:43.775 
Epoch 75/1000 
	 loss: 387.4161, MinusLogProbMetric: 387.4161, val_loss: 399.4329, val_MinusLogProbMetric: 399.4329

Epoch 75: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.4161 - MinusLogProbMetric: 387.4161 - val_loss: 399.4329 - val_MinusLogProbMetric: 399.4329 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 76/1000
2023-10-04 03:57:53.938 
Epoch 76/1000 
	 loss: 387.4592, MinusLogProbMetric: 387.4592, val_loss: 402.0717, val_MinusLogProbMetric: 402.0717

Epoch 76: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.4592 - MinusLogProbMetric: 387.4592 - val_loss: 402.0717 - val_MinusLogProbMetric: 402.0717 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 77/1000
2023-10-04 03:58:04.306 
Epoch 77/1000 
	 loss: 387.3935, MinusLogProbMetric: 387.3935, val_loss: 399.3689, val_MinusLogProbMetric: 399.3689

Epoch 77: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.3935 - MinusLogProbMetric: 387.3935 - val_loss: 399.3689 - val_MinusLogProbMetric: 399.3689 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 78/1000
2023-10-04 03:58:14.446 
Epoch 78/1000 
	 loss: 387.1558, MinusLogProbMetric: 387.1558, val_loss: 403.7651, val_MinusLogProbMetric: 403.7651

Epoch 78: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.1558 - MinusLogProbMetric: 387.1558 - val_loss: 403.7651 - val_MinusLogProbMetric: 403.7651 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 79/1000
2023-10-04 03:58:24.768 
Epoch 79/1000 
	 loss: 387.4736, MinusLogProbMetric: 387.4736, val_loss: 399.3944, val_MinusLogProbMetric: 399.3944

Epoch 79: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.4736 - MinusLogProbMetric: 387.4736 - val_loss: 399.3944 - val_MinusLogProbMetric: 399.3944 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 80/1000
2023-10-04 03:58:34.962 
Epoch 80/1000 
	 loss: 387.2699, MinusLogProbMetric: 387.2699, val_loss: 399.2163, val_MinusLogProbMetric: 399.2163

Epoch 80: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.2699 - MinusLogProbMetric: 387.2699 - val_loss: 399.2163 - val_MinusLogProbMetric: 399.2163 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 81/1000
2023-10-04 03:58:45.099 
Epoch 81/1000 
	 loss: 387.0028, MinusLogProbMetric: 387.0028, val_loss: 399.0321, val_MinusLogProbMetric: 399.0321

Epoch 81: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.0028 - MinusLogProbMetric: 387.0028 - val_loss: 399.0321 - val_MinusLogProbMetric: 399.0321 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 82/1000
2023-10-04 03:58:55.315 
Epoch 82/1000 
	 loss: 387.1952, MinusLogProbMetric: 387.1952, val_loss: 399.8887, val_MinusLogProbMetric: 399.8887

Epoch 82: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.1952 - MinusLogProbMetric: 387.1952 - val_loss: 399.8887 - val_MinusLogProbMetric: 399.8887 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 83/1000
2023-10-04 03:59:05.696 
Epoch 83/1000 
	 loss: 387.0914, MinusLogProbMetric: 387.0914, val_loss: 400.5462, val_MinusLogProbMetric: 400.5462

Epoch 83: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.0914 - MinusLogProbMetric: 387.0914 - val_loss: 400.5462 - val_MinusLogProbMetric: 400.5462 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 84/1000
2023-10-04 03:59:16.020 
Epoch 84/1000 
	 loss: 387.0846, MinusLogProbMetric: 387.0846, val_loss: 401.0007, val_MinusLogProbMetric: 401.0007

Epoch 84: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.0846 - MinusLogProbMetric: 387.0846 - val_loss: 401.0007 - val_MinusLogProbMetric: 401.0007 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 85/1000
2023-10-04 03:59:26.252 
Epoch 85/1000 
	 loss: 387.3975, MinusLogProbMetric: 387.3975, val_loss: 399.3691, val_MinusLogProbMetric: 399.3691

Epoch 85: val_loss did not improve from 397.71024
196/196 - 10s - loss: 387.3975 - MinusLogProbMetric: 387.3975 - val_loss: 399.3691 - val_MinusLogProbMetric: 399.3691 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 86/1000
2023-10-04 03:59:36.424 
Epoch 86/1000 
	 loss: 386.9267, MinusLogProbMetric: 386.9267, val_loss: 399.6000, val_MinusLogProbMetric: 399.6000

Epoch 86: val_loss did not improve from 397.71024
196/196 - 10s - loss: 386.9267 - MinusLogProbMetric: 386.9267 - val_loss: 399.6000 - val_MinusLogProbMetric: 399.6000 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 87/1000
2023-10-04 03:59:46.772 
Epoch 87/1000 
	 loss: 384.4119, MinusLogProbMetric: 384.4119, val_loss: 397.6298, val_MinusLogProbMetric: 397.6298

Epoch 87: val_loss improved from 397.71024 to 397.62982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 384.4119 - MinusLogProbMetric: 384.4119 - val_loss: 397.6298 - val_MinusLogProbMetric: 397.6298 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 88/1000
2023-10-04 03:59:57.805 
Epoch 88/1000 
	 loss: 384.1941, MinusLogProbMetric: 384.1941, val_loss: 397.7461, val_MinusLogProbMetric: 397.7461

Epoch 88: val_loss did not improve from 397.62982
196/196 - 11s - loss: 384.1941 - MinusLogProbMetric: 384.1941 - val_loss: 397.7461 - val_MinusLogProbMetric: 397.7461 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 89/1000
2023-10-04 04:00:08.212 
Epoch 89/1000 
	 loss: 384.3207, MinusLogProbMetric: 384.3207, val_loss: 397.2697, val_MinusLogProbMetric: 397.2697

Epoch 89: val_loss improved from 397.62982 to 397.26974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 384.3207 - MinusLogProbMetric: 384.3207 - val_loss: 397.2697 - val_MinusLogProbMetric: 397.2697 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 90/1000
2023-10-04 04:00:19.145 
Epoch 90/1000 
	 loss: 384.3531, MinusLogProbMetric: 384.3531, val_loss: 397.7706, val_MinusLogProbMetric: 397.7706

Epoch 90: val_loss did not improve from 397.26974
196/196 - 10s - loss: 384.3531 - MinusLogProbMetric: 384.3531 - val_loss: 397.7706 - val_MinusLogProbMetric: 397.7706 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 91/1000
2023-10-04 04:00:29.446 
Epoch 91/1000 
	 loss: 384.3277, MinusLogProbMetric: 384.3277, val_loss: 397.2425, val_MinusLogProbMetric: 397.2425

Epoch 91: val_loss improved from 397.26974 to 397.24246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 11s - loss: 384.3277 - MinusLogProbMetric: 384.3277 - val_loss: 397.2425 - val_MinusLogProbMetric: 397.2425 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 92/1000
2023-10-04 04:00:40.077 
Epoch 92/1000 
	 loss: 384.1688, MinusLogProbMetric: 384.1688, val_loss: 397.6630, val_MinusLogProbMetric: 397.6630

Epoch 92: val_loss did not improve from 397.24246
196/196 - 10s - loss: 384.1688 - MinusLogProbMetric: 384.1688 - val_loss: 397.6630 - val_MinusLogProbMetric: 397.6630 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 93/1000
2023-10-04 04:00:50.487 
Epoch 93/1000 
	 loss: 384.1562, MinusLogProbMetric: 384.1562, val_loss: 397.3722, val_MinusLogProbMetric: 397.3722

Epoch 93: val_loss did not improve from 397.24246
196/196 - 10s - loss: 384.1562 - MinusLogProbMetric: 384.1562 - val_loss: 397.3722 - val_MinusLogProbMetric: 397.3722 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-10-04 04:01:00.882 
Epoch 94/1000 
	 loss: 384.2002, MinusLogProbMetric: 384.2002, val_loss: 397.2493, val_MinusLogProbMetric: 397.2493

Epoch 94: val_loss did not improve from 397.24246
196/196 - 10s - loss: 384.2002 - MinusLogProbMetric: 384.2002 - val_loss: 397.2493 - val_MinusLogProbMetric: 397.2493 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 95/1000
2023-10-04 04:01:10.430 
Epoch 95/1000 
	 loss: 384.2032, MinusLogProbMetric: 384.2032, val_loss: 397.1437, val_MinusLogProbMetric: 397.1437

Epoch 95: val_loss improved from 397.24246 to 397.14371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_356/weights/best_weights.h5
196/196 - 10s - loss: 384.2032 - MinusLogProbMetric: 384.2032 - val_loss: 397.1437 - val_MinusLogProbMetric: 397.1437 - lr: 5.5556e-05 - 10s/epoch - 50ms/step
Epoch 96/1000
2023-10-04 04:01:20.176 
Epoch 96/1000 
	 loss: 384.4170, MinusLogProbMetric: 384.4170, val_loss: 398.0342, val_MinusLogProbMetric: 398.0342

Epoch 96: val_loss did not improve from 397.14371
196/196 - 9s - loss: 384.4170 - MinusLogProbMetric: 384.4170 - val_loss: 398.0342 - val_MinusLogProbMetric: 398.0342 - lr: 5.5556e-05 - 9s/epoch - 48ms/step
Epoch 97/1000
2023-10-04 04:01:29.756 
Epoch 97/1000 
	 loss: 384.1535, MinusLogProbMetric: 384.1535, val_loss: 397.7492, val_MinusLogProbMetric: 397.7492

Epoch 97: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.1535 - MinusLogProbMetric: 384.1535 - val_loss: 397.7492 - val_MinusLogProbMetric: 397.7492 - lr: 5.5556e-05 - 10s/epoch - 49ms/step
Epoch 98/1000
2023-10-04 04:01:39.922 
Epoch 98/1000 
	 loss: 384.2298, MinusLogProbMetric: 384.2298, val_loss: 397.7864, val_MinusLogProbMetric: 397.7864

Epoch 98: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.2298 - MinusLogProbMetric: 384.2298 - val_loss: 397.7864 - val_MinusLogProbMetric: 397.7864 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 99/1000
2023-10-04 04:01:50.036 
Epoch 99/1000 
	 loss: 384.1606, MinusLogProbMetric: 384.1606, val_loss: 398.1151, val_MinusLogProbMetric: 398.1151

Epoch 99: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.1606 - MinusLogProbMetric: 384.1606 - val_loss: 398.1151 - val_MinusLogProbMetric: 398.1151 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 100/1000
2023-10-04 04:02:00.194 
Epoch 100/1000 
	 loss: 384.3286, MinusLogProbMetric: 384.3286, val_loss: 397.8029, val_MinusLogProbMetric: 397.8029

Epoch 100: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.3286 - MinusLogProbMetric: 384.3286 - val_loss: 397.8029 - val_MinusLogProbMetric: 397.8029 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 101/1000
2023-10-04 04:02:10.331 
Epoch 101/1000 
	 loss: 384.1028, MinusLogProbMetric: 384.1028, val_loss: 397.7113, val_MinusLogProbMetric: 397.7113

Epoch 101: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.1028 - MinusLogProbMetric: 384.1028 - val_loss: 397.7113 - val_MinusLogProbMetric: 397.7113 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 102/1000
2023-10-04 04:02:20.425 
Epoch 102/1000 
	 loss: 384.2368, MinusLogProbMetric: 384.2368, val_loss: 397.3917, val_MinusLogProbMetric: 397.3917

Epoch 102: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.2368 - MinusLogProbMetric: 384.2368 - val_loss: 397.3917 - val_MinusLogProbMetric: 397.3917 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 103/1000
2023-10-04 04:02:30.752 
Epoch 103/1000 
	 loss: 384.2593, MinusLogProbMetric: 384.2593, val_loss: 397.3685, val_MinusLogProbMetric: 397.3685

Epoch 103: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.2593 - MinusLogProbMetric: 384.2593 - val_loss: 397.3685 - val_MinusLogProbMetric: 397.3685 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 104/1000
2023-10-04 04:02:41.163 
Epoch 104/1000 
	 loss: 384.0957, MinusLogProbMetric: 384.0957, val_loss: 397.6214, val_MinusLogProbMetric: 397.6214

Epoch 104: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.0957 - MinusLogProbMetric: 384.0957 - val_loss: 397.6214 - val_MinusLogProbMetric: 397.6214 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 105/1000
2023-10-04 04:02:51.480 
Epoch 105/1000 
	 loss: 384.3142, MinusLogProbMetric: 384.3142, val_loss: 397.7581, val_MinusLogProbMetric: 397.7581

Epoch 105: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.3142 - MinusLogProbMetric: 384.3142 - val_loss: 397.7581 - val_MinusLogProbMetric: 397.7581 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 106/1000
2023-10-04 04:03:01.745 
Epoch 106/1000 
	 loss: 383.8930, MinusLogProbMetric: 383.8930, val_loss: 397.5225, val_MinusLogProbMetric: 397.5225

Epoch 106: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8930 - MinusLogProbMetric: 383.8930 - val_loss: 397.5225 - val_MinusLogProbMetric: 397.5225 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 107/1000
2023-10-04 04:03:12.129 
Epoch 107/1000 
	 loss: 384.1318, MinusLogProbMetric: 384.1318, val_loss: 398.3879, val_MinusLogProbMetric: 398.3879

Epoch 107: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.1318 - MinusLogProbMetric: 384.1318 - val_loss: 398.3879 - val_MinusLogProbMetric: 398.3879 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 108/1000
2023-10-04 04:03:22.233 
Epoch 108/1000 
	 loss: 384.0187, MinusLogProbMetric: 384.0187, val_loss: 397.1862, val_MinusLogProbMetric: 397.1862

Epoch 108: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.0187 - MinusLogProbMetric: 384.0187 - val_loss: 397.1862 - val_MinusLogProbMetric: 397.1862 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 109/1000
2023-10-04 04:03:32.473 
Epoch 109/1000 
	 loss: 384.3244, MinusLogProbMetric: 384.3244, val_loss: 398.3097, val_MinusLogProbMetric: 398.3097

Epoch 109: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.3244 - MinusLogProbMetric: 384.3244 - val_loss: 398.3097 - val_MinusLogProbMetric: 398.3097 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 110/1000
2023-10-04 04:03:42.686 
Epoch 110/1000 
	 loss: 384.1991, MinusLogProbMetric: 384.1991, val_loss: 397.3474, val_MinusLogProbMetric: 397.3474

Epoch 110: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.1991 - MinusLogProbMetric: 384.1991 - val_loss: 397.3474 - val_MinusLogProbMetric: 397.3474 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 111/1000
2023-10-04 04:03:53.066 
Epoch 111/1000 
	 loss: 384.0376, MinusLogProbMetric: 384.0376, val_loss: 398.0478, val_MinusLogProbMetric: 398.0478

Epoch 111: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.0376 - MinusLogProbMetric: 384.0376 - val_loss: 398.0478 - val_MinusLogProbMetric: 398.0478 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 112/1000
2023-10-04 04:04:03.195 
Epoch 112/1000 
	 loss: 383.9313, MinusLogProbMetric: 383.9313, val_loss: 398.1705, val_MinusLogProbMetric: 398.1705

Epoch 112: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.9313 - MinusLogProbMetric: 383.9313 - val_loss: 398.1705 - val_MinusLogProbMetric: 398.1705 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 113/1000
2023-10-04 04:04:13.742 
Epoch 113/1000 
	 loss: 384.0990, MinusLogProbMetric: 384.0990, val_loss: 397.5832, val_MinusLogProbMetric: 397.5832

Epoch 113: val_loss did not improve from 397.14371
196/196 - 11s - loss: 384.0990 - MinusLogProbMetric: 384.0990 - val_loss: 397.5832 - val_MinusLogProbMetric: 397.5832 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 114/1000
2023-10-04 04:04:24.133 
Epoch 114/1000 
	 loss: 383.8380, MinusLogProbMetric: 383.8380, val_loss: 397.4787, val_MinusLogProbMetric: 397.4787

Epoch 114: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8380 - MinusLogProbMetric: 383.8380 - val_loss: 397.4787 - val_MinusLogProbMetric: 397.4787 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 115/1000
2023-10-04 04:04:34.659 
Epoch 115/1000 
	 loss: 384.1045, MinusLogProbMetric: 384.1045, val_loss: 397.6840, val_MinusLogProbMetric: 397.6840

Epoch 115: val_loss did not improve from 397.14371
196/196 - 11s - loss: 384.1045 - MinusLogProbMetric: 384.1045 - val_loss: 397.6840 - val_MinusLogProbMetric: 397.6840 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 116/1000
2023-10-04 04:04:44.893 
Epoch 116/1000 
	 loss: 383.8984, MinusLogProbMetric: 383.8984, val_loss: 397.5684, val_MinusLogProbMetric: 397.5684

Epoch 116: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8984 - MinusLogProbMetric: 383.8984 - val_loss: 397.5684 - val_MinusLogProbMetric: 397.5684 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 117/1000
2023-10-04 04:04:55.043 
Epoch 117/1000 
	 loss: 384.1410, MinusLogProbMetric: 384.1410, val_loss: 398.0895, val_MinusLogProbMetric: 398.0895

Epoch 117: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.1410 - MinusLogProbMetric: 384.1410 - val_loss: 398.0895 - val_MinusLogProbMetric: 398.0895 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 118/1000
2023-10-04 04:05:05.477 
Epoch 118/1000 
	 loss: 384.2417, MinusLogProbMetric: 384.2417, val_loss: 397.6016, val_MinusLogProbMetric: 397.6016

Epoch 118: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.2417 - MinusLogProbMetric: 384.2417 - val_loss: 397.6016 - val_MinusLogProbMetric: 397.6016 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 119/1000
2023-10-04 04:05:15.679 
Epoch 119/1000 
	 loss: 384.0600, MinusLogProbMetric: 384.0600, val_loss: 398.1602, val_MinusLogProbMetric: 398.1602

Epoch 119: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.0600 - MinusLogProbMetric: 384.0600 - val_loss: 398.1602 - val_MinusLogProbMetric: 398.1602 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 120/1000
2023-10-04 04:05:25.866 
Epoch 120/1000 
	 loss: 383.8253, MinusLogProbMetric: 383.8253, val_loss: 397.7789, val_MinusLogProbMetric: 397.7789

Epoch 120: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8253 - MinusLogProbMetric: 383.8253 - val_loss: 397.7789 - val_MinusLogProbMetric: 397.7789 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 121/1000
2023-10-04 04:05:35.913 
Epoch 121/1000 
	 loss: 384.0099, MinusLogProbMetric: 384.0099, val_loss: 397.8662, val_MinusLogProbMetric: 397.8662

Epoch 121: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.0099 - MinusLogProbMetric: 384.0099 - val_loss: 397.8662 - val_MinusLogProbMetric: 397.8662 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 122/1000
2023-10-04 04:05:46.136 
Epoch 122/1000 
	 loss: 383.9702, MinusLogProbMetric: 383.9702, val_loss: 398.7692, val_MinusLogProbMetric: 398.7692

Epoch 122: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.9702 - MinusLogProbMetric: 383.9702 - val_loss: 398.7692 - val_MinusLogProbMetric: 398.7692 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 123/1000
2023-10-04 04:05:56.132 
Epoch 123/1000 
	 loss: 383.9385, MinusLogProbMetric: 383.9385, val_loss: 397.9323, val_MinusLogProbMetric: 397.9323

Epoch 123: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.9385 - MinusLogProbMetric: 383.9385 - val_loss: 397.9323 - val_MinusLogProbMetric: 397.9323 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 124/1000
2023-10-04 04:06:06.247 
Epoch 124/1000 
	 loss: 383.8163, MinusLogProbMetric: 383.8163, val_loss: 397.5223, val_MinusLogProbMetric: 397.5223

Epoch 124: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8163 - MinusLogProbMetric: 383.8163 - val_loss: 397.5223 - val_MinusLogProbMetric: 397.5223 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 125/1000
2023-10-04 04:06:16.532 
Epoch 125/1000 
	 loss: 383.8191, MinusLogProbMetric: 383.8191, val_loss: 397.5734, val_MinusLogProbMetric: 397.5734

Epoch 125: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8191 - MinusLogProbMetric: 383.8191 - val_loss: 397.5734 - val_MinusLogProbMetric: 397.5734 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 126/1000
2023-10-04 04:06:27.371 
Epoch 126/1000 
	 loss: 383.7395, MinusLogProbMetric: 383.7395, val_loss: 398.0760, val_MinusLogProbMetric: 398.0760

Epoch 126: val_loss did not improve from 397.14371
196/196 - 11s - loss: 383.7395 - MinusLogProbMetric: 383.7395 - val_loss: 398.0760 - val_MinusLogProbMetric: 398.0760 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 127/1000
2023-10-04 04:06:37.493 
Epoch 127/1000 
	 loss: 384.0278, MinusLogProbMetric: 384.0278, val_loss: 399.0679, val_MinusLogProbMetric: 399.0679

Epoch 127: val_loss did not improve from 397.14371
196/196 - 10s - loss: 384.0278 - MinusLogProbMetric: 384.0278 - val_loss: 399.0679 - val_MinusLogProbMetric: 399.0679 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 128/1000
2023-10-04 04:06:47.682 
Epoch 128/1000 
	 loss: 383.9861, MinusLogProbMetric: 383.9861, val_loss: 398.4203, val_MinusLogProbMetric: 398.4203

Epoch 128: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.9861 - MinusLogProbMetric: 383.9861 - val_loss: 398.4203 - val_MinusLogProbMetric: 398.4203 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 129/1000
2023-10-04 04:06:57.793 
Epoch 129/1000 
	 loss: 383.7733, MinusLogProbMetric: 383.7733, val_loss: 398.5297, val_MinusLogProbMetric: 398.5297

Epoch 129: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.7733 - MinusLogProbMetric: 383.7733 - val_loss: 398.5297 - val_MinusLogProbMetric: 398.5297 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 130/1000
2023-10-04 04:07:08.156 
Epoch 130/1000 
	 loss: 383.9784, MinusLogProbMetric: 383.9784, val_loss: 397.9183, val_MinusLogProbMetric: 397.9183

Epoch 130: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.9784 - MinusLogProbMetric: 383.9784 - val_loss: 397.9183 - val_MinusLogProbMetric: 397.9183 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 131/1000
2023-10-04 04:07:18.394 
Epoch 131/1000 
	 loss: 383.9151, MinusLogProbMetric: 383.9151, val_loss: 399.4433, val_MinusLogProbMetric: 399.4433

Epoch 131: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.9151 - MinusLogProbMetric: 383.9151 - val_loss: 399.4433 - val_MinusLogProbMetric: 399.4433 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 132/1000
2023-10-04 04:07:28.861 
Epoch 132/1000 
	 loss: 383.7688, MinusLogProbMetric: 383.7688, val_loss: 397.5840, val_MinusLogProbMetric: 397.5840

Epoch 132: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.7688 - MinusLogProbMetric: 383.7688 - val_loss: 397.5840 - val_MinusLogProbMetric: 397.5840 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 133/1000
2023-10-04 04:07:39.377 
Epoch 133/1000 
	 loss: 383.7540, MinusLogProbMetric: 383.7540, val_loss: 398.3511, val_MinusLogProbMetric: 398.3511

Epoch 133: val_loss did not improve from 397.14371
196/196 - 11s - loss: 383.7540 - MinusLogProbMetric: 383.7540 - val_loss: 398.3511 - val_MinusLogProbMetric: 398.3511 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 134/1000
2023-10-04 04:07:49.633 
Epoch 134/1000 
	 loss: 383.8072, MinusLogProbMetric: 383.8072, val_loss: 398.2448, val_MinusLogProbMetric: 398.2448

Epoch 134: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8072 - MinusLogProbMetric: 383.8072 - val_loss: 398.2448 - val_MinusLogProbMetric: 398.2448 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 135/1000
2023-10-04 04:07:59.878 
Epoch 135/1000 
	 loss: 383.7899, MinusLogProbMetric: 383.7899, val_loss: 398.7873, val_MinusLogProbMetric: 398.7873

Epoch 135: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.7899 - MinusLogProbMetric: 383.7899 - val_loss: 398.7873 - val_MinusLogProbMetric: 398.7873 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 136/1000
2023-10-04 04:08:10.151 
Epoch 136/1000 
	 loss: 383.8122, MinusLogProbMetric: 383.8122, val_loss: 398.0011, val_MinusLogProbMetric: 398.0011

Epoch 136: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8122 - MinusLogProbMetric: 383.8122 - val_loss: 398.0011 - val_MinusLogProbMetric: 398.0011 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 137/1000
2023-10-04 04:08:20.615 
Epoch 137/1000 
	 loss: 383.8179, MinusLogProbMetric: 383.8179, val_loss: 398.3878, val_MinusLogProbMetric: 398.3878

Epoch 137: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8179 - MinusLogProbMetric: 383.8179 - val_loss: 398.3878 - val_MinusLogProbMetric: 398.3878 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 138/1000
2023-10-04 04:08:31.020 
Epoch 138/1000 
	 loss: 383.7042, MinusLogProbMetric: 383.7042, val_loss: 398.3316, val_MinusLogProbMetric: 398.3316

Epoch 138: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.7042 - MinusLogProbMetric: 383.7042 - val_loss: 398.3316 - val_MinusLogProbMetric: 398.3316 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 139/1000
2023-10-04 04:08:41.318 
Epoch 139/1000 
	 loss: 383.8542, MinusLogProbMetric: 383.8542, val_loss: 398.2855, val_MinusLogProbMetric: 398.2855

Epoch 139: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.8542 - MinusLogProbMetric: 383.8542 - val_loss: 398.2855 - val_MinusLogProbMetric: 398.2855 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 140/1000
2023-10-04 04:08:51.622 
Epoch 140/1000 
	 loss: 383.7229, MinusLogProbMetric: 383.7229, val_loss: 399.2883, val_MinusLogProbMetric: 399.2883

Epoch 140: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.7229 - MinusLogProbMetric: 383.7229 - val_loss: 399.2883 - val_MinusLogProbMetric: 399.2883 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 141/1000
2023-10-04 04:09:01.952 
Epoch 141/1000 
	 loss: 383.9626, MinusLogProbMetric: 383.9626, val_loss: 398.5547, val_MinusLogProbMetric: 398.5547

Epoch 141: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.9626 - MinusLogProbMetric: 383.9626 - val_loss: 398.5547 - val_MinusLogProbMetric: 398.5547 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 142/1000
2023-10-04 04:09:12.084 
Epoch 142/1000 
	 loss: 383.6650, MinusLogProbMetric: 383.6650, val_loss: 397.7569, val_MinusLogProbMetric: 397.7569

Epoch 142: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.6650 - MinusLogProbMetric: 383.6650 - val_loss: 397.7569 - val_MinusLogProbMetric: 397.7569 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 143/1000
2023-10-04 04:09:22.391 
Epoch 143/1000 
	 loss: 383.7778, MinusLogProbMetric: 383.7778, val_loss: 398.1285, val_MinusLogProbMetric: 398.1285

Epoch 143: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.7778 - MinusLogProbMetric: 383.7778 - val_loss: 398.1285 - val_MinusLogProbMetric: 398.1285 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 144/1000
2023-10-04 04:09:32.657 
Epoch 144/1000 
	 loss: 383.6877, MinusLogProbMetric: 383.6877, val_loss: 400.6300, val_MinusLogProbMetric: 400.6300

Epoch 144: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.6877 - MinusLogProbMetric: 383.6877 - val_loss: 400.6300 - val_MinusLogProbMetric: 400.6300 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 145/1000
2023-10-04 04:09:43.056 
Epoch 145/1000 
	 loss: 383.5182, MinusLogProbMetric: 383.5182, val_loss: 398.5869, val_MinusLogProbMetric: 398.5869

Epoch 145: val_loss did not improve from 397.14371
196/196 - 10s - loss: 383.5182 - MinusLogProbMetric: 383.5182 - val_loss: 398.5869 - val_MinusLogProbMetric: 398.5869 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 146/1000
2023-10-04 04:09:52.888 
Epoch 146/1000 
	 loss: 382.3398, MinusLogProbMetric: 382.3398, val_loss: 397.4908, val_MinusLogProbMetric: 397.4908

Epoch 146: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.3398 - MinusLogProbMetric: 382.3398 - val_loss: 397.4908 - val_MinusLogProbMetric: 397.4908 - lr: 2.7778e-05 - 10s/epoch - 50ms/step
Epoch 147/1000
2023-10-04 04:10:03.315 
Epoch 147/1000 
	 loss: 382.2603, MinusLogProbMetric: 382.2603, val_loss: 397.4328, val_MinusLogProbMetric: 397.4328

Epoch 147: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2603 - MinusLogProbMetric: 382.2603 - val_loss: 397.4328 - val_MinusLogProbMetric: 397.4328 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 148/1000
2023-10-04 04:10:13.773 
Epoch 148/1000 
	 loss: 382.2498, MinusLogProbMetric: 382.2498, val_loss: 397.5356, val_MinusLogProbMetric: 397.5356

Epoch 148: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2498 - MinusLogProbMetric: 382.2498 - val_loss: 397.5356 - val_MinusLogProbMetric: 397.5356 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 149/1000
2023-10-04 04:10:23.997 
Epoch 149/1000 
	 loss: 382.2060, MinusLogProbMetric: 382.2060, val_loss: 397.5400, val_MinusLogProbMetric: 397.5400

Epoch 149: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2060 - MinusLogProbMetric: 382.2060 - val_loss: 397.5400 - val_MinusLogProbMetric: 397.5400 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 150/1000
2023-10-04 04:10:34.055 
Epoch 150/1000 
	 loss: 382.2584, MinusLogProbMetric: 382.2584, val_loss: 397.4491, val_MinusLogProbMetric: 397.4491

Epoch 150: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2584 - MinusLogProbMetric: 382.2584 - val_loss: 397.4491 - val_MinusLogProbMetric: 397.4491 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 151/1000
2023-10-04 04:10:44.316 
Epoch 151/1000 
	 loss: 382.2778, MinusLogProbMetric: 382.2778, val_loss: 397.5922, val_MinusLogProbMetric: 397.5922

Epoch 151: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2778 - MinusLogProbMetric: 382.2778 - val_loss: 397.5922 - val_MinusLogProbMetric: 397.5922 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 152/1000
2023-10-04 04:10:54.904 
Epoch 152/1000 
	 loss: 382.2145, MinusLogProbMetric: 382.2145, val_loss: 397.7213, val_MinusLogProbMetric: 397.7213

Epoch 152: val_loss did not improve from 397.14371
196/196 - 11s - loss: 382.2145 - MinusLogProbMetric: 382.2145 - val_loss: 397.7213 - val_MinusLogProbMetric: 397.7213 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 153/1000
2023-10-04 04:11:05.247 
Epoch 153/1000 
	 loss: 382.1740, MinusLogProbMetric: 382.1740, val_loss: 397.6343, val_MinusLogProbMetric: 397.6343

Epoch 153: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1740 - MinusLogProbMetric: 382.1740 - val_loss: 397.6343 - val_MinusLogProbMetric: 397.6343 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 154/1000
2023-10-04 04:11:15.575 
Epoch 154/1000 
	 loss: 382.2014, MinusLogProbMetric: 382.2014, val_loss: 397.5172, val_MinusLogProbMetric: 397.5172

Epoch 154: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2014 - MinusLogProbMetric: 382.2014 - val_loss: 397.5172 - val_MinusLogProbMetric: 397.5172 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 155/1000
2023-10-04 04:11:25.824 
Epoch 155/1000 
	 loss: 382.1929, MinusLogProbMetric: 382.1929, val_loss: 397.6571, val_MinusLogProbMetric: 397.6571

Epoch 155: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1929 - MinusLogProbMetric: 382.1929 - val_loss: 397.6571 - val_MinusLogProbMetric: 397.6571 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 156/1000
2023-10-04 04:11:36.057 
Epoch 156/1000 
	 loss: 382.2371, MinusLogProbMetric: 382.2371, val_loss: 397.5076, val_MinusLogProbMetric: 397.5076

Epoch 156: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2371 - MinusLogProbMetric: 382.2371 - val_loss: 397.5076 - val_MinusLogProbMetric: 397.5076 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 157/1000
2023-10-04 04:11:46.222 
Epoch 157/1000 
	 loss: 382.1859, MinusLogProbMetric: 382.1859, val_loss: 397.5503, val_MinusLogProbMetric: 397.5503

Epoch 157: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1859 - MinusLogProbMetric: 382.1859 - val_loss: 397.5503 - val_MinusLogProbMetric: 397.5503 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 158/1000
2023-10-04 04:11:56.457 
Epoch 158/1000 
	 loss: 382.2018, MinusLogProbMetric: 382.2018, val_loss: 397.6322, val_MinusLogProbMetric: 397.6322

Epoch 158: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2018 - MinusLogProbMetric: 382.2018 - val_loss: 397.6322 - val_MinusLogProbMetric: 397.6322 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 159/1000
2023-10-04 04:12:06.656 
Epoch 159/1000 
	 loss: 382.2710, MinusLogProbMetric: 382.2710, val_loss: 397.5137, val_MinusLogProbMetric: 397.5137

Epoch 159: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2710 - MinusLogProbMetric: 382.2710 - val_loss: 397.5137 - val_MinusLogProbMetric: 397.5137 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 160/1000
2023-10-04 04:12:17.086 
Epoch 160/1000 
	 loss: 382.2071, MinusLogProbMetric: 382.2071, val_loss: 397.8474, val_MinusLogProbMetric: 397.8474

Epoch 160: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2071 - MinusLogProbMetric: 382.2071 - val_loss: 397.8474 - val_MinusLogProbMetric: 397.8474 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 161/1000
2023-10-04 04:12:27.235 
Epoch 161/1000 
	 loss: 382.2150, MinusLogProbMetric: 382.2150, val_loss: 397.8546, val_MinusLogProbMetric: 397.8546

Epoch 161: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2150 - MinusLogProbMetric: 382.2150 - val_loss: 397.8546 - val_MinusLogProbMetric: 397.8546 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 162/1000
2023-10-04 04:12:37.391 
Epoch 162/1000 
	 loss: 382.1888, MinusLogProbMetric: 382.1888, val_loss: 397.8360, val_MinusLogProbMetric: 397.8360

Epoch 162: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1888 - MinusLogProbMetric: 382.1888 - val_loss: 397.8360 - val_MinusLogProbMetric: 397.8360 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 163/1000
2023-10-04 04:12:48.001 
Epoch 163/1000 
	 loss: 382.2185, MinusLogProbMetric: 382.2185, val_loss: 397.8252, val_MinusLogProbMetric: 397.8252

Epoch 163: val_loss did not improve from 397.14371
196/196 - 11s - loss: 382.2185 - MinusLogProbMetric: 382.2185 - val_loss: 397.8252 - val_MinusLogProbMetric: 397.8252 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 164/1000
2023-10-04 04:12:58.460 
Epoch 164/1000 
	 loss: 382.2094, MinusLogProbMetric: 382.2094, val_loss: 397.9115, val_MinusLogProbMetric: 397.9115

Epoch 164: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2094 - MinusLogProbMetric: 382.2094 - val_loss: 397.9115 - val_MinusLogProbMetric: 397.9115 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 165/1000
2023-10-04 04:13:08.578 
Epoch 165/1000 
	 loss: 382.1811, MinusLogProbMetric: 382.1811, val_loss: 398.0370, val_MinusLogProbMetric: 398.0370

Epoch 165: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1811 - MinusLogProbMetric: 382.1811 - val_loss: 398.0370 - val_MinusLogProbMetric: 398.0370 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 166/1000
2023-10-04 04:13:18.681 
Epoch 166/1000 
	 loss: 382.1794, MinusLogProbMetric: 382.1794, val_loss: 398.3229, val_MinusLogProbMetric: 398.3229

Epoch 166: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1794 - MinusLogProbMetric: 382.1794 - val_loss: 398.3229 - val_MinusLogProbMetric: 398.3229 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 167/1000
2023-10-04 04:13:29.125 
Epoch 167/1000 
	 loss: 382.1162, MinusLogProbMetric: 382.1162, val_loss: 397.7583, val_MinusLogProbMetric: 397.7583

Epoch 167: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1162 - MinusLogProbMetric: 382.1162 - val_loss: 397.7583 - val_MinusLogProbMetric: 397.7583 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 168/1000
2023-10-04 04:13:39.353 
Epoch 168/1000 
	 loss: 382.1328, MinusLogProbMetric: 382.1328, val_loss: 397.9889, val_MinusLogProbMetric: 397.9889

Epoch 168: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1328 - MinusLogProbMetric: 382.1328 - val_loss: 397.9889 - val_MinusLogProbMetric: 397.9889 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 169/1000
2023-10-04 04:13:49.358 
Epoch 169/1000 
	 loss: 382.1253, MinusLogProbMetric: 382.1253, val_loss: 397.8664, val_MinusLogProbMetric: 397.8664

Epoch 169: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1253 - MinusLogProbMetric: 382.1253 - val_loss: 397.8664 - val_MinusLogProbMetric: 397.8664 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 170/1000
2023-10-04 04:13:59.650 
Epoch 170/1000 
	 loss: 382.0968, MinusLogProbMetric: 382.0968, val_loss: 397.8380, val_MinusLogProbMetric: 397.8380

Epoch 170: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0968 - MinusLogProbMetric: 382.0968 - val_loss: 397.8380 - val_MinusLogProbMetric: 397.8380 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 171/1000
2023-10-04 04:14:09.900 
Epoch 171/1000 
	 loss: 382.1031, MinusLogProbMetric: 382.1031, val_loss: 398.2903, val_MinusLogProbMetric: 398.2903

Epoch 171: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1031 - MinusLogProbMetric: 382.1031 - val_loss: 398.2903 - val_MinusLogProbMetric: 398.2903 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 172/1000
2023-10-04 04:14:19.797 
Epoch 172/1000 
	 loss: 382.0793, MinusLogProbMetric: 382.0793, val_loss: 398.3072, val_MinusLogProbMetric: 398.3072

Epoch 172: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0793 - MinusLogProbMetric: 382.0793 - val_loss: 398.3072 - val_MinusLogProbMetric: 398.3072 - lr: 2.7778e-05 - 10s/epoch - 50ms/step
Epoch 173/1000
2023-10-04 04:14:29.897 
Epoch 173/1000 
	 loss: 382.2917, MinusLogProbMetric: 382.2917, val_loss: 397.8034, val_MinusLogProbMetric: 397.8034

Epoch 173: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2917 - MinusLogProbMetric: 382.2917 - val_loss: 397.8034 - val_MinusLogProbMetric: 397.8034 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 174/1000
2023-10-04 04:14:40.478 
Epoch 174/1000 
	 loss: 382.2037, MinusLogProbMetric: 382.2037, val_loss: 398.1294, val_MinusLogProbMetric: 398.1294

Epoch 174: val_loss did not improve from 397.14371
196/196 - 11s - loss: 382.2037 - MinusLogProbMetric: 382.2037 - val_loss: 398.1294 - val_MinusLogProbMetric: 398.1294 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 175/1000
2023-10-04 04:14:50.712 
Epoch 175/1000 
	 loss: 382.0295, MinusLogProbMetric: 382.0295, val_loss: 398.1500, val_MinusLogProbMetric: 398.1500

Epoch 175: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0295 - MinusLogProbMetric: 382.0295 - val_loss: 398.1500 - val_MinusLogProbMetric: 398.1500 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 176/1000
2023-10-04 04:15:00.856 
Epoch 176/1000 
	 loss: 382.0260, MinusLogProbMetric: 382.0260, val_loss: 398.3448, val_MinusLogProbMetric: 398.3448

Epoch 176: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0260 - MinusLogProbMetric: 382.0260 - val_loss: 398.3448 - val_MinusLogProbMetric: 398.3448 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 177/1000
2023-10-04 04:15:11.011 
Epoch 177/1000 
	 loss: 382.2215, MinusLogProbMetric: 382.2215, val_loss: 397.8044, val_MinusLogProbMetric: 397.8044

Epoch 177: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.2215 - MinusLogProbMetric: 382.2215 - val_loss: 397.8044 - val_MinusLogProbMetric: 397.8044 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 178/1000
2023-10-04 04:15:21.437 
Epoch 178/1000 
	 loss: 382.0911, MinusLogProbMetric: 382.0911, val_loss: 398.7215, val_MinusLogProbMetric: 398.7215

Epoch 178: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0911 - MinusLogProbMetric: 382.0911 - val_loss: 398.7215 - val_MinusLogProbMetric: 398.7215 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 179/1000
2023-10-04 04:15:31.587 
Epoch 179/1000 
	 loss: 382.1406, MinusLogProbMetric: 382.1406, val_loss: 398.6570, val_MinusLogProbMetric: 398.6570

Epoch 179: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1406 - MinusLogProbMetric: 382.1406 - val_loss: 398.6570 - val_MinusLogProbMetric: 398.6570 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 180/1000
2023-10-04 04:15:41.873 
Epoch 180/1000 
	 loss: 382.1106, MinusLogProbMetric: 382.1106, val_loss: 398.5456, val_MinusLogProbMetric: 398.5456

Epoch 180: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1106 - MinusLogProbMetric: 382.1106 - val_loss: 398.5456 - val_MinusLogProbMetric: 398.5456 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 181/1000
2023-10-04 04:15:52.061 
Epoch 181/1000 
	 loss: 382.1267, MinusLogProbMetric: 382.1267, val_loss: 397.7997, val_MinusLogProbMetric: 397.7997

Epoch 181: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1267 - MinusLogProbMetric: 382.1267 - val_loss: 397.7997 - val_MinusLogProbMetric: 397.7997 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 182/1000
2023-10-04 04:16:02.453 
Epoch 182/1000 
	 loss: 382.0362, MinusLogProbMetric: 382.0362, val_loss: 398.0805, val_MinusLogProbMetric: 398.0805

Epoch 182: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0362 - MinusLogProbMetric: 382.0362 - val_loss: 398.0805 - val_MinusLogProbMetric: 398.0805 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 183/1000
2023-10-04 04:16:12.495 
Epoch 183/1000 
	 loss: 382.0491, MinusLogProbMetric: 382.0491, val_loss: 398.1502, val_MinusLogProbMetric: 398.1502

Epoch 183: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0491 - MinusLogProbMetric: 382.0491 - val_loss: 398.1502 - val_MinusLogProbMetric: 398.1502 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 184/1000
2023-10-04 04:16:22.578 
Epoch 184/1000 
	 loss: 382.0880, MinusLogProbMetric: 382.0880, val_loss: 397.7898, val_MinusLogProbMetric: 397.7898

Epoch 184: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0880 - MinusLogProbMetric: 382.0880 - val_loss: 397.7898 - val_MinusLogProbMetric: 397.7898 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 185/1000
2023-10-04 04:16:32.739 
Epoch 185/1000 
	 loss: 381.9883, MinusLogProbMetric: 381.9883, val_loss: 398.1199, val_MinusLogProbMetric: 398.1199

Epoch 185: val_loss did not improve from 397.14371
196/196 - 10s - loss: 381.9883 - MinusLogProbMetric: 381.9883 - val_loss: 398.1199 - val_MinusLogProbMetric: 398.1199 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 186/1000
2023-10-04 04:16:42.948 
Epoch 186/1000 
	 loss: 382.1339, MinusLogProbMetric: 382.1339, val_loss: 398.1700, val_MinusLogProbMetric: 398.1700

Epoch 186: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.1339 - MinusLogProbMetric: 382.1339 - val_loss: 398.1700 - val_MinusLogProbMetric: 398.1700 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 187/1000
2023-10-04 04:16:53.140 
Epoch 187/1000 
	 loss: 381.9741, MinusLogProbMetric: 381.9741, val_loss: 398.4695, val_MinusLogProbMetric: 398.4695

Epoch 187: val_loss did not improve from 397.14371
196/196 - 10s - loss: 381.9741 - MinusLogProbMetric: 381.9741 - val_loss: 398.4695 - val_MinusLogProbMetric: 398.4695 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 188/1000
2023-10-04 04:17:03.451 
Epoch 188/1000 
	 loss: 382.0394, MinusLogProbMetric: 382.0394, val_loss: 398.3944, val_MinusLogProbMetric: 398.3944

Epoch 188: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0394 - MinusLogProbMetric: 382.0394 - val_loss: 398.3944 - val_MinusLogProbMetric: 398.3944 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 189/1000
2023-10-04 04:17:13.827 
Epoch 189/1000 
	 loss: 381.9379, MinusLogProbMetric: 381.9379, val_loss: 398.2691, val_MinusLogProbMetric: 398.2691

Epoch 189: val_loss did not improve from 397.14371
196/196 - 10s - loss: 381.9379 - MinusLogProbMetric: 381.9379 - val_loss: 398.2691 - val_MinusLogProbMetric: 398.2691 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 190/1000
2023-10-04 04:17:24.009 
Epoch 190/1000 
	 loss: 381.9028, MinusLogProbMetric: 381.9028, val_loss: 398.2325, val_MinusLogProbMetric: 398.2325

Epoch 190: val_loss did not improve from 397.14371
196/196 - 10s - loss: 381.9028 - MinusLogProbMetric: 381.9028 - val_loss: 398.2325 - val_MinusLogProbMetric: 398.2325 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 191/1000
2023-10-04 04:17:34.106 
Epoch 191/1000 
	 loss: 381.8649, MinusLogProbMetric: 381.8649, val_loss: 398.2349, val_MinusLogProbMetric: 398.2349

Epoch 191: val_loss did not improve from 397.14371
196/196 - 10s - loss: 381.8649 - MinusLogProbMetric: 381.8649 - val_loss: 398.2349 - val_MinusLogProbMetric: 398.2349 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 192/1000
2023-10-04 04:17:44.451 
Epoch 192/1000 
	 loss: 382.0145, MinusLogProbMetric: 382.0145, val_loss: 398.1645, val_MinusLogProbMetric: 398.1645

Epoch 192: val_loss did not improve from 397.14371
196/196 - 10s - loss: 382.0145 - MinusLogProbMetric: 382.0145 - val_loss: 398.1645 - val_MinusLogProbMetric: 398.1645 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 193/1000
2023-10-04 04:17:54.839 
Epoch 193/1000 
	 loss: 381.8499, MinusLogProbMetric: 381.8499, val_loss: 398.1342, val_MinusLogProbMetric: 398.1342

Epoch 193: val_loss did not improve from 397.14371
196/196 - 10s - loss: 381.8499 - MinusLogProbMetric: 381.8499 - val_loss: 398.1342 - val_MinusLogProbMetric: 398.1342 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 194/1000
2023-10-04 04:18:04.874 
Epoch 194/1000 
	 loss: 381.8520, MinusLogProbMetric: 381.8520, val_loss: 398.2461, val_MinusLogProbMetric: 398.2461

Epoch 194: val_loss did not improve from 397.14371
196/196 - 10s - loss: 381.8520 - MinusLogProbMetric: 381.8520 - val_loss: 398.2461 - val_MinusLogProbMetric: 398.2461 - lr: 2.7778e-05 - 10s/epoch - 51ms/step
Epoch 195/1000
2023-10-04 04:18:15.097 
Epoch 195/1000 
	 loss: 381.9212, MinusLogProbMetric: 381.9212, val_loss: 398.2851, val_MinusLogProbMetric: 398.2851

Epoch 195: val_loss did not improve from 397.14371
Restoring model weights from the end of the best epoch: 95.
196/196 - 10s - loss: 381.9212 - MinusLogProbMetric: 381.9212 - val_loss: 398.2851 - val_MinusLogProbMetric: 398.2851 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 195: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 5380.721922504017 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 5412.99027838395 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
