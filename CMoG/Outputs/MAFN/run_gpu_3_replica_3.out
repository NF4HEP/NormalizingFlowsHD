2023-09-09 09:31:24.391995: Importing os...
2023-09-09 09:31:24.392073: Importing sys...
2023-09-09 09:31:24.392088: Importing and initializing argparse...
Visible devices: [3]
2023-09-09 09:31:24.409963: Importing timer from timeit...
2023-09-09 09:31:24.410587: Setting env variables for tf import (only device [3] will be available)...
2023-09-09 09:31:24.410634: Importing numpy...
2023-09-09 09:31:24.568910: Importing pandas...
2023-09-09 09:31:24.765639: Importing shutil...
2023-09-09 09:31:24.765668: Importing subprocess...
2023-09-09 09:31:24.765676: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-09 09:31:27.119343: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-09 09:31:27.551209: Importing textwrap...
2023-09-09 09:31:27.551247: Importing timeit...
2023-09-09 09:31:27.551257: Importing traceback...
2023-09-09 09:31:27.551264: Importing typing...
2023-09-09 09:31:27.551274: Setting tf configs...
2023-09-09 09:31:27.771791: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-09 09:31:29.173445: All modues imported successfully.
Directory ../../results/MAFN_new/ already exists.
Directory ../../results/MAFN_new/run_1/ already exists.
Skipping it.
===========
Run 1/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_2/ already exists.
Skipping it.
===========
Run 2/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_3/ already exists.
Skipping it.
===========
Run 3/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_4/ already exists.
Skipping it.
===========
Run 4/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_5/ already exists.
Skipping it.
===========
Run 5/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_6/ already exists.
Skipping it.
===========
Run 6/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_7/ already exists.
Skipping it.
===========
Run 7/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_8/ already exists.
Skipping it.
===========
Run 8/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_9/ already exists.
Skipping it.
===========
Run 9/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_10/ already exists.
Skipping it.
===========
Run 10/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_11/ already exists.
Skipping it.
===========
Run 11/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_12/ already exists.
Skipping it.
===========
Run 12/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_13/ already exists.
Skipping it.
===========
Run 13/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_14/ already exists.
Skipping it.
===========
Run 14/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_15/ already exists.
Skipping it.
===========
Run 15/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_16/ already exists.
Skipping it.
===========
Run 16/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_17/ already exists.
Skipping it.
===========
Run 17/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_18/ already exists.
Skipping it.
===========
Run 18/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_19/ already exists.
Skipping it.
===========
Run 19/360 already exists. Skipping it.
===========

===========
Generating train data for run 20.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[4], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 4)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_20/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_20/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[4.2548594, 7.0946026, 4.7615414, 9.43823  ],
       [4.232356 , 6.5302997, 4.4809017, 9.592666 ],
       [4.2134495, 7.179828 , 4.533561 , 8.779418 ],
       ...,
       [4.240699 , 7.4401965, 5.062128 , 9.433647 ],
       [4.2825274, 5.9462013, 2.819703 , 9.018203 ],
       [8.961808 , 3.052795 , 7.455145 , 5.2543044]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[4], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[4], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_20/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_20
self.data_kwargs: {'seed': 520}
self.x_data: [[ 4.2441406  6.7530947  3.3826632 10.332189 ]
 [10.195218   2.5649576  7.9873266  5.103083 ]
 [ 9.97571    4.4703245  8.199758   5.5030446]
 ...
 [ 4.2466564  8.298781   5.5362053 10.0680065]
 [ 4.262163   5.731845   3.7121477  8.4649315]
 [11.055306   5.1953883  7.2777805  4.4987473]]
self.y_data: []
self.ndims: 4
Model defined.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 4)]               0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1349200   
 r)                                                              
                                                                 
=================================================================
Total params: 1,349,200
Trainable params: 1,349,200
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7fa3d4044730>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fa3d52ff430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fa3d52ff430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fa3dc047670>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fa3b073f3a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fa3b073fdc0>, <keras.callbacks.ModelCheckpoint object at 0x7fa3b073ff10>, <keras.callbacks.EarlyStopping object at 0x7fa3b073ff70>, <keras.callbacks.ReduceLROnPlateau object at 0x7fa3b073ff40>, <keras.callbacks.TerminateOnNaN object at 0x7fa3b073fd60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[4.2548594, 7.0946026, 4.7615414, 9.43823  ],
       [4.232356 , 6.5302997, 4.4809017, 9.592666 ],
       [4.2134495, 7.179828 , 4.533561 , 8.779418 ],
       ...,
       [4.240699 , 7.4401965, 5.062128 , 9.433647 ],
       [4.2825274, 5.9462013, 2.819703 , 9.018203 ],
       [8.961808 , 3.052795 , 7.455145 , 5.2543044]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_20/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 20/360 with hyperparameters:
timestamp = 2023-09-09 09:31:33.003723
ndims = 4
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 1349200
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.2441406  6.7530947  3.3826632 10.332189 ]
Epoch 1/1000
2023-09-09 09:32:11.330 
Epoch 1/1000 
	 loss: 7.3500, MinusLogProbMetric: 7.3500, val_loss: 6.9927, val_MinusLogProbMetric: 6.9927

Epoch 1: val_loss improved from inf to 6.99269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 38s - loss: 7.3500 - MinusLogProbMetric: 7.3500 - val_loss: 6.9927 - val_MinusLogProbMetric: 6.9927 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 2/1000
2023-09-09 09:32:22.208 
Epoch 2/1000 
	 loss: 4.5361, MinusLogProbMetric: 4.5361, val_loss: 4.6512, val_MinusLogProbMetric: 4.6512

Epoch 2: val_loss improved from 6.99269 to 4.65116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 4.5361 - MinusLogProbMetric: 4.5361 - val_loss: 4.6512 - val_MinusLogProbMetric: 4.6512 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 3/1000
2023-09-09 09:32:33.001 
Epoch 3/1000 
	 loss: 4.7077, MinusLogProbMetric: 4.7077, val_loss: 3.2239, val_MinusLogProbMetric: 3.2239

Epoch 3: val_loss improved from 4.65116 to 3.22391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 4.7077 - MinusLogProbMetric: 4.7077 - val_loss: 3.2239 - val_MinusLogProbMetric: 3.2239 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 4/1000
2023-09-09 09:32:43.674 
Epoch 4/1000 
	 loss: 3.4845, MinusLogProbMetric: 3.4845, val_loss: 3.8549, val_MinusLogProbMetric: 3.8549

Epoch 4: val_loss did not improve from 3.22391
196/196 - 10s - loss: 3.4845 - MinusLogProbMetric: 3.4845 - val_loss: 3.8549 - val_MinusLogProbMetric: 3.8549 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 5/1000
2023-09-09 09:32:53.846 
Epoch 5/1000 
	 loss: 2.9452, MinusLogProbMetric: 2.9452, val_loss: 2.6397, val_MinusLogProbMetric: 2.6397

Epoch 5: val_loss improved from 3.22391 to 2.63972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 10s - loss: 2.9452 - MinusLogProbMetric: 2.9452 - val_loss: 2.6397 - val_MinusLogProbMetric: 2.6397 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 6/1000
2023-09-09 09:33:04.436 
Epoch 6/1000 
	 loss: 2.9912, MinusLogProbMetric: 2.9912, val_loss: 3.2571, val_MinusLogProbMetric: 3.2571

Epoch 6: val_loss did not improve from 2.63972
196/196 - 10s - loss: 2.9912 - MinusLogProbMetric: 2.9912 - val_loss: 3.2571 - val_MinusLogProbMetric: 3.2571 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 7/1000
2023-09-09 09:33:15.219 
Epoch 7/1000 
	 loss: 2.7148, MinusLogProbMetric: 2.7148, val_loss: 2.5347, val_MinusLogProbMetric: 2.5347

Epoch 7: val_loss improved from 2.63972 to 2.53466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 2.7148 - MinusLogProbMetric: 2.7148 - val_loss: 2.5347 - val_MinusLogProbMetric: 2.5347 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 8/1000
2023-09-09 09:33:25.472 
Epoch 8/1000 
	 loss: 2.6441, MinusLogProbMetric: 2.6441, val_loss: 2.5395, val_MinusLogProbMetric: 2.5395

Epoch 8: val_loss did not improve from 2.53466
196/196 - 10s - loss: 2.6441 - MinusLogProbMetric: 2.6441 - val_loss: 2.5395 - val_MinusLogProbMetric: 2.5395 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 9/1000
2023-09-09 09:33:35.905 
Epoch 9/1000 
	 loss: 2.5858, MinusLogProbMetric: 2.5858, val_loss: 2.6818, val_MinusLogProbMetric: 2.6818

Epoch 9: val_loss did not improve from 2.53466
196/196 - 10s - loss: 2.5858 - MinusLogProbMetric: 2.5858 - val_loss: 2.6818 - val_MinusLogProbMetric: 2.6818 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 10/1000
2023-09-09 09:33:46.305 
Epoch 10/1000 
	 loss: 2.5621, MinusLogProbMetric: 2.5621, val_loss: 2.7590, val_MinusLogProbMetric: 2.7590

Epoch 10: val_loss did not improve from 2.53466
196/196 - 10s - loss: 2.5621 - MinusLogProbMetric: 2.5621 - val_loss: 2.7590 - val_MinusLogProbMetric: 2.7590 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 11/1000
2023-09-09 09:33:56.742 
Epoch 11/1000 
	 loss: 2.5630, MinusLogProbMetric: 2.5630, val_loss: 2.5852, val_MinusLogProbMetric: 2.5852

Epoch 11: val_loss did not improve from 2.53466
196/196 - 10s - loss: 2.5630 - MinusLogProbMetric: 2.5630 - val_loss: 2.5852 - val_MinusLogProbMetric: 2.5852 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 12/1000
2023-09-09 09:34:07.212 
Epoch 12/1000 
	 loss: 2.5181, MinusLogProbMetric: 2.5181, val_loss: 2.5149, val_MinusLogProbMetric: 2.5149

Epoch 12: val_loss improved from 2.53466 to 2.51495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 2.5181 - MinusLogProbMetric: 2.5181 - val_loss: 2.5149 - val_MinusLogProbMetric: 2.5149 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 13/1000
2023-09-09 09:34:18.028 
Epoch 13/1000 
	 loss: 2.5822, MinusLogProbMetric: 2.5822, val_loss: 2.5847, val_MinusLogProbMetric: 2.5847

Epoch 13: val_loss did not improve from 2.51495
196/196 - 10s - loss: 2.5822 - MinusLogProbMetric: 2.5822 - val_loss: 2.5847 - val_MinusLogProbMetric: 2.5847 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 14/1000
2023-09-09 09:34:28.218 
Epoch 14/1000 
	 loss: 2.5138, MinusLogProbMetric: 2.5138, val_loss: 2.5572, val_MinusLogProbMetric: 2.5572

Epoch 14: val_loss did not improve from 2.51495
196/196 - 10s - loss: 2.5138 - MinusLogProbMetric: 2.5138 - val_loss: 2.5572 - val_MinusLogProbMetric: 2.5572 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 15/1000
2023-09-09 09:34:38.062 
Epoch 15/1000 
	 loss: 2.4948, MinusLogProbMetric: 2.4948, val_loss: 2.4898, val_MinusLogProbMetric: 2.4898

Epoch 15: val_loss improved from 2.51495 to 2.48984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 10s - loss: 2.4948 - MinusLogProbMetric: 2.4948 - val_loss: 2.4898 - val_MinusLogProbMetric: 2.4898 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 16/1000
2023-09-09 09:34:48.419 
Epoch 16/1000 
	 loss: 2.4904, MinusLogProbMetric: 2.4904, val_loss: 2.4363, val_MinusLogProbMetric: 2.4363

Epoch 16: val_loss improved from 2.48984 to 2.43625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 10s - loss: 2.4904 - MinusLogProbMetric: 2.4904 - val_loss: 2.4363 - val_MinusLogProbMetric: 2.4363 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 17/1000
2023-09-09 09:34:58.694 
Epoch 17/1000 
	 loss: 2.4965, MinusLogProbMetric: 2.4965, val_loss: 2.4555, val_MinusLogProbMetric: 2.4555

Epoch 17: val_loss did not improve from 2.43625
196/196 - 10s - loss: 2.4965 - MinusLogProbMetric: 2.4965 - val_loss: 2.4555 - val_MinusLogProbMetric: 2.4555 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 18/1000
2023-09-09 09:35:08.324 
Epoch 18/1000 
	 loss: 2.5061, MinusLogProbMetric: 2.5061, val_loss: 2.4336, val_MinusLogProbMetric: 2.4336

Epoch 18: val_loss improved from 2.43625 to 2.43356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 10s - loss: 2.5061 - MinusLogProbMetric: 2.5061 - val_loss: 2.4336 - val_MinusLogProbMetric: 2.4336 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 19/1000
2023-09-09 09:35:18.688 
Epoch 19/1000 
	 loss: 2.4833, MinusLogProbMetric: 2.4833, val_loss: 2.4039, val_MinusLogProbMetric: 2.4039

Epoch 19: val_loss improved from 2.43356 to 2.40389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 10s - loss: 2.4833 - MinusLogProbMetric: 2.4833 - val_loss: 2.4039 - val_MinusLogProbMetric: 2.4039 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 20/1000
2023-09-09 09:35:28.460 
Epoch 20/1000 
	 loss: 2.4802, MinusLogProbMetric: 2.4802, val_loss: 2.5966, val_MinusLogProbMetric: 2.5966

Epoch 20: val_loss did not improve from 2.40389
196/196 - 9s - loss: 2.4802 - MinusLogProbMetric: 2.4802 - val_loss: 2.5966 - val_MinusLogProbMetric: 2.5966 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 21/1000
2023-09-09 09:35:36.886 
Epoch 21/1000 
	 loss: 2.4850, MinusLogProbMetric: 2.4850, val_loss: 2.6336, val_MinusLogProbMetric: 2.6336

Epoch 21: val_loss did not improve from 2.40389
196/196 - 8s - loss: 2.4850 - MinusLogProbMetric: 2.4850 - val_loss: 2.6336 - val_MinusLogProbMetric: 2.6336 - lr: 0.0010 - 8s/epoch - 43ms/step
Epoch 22/1000
2023-09-09 09:35:46.879 
Epoch 22/1000 
	 loss: 2.4860, MinusLogProbMetric: 2.4860, val_loss: 2.4647, val_MinusLogProbMetric: 2.4647

Epoch 22: val_loss did not improve from 2.40389
196/196 - 10s - loss: 2.4860 - MinusLogProbMetric: 2.4860 - val_loss: 2.4647 - val_MinusLogProbMetric: 2.4647 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 23/1000
2023-09-09 09:35:56.548 
Epoch 23/1000 
	 loss: 2.4670, MinusLogProbMetric: 2.4670, val_loss: 2.4939, val_MinusLogProbMetric: 2.4939

Epoch 23: val_loss did not improve from 2.40389
196/196 - 10s - loss: 2.4670 - MinusLogProbMetric: 2.4670 - val_loss: 2.4939 - val_MinusLogProbMetric: 2.4939 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 24/1000
2023-09-09 09:36:05.486 
Epoch 24/1000 
	 loss: 2.4591, MinusLogProbMetric: 2.4591, val_loss: 2.4645, val_MinusLogProbMetric: 2.4645

Epoch 24: val_loss did not improve from 2.40389
196/196 - 9s - loss: 2.4591 - MinusLogProbMetric: 2.4591 - val_loss: 2.4645 - val_MinusLogProbMetric: 2.4645 - lr: 0.0010 - 9s/epoch - 46ms/step
Epoch 25/1000
2023-09-09 09:36:14.194 
Epoch 25/1000 
	 loss: 2.4602, MinusLogProbMetric: 2.4602, val_loss: 2.3950, val_MinusLogProbMetric: 2.3950

Epoch 25: val_loss improved from 2.40389 to 2.39501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 9s - loss: 2.4602 - MinusLogProbMetric: 2.4602 - val_loss: 2.3950 - val_MinusLogProbMetric: 2.3950 - lr: 0.0010 - 9s/epoch - 46ms/step
Epoch 26/1000
2023-09-09 09:36:22.848 
Epoch 26/1000 
	 loss: 2.4519, MinusLogProbMetric: 2.4519, val_loss: 2.4544, val_MinusLogProbMetric: 2.4544

Epoch 26: val_loss did not improve from 2.39501
196/196 - 8s - loss: 2.4519 - MinusLogProbMetric: 2.4519 - val_loss: 2.4544 - val_MinusLogProbMetric: 2.4544 - lr: 0.0010 - 8s/epoch - 43ms/step
Epoch 27/1000
2023-09-09 09:36:32.898 
Epoch 27/1000 
	 loss: 2.4369, MinusLogProbMetric: 2.4369, val_loss: 2.4394, val_MinusLogProbMetric: 2.4394

Epoch 27: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4369 - MinusLogProbMetric: 2.4369 - val_loss: 2.4394 - val_MinusLogProbMetric: 2.4394 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 28/1000
2023-09-09 09:36:42.061 
Epoch 28/1000 
	 loss: 2.4491, MinusLogProbMetric: 2.4491, val_loss: 2.4720, val_MinusLogProbMetric: 2.4720

Epoch 28: val_loss did not improve from 2.39501
196/196 - 9s - loss: 2.4491 - MinusLogProbMetric: 2.4491 - val_loss: 2.4720 - val_MinusLogProbMetric: 2.4720 - lr: 0.0010 - 9s/epoch - 47ms/step
Epoch 29/1000
2023-09-09 09:36:51.498 
Epoch 29/1000 
	 loss: 2.4568, MinusLogProbMetric: 2.4568, val_loss: 2.4616, val_MinusLogProbMetric: 2.4616

Epoch 29: val_loss did not improve from 2.39501
196/196 - 9s - loss: 2.4568 - MinusLogProbMetric: 2.4568 - val_loss: 2.4616 - val_MinusLogProbMetric: 2.4616 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 30/1000
2023-09-09 09:37:01.203 
Epoch 30/1000 
	 loss: 2.4469, MinusLogProbMetric: 2.4469, val_loss: 2.4521, val_MinusLogProbMetric: 2.4521

Epoch 30: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4469 - MinusLogProbMetric: 2.4469 - val_loss: 2.4521 - val_MinusLogProbMetric: 2.4521 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 31/1000
2023-09-09 09:37:11.744 
Epoch 31/1000 
	 loss: 2.4564, MinusLogProbMetric: 2.4564, val_loss: 2.5181, val_MinusLogProbMetric: 2.5181

Epoch 31: val_loss did not improve from 2.39501
196/196 - 11s - loss: 2.4564 - MinusLogProbMetric: 2.4564 - val_loss: 2.5181 - val_MinusLogProbMetric: 2.5181 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 32/1000
2023-09-09 09:37:22.363 
Epoch 32/1000 
	 loss: 2.4328, MinusLogProbMetric: 2.4328, val_loss: 2.4058, val_MinusLogProbMetric: 2.4058

Epoch 32: val_loss did not improve from 2.39501
196/196 - 11s - loss: 2.4328 - MinusLogProbMetric: 2.4328 - val_loss: 2.4058 - val_MinusLogProbMetric: 2.4058 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 33/1000
2023-09-09 09:37:33.101 
Epoch 33/1000 
	 loss: 2.4297, MinusLogProbMetric: 2.4297, val_loss: 2.4090, val_MinusLogProbMetric: 2.4090

Epoch 33: val_loss did not improve from 2.39501
196/196 - 11s - loss: 2.4297 - MinusLogProbMetric: 2.4297 - val_loss: 2.4090 - val_MinusLogProbMetric: 2.4090 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 34/1000
2023-09-09 09:37:43.781 
Epoch 34/1000 
	 loss: 2.4414, MinusLogProbMetric: 2.4414, val_loss: 2.4505, val_MinusLogProbMetric: 2.4505

Epoch 34: val_loss did not improve from 2.39501
196/196 - 11s - loss: 2.4414 - MinusLogProbMetric: 2.4414 - val_loss: 2.4505 - val_MinusLogProbMetric: 2.4505 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 35/1000
2023-09-09 09:37:53.778 
Epoch 35/1000 
	 loss: 2.4368, MinusLogProbMetric: 2.4368, val_loss: 2.4053, val_MinusLogProbMetric: 2.4053

Epoch 35: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4368 - MinusLogProbMetric: 2.4368 - val_loss: 2.4053 - val_MinusLogProbMetric: 2.4053 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 36/1000
2023-09-09 09:38:04.257 
Epoch 36/1000 
	 loss: 2.4393, MinusLogProbMetric: 2.4393, val_loss: 2.4868, val_MinusLogProbMetric: 2.4868

Epoch 36: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4393 - MinusLogProbMetric: 2.4393 - val_loss: 2.4868 - val_MinusLogProbMetric: 2.4868 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 37/1000
2023-09-09 09:38:14.239 
Epoch 37/1000 
	 loss: 2.4278, MinusLogProbMetric: 2.4278, val_loss: 2.6854, val_MinusLogProbMetric: 2.6854

Epoch 37: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4278 - MinusLogProbMetric: 2.4278 - val_loss: 2.6854 - val_MinusLogProbMetric: 2.6854 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 38/1000
2023-09-09 09:38:24.656 
Epoch 38/1000 
	 loss: 2.4434, MinusLogProbMetric: 2.4434, val_loss: 2.4559, val_MinusLogProbMetric: 2.4559

Epoch 38: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4434 - MinusLogProbMetric: 2.4434 - val_loss: 2.4559 - val_MinusLogProbMetric: 2.4559 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 39/1000
2023-09-09 09:38:34.748 
Epoch 39/1000 
	 loss: 2.4177, MinusLogProbMetric: 2.4177, val_loss: 2.4363, val_MinusLogProbMetric: 2.4363

Epoch 39: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4177 - MinusLogProbMetric: 2.4177 - val_loss: 2.4363 - val_MinusLogProbMetric: 2.4363 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 40/1000
2023-09-09 09:38:44.988 
Epoch 40/1000 
	 loss: 2.4143, MinusLogProbMetric: 2.4143, val_loss: 2.4113, val_MinusLogProbMetric: 2.4113

Epoch 40: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4143 - MinusLogProbMetric: 2.4143 - val_loss: 2.4113 - val_MinusLogProbMetric: 2.4113 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 41/1000
2023-09-09 09:38:55.197 
Epoch 41/1000 
	 loss: 2.4446, MinusLogProbMetric: 2.4446, val_loss: 2.4646, val_MinusLogProbMetric: 2.4646

Epoch 41: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4446 - MinusLogProbMetric: 2.4446 - val_loss: 2.4646 - val_MinusLogProbMetric: 2.4646 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 42/1000
2023-09-09 09:39:05.626 
Epoch 42/1000 
	 loss: 2.4087, MinusLogProbMetric: 2.4087, val_loss: 2.4134, val_MinusLogProbMetric: 2.4134

Epoch 42: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4087 - MinusLogProbMetric: 2.4087 - val_loss: 2.4134 - val_MinusLogProbMetric: 2.4134 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 43/1000
2023-09-09 09:39:16.071 
Epoch 43/1000 
	 loss: 2.4158, MinusLogProbMetric: 2.4158, val_loss: 2.4165, val_MinusLogProbMetric: 2.4165

Epoch 43: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4158 - MinusLogProbMetric: 2.4158 - val_loss: 2.4165 - val_MinusLogProbMetric: 2.4165 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 44/1000
2023-09-09 09:39:26.544 
Epoch 44/1000 
	 loss: 2.4247, MinusLogProbMetric: 2.4247, val_loss: 2.4287, val_MinusLogProbMetric: 2.4287

Epoch 44: val_loss did not improve from 2.39501
196/196 - 10s - loss: 2.4247 - MinusLogProbMetric: 2.4247 - val_loss: 2.4287 - val_MinusLogProbMetric: 2.4287 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 45/1000
2023-09-09 09:39:37.084 
Epoch 45/1000 
	 loss: 2.4243, MinusLogProbMetric: 2.4243, val_loss: 2.4498, val_MinusLogProbMetric: 2.4498

Epoch 45: val_loss did not improve from 2.39501
196/196 - 11s - loss: 2.4243 - MinusLogProbMetric: 2.4243 - val_loss: 2.4498 - val_MinusLogProbMetric: 2.4498 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 46/1000
2023-09-09 09:39:47.560 
Epoch 46/1000 
	 loss: 2.4171, MinusLogProbMetric: 2.4171, val_loss: 2.3940, val_MinusLogProbMetric: 2.3940

Epoch 46: val_loss improved from 2.39501 to 2.39403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 2.4171 - MinusLogProbMetric: 2.4171 - val_loss: 2.3940 - val_MinusLogProbMetric: 2.3940 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 47/1000
2023-09-09 09:39:58.361 
Epoch 47/1000 
	 loss: 2.4162, MinusLogProbMetric: 2.4162, val_loss: 2.4589, val_MinusLogProbMetric: 2.4589

Epoch 47: val_loss did not improve from 2.39403
196/196 - 10s - loss: 2.4162 - MinusLogProbMetric: 2.4162 - val_loss: 2.4589 - val_MinusLogProbMetric: 2.4589 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 48/1000
2023-09-09 09:40:08.680 
Epoch 48/1000 
	 loss: 2.4103, MinusLogProbMetric: 2.4103, val_loss: 2.4076, val_MinusLogProbMetric: 2.4076

Epoch 48: val_loss did not improve from 2.39403
196/196 - 10s - loss: 2.4103 - MinusLogProbMetric: 2.4103 - val_loss: 2.4076 - val_MinusLogProbMetric: 2.4076 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 49/1000
2023-09-09 09:40:18.459 
Epoch 49/1000 
	 loss: 2.4124, MinusLogProbMetric: 2.4124, val_loss: 2.4000, val_MinusLogProbMetric: 2.4000

Epoch 49: val_loss did not improve from 2.39403
196/196 - 10s - loss: 2.4124 - MinusLogProbMetric: 2.4124 - val_loss: 2.4000 - val_MinusLogProbMetric: 2.4000 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 50/1000
2023-09-09 09:40:27.855 
Epoch 50/1000 
	 loss: 2.4148, MinusLogProbMetric: 2.4148, val_loss: 2.4912, val_MinusLogProbMetric: 2.4912

Epoch 50: val_loss did not improve from 2.39403
196/196 - 9s - loss: 2.4148 - MinusLogProbMetric: 2.4148 - val_loss: 2.4912 - val_MinusLogProbMetric: 2.4912 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 51/1000
2023-09-09 09:40:38.249 
Epoch 51/1000 
	 loss: 2.4085, MinusLogProbMetric: 2.4085, val_loss: 2.5664, val_MinusLogProbMetric: 2.5664

Epoch 51: val_loss did not improve from 2.39403
196/196 - 10s - loss: 2.4085 - MinusLogProbMetric: 2.4085 - val_loss: 2.5664 - val_MinusLogProbMetric: 2.5664 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 52/1000
2023-09-09 09:40:48.459 
Epoch 52/1000 
	 loss: 2.4134, MinusLogProbMetric: 2.4134, val_loss: 2.3960, val_MinusLogProbMetric: 2.3960

Epoch 52: val_loss did not improve from 2.39403
196/196 - 10s - loss: 2.4134 - MinusLogProbMetric: 2.4134 - val_loss: 2.3960 - val_MinusLogProbMetric: 2.3960 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 53/1000
2023-09-09 09:40:59.005 
Epoch 53/1000 
	 loss: 2.4060, MinusLogProbMetric: 2.4060, val_loss: 2.3871, val_MinusLogProbMetric: 2.3871

Epoch 53: val_loss improved from 2.39403 to 2.38710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 2.4060 - MinusLogProbMetric: 2.4060 - val_loss: 2.3871 - val_MinusLogProbMetric: 2.3871 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 54/1000
2023-09-09 09:41:09.444 
Epoch 54/1000 
	 loss: 2.4153, MinusLogProbMetric: 2.4153, val_loss: 2.3916, val_MinusLogProbMetric: 2.3916

Epoch 54: val_loss did not improve from 2.38710
196/196 - 10s - loss: 2.4153 - MinusLogProbMetric: 2.4153 - val_loss: 2.3916 - val_MinusLogProbMetric: 2.3916 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 55/1000
2023-09-09 09:41:19.384 
Epoch 55/1000 
	 loss: 2.4019, MinusLogProbMetric: 2.4019, val_loss: 2.4711, val_MinusLogProbMetric: 2.4711

Epoch 55: val_loss did not improve from 2.38710
196/196 - 10s - loss: 2.4019 - MinusLogProbMetric: 2.4019 - val_loss: 2.4711 - val_MinusLogProbMetric: 2.4711 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 56/1000
2023-09-09 09:41:30.030 
Epoch 56/1000 
	 loss: 2.4172, MinusLogProbMetric: 2.4172, val_loss: 2.3913, val_MinusLogProbMetric: 2.3913

Epoch 56: val_loss did not improve from 2.38710
196/196 - 11s - loss: 2.4172 - MinusLogProbMetric: 2.4172 - val_loss: 2.3913 - val_MinusLogProbMetric: 2.3913 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 57/1000
2023-09-09 09:41:38.743 
Epoch 57/1000 
	 loss: 2.4013, MinusLogProbMetric: 2.4013, val_loss: 2.4247, val_MinusLogProbMetric: 2.4247

Epoch 57: val_loss did not improve from 2.38710
196/196 - 9s - loss: 2.4013 - MinusLogProbMetric: 2.4013 - val_loss: 2.4247 - val_MinusLogProbMetric: 2.4247 - lr: 0.0010 - 9s/epoch - 44ms/step
Epoch 58/1000
2023-09-09 09:41:49.059 
Epoch 58/1000 
	 loss: 2.4013, MinusLogProbMetric: 2.4013, val_loss: 2.5678, val_MinusLogProbMetric: 2.5678

Epoch 58: val_loss did not improve from 2.38710
196/196 - 10s - loss: 2.4013 - MinusLogProbMetric: 2.4013 - val_loss: 2.5678 - val_MinusLogProbMetric: 2.5678 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 59/1000
2023-09-09 09:41:59.223 
Epoch 59/1000 
	 loss: 2.4040, MinusLogProbMetric: 2.4040, val_loss: 2.4253, val_MinusLogProbMetric: 2.4253

Epoch 59: val_loss did not improve from 2.38710
196/196 - 10s - loss: 2.4040 - MinusLogProbMetric: 2.4040 - val_loss: 2.4253 - val_MinusLogProbMetric: 2.4253 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 60/1000
2023-09-09 09:42:09.736 
Epoch 60/1000 
	 loss: 2.4091, MinusLogProbMetric: 2.4091, val_loss: 2.4007, val_MinusLogProbMetric: 2.4007

Epoch 60: val_loss did not improve from 2.38710
196/196 - 11s - loss: 2.4091 - MinusLogProbMetric: 2.4091 - val_loss: 2.4007 - val_MinusLogProbMetric: 2.4007 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 61/1000
2023-09-09 09:42:20.080 
Epoch 61/1000 
	 loss: 2.4116, MinusLogProbMetric: 2.4116, val_loss: 2.4191, val_MinusLogProbMetric: 2.4191

Epoch 61: val_loss did not improve from 2.38710
196/196 - 10s - loss: 2.4116 - MinusLogProbMetric: 2.4116 - val_loss: 2.4191 - val_MinusLogProbMetric: 2.4191 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 62/1000
2023-09-09 09:42:30.441 
Epoch 62/1000 
	 loss: 2.4056, MinusLogProbMetric: 2.4056, val_loss: 2.4382, val_MinusLogProbMetric: 2.4382

Epoch 62: val_loss did not improve from 2.38710
196/196 - 10s - loss: 2.4056 - MinusLogProbMetric: 2.4056 - val_loss: 2.4382 - val_MinusLogProbMetric: 2.4382 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 63/1000
2023-09-09 09:42:40.946 
Epoch 63/1000 
	 loss: 2.4079, MinusLogProbMetric: 2.4079, val_loss: 2.3900, val_MinusLogProbMetric: 2.3900

Epoch 63: val_loss did not improve from 2.38710
196/196 - 11s - loss: 2.4079 - MinusLogProbMetric: 2.4079 - val_loss: 2.3900 - val_MinusLogProbMetric: 2.3900 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 64/1000
2023-09-09 09:42:51.564 
Epoch 64/1000 
	 loss: 2.4126, MinusLogProbMetric: 2.4126, val_loss: 2.3773, val_MinusLogProbMetric: 2.3773

Epoch 64: val_loss improved from 2.38710 to 2.37725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 2.4126 - MinusLogProbMetric: 2.4126 - val_loss: 2.3773 - val_MinusLogProbMetric: 2.3773 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 65/1000
2023-09-09 09:43:02.316 
Epoch 65/1000 
	 loss: 2.4144, MinusLogProbMetric: 2.4144, val_loss: 2.4200, val_MinusLogProbMetric: 2.4200

Epoch 65: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4144 - MinusLogProbMetric: 2.4144 - val_loss: 2.4200 - val_MinusLogProbMetric: 2.4200 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 66/1000
2023-09-09 09:43:12.486 
Epoch 66/1000 
	 loss: 2.3988, MinusLogProbMetric: 2.3988, val_loss: 2.3860, val_MinusLogProbMetric: 2.3860

Epoch 66: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3988 - MinusLogProbMetric: 2.3988 - val_loss: 2.3860 - val_MinusLogProbMetric: 2.3860 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 67/1000
2023-09-09 09:43:21.894 
Epoch 67/1000 
	 loss: 2.4112, MinusLogProbMetric: 2.4112, val_loss: 2.4235, val_MinusLogProbMetric: 2.4235

Epoch 67: val_loss did not improve from 2.37725
196/196 - 9s - loss: 2.4112 - MinusLogProbMetric: 2.4112 - val_loss: 2.4235 - val_MinusLogProbMetric: 2.4235 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 68/1000
2023-09-09 09:43:31.549 
Epoch 68/1000 
	 loss: 2.4010, MinusLogProbMetric: 2.4010, val_loss: 2.4033, val_MinusLogProbMetric: 2.4033

Epoch 68: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4010 - MinusLogProbMetric: 2.4010 - val_loss: 2.4033 - val_MinusLogProbMetric: 2.4033 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 69/1000
2023-09-09 09:43:41.874 
Epoch 69/1000 
	 loss: 2.4032, MinusLogProbMetric: 2.4032, val_loss: 2.4064, val_MinusLogProbMetric: 2.4064

Epoch 69: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4032 - MinusLogProbMetric: 2.4032 - val_loss: 2.4064 - val_MinusLogProbMetric: 2.4064 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 70/1000
2023-09-09 09:43:50.739 
Epoch 70/1000 
	 loss: 2.4044, MinusLogProbMetric: 2.4044, val_loss: 2.4821, val_MinusLogProbMetric: 2.4821

Epoch 70: val_loss did not improve from 2.37725
196/196 - 9s - loss: 2.4044 - MinusLogProbMetric: 2.4044 - val_loss: 2.4821 - val_MinusLogProbMetric: 2.4821 - lr: 0.0010 - 9s/epoch - 45ms/step
Epoch 71/1000
2023-09-09 09:44:00.522 
Epoch 71/1000 
	 loss: 2.4009, MinusLogProbMetric: 2.4009, val_loss: 2.3915, val_MinusLogProbMetric: 2.3915

Epoch 71: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4009 - MinusLogProbMetric: 2.4009 - val_loss: 2.3915 - val_MinusLogProbMetric: 2.3915 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 72/1000
2023-09-09 09:44:10.630 
Epoch 72/1000 
	 loss: 2.4010, MinusLogProbMetric: 2.4010, val_loss: 2.3902, val_MinusLogProbMetric: 2.3902

Epoch 72: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4010 - MinusLogProbMetric: 2.4010 - val_loss: 2.3902 - val_MinusLogProbMetric: 2.3902 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 73/1000
2023-09-09 09:44:21.196 
Epoch 73/1000 
	 loss: 2.3966, MinusLogProbMetric: 2.3966, val_loss: 2.4315, val_MinusLogProbMetric: 2.4315

Epoch 73: val_loss did not improve from 2.37725
196/196 - 11s - loss: 2.3966 - MinusLogProbMetric: 2.3966 - val_loss: 2.4315 - val_MinusLogProbMetric: 2.4315 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 74/1000
2023-09-09 09:44:31.409 
Epoch 74/1000 
	 loss: 2.4096, MinusLogProbMetric: 2.4096, val_loss: 2.3983, val_MinusLogProbMetric: 2.3983

Epoch 74: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4096 - MinusLogProbMetric: 2.4096 - val_loss: 2.3983 - val_MinusLogProbMetric: 2.3983 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 75/1000
2023-09-09 09:44:41.638 
Epoch 75/1000 
	 loss: 2.3978, MinusLogProbMetric: 2.3978, val_loss: 2.3840, val_MinusLogProbMetric: 2.3840

Epoch 75: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3978 - MinusLogProbMetric: 2.3978 - val_loss: 2.3840 - val_MinusLogProbMetric: 2.3840 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 76/1000
2023-09-09 09:44:51.416 
Epoch 76/1000 
	 loss: 2.4027, MinusLogProbMetric: 2.4027, val_loss: 2.3929, val_MinusLogProbMetric: 2.3929

Epoch 76: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4027 - MinusLogProbMetric: 2.4027 - val_loss: 2.3929 - val_MinusLogProbMetric: 2.3929 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 77/1000
2023-09-09 09:45:00.359 
Epoch 77/1000 
	 loss: 2.3934, MinusLogProbMetric: 2.3934, val_loss: 2.5704, val_MinusLogProbMetric: 2.5704

Epoch 77: val_loss did not improve from 2.37725
196/196 - 9s - loss: 2.3934 - MinusLogProbMetric: 2.3934 - val_loss: 2.5704 - val_MinusLogProbMetric: 2.5704 - lr: 0.0010 - 9s/epoch - 46ms/step
Epoch 78/1000
2023-09-09 09:45:10.544 
Epoch 78/1000 
	 loss: 2.4120, MinusLogProbMetric: 2.4120, val_loss: 2.5835, val_MinusLogProbMetric: 2.5835

Epoch 78: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4120 - MinusLogProbMetric: 2.4120 - val_loss: 2.5835 - val_MinusLogProbMetric: 2.5835 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 79/1000
2023-09-09 09:45:20.999 
Epoch 79/1000 
	 loss: 2.3957, MinusLogProbMetric: 2.3957, val_loss: 2.3866, val_MinusLogProbMetric: 2.3866

Epoch 79: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3957 - MinusLogProbMetric: 2.3957 - val_loss: 2.3866 - val_MinusLogProbMetric: 2.3866 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 80/1000
2023-09-09 09:45:31.277 
Epoch 80/1000 
	 loss: 2.3951, MinusLogProbMetric: 2.3951, val_loss: 2.3985, val_MinusLogProbMetric: 2.3985

Epoch 80: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3951 - MinusLogProbMetric: 2.3951 - val_loss: 2.3985 - val_MinusLogProbMetric: 2.3985 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 81/1000
2023-09-09 09:45:41.609 
Epoch 81/1000 
	 loss: 2.4068, MinusLogProbMetric: 2.4068, val_loss: 2.3850, val_MinusLogProbMetric: 2.3850

Epoch 81: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4068 - MinusLogProbMetric: 2.4068 - val_loss: 2.3850 - val_MinusLogProbMetric: 2.3850 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 82/1000
2023-09-09 09:45:51.791 
Epoch 82/1000 
	 loss: 2.3914, MinusLogProbMetric: 2.3914, val_loss: 2.4008, val_MinusLogProbMetric: 2.4008

Epoch 82: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3914 - MinusLogProbMetric: 2.3914 - val_loss: 2.4008 - val_MinusLogProbMetric: 2.4008 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 83/1000
2023-09-09 09:46:01.973 
Epoch 83/1000 
	 loss: 2.3913, MinusLogProbMetric: 2.3913, val_loss: 2.5591, val_MinusLogProbMetric: 2.5591

Epoch 83: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3913 - MinusLogProbMetric: 2.3913 - val_loss: 2.5591 - val_MinusLogProbMetric: 2.5591 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 84/1000
2023-09-09 09:46:12.399 
Epoch 84/1000 
	 loss: 2.4044, MinusLogProbMetric: 2.4044, val_loss: 2.4070, val_MinusLogProbMetric: 2.4070

Epoch 84: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4044 - MinusLogProbMetric: 2.4044 - val_loss: 2.4070 - val_MinusLogProbMetric: 2.4070 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 85/1000
2023-09-09 09:46:22.623 
Epoch 85/1000 
	 loss: 2.3990, MinusLogProbMetric: 2.3990, val_loss: 2.4637, val_MinusLogProbMetric: 2.4637

Epoch 85: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3990 - MinusLogProbMetric: 2.3990 - val_loss: 2.4637 - val_MinusLogProbMetric: 2.4637 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 86/1000
2023-09-09 09:46:32.850 
Epoch 86/1000 
	 loss: 2.3877, MinusLogProbMetric: 2.3877, val_loss: 2.4371, val_MinusLogProbMetric: 2.4371

Epoch 86: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3877 - MinusLogProbMetric: 2.3877 - val_loss: 2.4371 - val_MinusLogProbMetric: 2.4371 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 87/1000
2023-09-09 09:46:43.211 
Epoch 87/1000 
	 loss: 2.3932, MinusLogProbMetric: 2.3932, val_loss: 2.4173, val_MinusLogProbMetric: 2.4173

Epoch 87: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3932 - MinusLogProbMetric: 2.3932 - val_loss: 2.4173 - val_MinusLogProbMetric: 2.4173 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 88/1000
2023-09-09 09:46:53.688 
Epoch 88/1000 
	 loss: 2.3983, MinusLogProbMetric: 2.3983, val_loss: 2.3819, val_MinusLogProbMetric: 2.3819

Epoch 88: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3983 - MinusLogProbMetric: 2.3983 - val_loss: 2.3819 - val_MinusLogProbMetric: 2.3819 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 89/1000
2023-09-09 09:47:04.239 
Epoch 89/1000 
	 loss: 2.3995, MinusLogProbMetric: 2.3995, val_loss: 2.3968, val_MinusLogProbMetric: 2.3968

Epoch 89: val_loss did not improve from 2.37725
196/196 - 11s - loss: 2.3995 - MinusLogProbMetric: 2.3995 - val_loss: 2.3968 - val_MinusLogProbMetric: 2.3968 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 90/1000
2023-09-09 09:47:14.678 
Epoch 90/1000 
	 loss: 2.3953, MinusLogProbMetric: 2.3953, val_loss: 2.4219, val_MinusLogProbMetric: 2.4219

Epoch 90: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3953 - MinusLogProbMetric: 2.3953 - val_loss: 2.4219 - val_MinusLogProbMetric: 2.4219 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 91/1000
2023-09-09 09:47:25.132 
Epoch 91/1000 
	 loss: 2.3910, MinusLogProbMetric: 2.3910, val_loss: 2.3812, val_MinusLogProbMetric: 2.3812

Epoch 91: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3910 - MinusLogProbMetric: 2.3910 - val_loss: 2.3812 - val_MinusLogProbMetric: 2.3812 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 92/1000
2023-09-09 09:47:35.337 
Epoch 92/1000 
	 loss: 2.3953, MinusLogProbMetric: 2.3953, val_loss: 2.4343, val_MinusLogProbMetric: 2.4343

Epoch 92: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3953 - MinusLogProbMetric: 2.3953 - val_loss: 2.4343 - val_MinusLogProbMetric: 2.4343 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 93/1000
2023-09-09 09:47:45.666 
Epoch 93/1000 
	 loss: 2.4008, MinusLogProbMetric: 2.4008, val_loss: 2.3850, val_MinusLogProbMetric: 2.3850

Epoch 93: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.4008 - MinusLogProbMetric: 2.4008 - val_loss: 2.3850 - val_MinusLogProbMetric: 2.3850 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-09-09 09:47:55.998 
Epoch 94/1000 
	 loss: 2.3921, MinusLogProbMetric: 2.3921, val_loss: 2.4490, val_MinusLogProbMetric: 2.4490

Epoch 94: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3921 - MinusLogProbMetric: 2.3921 - val_loss: 2.4490 - val_MinusLogProbMetric: 2.4490 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 95/1000
2023-09-09 09:48:06.015 
Epoch 95/1000 
	 loss: 2.3955, MinusLogProbMetric: 2.3955, val_loss: 2.4361, val_MinusLogProbMetric: 2.4361

Epoch 95: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3955 - MinusLogProbMetric: 2.3955 - val_loss: 2.4361 - val_MinusLogProbMetric: 2.4361 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 96/1000
2023-09-09 09:48:15.818 
Epoch 96/1000 
	 loss: 2.3998, MinusLogProbMetric: 2.3998, val_loss: 2.3839, val_MinusLogProbMetric: 2.3839

Epoch 96: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3998 - MinusLogProbMetric: 2.3998 - val_loss: 2.3839 - val_MinusLogProbMetric: 2.3839 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 97/1000
2023-09-09 09:48:25.435 
Epoch 97/1000 
	 loss: 2.3886, MinusLogProbMetric: 2.3886, val_loss: 2.4031, val_MinusLogProbMetric: 2.4031

Epoch 97: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3886 - MinusLogProbMetric: 2.3886 - val_loss: 2.4031 - val_MinusLogProbMetric: 2.4031 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 98/1000
2023-09-09 09:48:35.598 
Epoch 98/1000 
	 loss: 2.3960, MinusLogProbMetric: 2.3960, val_loss: 2.4005, val_MinusLogProbMetric: 2.4005

Epoch 98: val_loss did not improve from 2.37725
196/196 - 10s - loss: 2.3960 - MinusLogProbMetric: 2.3960 - val_loss: 2.4005 - val_MinusLogProbMetric: 2.4005 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 99/1000
2023-09-09 09:48:45.303 
Epoch 99/1000 
	 loss: 2.3803, MinusLogProbMetric: 2.3803, val_loss: 2.3738, val_MinusLogProbMetric: 2.3738

Epoch 99: val_loss improved from 2.37725 to 2.37385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 10s - loss: 2.3803 - MinusLogProbMetric: 2.3803 - val_loss: 2.3738 - val_MinusLogProbMetric: 2.3738 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 100/1000
2023-09-09 09:48:55.420 
Epoch 100/1000 
	 loss: 2.3978, MinusLogProbMetric: 2.3978, val_loss: 2.3765, val_MinusLogProbMetric: 2.3765

Epoch 100: val_loss did not improve from 2.37385
196/196 - 10s - loss: 2.3978 - MinusLogProbMetric: 2.3978 - val_loss: 2.3765 - val_MinusLogProbMetric: 2.3765 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 101/1000
2023-09-09 09:49:05.832 
Epoch 101/1000 
	 loss: 2.3883, MinusLogProbMetric: 2.3883, val_loss: 2.4318, val_MinusLogProbMetric: 2.4318

Epoch 101: val_loss did not improve from 2.37385
196/196 - 10s - loss: 2.3883 - MinusLogProbMetric: 2.3883 - val_loss: 2.4318 - val_MinusLogProbMetric: 2.4318 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 102/1000
2023-09-09 09:49:15.747 
Epoch 102/1000 
	 loss: 2.3957, MinusLogProbMetric: 2.3957, val_loss: 2.3885, val_MinusLogProbMetric: 2.3885

Epoch 102: val_loss did not improve from 2.37385
196/196 - 10s - loss: 2.3957 - MinusLogProbMetric: 2.3957 - val_loss: 2.3885 - val_MinusLogProbMetric: 2.3885 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 103/1000
2023-09-09 09:49:26.092 
Epoch 103/1000 
	 loss: 2.3867, MinusLogProbMetric: 2.3867, val_loss: 2.4358, val_MinusLogProbMetric: 2.4358

Epoch 103: val_loss did not improve from 2.37385
196/196 - 10s - loss: 2.3867 - MinusLogProbMetric: 2.3867 - val_loss: 2.4358 - val_MinusLogProbMetric: 2.4358 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 104/1000
2023-09-09 09:49:36.484 
Epoch 104/1000 
	 loss: 2.3911, MinusLogProbMetric: 2.3911, val_loss: 2.4125, val_MinusLogProbMetric: 2.4125

Epoch 104: val_loss did not improve from 2.37385
196/196 - 10s - loss: 2.3911 - MinusLogProbMetric: 2.3911 - val_loss: 2.4125 - val_MinusLogProbMetric: 2.4125 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 105/1000
2023-09-09 09:49:46.768 
Epoch 105/1000 
	 loss: 2.3906, MinusLogProbMetric: 2.3906, val_loss: 2.3907, val_MinusLogProbMetric: 2.3907

Epoch 105: val_loss did not improve from 2.37385
196/196 - 10s - loss: 2.3906 - MinusLogProbMetric: 2.3906 - val_loss: 2.3907 - val_MinusLogProbMetric: 2.3907 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 106/1000
2023-09-09 09:49:57.037 
Epoch 106/1000 
	 loss: 2.3960, MinusLogProbMetric: 2.3960, val_loss: 2.3854, val_MinusLogProbMetric: 2.3854

Epoch 106: val_loss did not improve from 2.37385
196/196 - 10s - loss: 2.3960 - MinusLogProbMetric: 2.3960 - val_loss: 2.3854 - val_MinusLogProbMetric: 2.3854 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 107/1000
2023-09-09 09:50:07.326 
Epoch 107/1000 
	 loss: 2.3833, MinusLogProbMetric: 2.3833, val_loss: 2.3714, val_MinusLogProbMetric: 2.3714

Epoch 107: val_loss improved from 2.37385 to 2.37142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 2.3833 - MinusLogProbMetric: 2.3833 - val_loss: 2.3714 - val_MinusLogProbMetric: 2.3714 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 108/1000
2023-09-09 09:50:18.174 
Epoch 108/1000 
	 loss: 2.3849, MinusLogProbMetric: 2.3849, val_loss: 2.3919, val_MinusLogProbMetric: 2.3919

Epoch 108: val_loss did not improve from 2.37142
196/196 - 11s - loss: 2.3849 - MinusLogProbMetric: 2.3849 - val_loss: 2.3919 - val_MinusLogProbMetric: 2.3919 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 109/1000
2023-09-09 09:50:27.514 
Epoch 109/1000 
	 loss: 2.3865, MinusLogProbMetric: 2.3865, val_loss: 2.3938, val_MinusLogProbMetric: 2.3938

Epoch 109: val_loss did not improve from 2.37142
196/196 - 9s - loss: 2.3865 - MinusLogProbMetric: 2.3865 - val_loss: 2.3938 - val_MinusLogProbMetric: 2.3938 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 110/1000
2023-09-09 09:50:37.695 
Epoch 110/1000 
	 loss: 2.3969, MinusLogProbMetric: 2.3969, val_loss: 2.4117, val_MinusLogProbMetric: 2.4117

Epoch 110: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3969 - MinusLogProbMetric: 2.3969 - val_loss: 2.4117 - val_MinusLogProbMetric: 2.4117 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 111/1000
2023-09-09 09:50:48.367 
Epoch 111/1000 
	 loss: 2.3873, MinusLogProbMetric: 2.3873, val_loss: 2.3908, val_MinusLogProbMetric: 2.3908

Epoch 111: val_loss did not improve from 2.37142
196/196 - 11s - loss: 2.3873 - MinusLogProbMetric: 2.3873 - val_loss: 2.3908 - val_MinusLogProbMetric: 2.3908 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 112/1000
2023-09-09 09:50:58.466 
Epoch 112/1000 
	 loss: 2.3813, MinusLogProbMetric: 2.3813, val_loss: 2.3764, val_MinusLogProbMetric: 2.3764

Epoch 112: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3813 - MinusLogProbMetric: 2.3813 - val_loss: 2.3764 - val_MinusLogProbMetric: 2.3764 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 113/1000
2023-09-09 09:51:08.191 
Epoch 113/1000 
	 loss: 2.3860, MinusLogProbMetric: 2.3860, val_loss: 2.3791, val_MinusLogProbMetric: 2.3791

Epoch 113: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3860 - MinusLogProbMetric: 2.3860 - val_loss: 2.3791 - val_MinusLogProbMetric: 2.3791 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 114/1000
2023-09-09 09:51:18.724 
Epoch 114/1000 
	 loss: 2.3834, MinusLogProbMetric: 2.3834, val_loss: 2.3966, val_MinusLogProbMetric: 2.3966

Epoch 114: val_loss did not improve from 2.37142
196/196 - 11s - loss: 2.3834 - MinusLogProbMetric: 2.3834 - val_loss: 2.3966 - val_MinusLogProbMetric: 2.3966 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 115/1000
2023-09-09 09:51:28.344 
Epoch 115/1000 
	 loss: 2.3852, MinusLogProbMetric: 2.3852, val_loss: 2.3908, val_MinusLogProbMetric: 2.3908

Epoch 115: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3852 - MinusLogProbMetric: 2.3852 - val_loss: 2.3908 - val_MinusLogProbMetric: 2.3908 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 116/1000
2023-09-09 09:51:38.592 
Epoch 116/1000 
	 loss: 2.3867, MinusLogProbMetric: 2.3867, val_loss: 2.3874, val_MinusLogProbMetric: 2.3874

Epoch 116: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3867 - MinusLogProbMetric: 2.3867 - val_loss: 2.3874 - val_MinusLogProbMetric: 2.3874 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 117/1000
2023-09-09 09:51:48.498 
Epoch 117/1000 
	 loss: 2.3854, MinusLogProbMetric: 2.3854, val_loss: 2.4497, val_MinusLogProbMetric: 2.4497

Epoch 117: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3854 - MinusLogProbMetric: 2.3854 - val_loss: 2.4497 - val_MinusLogProbMetric: 2.4497 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 118/1000
2023-09-09 09:51:58.025 
Epoch 118/1000 
	 loss: 2.3894, MinusLogProbMetric: 2.3894, val_loss: 2.3872, val_MinusLogProbMetric: 2.3872

Epoch 118: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3894 - MinusLogProbMetric: 2.3894 - val_loss: 2.3872 - val_MinusLogProbMetric: 2.3872 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 119/1000
2023-09-09 09:52:08.090 
Epoch 119/1000 
	 loss: 2.3832, MinusLogProbMetric: 2.3832, val_loss: 2.3821, val_MinusLogProbMetric: 2.3821

Epoch 119: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3832 - MinusLogProbMetric: 2.3832 - val_loss: 2.3821 - val_MinusLogProbMetric: 2.3821 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 120/1000
2023-09-09 09:52:18.354 
Epoch 120/1000 
	 loss: 2.3831, MinusLogProbMetric: 2.3831, val_loss: 2.3781, val_MinusLogProbMetric: 2.3781

Epoch 120: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3831 - MinusLogProbMetric: 2.3831 - val_loss: 2.3781 - val_MinusLogProbMetric: 2.3781 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 121/1000
2023-09-09 09:52:28.814 
Epoch 121/1000 
	 loss: 2.3916, MinusLogProbMetric: 2.3916, val_loss: 2.4483, val_MinusLogProbMetric: 2.4483

Epoch 121: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3916 - MinusLogProbMetric: 2.3916 - val_loss: 2.4483 - val_MinusLogProbMetric: 2.4483 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 122/1000
2023-09-09 09:52:39.079 
Epoch 122/1000 
	 loss: 2.3866, MinusLogProbMetric: 2.3866, val_loss: 2.3792, val_MinusLogProbMetric: 2.3792

Epoch 122: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3866 - MinusLogProbMetric: 2.3866 - val_loss: 2.3792 - val_MinusLogProbMetric: 2.3792 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 123/1000
2023-09-09 09:52:49.467 
Epoch 123/1000 
	 loss: 2.3873, MinusLogProbMetric: 2.3873, val_loss: 2.4238, val_MinusLogProbMetric: 2.4238

Epoch 123: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3873 - MinusLogProbMetric: 2.3873 - val_loss: 2.4238 - val_MinusLogProbMetric: 2.4238 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 124/1000
2023-09-09 09:52:59.865 
Epoch 124/1000 
	 loss: 2.3916, MinusLogProbMetric: 2.3916, val_loss: 2.4089, val_MinusLogProbMetric: 2.4089

Epoch 124: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3916 - MinusLogProbMetric: 2.3916 - val_loss: 2.4089 - val_MinusLogProbMetric: 2.4089 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 125/1000
2023-09-09 09:53:10.241 
Epoch 125/1000 
	 loss: 2.3813, MinusLogProbMetric: 2.3813, val_loss: 2.4122, val_MinusLogProbMetric: 2.4122

Epoch 125: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3813 - MinusLogProbMetric: 2.3813 - val_loss: 2.4122 - val_MinusLogProbMetric: 2.4122 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 126/1000
2023-09-09 09:53:20.445 
Epoch 126/1000 
	 loss: 2.3961, MinusLogProbMetric: 2.3961, val_loss: 2.4669, val_MinusLogProbMetric: 2.4669

Epoch 126: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3961 - MinusLogProbMetric: 2.3961 - val_loss: 2.4669 - val_MinusLogProbMetric: 2.4669 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 127/1000
2023-09-09 09:53:30.787 
Epoch 127/1000 
	 loss: 2.3883, MinusLogProbMetric: 2.3883, val_loss: 2.4333, val_MinusLogProbMetric: 2.4333

Epoch 127: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3883 - MinusLogProbMetric: 2.3883 - val_loss: 2.4333 - val_MinusLogProbMetric: 2.4333 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 128/1000
2023-09-09 09:53:40.430 
Epoch 128/1000 
	 loss: 2.3966, MinusLogProbMetric: 2.3966, val_loss: 2.3906, val_MinusLogProbMetric: 2.3906

Epoch 128: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3966 - MinusLogProbMetric: 2.3966 - val_loss: 2.3906 - val_MinusLogProbMetric: 2.3906 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 129/1000
2023-09-09 09:53:50.550 
Epoch 129/1000 
	 loss: 2.3835, MinusLogProbMetric: 2.3835, val_loss: 2.3728, val_MinusLogProbMetric: 2.3728

Epoch 129: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3835 - MinusLogProbMetric: 2.3835 - val_loss: 2.3728 - val_MinusLogProbMetric: 2.3728 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 130/1000
2023-09-09 09:54:00.848 
Epoch 130/1000 
	 loss: 2.3762, MinusLogProbMetric: 2.3762, val_loss: 2.4038, val_MinusLogProbMetric: 2.4038

Epoch 130: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3762 - MinusLogProbMetric: 2.3762 - val_loss: 2.4038 - val_MinusLogProbMetric: 2.4038 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 131/1000
2023-09-09 09:54:11.260 
Epoch 131/1000 
	 loss: 2.3800, MinusLogProbMetric: 2.3800, val_loss: 2.4139, val_MinusLogProbMetric: 2.4139

Epoch 131: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3800 - MinusLogProbMetric: 2.3800 - val_loss: 2.4139 - val_MinusLogProbMetric: 2.4139 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 132/1000
2023-09-09 09:54:21.568 
Epoch 132/1000 
	 loss: 2.3819, MinusLogProbMetric: 2.3819, val_loss: 2.4222, val_MinusLogProbMetric: 2.4222

Epoch 132: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3819 - MinusLogProbMetric: 2.3819 - val_loss: 2.4222 - val_MinusLogProbMetric: 2.4222 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 133/1000
2023-09-09 09:54:31.972 
Epoch 133/1000 
	 loss: 2.3806, MinusLogProbMetric: 2.3806, val_loss: 2.3741, val_MinusLogProbMetric: 2.3741

Epoch 133: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3806 - MinusLogProbMetric: 2.3806 - val_loss: 2.3741 - val_MinusLogProbMetric: 2.3741 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 134/1000
2023-09-09 09:54:42.381 
Epoch 134/1000 
	 loss: 2.3836, MinusLogProbMetric: 2.3836, val_loss: 2.3801, val_MinusLogProbMetric: 2.3801

Epoch 134: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3836 - MinusLogProbMetric: 2.3836 - val_loss: 2.3801 - val_MinusLogProbMetric: 2.3801 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 135/1000
2023-09-09 09:54:52.284 
Epoch 135/1000 
	 loss: 2.3854, MinusLogProbMetric: 2.3854, val_loss: 2.3845, val_MinusLogProbMetric: 2.3845

Epoch 135: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3854 - MinusLogProbMetric: 2.3854 - val_loss: 2.3845 - val_MinusLogProbMetric: 2.3845 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 136/1000
2023-09-09 09:55:02.711 
Epoch 136/1000 
	 loss: 2.3876, MinusLogProbMetric: 2.3876, val_loss: 2.4089, val_MinusLogProbMetric: 2.4089

Epoch 136: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3876 - MinusLogProbMetric: 2.3876 - val_loss: 2.4089 - val_MinusLogProbMetric: 2.4089 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 137/1000
2023-09-09 09:55:13.067 
Epoch 137/1000 
	 loss: 2.3857, MinusLogProbMetric: 2.3857, val_loss: 2.4223, val_MinusLogProbMetric: 2.4223

Epoch 137: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3857 - MinusLogProbMetric: 2.3857 - val_loss: 2.4223 - val_MinusLogProbMetric: 2.4223 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 138/1000
2023-09-09 09:55:23.456 
Epoch 138/1000 
	 loss: 2.3881, MinusLogProbMetric: 2.3881, val_loss: 2.3829, val_MinusLogProbMetric: 2.3829

Epoch 138: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3881 - MinusLogProbMetric: 2.3881 - val_loss: 2.3829 - val_MinusLogProbMetric: 2.3829 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 139/1000
2023-09-09 09:55:33.718 
Epoch 139/1000 
	 loss: 2.3845, MinusLogProbMetric: 2.3845, val_loss: 2.3772, val_MinusLogProbMetric: 2.3772

Epoch 139: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3845 - MinusLogProbMetric: 2.3845 - val_loss: 2.3772 - val_MinusLogProbMetric: 2.3772 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 140/1000
2023-09-09 09:55:44.105 
Epoch 140/1000 
	 loss: 2.3926, MinusLogProbMetric: 2.3926, val_loss: 2.4377, val_MinusLogProbMetric: 2.4377

Epoch 140: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3926 - MinusLogProbMetric: 2.3926 - val_loss: 2.4377 - val_MinusLogProbMetric: 2.4377 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 141/1000
2023-09-09 09:55:53.773 
Epoch 141/1000 
	 loss: 2.3871, MinusLogProbMetric: 2.3871, val_loss: 2.4093, val_MinusLogProbMetric: 2.4093

Epoch 141: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3871 - MinusLogProbMetric: 2.3871 - val_loss: 2.4093 - val_MinusLogProbMetric: 2.4093 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 142/1000
2023-09-09 09:56:02.664 
Epoch 142/1000 
	 loss: 2.3816, MinusLogProbMetric: 2.3816, val_loss: 2.3934, val_MinusLogProbMetric: 2.3934

Epoch 142: val_loss did not improve from 2.37142
196/196 - 9s - loss: 2.3816 - MinusLogProbMetric: 2.3816 - val_loss: 2.3934 - val_MinusLogProbMetric: 2.3934 - lr: 0.0010 - 9s/epoch - 45ms/step
Epoch 143/1000
2023-09-09 09:56:12.852 
Epoch 143/1000 
	 loss: 2.3873, MinusLogProbMetric: 2.3873, val_loss: 2.3719, val_MinusLogProbMetric: 2.3719

Epoch 143: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3873 - MinusLogProbMetric: 2.3873 - val_loss: 2.3719 - val_MinusLogProbMetric: 2.3719 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 144/1000
2023-09-09 09:56:23.235 
Epoch 144/1000 
	 loss: 2.3771, MinusLogProbMetric: 2.3771, val_loss: 2.4116, val_MinusLogProbMetric: 2.4116

Epoch 144: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3771 - MinusLogProbMetric: 2.3771 - val_loss: 2.4116 - val_MinusLogProbMetric: 2.4116 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 145/1000
2023-09-09 09:56:32.581 
Epoch 145/1000 
	 loss: 2.3799, MinusLogProbMetric: 2.3799, val_loss: 2.3946, val_MinusLogProbMetric: 2.3946

Epoch 145: val_loss did not improve from 2.37142
196/196 - 9s - loss: 2.3799 - MinusLogProbMetric: 2.3799 - val_loss: 2.3946 - val_MinusLogProbMetric: 2.3946 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 146/1000
2023-09-09 09:56:42.770 
Epoch 146/1000 
	 loss: 2.3822, MinusLogProbMetric: 2.3822, val_loss: 2.3790, val_MinusLogProbMetric: 2.3790

Epoch 146: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3822 - MinusLogProbMetric: 2.3822 - val_loss: 2.3790 - val_MinusLogProbMetric: 2.3790 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 147/1000
2023-09-09 09:56:53.045 
Epoch 147/1000 
	 loss: 2.3863, MinusLogProbMetric: 2.3863, val_loss: 2.4349, val_MinusLogProbMetric: 2.4349

Epoch 147: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3863 - MinusLogProbMetric: 2.3863 - val_loss: 2.4349 - val_MinusLogProbMetric: 2.4349 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 148/1000
2023-09-09 09:57:03.454 
Epoch 148/1000 
	 loss: 2.3796, MinusLogProbMetric: 2.3796, val_loss: 2.3942, val_MinusLogProbMetric: 2.3942

Epoch 148: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3796 - MinusLogProbMetric: 2.3796 - val_loss: 2.3942 - val_MinusLogProbMetric: 2.3942 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 149/1000
2023-09-09 09:57:14.083 
Epoch 149/1000 
	 loss: 2.3814, MinusLogProbMetric: 2.3814, val_loss: 2.4262, val_MinusLogProbMetric: 2.4262

Epoch 149: val_loss did not improve from 2.37142
196/196 - 11s - loss: 2.3814 - MinusLogProbMetric: 2.3814 - val_loss: 2.4262 - val_MinusLogProbMetric: 2.4262 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 150/1000
2023-09-09 09:57:24.491 
Epoch 150/1000 
	 loss: 2.3977, MinusLogProbMetric: 2.3977, val_loss: 2.3941, val_MinusLogProbMetric: 2.3941

Epoch 150: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3977 - MinusLogProbMetric: 2.3977 - val_loss: 2.3941 - val_MinusLogProbMetric: 2.3941 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 151/1000
2023-09-09 09:57:35.003 
Epoch 151/1000 
	 loss: 2.3941, MinusLogProbMetric: 2.3941, val_loss: 2.3780, val_MinusLogProbMetric: 2.3780

Epoch 151: val_loss did not improve from 2.37142
196/196 - 11s - loss: 2.3941 - MinusLogProbMetric: 2.3941 - val_loss: 2.3780 - val_MinusLogProbMetric: 2.3780 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 152/1000
2023-09-09 09:57:44.965 
Epoch 152/1000 
	 loss: 2.3818, MinusLogProbMetric: 2.3818, val_loss: 2.4113, val_MinusLogProbMetric: 2.4113

Epoch 152: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3818 - MinusLogProbMetric: 2.3818 - val_loss: 2.4113 - val_MinusLogProbMetric: 2.4113 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 153/1000
2023-09-09 09:57:55.248 
Epoch 153/1000 
	 loss: 2.3807, MinusLogProbMetric: 2.3807, val_loss: 2.3869, val_MinusLogProbMetric: 2.3869

Epoch 153: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3807 - MinusLogProbMetric: 2.3807 - val_loss: 2.3869 - val_MinusLogProbMetric: 2.3869 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 154/1000
2023-09-09 09:58:05.671 
Epoch 154/1000 
	 loss: 2.3828, MinusLogProbMetric: 2.3828, val_loss: 2.4119, val_MinusLogProbMetric: 2.4120

Epoch 154: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3828 - MinusLogProbMetric: 2.3828 - val_loss: 2.4119 - val_MinusLogProbMetric: 2.4120 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 155/1000
2023-09-09 09:58:15.893 
Epoch 155/1000 
	 loss: 2.3827, MinusLogProbMetric: 2.3827, val_loss: 2.4118, val_MinusLogProbMetric: 2.4118

Epoch 155: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3827 - MinusLogProbMetric: 2.3827 - val_loss: 2.4118 - val_MinusLogProbMetric: 2.4118 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 156/1000
2023-09-09 09:58:26.222 
Epoch 156/1000 
	 loss: 2.3783, MinusLogProbMetric: 2.3783, val_loss: 2.3901, val_MinusLogProbMetric: 2.3901

Epoch 156: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3783 - MinusLogProbMetric: 2.3783 - val_loss: 2.3901 - val_MinusLogProbMetric: 2.3901 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 157/1000
2023-09-09 09:58:36.600 
Epoch 157/1000 
	 loss: 2.3822, MinusLogProbMetric: 2.3822, val_loss: 2.3890, val_MinusLogProbMetric: 2.3890

Epoch 157: val_loss did not improve from 2.37142
196/196 - 10s - loss: 2.3822 - MinusLogProbMetric: 2.3822 - val_loss: 2.3890 - val_MinusLogProbMetric: 2.3890 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 158/1000
2023-09-09 09:58:46.721 
Epoch 158/1000 
	 loss: 2.3616, MinusLogProbMetric: 2.3616, val_loss: 2.3663, val_MinusLogProbMetric: 2.3663

Epoch 158: val_loss improved from 2.37142 to 2.36632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 10s - loss: 2.3616 - MinusLogProbMetric: 2.3616 - val_loss: 2.3663 - val_MinusLogProbMetric: 2.3663 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 159/1000
2023-09-09 09:58:55.992 
Epoch 159/1000 
	 loss: 2.3580, MinusLogProbMetric: 2.3580, val_loss: 2.3636, val_MinusLogProbMetric: 2.3636

Epoch 159: val_loss improved from 2.36632 to 2.36355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 9s - loss: 2.3580 - MinusLogProbMetric: 2.3580 - val_loss: 2.3636 - val_MinusLogProbMetric: 2.3636 - lr: 5.0000e-04 - 9s/epoch - 48ms/step
Epoch 160/1000
2023-09-09 09:59:06.705 
Epoch 160/1000 
	 loss: 2.3599, MinusLogProbMetric: 2.3599, val_loss: 2.3715, val_MinusLogProbMetric: 2.3715

Epoch 160: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3599 - MinusLogProbMetric: 2.3599 - val_loss: 2.3715 - val_MinusLogProbMetric: 2.3715 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 161/1000
2023-09-09 09:59:16.961 
Epoch 161/1000 
	 loss: 2.3596, MinusLogProbMetric: 2.3596, val_loss: 2.3906, val_MinusLogProbMetric: 2.3906

Epoch 161: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3596 - MinusLogProbMetric: 2.3596 - val_loss: 2.3906 - val_MinusLogProbMetric: 2.3906 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 162/1000
2023-09-09 09:59:27.430 
Epoch 162/1000 
	 loss: 2.3557, MinusLogProbMetric: 2.3557, val_loss: 2.3714, val_MinusLogProbMetric: 2.3714

Epoch 162: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3557 - MinusLogProbMetric: 2.3557 - val_loss: 2.3714 - val_MinusLogProbMetric: 2.3714 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 163/1000
2023-09-09 09:59:37.767 
Epoch 163/1000 
	 loss: 2.3549, MinusLogProbMetric: 2.3549, val_loss: 2.3643, val_MinusLogProbMetric: 2.3643

Epoch 163: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3549 - MinusLogProbMetric: 2.3549 - val_loss: 2.3643 - val_MinusLogProbMetric: 2.3643 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 164/1000
2023-09-09 09:59:48.146 
Epoch 164/1000 
	 loss: 2.3588, MinusLogProbMetric: 2.3588, val_loss: 2.3855, val_MinusLogProbMetric: 2.3855

Epoch 164: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3588 - MinusLogProbMetric: 2.3588 - val_loss: 2.3855 - val_MinusLogProbMetric: 2.3855 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 165/1000
2023-09-09 09:59:57.408 
Epoch 165/1000 
	 loss: 2.3596, MinusLogProbMetric: 2.3596, val_loss: 2.3752, val_MinusLogProbMetric: 2.3752

Epoch 165: val_loss did not improve from 2.36355
196/196 - 9s - loss: 2.3596 - MinusLogProbMetric: 2.3596 - val_loss: 2.3752 - val_MinusLogProbMetric: 2.3752 - lr: 5.0000e-04 - 9s/epoch - 47ms/step
Epoch 166/1000
2023-09-09 10:00:06.909 
Epoch 166/1000 
	 loss: 2.3552, MinusLogProbMetric: 2.3552, val_loss: 2.3928, val_MinusLogProbMetric: 2.3928

Epoch 166: val_loss did not improve from 2.36355
196/196 - 9s - loss: 2.3552 - MinusLogProbMetric: 2.3552 - val_loss: 2.3928 - val_MinusLogProbMetric: 2.3928 - lr: 5.0000e-04 - 9s/epoch - 48ms/step
Epoch 167/1000
2023-09-09 10:00:15.924 
Epoch 167/1000 
	 loss: 2.3584, MinusLogProbMetric: 2.3584, val_loss: 2.3913, val_MinusLogProbMetric: 2.3913

Epoch 167: val_loss did not improve from 2.36355
196/196 - 9s - loss: 2.3584 - MinusLogProbMetric: 2.3584 - val_loss: 2.3913 - val_MinusLogProbMetric: 2.3913 - lr: 5.0000e-04 - 9s/epoch - 46ms/step
Epoch 168/1000
2023-09-09 10:00:25.143 
Epoch 168/1000 
	 loss: 2.3601, MinusLogProbMetric: 2.3601, val_loss: 2.3761, val_MinusLogProbMetric: 2.3761

Epoch 168: val_loss did not improve from 2.36355
196/196 - 9s - loss: 2.3601 - MinusLogProbMetric: 2.3601 - val_loss: 2.3761 - val_MinusLogProbMetric: 2.3761 - lr: 5.0000e-04 - 9s/epoch - 47ms/step
Epoch 169/1000
2023-09-09 10:00:34.679 
Epoch 169/1000 
	 loss: 2.3557, MinusLogProbMetric: 2.3557, val_loss: 2.3776, val_MinusLogProbMetric: 2.3776

Epoch 169: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3557 - MinusLogProbMetric: 2.3557 - val_loss: 2.3776 - val_MinusLogProbMetric: 2.3776 - lr: 5.0000e-04 - 10s/epoch - 49ms/step
Epoch 170/1000
2023-09-09 10:00:44.762 
Epoch 170/1000 
	 loss: 2.3551, MinusLogProbMetric: 2.3551, val_loss: 2.3723, val_MinusLogProbMetric: 2.3723

Epoch 170: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3551 - MinusLogProbMetric: 2.3551 - val_loss: 2.3723 - val_MinusLogProbMetric: 2.3723 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 171/1000
2023-09-09 10:00:55.152 
Epoch 171/1000 
	 loss: 2.3569, MinusLogProbMetric: 2.3569, val_loss: 2.3766, val_MinusLogProbMetric: 2.3766

Epoch 171: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3569 - MinusLogProbMetric: 2.3569 - val_loss: 2.3766 - val_MinusLogProbMetric: 2.3766 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 172/1000
2023-09-09 10:01:04.997 
Epoch 172/1000 
	 loss: 2.3550, MinusLogProbMetric: 2.3550, val_loss: 2.3640, val_MinusLogProbMetric: 2.3640

Epoch 172: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3550 - MinusLogProbMetric: 2.3550 - val_loss: 2.3640 - val_MinusLogProbMetric: 2.3640 - lr: 5.0000e-04 - 10s/epoch - 50ms/step
Epoch 173/1000
2023-09-09 10:01:14.621 
Epoch 173/1000 
	 loss: 2.3563, MinusLogProbMetric: 2.3563, val_loss: 2.3668, val_MinusLogProbMetric: 2.3668

Epoch 173: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3563 - MinusLogProbMetric: 2.3563 - val_loss: 2.3668 - val_MinusLogProbMetric: 2.3668 - lr: 5.0000e-04 - 10s/epoch - 49ms/step
Epoch 174/1000
2023-09-09 10:01:24.720 
Epoch 174/1000 
	 loss: 2.3548, MinusLogProbMetric: 2.3548, val_loss: 2.3935, val_MinusLogProbMetric: 2.3935

Epoch 174: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3548 - MinusLogProbMetric: 2.3548 - val_loss: 2.3935 - val_MinusLogProbMetric: 2.3935 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 175/1000
2023-09-09 10:01:35.036 
Epoch 175/1000 
	 loss: 2.3605, MinusLogProbMetric: 2.3605, val_loss: 2.3700, val_MinusLogProbMetric: 2.3700

Epoch 175: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3605 - MinusLogProbMetric: 2.3605 - val_loss: 2.3700 - val_MinusLogProbMetric: 2.3700 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 176/1000
2023-09-09 10:01:45.199 
Epoch 176/1000 
	 loss: 2.3605, MinusLogProbMetric: 2.3605, val_loss: 2.3712, val_MinusLogProbMetric: 2.3712

Epoch 176: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3605 - MinusLogProbMetric: 2.3605 - val_loss: 2.3712 - val_MinusLogProbMetric: 2.3712 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 177/1000
2023-09-09 10:01:55.623 
Epoch 177/1000 
	 loss: 2.3586, MinusLogProbMetric: 2.3586, val_loss: 2.4004, val_MinusLogProbMetric: 2.4004

Epoch 177: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3586 - MinusLogProbMetric: 2.3586 - val_loss: 2.4004 - val_MinusLogProbMetric: 2.4004 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 178/1000
2023-09-09 10:02:06.053 
Epoch 178/1000 
	 loss: 2.3561, MinusLogProbMetric: 2.3561, val_loss: 2.3839, val_MinusLogProbMetric: 2.3839

Epoch 178: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3561 - MinusLogProbMetric: 2.3561 - val_loss: 2.3839 - val_MinusLogProbMetric: 2.3839 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 179/1000
2023-09-09 10:02:16.021 
Epoch 179/1000 
	 loss: 2.3559, MinusLogProbMetric: 2.3559, val_loss: 2.3779, val_MinusLogProbMetric: 2.3779

Epoch 179: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3559 - MinusLogProbMetric: 2.3559 - val_loss: 2.3779 - val_MinusLogProbMetric: 2.3779 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 180/1000
2023-09-09 10:02:26.255 
Epoch 180/1000 
	 loss: 2.3542, MinusLogProbMetric: 2.3542, val_loss: 2.3785, val_MinusLogProbMetric: 2.3785

Epoch 180: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3542 - MinusLogProbMetric: 2.3542 - val_loss: 2.3785 - val_MinusLogProbMetric: 2.3785 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 181/1000
2023-09-09 10:02:36.479 
Epoch 181/1000 
	 loss: 2.3553, MinusLogProbMetric: 2.3553, val_loss: 2.3852, val_MinusLogProbMetric: 2.3852

Epoch 181: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3553 - MinusLogProbMetric: 2.3553 - val_loss: 2.3852 - val_MinusLogProbMetric: 2.3852 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 182/1000
2023-09-09 10:02:47.007 
Epoch 182/1000 
	 loss: 2.3578, MinusLogProbMetric: 2.3578, val_loss: 2.4219, val_MinusLogProbMetric: 2.4219

Epoch 182: val_loss did not improve from 2.36355
196/196 - 11s - loss: 2.3578 - MinusLogProbMetric: 2.3578 - val_loss: 2.4219 - val_MinusLogProbMetric: 2.4219 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 183/1000
2023-09-09 10:02:57.144 
Epoch 183/1000 
	 loss: 2.3562, MinusLogProbMetric: 2.3562, val_loss: 2.3691, val_MinusLogProbMetric: 2.3691

Epoch 183: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3562 - MinusLogProbMetric: 2.3562 - val_loss: 2.3691 - val_MinusLogProbMetric: 2.3691 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 184/1000
2023-09-09 10:03:06.876 
Epoch 184/1000 
	 loss: 2.3587, MinusLogProbMetric: 2.3587, val_loss: 2.3723, val_MinusLogProbMetric: 2.3723

Epoch 184: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3587 - MinusLogProbMetric: 2.3587 - val_loss: 2.3723 - val_MinusLogProbMetric: 2.3723 - lr: 5.0000e-04 - 10s/epoch - 50ms/step
Epoch 185/1000
2023-09-09 10:03:16.543 
Epoch 185/1000 
	 loss: 2.3566, MinusLogProbMetric: 2.3566, val_loss: 2.3669, val_MinusLogProbMetric: 2.3669

Epoch 185: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3566 - MinusLogProbMetric: 2.3566 - val_loss: 2.3669 - val_MinusLogProbMetric: 2.3669 - lr: 5.0000e-04 - 10s/epoch - 49ms/step
Epoch 186/1000
2023-09-09 10:03:25.962 
Epoch 186/1000 
	 loss: 2.3609, MinusLogProbMetric: 2.3609, val_loss: 2.3690, val_MinusLogProbMetric: 2.3690

Epoch 186: val_loss did not improve from 2.36355
196/196 - 9s - loss: 2.3609 - MinusLogProbMetric: 2.3609 - val_loss: 2.3690 - val_MinusLogProbMetric: 2.3690 - lr: 5.0000e-04 - 9s/epoch - 48ms/step
Epoch 187/1000
2023-09-09 10:03:36.071 
Epoch 187/1000 
	 loss: 2.3539, MinusLogProbMetric: 2.3539, val_loss: 2.3667, val_MinusLogProbMetric: 2.3667

Epoch 187: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3539 - MinusLogProbMetric: 2.3539 - val_loss: 2.3667 - val_MinusLogProbMetric: 2.3667 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 188/1000
2023-09-09 10:03:46.105 
Epoch 188/1000 
	 loss: 2.3532, MinusLogProbMetric: 2.3532, val_loss: 2.3700, val_MinusLogProbMetric: 2.3700

Epoch 188: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3532 - MinusLogProbMetric: 2.3532 - val_loss: 2.3700 - val_MinusLogProbMetric: 2.3700 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 189/1000
2023-09-09 10:03:55.360 
Epoch 189/1000 
	 loss: 2.3559, MinusLogProbMetric: 2.3559, val_loss: 2.3731, val_MinusLogProbMetric: 2.3731

Epoch 189: val_loss did not improve from 2.36355
196/196 - 9s - loss: 2.3559 - MinusLogProbMetric: 2.3559 - val_loss: 2.3731 - val_MinusLogProbMetric: 2.3731 - lr: 5.0000e-04 - 9s/epoch - 47ms/step
Epoch 190/1000
2023-09-09 10:04:05.692 
Epoch 190/1000 
	 loss: 2.3540, MinusLogProbMetric: 2.3540, val_loss: 2.3802, val_MinusLogProbMetric: 2.3802

Epoch 190: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3540 - MinusLogProbMetric: 2.3540 - val_loss: 2.3802 - val_MinusLogProbMetric: 2.3802 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 191/1000
2023-09-09 10:04:15.990 
Epoch 191/1000 
	 loss: 2.3571, MinusLogProbMetric: 2.3571, val_loss: 2.3666, val_MinusLogProbMetric: 2.3666

Epoch 191: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3571 - MinusLogProbMetric: 2.3571 - val_loss: 2.3666 - val_MinusLogProbMetric: 2.3666 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 192/1000
2023-09-09 10:04:26.138 
Epoch 192/1000 
	 loss: 2.3549, MinusLogProbMetric: 2.3549, val_loss: 2.3796, val_MinusLogProbMetric: 2.3796

Epoch 192: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3549 - MinusLogProbMetric: 2.3549 - val_loss: 2.3796 - val_MinusLogProbMetric: 2.3796 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 193/1000
2023-09-09 10:04:36.712 
Epoch 193/1000 
	 loss: 2.3594, MinusLogProbMetric: 2.3594, val_loss: 2.4021, val_MinusLogProbMetric: 2.4021

Epoch 193: val_loss did not improve from 2.36355
196/196 - 11s - loss: 2.3594 - MinusLogProbMetric: 2.3594 - val_loss: 2.4021 - val_MinusLogProbMetric: 2.4021 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 194/1000
2023-09-09 10:04:47.029 
Epoch 194/1000 
	 loss: 2.3595, MinusLogProbMetric: 2.3595, val_loss: 2.3728, val_MinusLogProbMetric: 2.3728

Epoch 194: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3595 - MinusLogProbMetric: 2.3595 - val_loss: 2.3728 - val_MinusLogProbMetric: 2.3728 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 195/1000
2023-09-09 10:04:57.393 
Epoch 195/1000 
	 loss: 2.3536, MinusLogProbMetric: 2.3536, val_loss: 2.3779, val_MinusLogProbMetric: 2.3779

Epoch 195: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3536 - MinusLogProbMetric: 2.3536 - val_loss: 2.3779 - val_MinusLogProbMetric: 2.3779 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 196/1000
2023-09-09 10:05:07.743 
Epoch 196/1000 
	 loss: 2.3568, MinusLogProbMetric: 2.3568, val_loss: 2.3660, val_MinusLogProbMetric: 2.3660

Epoch 196: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3568 - MinusLogProbMetric: 2.3568 - val_loss: 2.3660 - val_MinusLogProbMetric: 2.3660 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 197/1000
2023-09-09 10:05:17.494 
Epoch 197/1000 
	 loss: 2.3560, MinusLogProbMetric: 2.3560, val_loss: 2.3869, val_MinusLogProbMetric: 2.3869

Epoch 197: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3560 - MinusLogProbMetric: 2.3560 - val_loss: 2.3869 - val_MinusLogProbMetric: 2.3869 - lr: 5.0000e-04 - 10s/epoch - 50ms/step
Epoch 198/1000
2023-09-09 10:05:27.451 
Epoch 198/1000 
	 loss: 2.3543, MinusLogProbMetric: 2.3543, val_loss: 2.3675, val_MinusLogProbMetric: 2.3675

Epoch 198: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3543 - MinusLogProbMetric: 2.3543 - val_loss: 2.3675 - val_MinusLogProbMetric: 2.3675 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 199/1000
2023-09-09 10:05:37.669 
Epoch 199/1000 
	 loss: 2.3567, MinusLogProbMetric: 2.3567, val_loss: 2.3752, val_MinusLogProbMetric: 2.3752

Epoch 199: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3567 - MinusLogProbMetric: 2.3567 - val_loss: 2.3752 - val_MinusLogProbMetric: 2.3752 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 200/1000
2023-09-09 10:05:47.463 
Epoch 200/1000 
	 loss: 2.3583, MinusLogProbMetric: 2.3583, val_loss: 2.3738, val_MinusLogProbMetric: 2.3738

Epoch 200: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3583 - MinusLogProbMetric: 2.3583 - val_loss: 2.3738 - val_MinusLogProbMetric: 2.3738 - lr: 5.0000e-04 - 10s/epoch - 50ms/step
Epoch 201/1000
2023-09-09 10:05:57.900 
Epoch 201/1000 
	 loss: 2.3584, MinusLogProbMetric: 2.3584, val_loss: 2.3716, val_MinusLogProbMetric: 2.3716

Epoch 201: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3584 - MinusLogProbMetric: 2.3584 - val_loss: 2.3716 - val_MinusLogProbMetric: 2.3716 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 202/1000
2023-09-09 10:06:08.333 
Epoch 202/1000 
	 loss: 2.3555, MinusLogProbMetric: 2.3555, val_loss: 2.3657, val_MinusLogProbMetric: 2.3657

Epoch 202: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3555 - MinusLogProbMetric: 2.3555 - val_loss: 2.3657 - val_MinusLogProbMetric: 2.3657 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 203/1000
2023-09-09 10:06:17.366 
Epoch 203/1000 
	 loss: 2.3529, MinusLogProbMetric: 2.3529, val_loss: 2.3718, val_MinusLogProbMetric: 2.3718

Epoch 203: val_loss did not improve from 2.36355
196/196 - 9s - loss: 2.3529 - MinusLogProbMetric: 2.3529 - val_loss: 2.3718 - val_MinusLogProbMetric: 2.3718 - lr: 5.0000e-04 - 9s/epoch - 46ms/step
Epoch 204/1000
2023-09-09 10:06:27.596 
Epoch 204/1000 
	 loss: 2.3538, MinusLogProbMetric: 2.3538, val_loss: 2.3756, val_MinusLogProbMetric: 2.3756

Epoch 204: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3538 - MinusLogProbMetric: 2.3538 - val_loss: 2.3756 - val_MinusLogProbMetric: 2.3756 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 205/1000
2023-09-09 10:06:38.163 
Epoch 205/1000 
	 loss: 2.3555, MinusLogProbMetric: 2.3555, val_loss: 2.3759, val_MinusLogProbMetric: 2.3759

Epoch 205: val_loss did not improve from 2.36355
196/196 - 11s - loss: 2.3555 - MinusLogProbMetric: 2.3555 - val_loss: 2.3759 - val_MinusLogProbMetric: 2.3759 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 206/1000
2023-09-09 10:06:48.421 
Epoch 206/1000 
	 loss: 2.3560, MinusLogProbMetric: 2.3560, val_loss: 2.3684, val_MinusLogProbMetric: 2.3684

Epoch 206: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3560 - MinusLogProbMetric: 2.3560 - val_loss: 2.3684 - val_MinusLogProbMetric: 2.3684 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 207/1000
2023-09-09 10:06:58.489 
Epoch 207/1000 
	 loss: 2.3588, MinusLogProbMetric: 2.3588, val_loss: 2.4196, val_MinusLogProbMetric: 2.4196

Epoch 207: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3588 - MinusLogProbMetric: 2.3588 - val_loss: 2.4196 - val_MinusLogProbMetric: 2.4196 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 208/1000
2023-09-09 10:07:08.600 
Epoch 208/1000 
	 loss: 2.3580, MinusLogProbMetric: 2.3580, val_loss: 2.3733, val_MinusLogProbMetric: 2.3733

Epoch 208: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3580 - MinusLogProbMetric: 2.3580 - val_loss: 2.3733 - val_MinusLogProbMetric: 2.3733 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 209/1000
2023-09-09 10:07:18.909 
Epoch 209/1000 
	 loss: 2.3582, MinusLogProbMetric: 2.3582, val_loss: 2.3714, val_MinusLogProbMetric: 2.3714

Epoch 209: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3582 - MinusLogProbMetric: 2.3582 - val_loss: 2.3714 - val_MinusLogProbMetric: 2.3714 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 210/1000
2023-09-09 10:07:28.973 
Epoch 210/1000 
	 loss: 2.3478, MinusLogProbMetric: 2.3478, val_loss: 2.3657, val_MinusLogProbMetric: 2.3657

Epoch 210: val_loss did not improve from 2.36355
196/196 - 10s - loss: 2.3478 - MinusLogProbMetric: 2.3478 - val_loss: 2.3657 - val_MinusLogProbMetric: 2.3657 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 211/1000
2023-09-09 10:07:39.552 
Epoch 211/1000 
	 loss: 2.3466, MinusLogProbMetric: 2.3466, val_loss: 2.3633, val_MinusLogProbMetric: 2.3633

Epoch 211: val_loss improved from 2.36355 to 2.36326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 11s - loss: 2.3466 - MinusLogProbMetric: 2.3466 - val_loss: 2.3633 - val_MinusLogProbMetric: 2.3633 - lr: 2.5000e-04 - 11s/epoch - 56ms/step
Epoch 212/1000
2023-09-09 10:07:50.450 
Epoch 212/1000 
	 loss: 2.3470, MinusLogProbMetric: 2.3470, val_loss: 2.3666, val_MinusLogProbMetric: 2.3666

Epoch 212: val_loss did not improve from 2.36326
196/196 - 11s - loss: 2.3470 - MinusLogProbMetric: 2.3470 - val_loss: 2.3666 - val_MinusLogProbMetric: 2.3666 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 213/1000
2023-09-09 10:07:59.680 
Epoch 213/1000 
	 loss: 2.3472, MinusLogProbMetric: 2.3472, val_loss: 2.3690, val_MinusLogProbMetric: 2.3690

Epoch 213: val_loss did not improve from 2.36326
196/196 - 9s - loss: 2.3472 - MinusLogProbMetric: 2.3472 - val_loss: 2.3690 - val_MinusLogProbMetric: 2.3690 - lr: 2.5000e-04 - 9s/epoch - 47ms/step
Epoch 214/1000
2023-09-09 10:08:10.097 
Epoch 214/1000 
	 loss: 2.3467, MinusLogProbMetric: 2.3467, val_loss: 2.3701, val_MinusLogProbMetric: 2.3701

Epoch 214: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3467 - MinusLogProbMetric: 2.3467 - val_loss: 2.3701 - val_MinusLogProbMetric: 2.3701 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 215/1000
2023-09-09 10:08:20.625 
Epoch 215/1000 
	 loss: 2.3456, MinusLogProbMetric: 2.3456, val_loss: 2.3683, val_MinusLogProbMetric: 2.3683

Epoch 215: val_loss did not improve from 2.36326
196/196 - 11s - loss: 2.3456 - MinusLogProbMetric: 2.3456 - val_loss: 2.3683 - val_MinusLogProbMetric: 2.3683 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 216/1000
2023-09-09 10:08:31.020 
Epoch 216/1000 
	 loss: 2.3461, MinusLogProbMetric: 2.3461, val_loss: 2.3639, val_MinusLogProbMetric: 2.3639

Epoch 216: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3461 - MinusLogProbMetric: 2.3461 - val_loss: 2.3639 - val_MinusLogProbMetric: 2.3639 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 217/1000
2023-09-09 10:08:41.337 
Epoch 217/1000 
	 loss: 2.3463, MinusLogProbMetric: 2.3463, val_loss: 2.3674, val_MinusLogProbMetric: 2.3674

Epoch 217: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3463 - MinusLogProbMetric: 2.3463 - val_loss: 2.3674 - val_MinusLogProbMetric: 2.3674 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 218/1000
2023-09-09 10:08:51.900 
Epoch 218/1000 
	 loss: 2.3459, MinusLogProbMetric: 2.3459, val_loss: 2.3663, val_MinusLogProbMetric: 2.3663

Epoch 218: val_loss did not improve from 2.36326
196/196 - 11s - loss: 2.3459 - MinusLogProbMetric: 2.3459 - val_loss: 2.3663 - val_MinusLogProbMetric: 2.3663 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 219/1000
2023-09-09 10:09:01.723 
Epoch 219/1000 
	 loss: 2.3472, MinusLogProbMetric: 2.3472, val_loss: 2.3647, val_MinusLogProbMetric: 2.3647

Epoch 219: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3472 - MinusLogProbMetric: 2.3472 - val_loss: 2.3647 - val_MinusLogProbMetric: 2.3647 - lr: 2.5000e-04 - 10s/epoch - 50ms/step
Epoch 220/1000
2023-09-09 10:09:11.819 
Epoch 220/1000 
	 loss: 2.3463, MinusLogProbMetric: 2.3463, val_loss: 2.3854, val_MinusLogProbMetric: 2.3854

Epoch 220: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3463 - MinusLogProbMetric: 2.3463 - val_loss: 2.3854 - val_MinusLogProbMetric: 2.3854 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 221/1000
2023-09-09 10:09:22.022 
Epoch 221/1000 
	 loss: 2.3463, MinusLogProbMetric: 2.3463, val_loss: 2.3723, val_MinusLogProbMetric: 2.3723

Epoch 221: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3463 - MinusLogProbMetric: 2.3463 - val_loss: 2.3723 - val_MinusLogProbMetric: 2.3723 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 222/1000
2023-09-09 10:09:31.753 
Epoch 222/1000 
	 loss: 2.3462, MinusLogProbMetric: 2.3462, val_loss: 2.3666, val_MinusLogProbMetric: 2.3666

Epoch 222: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3462 - MinusLogProbMetric: 2.3462 - val_loss: 2.3666 - val_MinusLogProbMetric: 2.3666 - lr: 2.5000e-04 - 10s/epoch - 50ms/step
Epoch 223/1000
2023-09-09 10:09:42.055 
Epoch 223/1000 
	 loss: 2.3471, MinusLogProbMetric: 2.3471, val_loss: 2.3667, val_MinusLogProbMetric: 2.3667

Epoch 223: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3471 - MinusLogProbMetric: 2.3471 - val_loss: 2.3667 - val_MinusLogProbMetric: 2.3667 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 224/1000
2023-09-09 10:09:50.852 
Epoch 224/1000 
	 loss: 2.3456, MinusLogProbMetric: 2.3456, val_loss: 2.3636, val_MinusLogProbMetric: 2.3636

Epoch 224: val_loss did not improve from 2.36326
196/196 - 9s - loss: 2.3456 - MinusLogProbMetric: 2.3456 - val_loss: 2.3636 - val_MinusLogProbMetric: 2.3636 - lr: 2.5000e-04 - 9s/epoch - 45ms/step
Epoch 225/1000
2023-09-09 10:10:00.990 
Epoch 225/1000 
	 loss: 2.3467, MinusLogProbMetric: 2.3467, val_loss: 2.3659, val_MinusLogProbMetric: 2.3659

Epoch 225: val_loss did not improve from 2.36326
196/196 - 10s - loss: 2.3467 - MinusLogProbMetric: 2.3467 - val_loss: 2.3659 - val_MinusLogProbMetric: 2.3659 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 226/1000
2023-09-09 10:10:10.471 
Epoch 226/1000 
	 loss: 2.3448, MinusLogProbMetric: 2.3448, val_loss: 2.3684, val_MinusLogProbMetric: 2.3684

Epoch 226: val_loss did not improve from 2.36326
196/196 - 9s - loss: 2.3448 - MinusLogProbMetric: 2.3448 - val_loss: 2.3684 - val_MinusLogProbMetric: 2.3684 - lr: 2.5000e-04 - 9s/epoch - 48ms/step
Epoch 227/1000
2023-09-09 10:10:19.219 
Epoch 227/1000 
	 loss: 2.3469, MinusLogProbMetric: 2.3469, val_loss: 2.3631, val_MinusLogProbMetric: 2.3631

Epoch 227: val_loss improved from 2.36326 to 2.36307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_20/weights/best_weights.h5
196/196 - 9s - loss: 2.3469 - MinusLogProbMetric: 2.3469 - val_loss: 2.3631 - val_MinusLogProbMetric: 2.3631 - lr: 2.5000e-04 - 9s/epoch - 46ms/step
Epoch 228/1000
2023-09-09 10:10:29.251 
Epoch 228/1000 
	 loss: 2.3458, MinusLogProbMetric: 2.3458, val_loss: 2.3679, val_MinusLogProbMetric: 2.3679

Epoch 228: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3458 - MinusLogProbMetric: 2.3458 - val_loss: 2.3679 - val_MinusLogProbMetric: 2.3679 - lr: 2.5000e-04 - 10s/epoch - 50ms/step
Epoch 229/1000
2023-09-09 10:10:39.738 
Epoch 229/1000 
	 loss: 2.3461, MinusLogProbMetric: 2.3461, val_loss: 2.3648, val_MinusLogProbMetric: 2.3648

Epoch 229: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3461 - MinusLogProbMetric: 2.3461 - val_loss: 2.3648 - val_MinusLogProbMetric: 2.3648 - lr: 2.5000e-04 - 10s/epoch - 54ms/step
Epoch 230/1000
2023-09-09 10:10:49.222 
Epoch 230/1000 
	 loss: 2.3449, MinusLogProbMetric: 2.3449, val_loss: 2.3633, val_MinusLogProbMetric: 2.3633

Epoch 230: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3449 - MinusLogProbMetric: 2.3449 - val_loss: 2.3633 - val_MinusLogProbMetric: 2.3633 - lr: 2.5000e-04 - 9s/epoch - 48ms/step
Epoch 231/1000
2023-09-09 10:10:59.474 
Epoch 231/1000 
	 loss: 2.3460, MinusLogProbMetric: 2.3460, val_loss: 2.3711, val_MinusLogProbMetric: 2.3711

Epoch 231: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3460 - MinusLogProbMetric: 2.3460 - val_loss: 2.3711 - val_MinusLogProbMetric: 2.3711 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 232/1000
2023-09-09 10:11:09.896 
Epoch 232/1000 
	 loss: 2.3471, MinusLogProbMetric: 2.3471, val_loss: 2.3658, val_MinusLogProbMetric: 2.3658

Epoch 232: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3471 - MinusLogProbMetric: 2.3471 - val_loss: 2.3658 - val_MinusLogProbMetric: 2.3658 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 233/1000
2023-09-09 10:11:20.519 
Epoch 233/1000 
	 loss: 2.3450, MinusLogProbMetric: 2.3450, val_loss: 2.3670, val_MinusLogProbMetric: 2.3670

Epoch 233: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3450 - MinusLogProbMetric: 2.3450 - val_loss: 2.3670 - val_MinusLogProbMetric: 2.3670 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 234/1000
2023-09-09 10:11:30.762 
Epoch 234/1000 
	 loss: 2.3459, MinusLogProbMetric: 2.3459, val_loss: 2.3650, val_MinusLogProbMetric: 2.3650

Epoch 234: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3459 - MinusLogProbMetric: 2.3459 - val_loss: 2.3650 - val_MinusLogProbMetric: 2.3650 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 235/1000
2023-09-09 10:11:40.609 
Epoch 235/1000 
	 loss: 2.3449, MinusLogProbMetric: 2.3449, val_loss: 2.3645, val_MinusLogProbMetric: 2.3645

Epoch 235: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3449 - MinusLogProbMetric: 2.3449 - val_loss: 2.3645 - val_MinusLogProbMetric: 2.3645 - lr: 2.5000e-04 - 10s/epoch - 50ms/step
Epoch 236/1000
2023-09-09 10:11:51.182 
Epoch 236/1000 
	 loss: 2.3458, MinusLogProbMetric: 2.3458, val_loss: 2.3649, val_MinusLogProbMetric: 2.3649

Epoch 236: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3458 - MinusLogProbMetric: 2.3458 - val_loss: 2.3649 - val_MinusLogProbMetric: 2.3649 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 237/1000
2023-09-09 10:12:01.513 
Epoch 237/1000 
	 loss: 2.3447, MinusLogProbMetric: 2.3447, val_loss: 2.3653, val_MinusLogProbMetric: 2.3653

Epoch 237: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3447 - MinusLogProbMetric: 2.3447 - val_loss: 2.3653 - val_MinusLogProbMetric: 2.3653 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 238/1000
2023-09-09 10:12:12.155 
Epoch 238/1000 
	 loss: 2.3454, MinusLogProbMetric: 2.3454, val_loss: 2.3720, val_MinusLogProbMetric: 2.3720

Epoch 238: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3454 - MinusLogProbMetric: 2.3454 - val_loss: 2.3720 - val_MinusLogProbMetric: 2.3720 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 239/1000
2023-09-09 10:12:22.436 
Epoch 239/1000 
	 loss: 2.3456, MinusLogProbMetric: 2.3456, val_loss: 2.3659, val_MinusLogProbMetric: 2.3659

Epoch 239: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3456 - MinusLogProbMetric: 2.3456 - val_loss: 2.3659 - val_MinusLogProbMetric: 2.3659 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 240/1000
2023-09-09 10:12:32.818 
Epoch 240/1000 
	 loss: 2.3441, MinusLogProbMetric: 2.3441, val_loss: 2.3684, val_MinusLogProbMetric: 2.3684

Epoch 240: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3441 - MinusLogProbMetric: 2.3441 - val_loss: 2.3684 - val_MinusLogProbMetric: 2.3684 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 241/1000
2023-09-09 10:12:43.289 
Epoch 241/1000 
	 loss: 2.3449, MinusLogProbMetric: 2.3449, val_loss: 2.3686, val_MinusLogProbMetric: 2.3686

Epoch 241: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3449 - MinusLogProbMetric: 2.3449 - val_loss: 2.3686 - val_MinusLogProbMetric: 2.3686 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 242/1000
2023-09-09 10:12:53.798 
Epoch 242/1000 
	 loss: 2.3461, MinusLogProbMetric: 2.3461, val_loss: 2.3709, val_MinusLogProbMetric: 2.3709

Epoch 242: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3461 - MinusLogProbMetric: 2.3461 - val_loss: 2.3709 - val_MinusLogProbMetric: 2.3709 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 243/1000
2023-09-09 10:13:04.051 
Epoch 243/1000 
	 loss: 2.3459, MinusLogProbMetric: 2.3459, val_loss: 2.3667, val_MinusLogProbMetric: 2.3667

Epoch 243: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3459 - MinusLogProbMetric: 2.3459 - val_loss: 2.3667 - val_MinusLogProbMetric: 2.3667 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 244/1000
2023-09-09 10:13:14.682 
Epoch 244/1000 
	 loss: 2.3445, MinusLogProbMetric: 2.3445, val_loss: 2.3800, val_MinusLogProbMetric: 2.3800

Epoch 244: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3445 - MinusLogProbMetric: 2.3445 - val_loss: 2.3800 - val_MinusLogProbMetric: 2.3800 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 245/1000
2023-09-09 10:13:25.064 
Epoch 245/1000 
	 loss: 2.3445, MinusLogProbMetric: 2.3445, val_loss: 2.3687, val_MinusLogProbMetric: 2.3687

Epoch 245: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3445 - MinusLogProbMetric: 2.3445 - val_loss: 2.3687 - val_MinusLogProbMetric: 2.3687 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 246/1000
2023-09-09 10:13:35.277 
Epoch 246/1000 
	 loss: 2.3461, MinusLogProbMetric: 2.3461, val_loss: 2.3691, val_MinusLogProbMetric: 2.3691

Epoch 246: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3461 - MinusLogProbMetric: 2.3461 - val_loss: 2.3691 - val_MinusLogProbMetric: 2.3691 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 247/1000
2023-09-09 10:13:45.809 
Epoch 247/1000 
	 loss: 2.3446, MinusLogProbMetric: 2.3446, val_loss: 2.3796, val_MinusLogProbMetric: 2.3796

Epoch 247: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3446 - MinusLogProbMetric: 2.3446 - val_loss: 2.3796 - val_MinusLogProbMetric: 2.3796 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 248/1000
2023-09-09 10:13:56.375 
Epoch 248/1000 
	 loss: 2.3478, MinusLogProbMetric: 2.3478, val_loss: 2.3666, val_MinusLogProbMetric: 2.3666

Epoch 248: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3478 - MinusLogProbMetric: 2.3478 - val_loss: 2.3666 - val_MinusLogProbMetric: 2.3666 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 249/1000
2023-09-09 10:14:06.902 
Epoch 249/1000 
	 loss: 2.3459, MinusLogProbMetric: 2.3459, val_loss: 2.3649, val_MinusLogProbMetric: 2.3649

Epoch 249: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3459 - MinusLogProbMetric: 2.3459 - val_loss: 2.3649 - val_MinusLogProbMetric: 2.3649 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 250/1000
2023-09-09 10:14:17.403 
Epoch 250/1000 
	 loss: 2.3451, MinusLogProbMetric: 2.3451, val_loss: 2.3692, val_MinusLogProbMetric: 2.3692

Epoch 250: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3451 - MinusLogProbMetric: 2.3451 - val_loss: 2.3692 - val_MinusLogProbMetric: 2.3692 - lr: 2.5000e-04 - 10s/epoch - 54ms/step
Epoch 251/1000
2023-09-09 10:14:28.169 
Epoch 251/1000 
	 loss: 2.3443, MinusLogProbMetric: 2.3443, val_loss: 2.3654, val_MinusLogProbMetric: 2.3654

Epoch 251: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3443 - MinusLogProbMetric: 2.3443 - val_loss: 2.3654 - val_MinusLogProbMetric: 2.3654 - lr: 2.5000e-04 - 11s/epoch - 55ms/step
Epoch 252/1000
2023-09-09 10:14:38.582 
Epoch 252/1000 
	 loss: 2.3449, MinusLogProbMetric: 2.3449, val_loss: 2.3671, val_MinusLogProbMetric: 2.3671

Epoch 252: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3449 - MinusLogProbMetric: 2.3449 - val_loss: 2.3671 - val_MinusLogProbMetric: 2.3671 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 253/1000
2023-09-09 10:14:49.071 
Epoch 253/1000 
	 loss: 2.3471, MinusLogProbMetric: 2.3471, val_loss: 2.3654, val_MinusLogProbMetric: 2.3654

Epoch 253: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3471 - MinusLogProbMetric: 2.3471 - val_loss: 2.3654 - val_MinusLogProbMetric: 2.3654 - lr: 2.5000e-04 - 10s/epoch - 54ms/step
Epoch 254/1000
2023-09-09 10:14:59.543 
Epoch 254/1000 
	 loss: 2.3446, MinusLogProbMetric: 2.3446, val_loss: 2.3683, val_MinusLogProbMetric: 2.3683

Epoch 254: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3446 - MinusLogProbMetric: 2.3446 - val_loss: 2.3683 - val_MinusLogProbMetric: 2.3683 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 255/1000
2023-09-09 10:15:09.933 
Epoch 255/1000 
	 loss: 2.3456, MinusLogProbMetric: 2.3456, val_loss: 2.3681, val_MinusLogProbMetric: 2.3681

Epoch 255: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3456 - MinusLogProbMetric: 2.3456 - val_loss: 2.3681 - val_MinusLogProbMetric: 2.3681 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 256/1000
2023-09-09 10:15:19.850 
Epoch 256/1000 
	 loss: 2.3464, MinusLogProbMetric: 2.3464, val_loss: 2.3664, val_MinusLogProbMetric: 2.3664

Epoch 256: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3464 - MinusLogProbMetric: 2.3464 - val_loss: 2.3664 - val_MinusLogProbMetric: 2.3664 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 257/1000
2023-09-09 10:15:30.094 
Epoch 257/1000 
	 loss: 2.3458, MinusLogProbMetric: 2.3458, val_loss: 2.3670, val_MinusLogProbMetric: 2.3670

Epoch 257: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3458 - MinusLogProbMetric: 2.3458 - val_loss: 2.3670 - val_MinusLogProbMetric: 2.3670 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 258/1000
2023-09-09 10:15:40.201 
Epoch 258/1000 
	 loss: 2.3441, MinusLogProbMetric: 2.3441, val_loss: 2.3781, val_MinusLogProbMetric: 2.3781

Epoch 258: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3441 - MinusLogProbMetric: 2.3441 - val_loss: 2.3781 - val_MinusLogProbMetric: 2.3781 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 259/1000
2023-09-09 10:15:49.869 
Epoch 259/1000 
	 loss: 2.3436, MinusLogProbMetric: 2.3436, val_loss: 2.3779, val_MinusLogProbMetric: 2.3779

Epoch 259: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3436 - MinusLogProbMetric: 2.3436 - val_loss: 2.3779 - val_MinusLogProbMetric: 2.3779 - lr: 2.5000e-04 - 10s/epoch - 49ms/step
Epoch 260/1000
2023-09-09 10:16:00.426 
Epoch 260/1000 
	 loss: 2.3450, MinusLogProbMetric: 2.3450, val_loss: 2.3663, val_MinusLogProbMetric: 2.3663

Epoch 260: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3450 - MinusLogProbMetric: 2.3450 - val_loss: 2.3663 - val_MinusLogProbMetric: 2.3663 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 261/1000
2023-09-09 10:16:10.520 
Epoch 261/1000 
	 loss: 2.3447, MinusLogProbMetric: 2.3447, val_loss: 2.3669, val_MinusLogProbMetric: 2.3669

Epoch 261: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3447 - MinusLogProbMetric: 2.3447 - val_loss: 2.3669 - val_MinusLogProbMetric: 2.3669 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 262/1000
2023-09-09 10:16:20.910 
Epoch 262/1000 
	 loss: 2.3446, MinusLogProbMetric: 2.3446, val_loss: 2.3690, val_MinusLogProbMetric: 2.3690

Epoch 262: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3446 - MinusLogProbMetric: 2.3446 - val_loss: 2.3690 - val_MinusLogProbMetric: 2.3690 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 263/1000
2023-09-09 10:16:31.055 
Epoch 263/1000 
	 loss: 2.3452, MinusLogProbMetric: 2.3452, val_loss: 2.3649, val_MinusLogProbMetric: 2.3649

Epoch 263: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3452 - MinusLogProbMetric: 2.3452 - val_loss: 2.3649 - val_MinusLogProbMetric: 2.3649 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 264/1000
2023-09-09 10:16:41.310 
Epoch 264/1000 
	 loss: 2.3445, MinusLogProbMetric: 2.3445, val_loss: 2.3701, val_MinusLogProbMetric: 2.3701

Epoch 264: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3445 - MinusLogProbMetric: 2.3445 - val_loss: 2.3701 - val_MinusLogProbMetric: 2.3701 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 265/1000
2023-09-09 10:16:50.991 
Epoch 265/1000 
	 loss: 2.3456, MinusLogProbMetric: 2.3456, val_loss: 2.3674, val_MinusLogProbMetric: 2.3674

Epoch 265: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3456 - MinusLogProbMetric: 2.3456 - val_loss: 2.3674 - val_MinusLogProbMetric: 2.3674 - lr: 2.5000e-04 - 10s/epoch - 49ms/step
Epoch 266/1000
2023-09-09 10:17:01.245 
Epoch 266/1000 
	 loss: 2.3448, MinusLogProbMetric: 2.3448, val_loss: 2.3753, val_MinusLogProbMetric: 2.3753

Epoch 266: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3448 - MinusLogProbMetric: 2.3448 - val_loss: 2.3753 - val_MinusLogProbMetric: 2.3753 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 267/1000
2023-09-09 10:17:11.722 
Epoch 267/1000 
	 loss: 2.3455, MinusLogProbMetric: 2.3455, val_loss: 2.3793, val_MinusLogProbMetric: 2.3793

Epoch 267: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3455 - MinusLogProbMetric: 2.3455 - val_loss: 2.3793 - val_MinusLogProbMetric: 2.3793 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 268/1000
2023-09-09 10:17:22.158 
Epoch 268/1000 
	 loss: 2.3452, MinusLogProbMetric: 2.3452, val_loss: 2.3710, val_MinusLogProbMetric: 2.3710

Epoch 268: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3452 - MinusLogProbMetric: 2.3452 - val_loss: 2.3710 - val_MinusLogProbMetric: 2.3710 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 269/1000
2023-09-09 10:17:32.580 
Epoch 269/1000 
	 loss: 2.3456, MinusLogProbMetric: 2.3456, val_loss: 2.3760, val_MinusLogProbMetric: 2.3760

Epoch 269: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3456 - MinusLogProbMetric: 2.3456 - val_loss: 2.3760 - val_MinusLogProbMetric: 2.3760 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 270/1000
2023-09-09 10:17:42.880 
Epoch 270/1000 
	 loss: 2.3437, MinusLogProbMetric: 2.3437, val_loss: 2.3706, val_MinusLogProbMetric: 2.3706

Epoch 270: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3437 - MinusLogProbMetric: 2.3437 - val_loss: 2.3706 - val_MinusLogProbMetric: 2.3706 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 271/1000
2023-09-09 10:17:52.786 
Epoch 271/1000 
	 loss: 2.3445, MinusLogProbMetric: 2.3445, val_loss: 2.3756, val_MinusLogProbMetric: 2.3756

Epoch 271: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3445 - MinusLogProbMetric: 2.3445 - val_loss: 2.3756 - val_MinusLogProbMetric: 2.3756 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 272/1000
2023-09-09 10:18:03.050 
Epoch 272/1000 
	 loss: 2.3451, MinusLogProbMetric: 2.3451, val_loss: 2.3666, val_MinusLogProbMetric: 2.3666

Epoch 272: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3451 - MinusLogProbMetric: 2.3451 - val_loss: 2.3666 - val_MinusLogProbMetric: 2.3666 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 273/1000
2023-09-09 10:18:13.017 
Epoch 273/1000 
	 loss: 2.3443, MinusLogProbMetric: 2.3443, val_loss: 2.3802, val_MinusLogProbMetric: 2.3802

Epoch 273: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3443 - MinusLogProbMetric: 2.3443 - val_loss: 2.3802 - val_MinusLogProbMetric: 2.3802 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 274/1000
2023-09-09 10:18:23.368 
Epoch 274/1000 
	 loss: 2.3451, MinusLogProbMetric: 2.3451, val_loss: 2.3723, val_MinusLogProbMetric: 2.3723

Epoch 274: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3451 - MinusLogProbMetric: 2.3451 - val_loss: 2.3723 - val_MinusLogProbMetric: 2.3723 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 275/1000
2023-09-09 10:18:33.535 
Epoch 275/1000 
	 loss: 2.3441, MinusLogProbMetric: 2.3441, val_loss: 2.3766, val_MinusLogProbMetric: 2.3766

Epoch 275: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3441 - MinusLogProbMetric: 2.3441 - val_loss: 2.3766 - val_MinusLogProbMetric: 2.3766 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 276/1000
2023-09-09 10:18:43.445 
Epoch 276/1000 
	 loss: 2.3454, MinusLogProbMetric: 2.3454, val_loss: 2.3807, val_MinusLogProbMetric: 2.3807

Epoch 276: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3454 - MinusLogProbMetric: 2.3454 - val_loss: 2.3807 - val_MinusLogProbMetric: 2.3807 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 277/1000
2023-09-09 10:18:53.881 
Epoch 277/1000 
	 loss: 2.3441, MinusLogProbMetric: 2.3441, val_loss: 2.3694, val_MinusLogProbMetric: 2.3694

Epoch 277: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3441 - MinusLogProbMetric: 2.3441 - val_loss: 2.3694 - val_MinusLogProbMetric: 2.3694 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 278/1000
2023-09-09 10:19:03.932 
Epoch 278/1000 
	 loss: 2.3401, MinusLogProbMetric: 2.3401, val_loss: 2.3656, val_MinusLogProbMetric: 2.3656

Epoch 278: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3401 - MinusLogProbMetric: 2.3401 - val_loss: 2.3656 - val_MinusLogProbMetric: 2.3656 - lr: 1.2500e-04 - 10s/epoch - 51ms/step
Epoch 279/1000
2023-09-09 10:19:14.553 
Epoch 279/1000 
	 loss: 2.3394, MinusLogProbMetric: 2.3394, val_loss: 2.3697, val_MinusLogProbMetric: 2.3697

Epoch 279: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3394 - MinusLogProbMetric: 2.3394 - val_loss: 2.3697 - val_MinusLogProbMetric: 2.3697 - lr: 1.2500e-04 - 11s/epoch - 54ms/step
Epoch 280/1000
2023-09-09 10:19:24.977 
Epoch 280/1000 
	 loss: 2.3400, MinusLogProbMetric: 2.3400, val_loss: 2.3700, val_MinusLogProbMetric: 2.3700

Epoch 280: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3400 - MinusLogProbMetric: 2.3400 - val_loss: 2.3700 - val_MinusLogProbMetric: 2.3700 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 281/1000
2023-09-09 10:19:35.454 
Epoch 281/1000 
	 loss: 2.3404, MinusLogProbMetric: 2.3404, val_loss: 2.3650, val_MinusLogProbMetric: 2.3650

Epoch 281: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3404 - MinusLogProbMetric: 2.3404 - val_loss: 2.3650 - val_MinusLogProbMetric: 2.3650 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 282/1000
2023-09-09 10:19:45.854 
Epoch 282/1000 
	 loss: 2.3394, MinusLogProbMetric: 2.3394, val_loss: 2.3670, val_MinusLogProbMetric: 2.3670

Epoch 282: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3394 - MinusLogProbMetric: 2.3394 - val_loss: 2.3670 - val_MinusLogProbMetric: 2.3670 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 283/1000
2023-09-09 10:19:56.162 
Epoch 283/1000 
	 loss: 2.3393, MinusLogProbMetric: 2.3393, val_loss: 2.3692, val_MinusLogProbMetric: 2.3692

Epoch 283: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3393 - MinusLogProbMetric: 2.3393 - val_loss: 2.3692 - val_MinusLogProbMetric: 2.3692 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 284/1000
2023-09-09 10:20:06.557 
Epoch 284/1000 
	 loss: 2.3392, MinusLogProbMetric: 2.3392, val_loss: 2.3654, val_MinusLogProbMetric: 2.3654

Epoch 284: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3392 - MinusLogProbMetric: 2.3392 - val_loss: 2.3654 - val_MinusLogProbMetric: 2.3654 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 285/1000
2023-09-09 10:20:16.765 
Epoch 285/1000 
	 loss: 2.3400, MinusLogProbMetric: 2.3400, val_loss: 2.3702, val_MinusLogProbMetric: 2.3702

Epoch 285: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3400 - MinusLogProbMetric: 2.3400 - val_loss: 2.3702 - val_MinusLogProbMetric: 2.3702 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 286/1000
2023-09-09 10:20:26.964 
Epoch 286/1000 
	 loss: 2.3395, MinusLogProbMetric: 2.3395, val_loss: 2.3663, val_MinusLogProbMetric: 2.3663

Epoch 286: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3395 - MinusLogProbMetric: 2.3395 - val_loss: 2.3663 - val_MinusLogProbMetric: 2.3663 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 287/1000
2023-09-09 10:20:36.977 
Epoch 287/1000 
	 loss: 2.3397, MinusLogProbMetric: 2.3397, val_loss: 2.3659, val_MinusLogProbMetric: 2.3659

Epoch 287: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3397 - MinusLogProbMetric: 2.3397 - val_loss: 2.3659 - val_MinusLogProbMetric: 2.3659 - lr: 1.2500e-04 - 10s/epoch - 51ms/step
Epoch 288/1000
2023-09-09 10:20:47.292 
Epoch 288/1000 
	 loss: 2.3405, MinusLogProbMetric: 2.3405, val_loss: 2.3664, val_MinusLogProbMetric: 2.3664

Epoch 288: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3405 - MinusLogProbMetric: 2.3405 - val_loss: 2.3664 - val_MinusLogProbMetric: 2.3664 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 289/1000
2023-09-09 10:20:57.004 
Epoch 289/1000 
	 loss: 2.3399, MinusLogProbMetric: 2.3399, val_loss: 2.3681, val_MinusLogProbMetric: 2.3681

Epoch 289: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3399 - MinusLogProbMetric: 2.3399 - val_loss: 2.3681 - val_MinusLogProbMetric: 2.3681 - lr: 1.2500e-04 - 10s/epoch - 50ms/step
Epoch 290/1000
2023-09-09 10:21:07.333 
Epoch 290/1000 
	 loss: 2.3397, MinusLogProbMetric: 2.3397, val_loss: 2.3694, val_MinusLogProbMetric: 2.3694

Epoch 290: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3397 - MinusLogProbMetric: 2.3397 - val_loss: 2.3694 - val_MinusLogProbMetric: 2.3694 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 291/1000
2023-09-09 10:21:17.852 
Epoch 291/1000 
	 loss: 2.3404, MinusLogProbMetric: 2.3404, val_loss: 2.3709, val_MinusLogProbMetric: 2.3709

Epoch 291: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3404 - MinusLogProbMetric: 2.3404 - val_loss: 2.3709 - val_MinusLogProbMetric: 2.3709 - lr: 1.2500e-04 - 11s/epoch - 54ms/step
Epoch 292/1000
2023-09-09 10:21:28.098 
Epoch 292/1000 
	 loss: 2.3394, MinusLogProbMetric: 2.3394, val_loss: 2.3682, val_MinusLogProbMetric: 2.3682

Epoch 292: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3394 - MinusLogProbMetric: 2.3394 - val_loss: 2.3682 - val_MinusLogProbMetric: 2.3682 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 293/1000
2023-09-09 10:21:38.633 
Epoch 293/1000 
	 loss: 2.3395, MinusLogProbMetric: 2.3395, val_loss: 2.3672, val_MinusLogProbMetric: 2.3672

Epoch 293: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3395 - MinusLogProbMetric: 2.3395 - val_loss: 2.3672 - val_MinusLogProbMetric: 2.3672 - lr: 1.2500e-04 - 11s/epoch - 54ms/step
Epoch 294/1000
2023-09-09 10:21:48.371 
Epoch 294/1000 
	 loss: 2.3400, MinusLogProbMetric: 2.3400, val_loss: 2.3676, val_MinusLogProbMetric: 2.3676

Epoch 294: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3400 - MinusLogProbMetric: 2.3400 - val_loss: 2.3676 - val_MinusLogProbMetric: 2.3676 - lr: 1.2500e-04 - 10s/epoch - 50ms/step
Epoch 295/1000
2023-09-09 10:21:58.345 
Epoch 295/1000 
	 loss: 2.3401, MinusLogProbMetric: 2.3401, val_loss: 2.3656, val_MinusLogProbMetric: 2.3656

Epoch 295: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3401 - MinusLogProbMetric: 2.3401 - val_loss: 2.3656 - val_MinusLogProbMetric: 2.3656 - lr: 1.2500e-04 - 10s/epoch - 51ms/step
Epoch 296/1000
2023-09-09 10:22:08.673 
Epoch 296/1000 
	 loss: 2.3403, MinusLogProbMetric: 2.3403, val_loss: 2.3678, val_MinusLogProbMetric: 2.3678

Epoch 296: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3403 - MinusLogProbMetric: 2.3403 - val_loss: 2.3678 - val_MinusLogProbMetric: 2.3678 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 297/1000
2023-09-09 10:22:18.828 
Epoch 297/1000 
	 loss: 2.3396, MinusLogProbMetric: 2.3396, val_loss: 2.3677, val_MinusLogProbMetric: 2.3677

Epoch 297: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3396 - MinusLogProbMetric: 2.3396 - val_loss: 2.3677 - val_MinusLogProbMetric: 2.3677 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 298/1000
2023-09-09 10:22:29.262 
Epoch 298/1000 
	 loss: 2.3393, MinusLogProbMetric: 2.3393, val_loss: 2.3698, val_MinusLogProbMetric: 2.3698

Epoch 298: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3393 - MinusLogProbMetric: 2.3393 - val_loss: 2.3698 - val_MinusLogProbMetric: 2.3698 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 299/1000
2023-09-09 10:22:39.187 
Epoch 299/1000 
	 loss: 2.3401, MinusLogProbMetric: 2.3401, val_loss: 2.3713, val_MinusLogProbMetric: 2.3713

Epoch 299: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3401 - MinusLogProbMetric: 2.3401 - val_loss: 2.3713 - val_MinusLogProbMetric: 2.3713 - lr: 1.2500e-04 - 10s/epoch - 51ms/step
Epoch 300/1000
2023-09-09 10:22:48.274 
Epoch 300/1000 
	 loss: 2.3394, MinusLogProbMetric: 2.3394, val_loss: 2.3677, val_MinusLogProbMetric: 2.3677

Epoch 300: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3394 - MinusLogProbMetric: 2.3394 - val_loss: 2.3677 - val_MinusLogProbMetric: 2.3677 - lr: 1.2500e-04 - 9s/epoch - 46ms/step
Epoch 301/1000
2023-09-09 10:22:58.343 
Epoch 301/1000 
	 loss: 2.3391, MinusLogProbMetric: 2.3391, val_loss: 2.3669, val_MinusLogProbMetric: 2.3669

Epoch 301: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3391 - MinusLogProbMetric: 2.3391 - val_loss: 2.3669 - val_MinusLogProbMetric: 2.3669 - lr: 1.2500e-04 - 10s/epoch - 51ms/step
Epoch 302/1000
2023-09-09 10:23:08.655 
Epoch 302/1000 
	 loss: 2.3394, MinusLogProbMetric: 2.3394, val_loss: 2.3730, val_MinusLogProbMetric: 2.3730

Epoch 302: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3394 - MinusLogProbMetric: 2.3394 - val_loss: 2.3730 - val_MinusLogProbMetric: 2.3730 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 303/1000
2023-09-09 10:23:19.074 
Epoch 303/1000 
	 loss: 2.3397, MinusLogProbMetric: 2.3397, val_loss: 2.3696, val_MinusLogProbMetric: 2.3696

Epoch 303: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3397 - MinusLogProbMetric: 2.3397 - val_loss: 2.3696 - val_MinusLogProbMetric: 2.3696 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 304/1000
2023-09-09 10:23:29.524 
Epoch 304/1000 
	 loss: 2.3399, MinusLogProbMetric: 2.3399, val_loss: 2.3700, val_MinusLogProbMetric: 2.3700

Epoch 304: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3399 - MinusLogProbMetric: 2.3399 - val_loss: 2.3700 - val_MinusLogProbMetric: 2.3700 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 305/1000
2023-09-09 10:23:40.136 
Epoch 305/1000 
	 loss: 2.3398, MinusLogProbMetric: 2.3398, val_loss: 2.3691, val_MinusLogProbMetric: 2.3691

Epoch 305: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3398 - MinusLogProbMetric: 2.3398 - val_loss: 2.3691 - val_MinusLogProbMetric: 2.3691 - lr: 1.2500e-04 - 11s/epoch - 54ms/step
Epoch 306/1000
2023-09-09 10:23:49.497 
Epoch 306/1000 
	 loss: 2.3394, MinusLogProbMetric: 2.3394, val_loss: 2.3687, val_MinusLogProbMetric: 2.3687

Epoch 306: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3394 - MinusLogProbMetric: 2.3394 - val_loss: 2.3687 - val_MinusLogProbMetric: 2.3687 - lr: 1.2500e-04 - 9s/epoch - 48ms/step
Epoch 307/1000
2023-09-09 10:23:59.879 
Epoch 307/1000 
	 loss: 2.3397, MinusLogProbMetric: 2.3397, val_loss: 2.3711, val_MinusLogProbMetric: 2.3711

Epoch 307: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3397 - MinusLogProbMetric: 2.3397 - val_loss: 2.3711 - val_MinusLogProbMetric: 2.3711 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 308/1000
2023-09-09 10:24:09.285 
Epoch 308/1000 
	 loss: 2.3397, MinusLogProbMetric: 2.3397, val_loss: 2.3685, val_MinusLogProbMetric: 2.3685

Epoch 308: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3397 - MinusLogProbMetric: 2.3397 - val_loss: 2.3685 - val_MinusLogProbMetric: 2.3685 - lr: 1.2500e-04 - 9s/epoch - 48ms/step
Epoch 309/1000
2023-09-09 10:24:19.207 
Epoch 309/1000 
	 loss: 2.3396, MinusLogProbMetric: 2.3396, val_loss: 2.3699, val_MinusLogProbMetric: 2.3699

Epoch 309: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3396 - MinusLogProbMetric: 2.3396 - val_loss: 2.3699 - val_MinusLogProbMetric: 2.3699 - lr: 1.2500e-04 - 10s/epoch - 51ms/step
Epoch 310/1000
2023-09-09 10:24:29.576 
Epoch 310/1000 
	 loss: 2.3397, MinusLogProbMetric: 2.3397, val_loss: 2.3718, val_MinusLogProbMetric: 2.3718

Epoch 310: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3397 - MinusLogProbMetric: 2.3397 - val_loss: 2.3718 - val_MinusLogProbMetric: 2.3718 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 311/1000
2023-09-09 10:24:39.808 
Epoch 311/1000 
	 loss: 2.3398, MinusLogProbMetric: 2.3398, val_loss: 2.3727, val_MinusLogProbMetric: 2.3727

Epoch 311: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3398 - MinusLogProbMetric: 2.3398 - val_loss: 2.3727 - val_MinusLogProbMetric: 2.3727 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 312/1000
2023-09-09 10:24:50.006 
Epoch 312/1000 
	 loss: 2.3402, MinusLogProbMetric: 2.3402, val_loss: 2.3723, val_MinusLogProbMetric: 2.3723

Epoch 312: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3402 - MinusLogProbMetric: 2.3402 - val_loss: 2.3723 - val_MinusLogProbMetric: 2.3723 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 313/1000
2023-09-09 10:25:00.527 
Epoch 313/1000 
	 loss: 2.3402, MinusLogProbMetric: 2.3402, val_loss: 2.3692, val_MinusLogProbMetric: 2.3692

Epoch 313: val_loss did not improve from 2.36307
196/196 - 11s - loss: 2.3402 - MinusLogProbMetric: 2.3402 - val_loss: 2.3692 - val_MinusLogProbMetric: 2.3692 - lr: 1.2500e-04 - 11s/epoch - 54ms/step
Epoch 314/1000
2023-09-09 10:25:10.740 
Epoch 314/1000 
	 loss: 2.3390, MinusLogProbMetric: 2.3390, val_loss: 2.3662, val_MinusLogProbMetric: 2.3662

Epoch 314: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3390 - MinusLogProbMetric: 2.3390 - val_loss: 2.3662 - val_MinusLogProbMetric: 2.3662 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 315/1000
2023-09-09 10:25:21.146 
Epoch 315/1000 
	 loss: 2.3390, MinusLogProbMetric: 2.3390, val_loss: 2.3721, val_MinusLogProbMetric: 2.3721

Epoch 315: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3390 - MinusLogProbMetric: 2.3390 - val_loss: 2.3721 - val_MinusLogProbMetric: 2.3721 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 316/1000
2023-09-09 10:25:31.592 
Epoch 316/1000 
	 loss: 2.3385, MinusLogProbMetric: 2.3385, val_loss: 2.3722, val_MinusLogProbMetric: 2.3722

Epoch 316: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3385 - MinusLogProbMetric: 2.3385 - val_loss: 2.3722 - val_MinusLogProbMetric: 2.3722 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 317/1000
2023-09-09 10:25:40.866 
Epoch 317/1000 
	 loss: 2.3401, MinusLogProbMetric: 2.3401, val_loss: 2.3680, val_MinusLogProbMetric: 2.3680

Epoch 317: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3401 - MinusLogProbMetric: 2.3401 - val_loss: 2.3680 - val_MinusLogProbMetric: 2.3680 - lr: 1.2500e-04 - 9s/epoch - 47ms/step
Epoch 318/1000
2023-09-09 10:25:51.144 
Epoch 318/1000 
	 loss: 2.3393, MinusLogProbMetric: 2.3393, val_loss: 2.3681, val_MinusLogProbMetric: 2.3681

Epoch 318: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3393 - MinusLogProbMetric: 2.3393 - val_loss: 2.3681 - val_MinusLogProbMetric: 2.3681 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 319/1000
2023-09-09 10:26:00.056 
Epoch 319/1000 
	 loss: 2.3388, MinusLogProbMetric: 2.3388, val_loss: 2.3667, val_MinusLogProbMetric: 2.3667

Epoch 319: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3388 - MinusLogProbMetric: 2.3388 - val_loss: 2.3667 - val_MinusLogProbMetric: 2.3667 - lr: 1.2500e-04 - 9s/epoch - 45ms/step
Epoch 320/1000
2023-09-09 10:26:09.695 
Epoch 320/1000 
	 loss: 2.3390, MinusLogProbMetric: 2.3390, val_loss: 2.3694, val_MinusLogProbMetric: 2.3694

Epoch 320: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3390 - MinusLogProbMetric: 2.3390 - val_loss: 2.3694 - val_MinusLogProbMetric: 2.3694 - lr: 1.2500e-04 - 10s/epoch - 49ms/step
Epoch 321/1000
2023-09-09 10:26:19.962 
Epoch 321/1000 
	 loss: 2.3399, MinusLogProbMetric: 2.3399, val_loss: 2.3693, val_MinusLogProbMetric: 2.3693

Epoch 321: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3399 - MinusLogProbMetric: 2.3399 - val_loss: 2.3693 - val_MinusLogProbMetric: 2.3693 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 322/1000
2023-09-09 10:26:29.288 
Epoch 322/1000 
	 loss: 2.3400, MinusLogProbMetric: 2.3400, val_loss: 2.3727, val_MinusLogProbMetric: 2.3727

Epoch 322: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3400 - MinusLogProbMetric: 2.3400 - val_loss: 2.3727 - val_MinusLogProbMetric: 2.3727 - lr: 1.2500e-04 - 9s/epoch - 48ms/step
Epoch 323/1000
2023-09-09 10:26:39.560 
Epoch 323/1000 
	 loss: 2.3403, MinusLogProbMetric: 2.3403, val_loss: 2.3693, val_MinusLogProbMetric: 2.3693

Epoch 323: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3403 - MinusLogProbMetric: 2.3403 - val_loss: 2.3693 - val_MinusLogProbMetric: 2.3693 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 324/1000
2023-09-09 10:26:49.949 
Epoch 324/1000 
	 loss: 2.3389, MinusLogProbMetric: 2.3389, val_loss: 2.3675, val_MinusLogProbMetric: 2.3675

Epoch 324: val_loss did not improve from 2.36307
196/196 - 10s - loss: 2.3389 - MinusLogProbMetric: 2.3389 - val_loss: 2.3675 - val_MinusLogProbMetric: 2.3675 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 325/1000
2023-09-09 10:26:59.159 
Epoch 325/1000 
	 loss: 2.3393, MinusLogProbMetric: 2.3393, val_loss: 2.3711, val_MinusLogProbMetric: 2.3711

Epoch 325: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3393 - MinusLogProbMetric: 2.3393 - val_loss: 2.3711 - val_MinusLogProbMetric: 2.3711 - lr: 1.2500e-04 - 9s/epoch - 47ms/step
Epoch 326/1000
2023-09-09 10:27:08.049 
Epoch 326/1000 
	 loss: 2.3388, MinusLogProbMetric: 2.3388, val_loss: 2.3702, val_MinusLogProbMetric: 2.3702

Epoch 326: val_loss did not improve from 2.36307
196/196 - 9s - loss: 2.3388 - MinusLogProbMetric: 2.3388 - val_loss: 2.3702 - val_MinusLogProbMetric: 2.3702 - lr: 1.2500e-04 - 9s/epoch - 45ms/step
Epoch 327/1000
2023-09-09 10:27:18.071 
Epoch 327/1000 
	 loss: 2.3398, MinusLogProbMetric: 2.3398, val_loss: 2.3722, val_MinusLogProbMetric: 2.3722

Epoch 327: val_loss did not improve from 2.36307
Restoring model weights from the end of the best epoch: 227.
196/196 - 10s - loss: 2.3398 - MinusLogProbMetric: 2.3398 - val_loss: 2.3722 - val_MinusLogProbMetric: 2.3722 - lr: 1.2500e-04 - 10s/epoch - 52ms/step
Epoch 327: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 6.812359835021198 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
KS tests calculation completed in 5.19801233895123 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
SWD metric calculation completed in 2.6246305210515857 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
FN metric calculation completed in 3.1331518749939278 seconds.
Training succeeded with seed 520.
Model trained in 3345.28 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 23.54 s.
Plots done in 2.21 s.
results.txt saved
results.json saved
Results log saved
Model predictions computed in 25.75 s.
===========
Run 20/360 done in 3373.46 s.
===========

Directory ../../results/MAFN_new/run_21/ already exists.
Skipping it.
===========
Run 21/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_22/ already exists.
Skipping it.
===========
Run 22/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_23/ already exists.
Skipping it.
===========
Run 23/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_24/ already exists.
Skipping it.
===========
Run 24/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_25/ already exists.
Skipping it.
===========
Run 25/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_26/ already exists.
Skipping it.
===========
Run 26/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_27/ already exists.
Skipping it.
===========
Run 27/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_28/ already exists.
Skipping it.
===========
Run 28/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_29/ already exists.
Skipping it.
===========
Run 29/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_30/ already exists.
Skipping it.
===========
Run 30/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_31/ already exists.
Skipping it.
===========
Run 31/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_32/ already exists.
Skipping it.
===========
Run 32/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_33/ already exists.
Skipping it.
===========
Run 33/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_34/ already exists.
Skipping it.
===========
Run 34/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_35/ already exists.
Skipping it.
===========
Run 35/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_36/ already exists.
Skipping it.
===========
Run 36/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_37/ already exists.
Skipping it.
===========
Run 37/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_38/ already exists.
Skipping it.
===========
Run 38/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_39/ already exists.
Skipping it.
===========
Run 39/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_40/ already exists.
Skipping it.
===========
Run 40/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_41/ already exists.
Skipping it.
===========
Run 41/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_42/ already exists.
Skipping it.
===========
Run 42/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_43/ already exists.
Skipping it.
===========
Run 43/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_44/ already exists.
Skipping it.
===========
Run 44/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_45/ already exists.
Skipping it.
===========
Run 45/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_46/ already exists.
Skipping it.
===========
Run 46/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_47/ already exists.
Skipping it.
===========
Run 47/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_48/ already exists.
Skipping it.
===========
Run 48/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_49/ already exists.
Skipping it.
===========
Run 49/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_50/ already exists.
Skipping it.
===========
Run 50/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_51/ already exists.
Skipping it.
===========
Run 51/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_52/ already exists.
Skipping it.
===========
Run 52/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_53/ already exists.
Skipping it.
===========
Run 53/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_54/ already exists.
Skipping it.
===========
Run 54/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_55/ already exists.
Skipping it.
===========
Run 55/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_56/ already exists.
Skipping it.
===========
Run 56/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_57/ already exists.
Skipping it.
===========
Run 57/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_58/ already exists.
Skipping it.
===========
Run 58/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_59/ already exists.
Skipping it.
===========
Run 59/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_60/ already exists.
Skipping it.
===========
Run 60/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_61/ already exists.
Skipping it.
===========
Run 61/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_62/ already exists.
Skipping it.
===========
Run 62/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_63/ already exists.
Skipping it.
===========
Run 63/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_64/ already exists.
Skipping it.
===========
Run 64/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_65/ already exists.
Skipping it.
===========
Run 65/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_66/ already exists.
Skipping it.
===========
Run 66/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_67/ already exists.
Skipping it.
===========
Run 67/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_68/ already exists.
Skipping it.
===========
Run 68/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_69/ already exists.
Skipping it.
===========
Run 69/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_70/ already exists.
Skipping it.
===========
Run 70/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_71/ already exists.
Skipping it.
===========
Run 71/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_72/ already exists.
Skipping it.
===========
Run 72/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_73/ already exists.
Skipping it.
===========
Run 73/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_74/ already exists.
Skipping it.
===========
Run 74/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_75/ already exists.
Skipping it.
===========
Run 75/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_76/ already exists.
Skipping it.
===========
Run 76/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_77/ already exists.
Skipping it.
===========
Run 77/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_78/ already exists.
Skipping it.
===========
Run 78/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_79/ already exists.
Skipping it.
===========
Run 79/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_80/ already exists.
Skipping it.
===========
Run 80/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_81/ already exists.
Skipping it.
===========
Run 81/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_82/ already exists.
Skipping it.
===========
Run 82/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_83/ already exists.
Skipping it.
===========
Run 83/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_84/ already exists.
Skipping it.
===========
Run 84/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_85/ already exists.
Skipping it.
===========
Run 85/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_86/ already exists.
Skipping it.
===========
Run 86/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_87/ already exists.
Skipping it.
===========
Run 87/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_88/ already exists.
Skipping it.
===========
Run 88/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_89/ already exists.
Skipping it.
===========
Run 89/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_90/ already exists.
Skipping it.
===========
Run 90/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_91/ already exists.
Skipping it.
===========
Run 91/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_92/ already exists.
Skipping it.
===========
Run 92/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_93/ already exists.
Skipping it.
===========
Run 93/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_94/ already exists.
Skipping it.
===========
Run 94/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_95/ already exists.
Skipping it.
===========
Run 95/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_96/ already exists.
Skipping it.
===========
Run 96/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_97/ already exists.
Skipping it.
===========
Run 97/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_98/ already exists.
Skipping it.
===========
Run 98/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_99/ already exists.
Skipping it.
===========
Run 99/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_100/ already exists.
Skipping it.
===========
Run 100/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_101/ already exists.
Skipping it.
===========
Run 101/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_102/ already exists.
Skipping it.
===========
Run 102/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_103/ already exists.
Skipping it.
===========
Run 103/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_104/ already exists.
Skipping it.
===========
Run 104/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_105/ already exists.
Skipping it.
===========
Run 105/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_106/ already exists.
Skipping it.
===========
Run 106/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_107/ already exists.
Skipping it.
===========
Run 107/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_108/ already exists.
Skipping it.
===========
Run 108/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_109/ already exists.
Skipping it.
===========
Run 109/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_110/ already exists.
Skipping it.
===========
Run 110/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_111/ already exists.
Skipping it.
===========
Run 111/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_112/ already exists.
Skipping it.
===========
Run 112/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_113/ already exists.
Skipping it.
===========
Run 113/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_114/ already exists.
Skipping it.
===========
Run 114/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_115/ already exists.
Skipping it.
===========
Run 115/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_116/ already exists.
Skipping it.
===========
Run 116/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_117/ already exists.
Skipping it.
===========
Run 117/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_118/ already exists.
Skipping it.
===========
Run 118/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_119/ already exists.
Skipping it.
===========
Run 119/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_120/ already exists.
Skipping it.
===========
Run 120/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_121/ already exists.
Skipping it.
===========
Run 121/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_122/ already exists.
Skipping it.
===========
Run 122/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_123/ already exists.
Skipping it.
===========
Run 123/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_124/ already exists.
Skipping it.
===========
Run 124/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_125/ already exists.
Skipping it.
===========
Run 125/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_126/ already exists.
Skipping it.
===========
Run 126/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_127/ already exists.
Skipping it.
===========
Run 127/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_128/ already exists.
Skipping it.
===========
Run 128/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_129/ already exists.
Skipping it.
===========
Run 129/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_130/ already exists.
Skipping it.
===========
Run 130/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_131/ already exists.
Skipping it.
===========
Run 131/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_132/ already exists.
Skipping it.
===========
Run 132/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_133/ already exists.
Skipping it.
===========
Run 133/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_134/ already exists.
Skipping it.
===========
Run 134/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_135/ already exists.
Skipping it.
===========
Run 135/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_136/ already exists.
Skipping it.
===========
Run 136/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_137/ already exists.
Skipping it.
===========
Run 137/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_138/ already exists.
Skipping it.
===========
Run 138/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_139/ already exists.
Skipping it.
===========
Run 139/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_140/ already exists.
Skipping it.
===========
Run 140/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_141/ already exists.
Skipping it.
===========
Run 141/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_142/ already exists.
Skipping it.
===========
Run 142/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_143/ already exists.
Skipping it.
===========
Run 143/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_144/ already exists.
Skipping it.
===========
Run 144/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_145/ already exists.
Skipping it.
===========
Run 145/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_146/ already exists.
Skipping it.
===========
Run 146/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_147/ already exists.
Skipping it.
===========
Run 147/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_148/ already exists.
Skipping it.
===========
Run 148/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_149/ already exists.
Skipping it.
===========
Run 149/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_150/ already exists.
Skipping it.
===========
Run 150/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_151/ already exists.
Skipping it.
===========
Run 151/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_152/ already exists.
Skipping it.
===========
Run 152/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_153/ already exists.
Skipping it.
===========
Run 153/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_154/ already exists.
Skipping it.
===========
Run 154/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_155/ already exists.
Skipping it.
===========
Run 155/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_156/ already exists.
Skipping it.
===========
Run 156/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_157/ already exists.
Skipping it.
===========
Run 157/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_158/ already exists.
Skipping it.
===========
Run 158/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_159/ already exists.
Skipping it.
===========
Run 159/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_160/ already exists.
Skipping it.
===========
Run 160/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_161/ already exists.
Skipping it.
===========
Run 161/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_162/ already exists.
Skipping it.
===========
Run 162/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_163/ already exists.
Skipping it.
===========
Run 163/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_164/ already exists.
Skipping it.
===========
Run 164/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_165/ already exists.
Skipping it.
===========
Run 165/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_166/ already exists.
Skipping it.
===========
Run 166/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_167/ already exists.
Skipping it.
===========
Run 167/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_168/ already exists.
Skipping it.
===========
Run 168/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_169/ already exists.
Skipping it.
===========
Run 169/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_170/ already exists.
Skipping it.
===========
Run 170/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_171/ already exists.
Skipping it.
===========
Run 171/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_172/ already exists.
Skipping it.
===========
Run 172/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_173/ already exists.
Skipping it.
===========
Run 173/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_174/ already exists.
Skipping it.
===========
Run 174/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_175/ already exists.
Skipping it.
===========
Run 175/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_176/ already exists.
Skipping it.
===========
Run 176/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_177/ already exists.
Skipping it.
===========
Run 177/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_178/ already exists.
Skipping it.
===========
Run 178/360 already exists. Skipping it.
===========

===========
Generating train data for run 179.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_179/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_179/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_179/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_179
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  578560    
 yer)                                                            
                                                                 
=================================================================
Total params: 578,560
Trainable params: 578,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7fa314344850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fa314386bf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fa314386bf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fa3143a88b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fa3142d1a20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fa3142d1de0>, <keras.callbacks.ModelCheckpoint object at 0x7fa3142d1ea0>, <keras.callbacks.EarlyStopping object at 0x7fa3142d2110>, <keras.callbacks.ReduceLROnPlateau object at 0x7fa3142d2140>, <keras.callbacks.TerminateOnNaN object at 0x7fa3142d1d80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_179/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 179/360 with hyperparameters:
timestamp = 2023-09-09 10:27:46.247991
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 578560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-09-09 10:28:16.487 
Epoch 1/1000 
	 loss: 124.0475, MinusLogProbMetric: 124.0475, val_loss: 50.7912, val_MinusLogProbMetric: 50.7912

Epoch 1: val_loss improved from inf to 50.79118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 30s - loss: 124.0475 - MinusLogProbMetric: 124.0475 - val_loss: 50.7912 - val_MinusLogProbMetric: 50.7912 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 2/1000
2023-09-09 10:28:27.159 
Epoch 2/1000 
	 loss: 44.1754, MinusLogProbMetric: 44.1754, val_loss: 39.8122, val_MinusLogProbMetric: 39.8122

Epoch 2: val_loss improved from 50.79118 to 39.81218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 44.1754 - MinusLogProbMetric: 44.1754 - val_loss: 39.8122 - val_MinusLogProbMetric: 39.8122 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 3/1000
2023-09-09 10:28:37.767 
Epoch 3/1000 
	 loss: 39.9273, MinusLogProbMetric: 39.9273, val_loss: 37.3323, val_MinusLogProbMetric: 37.3323

Epoch 3: val_loss improved from 39.81218 to 37.33232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 39.9273 - MinusLogProbMetric: 39.9273 - val_loss: 37.3323 - val_MinusLogProbMetric: 37.3323 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 4/1000
2023-09-09 10:28:47.548 
Epoch 4/1000 
	 loss: 36.8447, MinusLogProbMetric: 36.8447, val_loss: 35.0752, val_MinusLogProbMetric: 35.0752

Epoch 4: val_loss improved from 37.33232 to 35.07521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 36.8447 - MinusLogProbMetric: 36.8447 - val_loss: 35.0752 - val_MinusLogProbMetric: 35.0752 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 5/1000
2023-09-09 10:28:58.122 
Epoch 5/1000 
	 loss: 35.7248, MinusLogProbMetric: 35.7248, val_loss: 34.3406, val_MinusLogProbMetric: 34.3406

Epoch 5: val_loss improved from 35.07521 to 34.34060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 35.7248 - MinusLogProbMetric: 35.7248 - val_loss: 34.3406 - val_MinusLogProbMetric: 34.3406 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 6/1000
2023-09-09 10:29:08.244 
Epoch 6/1000 
	 loss: 36.3528, MinusLogProbMetric: 36.3528, val_loss: 36.4680, val_MinusLogProbMetric: 36.4680

Epoch 6: val_loss did not improve from 34.34060
196/196 - 10s - loss: 36.3528 - MinusLogProbMetric: 36.3528 - val_loss: 36.4680 - val_MinusLogProbMetric: 36.4680 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 7/1000
2023-09-09 10:29:18.074 
Epoch 7/1000 
	 loss: 35.0541, MinusLogProbMetric: 35.0541, val_loss: 34.2615, val_MinusLogProbMetric: 34.2615

Epoch 7: val_loss improved from 34.34060 to 34.26154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 35.0541 - MinusLogProbMetric: 35.0541 - val_loss: 34.2615 - val_MinusLogProbMetric: 34.2615 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 8/1000
2023-09-09 10:29:28.552 
Epoch 8/1000 
	 loss: 34.3047, MinusLogProbMetric: 34.3047, val_loss: 32.6323, val_MinusLogProbMetric: 32.6323

Epoch 8: val_loss improved from 34.26154 to 32.63230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 34.3047 - MinusLogProbMetric: 34.3047 - val_loss: 32.6323 - val_MinusLogProbMetric: 32.6323 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 9/1000
2023-09-09 10:29:39.304 
Epoch 9/1000 
	 loss: 33.3342, MinusLogProbMetric: 33.3342, val_loss: 32.3239, val_MinusLogProbMetric: 32.3239

Epoch 9: val_loss improved from 32.63230 to 32.32393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 33.3342 - MinusLogProbMetric: 33.3342 - val_loss: 32.3239 - val_MinusLogProbMetric: 32.3239 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 10/1000
2023-09-09 10:29:49.980 
Epoch 10/1000 
	 loss: 33.1846, MinusLogProbMetric: 33.1846, val_loss: 34.3060, val_MinusLogProbMetric: 34.3060

Epoch 10: val_loss did not improve from 32.32393
196/196 - 10s - loss: 33.1846 - MinusLogProbMetric: 33.1846 - val_loss: 34.3060 - val_MinusLogProbMetric: 34.3060 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 11/1000
2023-09-09 10:30:00.342 
Epoch 11/1000 
	 loss: 33.2016, MinusLogProbMetric: 33.2016, val_loss: 32.0191, val_MinusLogProbMetric: 32.0191

Epoch 11: val_loss improved from 32.32393 to 32.01909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 33.2016 - MinusLogProbMetric: 33.2016 - val_loss: 32.0191 - val_MinusLogProbMetric: 32.0191 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 12/1000
2023-09-09 10:30:10.918 
Epoch 12/1000 
	 loss: 33.2070, MinusLogProbMetric: 33.2070, val_loss: 32.3785, val_MinusLogProbMetric: 32.3785

Epoch 12: val_loss did not improve from 32.01909
196/196 - 10s - loss: 33.2070 - MinusLogProbMetric: 33.2070 - val_loss: 32.3785 - val_MinusLogProbMetric: 32.3785 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 13/1000
2023-09-09 10:30:20.938 
Epoch 13/1000 
	 loss: 31.7892, MinusLogProbMetric: 31.7892, val_loss: 31.2863, val_MinusLogProbMetric: 31.2863

Epoch 13: val_loss improved from 32.01909 to 31.28626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 31.7892 - MinusLogProbMetric: 31.7892 - val_loss: 31.2863 - val_MinusLogProbMetric: 31.2863 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 14/1000
2023-09-09 10:30:30.750 
Epoch 14/1000 
	 loss: 32.6464, MinusLogProbMetric: 32.6464, val_loss: 33.4553, val_MinusLogProbMetric: 33.4553

Epoch 14: val_loss did not improve from 31.28626
196/196 - 10s - loss: 32.6464 - MinusLogProbMetric: 32.6464 - val_loss: 33.4553 - val_MinusLogProbMetric: 33.4553 - lr: 0.0010 - 10s/epoch - 48ms/step
Epoch 15/1000
2023-09-09 10:30:40.652 
Epoch 15/1000 
	 loss: 32.3905, MinusLogProbMetric: 32.3905, val_loss: 31.2318, val_MinusLogProbMetric: 31.2318

Epoch 15: val_loss improved from 31.28626 to 31.23176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 32.3905 - MinusLogProbMetric: 32.3905 - val_loss: 31.2318 - val_MinusLogProbMetric: 31.2318 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 16/1000
2023-09-09 10:30:50.822 
Epoch 16/1000 
	 loss: 31.5602, MinusLogProbMetric: 31.5602, val_loss: 31.2044, val_MinusLogProbMetric: 31.2044

Epoch 16: val_loss improved from 31.23176 to 31.20438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 31.5602 - MinusLogProbMetric: 31.5602 - val_loss: 31.2044 - val_MinusLogProbMetric: 31.2044 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 17/1000
2023-09-09 10:31:00.739 
Epoch 17/1000 
	 loss: 30.9949, MinusLogProbMetric: 30.9949, val_loss: 31.7721, val_MinusLogProbMetric: 31.7721

Epoch 17: val_loss did not improve from 31.20438
196/196 - 10s - loss: 30.9949 - MinusLogProbMetric: 30.9949 - val_loss: 31.7721 - val_MinusLogProbMetric: 31.7721 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 18/1000
2023-09-09 10:31:10.884 
Epoch 18/1000 
	 loss: 31.7683, MinusLogProbMetric: 31.7683, val_loss: 40.1466, val_MinusLogProbMetric: 40.1466

Epoch 18: val_loss did not improve from 31.20438
196/196 - 10s - loss: 31.7683 - MinusLogProbMetric: 31.7683 - val_loss: 40.1466 - val_MinusLogProbMetric: 40.1466 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 19/1000
2023-09-09 10:31:21.208 
Epoch 19/1000 
	 loss: 31.1404, MinusLogProbMetric: 31.1404, val_loss: 30.7080, val_MinusLogProbMetric: 30.7080

Epoch 19: val_loss improved from 31.20438 to 30.70802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 31.1404 - MinusLogProbMetric: 31.1404 - val_loss: 30.7080 - val_MinusLogProbMetric: 30.7080 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 20/1000
2023-09-09 10:31:31.792 
Epoch 20/1000 
	 loss: 30.7402, MinusLogProbMetric: 30.7402, val_loss: 30.0195, val_MinusLogProbMetric: 30.0195

Epoch 20: val_loss improved from 30.70802 to 30.01949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 30.7402 - MinusLogProbMetric: 30.7402 - val_loss: 30.0195 - val_MinusLogProbMetric: 30.0195 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 21/1000
2023-09-09 10:31:42.707 
Epoch 21/1000 
	 loss: 30.9481, MinusLogProbMetric: 30.9481, val_loss: 30.3113, val_MinusLogProbMetric: 30.3113

Epoch 21: val_loss did not improve from 30.01949
196/196 - 11s - loss: 30.9481 - MinusLogProbMetric: 30.9481 - val_loss: 30.3113 - val_MinusLogProbMetric: 30.3113 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 22/1000
2023-09-09 10:31:52.364 
Epoch 22/1000 
	 loss: 30.8653, MinusLogProbMetric: 30.8653, val_loss: 30.2237, val_MinusLogProbMetric: 30.2237

Epoch 22: val_loss did not improve from 30.01949
196/196 - 10s - loss: 30.8653 - MinusLogProbMetric: 30.8653 - val_loss: 30.2237 - val_MinusLogProbMetric: 30.2237 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 23/1000
2023-09-09 10:32:02.978 
Epoch 23/1000 
	 loss: 30.5383, MinusLogProbMetric: 30.5383, val_loss: 32.3509, val_MinusLogProbMetric: 32.3509

Epoch 23: val_loss did not improve from 30.01949
196/196 - 11s - loss: 30.5383 - MinusLogProbMetric: 30.5383 - val_loss: 32.3509 - val_MinusLogProbMetric: 32.3509 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 24/1000
2023-09-09 10:32:13.077 
Epoch 24/1000 
	 loss: 30.7300, MinusLogProbMetric: 30.7300, val_loss: 30.8309, val_MinusLogProbMetric: 30.8309

Epoch 24: val_loss did not improve from 30.01949
196/196 - 10s - loss: 30.7300 - MinusLogProbMetric: 30.7300 - val_loss: 30.8309 - val_MinusLogProbMetric: 30.8309 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 25/1000
2023-09-09 10:32:23.522 
Epoch 25/1000 
	 loss: 30.6660, MinusLogProbMetric: 30.6660, val_loss: 31.3590, val_MinusLogProbMetric: 31.3590

Epoch 25: val_loss did not improve from 30.01949
196/196 - 10s - loss: 30.6660 - MinusLogProbMetric: 30.6660 - val_loss: 31.3590 - val_MinusLogProbMetric: 31.3590 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 26/1000
2023-09-09 10:32:32.873 
Epoch 26/1000 
	 loss: 30.1409, MinusLogProbMetric: 30.1409, val_loss: 30.1912, val_MinusLogProbMetric: 30.1912

Epoch 26: val_loss did not improve from 30.01949
196/196 - 9s - loss: 30.1409 - MinusLogProbMetric: 30.1409 - val_loss: 30.1912 - val_MinusLogProbMetric: 30.1912 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 27/1000
2023-09-09 10:32:42.791 
Epoch 27/1000 
	 loss: 30.4457, MinusLogProbMetric: 30.4457, val_loss: 31.2840, val_MinusLogProbMetric: 31.2840

Epoch 27: val_loss did not improve from 30.01949
196/196 - 10s - loss: 30.4457 - MinusLogProbMetric: 30.4457 - val_loss: 31.2840 - val_MinusLogProbMetric: 31.2840 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 28/1000
2023-09-09 10:32:53.163 
Epoch 28/1000 
	 loss: 30.1499, MinusLogProbMetric: 30.1499, val_loss: 30.9554, val_MinusLogProbMetric: 30.9554

Epoch 28: val_loss did not improve from 30.01949
196/196 - 10s - loss: 30.1499 - MinusLogProbMetric: 30.1499 - val_loss: 30.9554 - val_MinusLogProbMetric: 30.9554 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 29/1000
2023-09-09 10:33:03.759 
Epoch 29/1000 
	 loss: 30.0848, MinusLogProbMetric: 30.0848, val_loss: 29.7781, val_MinusLogProbMetric: 29.7781

Epoch 29: val_loss improved from 30.01949 to 29.77809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 30.0848 - MinusLogProbMetric: 30.0848 - val_loss: 29.7781 - val_MinusLogProbMetric: 29.7781 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 30/1000
2023-09-09 10:33:13.235 
Epoch 30/1000 
	 loss: 29.9034, MinusLogProbMetric: 29.9034, val_loss: 29.7925, val_MinusLogProbMetric: 29.7925

Epoch 30: val_loss did not improve from 29.77809
196/196 - 9s - loss: 29.9034 - MinusLogProbMetric: 29.9034 - val_loss: 29.7925 - val_MinusLogProbMetric: 29.7925 - lr: 0.0010 - 9s/epoch - 47ms/step
Epoch 31/1000
2023-09-09 10:33:23.381 
Epoch 31/1000 
	 loss: 29.8443, MinusLogProbMetric: 29.8443, val_loss: 29.6476, val_MinusLogProbMetric: 29.6476

Epoch 31: val_loss improved from 29.77809 to 29.64760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 29.8443 - MinusLogProbMetric: 29.8443 - val_loss: 29.6476 - val_MinusLogProbMetric: 29.6476 - lr: 0.0010 - 10s/epoch - 54ms/step
Epoch 32/1000
2023-09-09 10:33:33.949 
Epoch 32/1000 
	 loss: 30.2105, MinusLogProbMetric: 30.2105, val_loss: 30.2677, val_MinusLogProbMetric: 30.2677

Epoch 32: val_loss did not improve from 29.64760
196/196 - 10s - loss: 30.2105 - MinusLogProbMetric: 30.2105 - val_loss: 30.2677 - val_MinusLogProbMetric: 30.2677 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 33/1000
2023-09-09 10:33:44.307 
Epoch 33/1000 
	 loss: 29.8384, MinusLogProbMetric: 29.8384, val_loss: 29.7093, val_MinusLogProbMetric: 29.7093

Epoch 33: val_loss did not improve from 29.64760
196/196 - 10s - loss: 29.8384 - MinusLogProbMetric: 29.8384 - val_loss: 29.7093 - val_MinusLogProbMetric: 29.7093 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 34/1000
2023-09-09 10:33:54.767 
Epoch 34/1000 
	 loss: 29.7253, MinusLogProbMetric: 29.7253, val_loss: 30.1741, val_MinusLogProbMetric: 30.1741

Epoch 34: val_loss did not improve from 29.64760
196/196 - 10s - loss: 29.7253 - MinusLogProbMetric: 29.7253 - val_loss: 30.1741 - val_MinusLogProbMetric: 30.1741 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 35/1000
2023-09-09 10:34:05.188 
Epoch 35/1000 
	 loss: 29.8008, MinusLogProbMetric: 29.8008, val_loss: 30.5655, val_MinusLogProbMetric: 30.5655

Epoch 35: val_loss did not improve from 29.64760
196/196 - 10s - loss: 29.8008 - MinusLogProbMetric: 29.8008 - val_loss: 30.5655 - val_MinusLogProbMetric: 30.5655 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 36/1000
2023-09-09 10:34:15.568 
Epoch 36/1000 
	 loss: 29.6962, MinusLogProbMetric: 29.6962, val_loss: 30.1804, val_MinusLogProbMetric: 30.1804

Epoch 36: val_loss did not improve from 29.64760
196/196 - 10s - loss: 29.6962 - MinusLogProbMetric: 29.6962 - val_loss: 30.1804 - val_MinusLogProbMetric: 30.1804 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 37/1000
2023-09-09 10:34:26.081 
Epoch 37/1000 
	 loss: 29.5369, MinusLogProbMetric: 29.5369, val_loss: 29.3018, val_MinusLogProbMetric: 29.3018

Epoch 37: val_loss improved from 29.64760 to 29.30182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 29.5369 - MinusLogProbMetric: 29.5369 - val_loss: 29.3018 - val_MinusLogProbMetric: 29.3018 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 38/1000
2023-09-09 10:34:36.612 
Epoch 38/1000 
	 loss: 29.9025, MinusLogProbMetric: 29.9025, val_loss: 29.4492, val_MinusLogProbMetric: 29.4492

Epoch 38: val_loss did not improve from 29.30182
196/196 - 10s - loss: 29.9025 - MinusLogProbMetric: 29.9025 - val_loss: 29.4492 - val_MinusLogProbMetric: 29.4492 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 39/1000
2023-09-09 10:34:46.996 
Epoch 39/1000 
	 loss: 29.9611, MinusLogProbMetric: 29.9611, val_loss: 30.5272, val_MinusLogProbMetric: 30.5272

Epoch 39: val_loss did not improve from 29.30182
196/196 - 10s - loss: 29.9611 - MinusLogProbMetric: 29.9611 - val_loss: 30.5272 - val_MinusLogProbMetric: 30.5272 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 40/1000
2023-09-09 10:34:57.370 
Epoch 40/1000 
	 loss: 29.8175, MinusLogProbMetric: 29.8175, val_loss: 29.6750, val_MinusLogProbMetric: 29.6750

Epoch 40: val_loss did not improve from 29.30182
196/196 - 10s - loss: 29.8175 - MinusLogProbMetric: 29.8175 - val_loss: 29.6750 - val_MinusLogProbMetric: 29.6750 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 41/1000
2023-09-09 10:35:06.816 
Epoch 41/1000 
	 loss: 29.4104, MinusLogProbMetric: 29.4104, val_loss: 29.3425, val_MinusLogProbMetric: 29.3425

Epoch 41: val_loss did not improve from 29.30182
196/196 - 9s - loss: 29.4104 - MinusLogProbMetric: 29.4104 - val_loss: 29.3425 - val_MinusLogProbMetric: 29.3425 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 42/1000
2023-09-09 10:35:16.519 
Epoch 42/1000 
	 loss: 29.2394, MinusLogProbMetric: 29.2394, val_loss: 29.4459, val_MinusLogProbMetric: 29.4459

Epoch 42: val_loss did not improve from 29.30182
196/196 - 10s - loss: 29.2394 - MinusLogProbMetric: 29.2394 - val_loss: 29.4459 - val_MinusLogProbMetric: 29.4459 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 43/1000
2023-09-09 10:35:26.691 
Epoch 43/1000 
	 loss: 29.2386, MinusLogProbMetric: 29.2386, val_loss: 29.1367, val_MinusLogProbMetric: 29.1367

Epoch 43: val_loss improved from 29.30182 to 29.13671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 29.2386 - MinusLogProbMetric: 29.2386 - val_loss: 29.1367 - val_MinusLogProbMetric: 29.1367 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 44/1000
2023-09-09 10:35:37.210 
Epoch 44/1000 
	 loss: 29.3110, MinusLogProbMetric: 29.3110, val_loss: 29.2702, val_MinusLogProbMetric: 29.2702

Epoch 44: val_loss did not improve from 29.13671
196/196 - 10s - loss: 29.3110 - MinusLogProbMetric: 29.3110 - val_loss: 29.2702 - val_MinusLogProbMetric: 29.2702 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 45/1000
2023-09-09 10:35:47.640 
Epoch 45/1000 
	 loss: 29.1227, MinusLogProbMetric: 29.1227, val_loss: 30.0087, val_MinusLogProbMetric: 30.0087

Epoch 45: val_loss did not improve from 29.13671
196/196 - 10s - loss: 29.1227 - MinusLogProbMetric: 29.1227 - val_loss: 30.0087 - val_MinusLogProbMetric: 30.0087 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 46/1000
2023-09-09 10:35:56.895 
Epoch 46/1000 
	 loss: 29.3648, MinusLogProbMetric: 29.3648, val_loss: 29.2903, val_MinusLogProbMetric: 29.2903

Epoch 46: val_loss did not improve from 29.13671
196/196 - 9s - loss: 29.3648 - MinusLogProbMetric: 29.3648 - val_loss: 29.2903 - val_MinusLogProbMetric: 29.2903 - lr: 0.0010 - 9s/epoch - 47ms/step
Epoch 47/1000
2023-09-09 10:36:07.149 
Epoch 47/1000 
	 loss: 29.0946, MinusLogProbMetric: 29.0946, val_loss: 29.2025, val_MinusLogProbMetric: 29.2025

Epoch 47: val_loss did not improve from 29.13671
196/196 - 10s - loss: 29.0946 - MinusLogProbMetric: 29.0946 - val_loss: 29.2025 - val_MinusLogProbMetric: 29.2025 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 48/1000
2023-09-09 10:36:17.592 
Epoch 48/1000 
	 loss: 29.3293, MinusLogProbMetric: 29.3293, val_loss: 29.2463, val_MinusLogProbMetric: 29.2463

Epoch 48: val_loss did not improve from 29.13671
196/196 - 10s - loss: 29.3293 - MinusLogProbMetric: 29.3293 - val_loss: 29.2463 - val_MinusLogProbMetric: 29.2463 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 49/1000
2023-09-09 10:36:27.991 
Epoch 49/1000 
	 loss: 29.0573, MinusLogProbMetric: 29.0573, val_loss: 29.1282, val_MinusLogProbMetric: 29.1282

Epoch 49: val_loss improved from 29.13671 to 29.12823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 29.0573 - MinusLogProbMetric: 29.0573 - val_loss: 29.1282 - val_MinusLogProbMetric: 29.1282 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 50/1000
2023-09-09 10:36:38.733 
Epoch 50/1000 
	 loss: 29.2034, MinusLogProbMetric: 29.2034, val_loss: 28.9837, val_MinusLogProbMetric: 28.9837

Epoch 50: val_loss improved from 29.12823 to 28.98370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 29.2034 - MinusLogProbMetric: 29.2034 - val_loss: 28.9837 - val_MinusLogProbMetric: 28.9837 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 51/1000
2023-09-09 10:36:48.796 
Epoch 51/1000 
	 loss: 29.0353, MinusLogProbMetric: 29.0353, val_loss: 29.2256, val_MinusLogProbMetric: 29.2256

Epoch 51: val_loss did not improve from 28.98370
196/196 - 10s - loss: 29.0353 - MinusLogProbMetric: 29.0353 - val_loss: 29.2256 - val_MinusLogProbMetric: 29.2256 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 52/1000
2023-09-09 10:36:57.799 
Epoch 52/1000 
	 loss: 29.0940, MinusLogProbMetric: 29.0940, val_loss: 29.4403, val_MinusLogProbMetric: 29.4403

Epoch 52: val_loss did not improve from 28.98370
196/196 - 9s - loss: 29.0940 - MinusLogProbMetric: 29.0940 - val_loss: 29.4403 - val_MinusLogProbMetric: 29.4403 - lr: 0.0010 - 9s/epoch - 46ms/step
Epoch 53/1000
2023-09-09 10:37:07.075 
Epoch 53/1000 
	 loss: 28.8736, MinusLogProbMetric: 28.8736, val_loss: 28.9889, val_MinusLogProbMetric: 28.9889

Epoch 53: val_loss did not improve from 28.98370
196/196 - 9s - loss: 28.8736 - MinusLogProbMetric: 28.8736 - val_loss: 28.9889 - val_MinusLogProbMetric: 28.9889 - lr: 0.0010 - 9s/epoch - 47ms/step
Epoch 54/1000
2023-09-09 10:37:17.346 
Epoch 54/1000 
	 loss: 28.8734, MinusLogProbMetric: 28.8734, val_loss: 29.4821, val_MinusLogProbMetric: 29.4821

Epoch 54: val_loss did not improve from 28.98370
196/196 - 10s - loss: 28.8734 - MinusLogProbMetric: 28.8734 - val_loss: 29.4821 - val_MinusLogProbMetric: 29.4821 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 55/1000
2023-09-09 10:37:27.917 
Epoch 55/1000 
	 loss: 28.8586, MinusLogProbMetric: 28.8586, val_loss: 29.0566, val_MinusLogProbMetric: 29.0566

Epoch 55: val_loss did not improve from 28.98370
196/196 - 11s - loss: 28.8586 - MinusLogProbMetric: 28.8586 - val_loss: 29.0566 - val_MinusLogProbMetric: 29.0566 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 56/1000
2023-09-09 10:37:38.207 
Epoch 56/1000 
	 loss: 28.8239, MinusLogProbMetric: 28.8239, val_loss: 28.8790, val_MinusLogProbMetric: 28.8790

Epoch 56: val_loss improved from 28.98370 to 28.87900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.8239 - MinusLogProbMetric: 28.8239 - val_loss: 28.8790 - val_MinusLogProbMetric: 28.8790 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 57/1000
2023-09-09 10:37:48.882 
Epoch 57/1000 
	 loss: 28.8405, MinusLogProbMetric: 28.8405, val_loss: 30.5878, val_MinusLogProbMetric: 30.5878

Epoch 57: val_loss did not improve from 28.87900
196/196 - 10s - loss: 28.8405 - MinusLogProbMetric: 28.8405 - val_loss: 30.5878 - val_MinusLogProbMetric: 30.5878 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 58/1000
2023-09-09 10:37:59.168 
Epoch 58/1000 
	 loss: 28.8278, MinusLogProbMetric: 28.8278, val_loss: 29.0568, val_MinusLogProbMetric: 29.0568

Epoch 58: val_loss did not improve from 28.87900
196/196 - 10s - loss: 28.8278 - MinusLogProbMetric: 28.8278 - val_loss: 29.0568 - val_MinusLogProbMetric: 29.0568 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 59/1000
2023-09-09 10:38:08.624 
Epoch 59/1000 
	 loss: 28.7289, MinusLogProbMetric: 28.7289, val_loss: 28.9275, val_MinusLogProbMetric: 28.9275

Epoch 59: val_loss did not improve from 28.87900
196/196 - 9s - loss: 28.7289 - MinusLogProbMetric: 28.7289 - val_loss: 28.9275 - val_MinusLogProbMetric: 28.9275 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 60/1000
2023-09-09 10:38:18.965 
Epoch 60/1000 
	 loss: 28.8312, MinusLogProbMetric: 28.8312, val_loss: 28.7493, val_MinusLogProbMetric: 28.7493

Epoch 60: val_loss improved from 28.87900 to 28.74928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.8312 - MinusLogProbMetric: 28.8312 - val_loss: 28.7493 - val_MinusLogProbMetric: 28.7493 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 61/1000
2023-09-09 10:38:29.647 
Epoch 61/1000 
	 loss: 28.7190, MinusLogProbMetric: 28.7190, val_loss: 28.6923, val_MinusLogProbMetric: 28.6923

Epoch 61: val_loss improved from 28.74928 to 28.69234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.7190 - MinusLogProbMetric: 28.7190 - val_loss: 28.6923 - val_MinusLogProbMetric: 28.6923 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 62/1000
2023-09-09 10:38:40.378 
Epoch 62/1000 
	 loss: 28.7757, MinusLogProbMetric: 28.7757, val_loss: 28.7905, val_MinusLogProbMetric: 28.7905

Epoch 62: val_loss did not improve from 28.69234
196/196 - 10s - loss: 28.7757 - MinusLogProbMetric: 28.7757 - val_loss: 28.7905 - val_MinusLogProbMetric: 28.7905 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 63/1000
2023-09-09 10:38:49.556 
Epoch 63/1000 
	 loss: 28.6693, MinusLogProbMetric: 28.6693, val_loss: 28.6081, val_MinusLogProbMetric: 28.6081

Epoch 63: val_loss improved from 28.69234 to 28.60810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 9s - loss: 28.6693 - MinusLogProbMetric: 28.6693 - val_loss: 28.6081 - val_MinusLogProbMetric: 28.6081 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 64/1000
2023-09-09 10:38:59.965 
Epoch 64/1000 
	 loss: 28.7025, MinusLogProbMetric: 28.7025, val_loss: 29.3379, val_MinusLogProbMetric: 29.3379

Epoch 64: val_loss did not improve from 28.60810
196/196 - 10s - loss: 28.7025 - MinusLogProbMetric: 28.7025 - val_loss: 29.3379 - val_MinusLogProbMetric: 29.3379 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 65/1000
2023-09-09 10:39:10.322 
Epoch 65/1000 
	 loss: 28.6461, MinusLogProbMetric: 28.6461, val_loss: 29.0636, val_MinusLogProbMetric: 29.0636

Epoch 65: val_loss did not improve from 28.60810
196/196 - 10s - loss: 28.6461 - MinusLogProbMetric: 28.6461 - val_loss: 29.0636 - val_MinusLogProbMetric: 29.0636 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 66/1000
2023-09-09 10:39:20.835 
Epoch 66/1000 
	 loss: 28.6560, MinusLogProbMetric: 28.6560, val_loss: 28.6639, val_MinusLogProbMetric: 28.6639

Epoch 66: val_loss did not improve from 28.60810
196/196 - 11s - loss: 28.6560 - MinusLogProbMetric: 28.6560 - val_loss: 28.6639 - val_MinusLogProbMetric: 28.6639 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 67/1000
2023-09-09 10:39:31.309 
Epoch 67/1000 
	 loss: 28.5873, MinusLogProbMetric: 28.5873, val_loss: 28.5672, val_MinusLogProbMetric: 28.5672

Epoch 67: val_loss improved from 28.60810 to 28.56716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.5873 - MinusLogProbMetric: 28.5873 - val_loss: 28.5672 - val_MinusLogProbMetric: 28.5672 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 68/1000
2023-09-09 10:39:40.961 
Epoch 68/1000 
	 loss: 28.5929, MinusLogProbMetric: 28.5929, val_loss: 28.5011, val_MinusLogProbMetric: 28.5011

Epoch 68: val_loss improved from 28.56716 to 28.50108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 28.5929 - MinusLogProbMetric: 28.5929 - val_loss: 28.5011 - val_MinusLogProbMetric: 28.5011 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 69/1000
2023-09-09 10:39:51.412 
Epoch 69/1000 
	 loss: 28.5894, MinusLogProbMetric: 28.5894, val_loss: 28.9960, val_MinusLogProbMetric: 28.9960

Epoch 69: val_loss did not improve from 28.50108
196/196 - 10s - loss: 28.5894 - MinusLogProbMetric: 28.5894 - val_loss: 28.9960 - val_MinusLogProbMetric: 28.9960 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 70/1000
2023-09-09 10:40:01.838 
Epoch 70/1000 
	 loss: 28.5576, MinusLogProbMetric: 28.5576, val_loss: 28.4585, val_MinusLogProbMetric: 28.4585

Epoch 70: val_loss improved from 28.50108 to 28.45849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.5576 - MinusLogProbMetric: 28.5576 - val_loss: 28.4585 - val_MinusLogProbMetric: 28.4585 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 71/1000
2023-09-09 10:40:12.693 
Epoch 71/1000 
	 loss: 28.5365, MinusLogProbMetric: 28.5365, val_loss: 28.7575, val_MinusLogProbMetric: 28.7575

Epoch 71: val_loss did not improve from 28.45849
196/196 - 10s - loss: 28.5365 - MinusLogProbMetric: 28.5365 - val_loss: 28.7575 - val_MinusLogProbMetric: 28.7575 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 72/1000
2023-09-09 10:40:23.279 
Epoch 72/1000 
	 loss: 28.5143, MinusLogProbMetric: 28.5143, val_loss: 28.6173, val_MinusLogProbMetric: 28.6173

Epoch 72: val_loss did not improve from 28.45849
196/196 - 11s - loss: 28.5143 - MinusLogProbMetric: 28.5143 - val_loss: 28.6173 - val_MinusLogProbMetric: 28.6173 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 73/1000
2023-09-09 10:40:33.374 
Epoch 73/1000 
	 loss: 28.6387, MinusLogProbMetric: 28.6387, val_loss: 28.6329, val_MinusLogProbMetric: 28.6329

Epoch 73: val_loss did not improve from 28.45849
196/196 - 10s - loss: 28.6387 - MinusLogProbMetric: 28.6387 - val_loss: 28.6329 - val_MinusLogProbMetric: 28.6329 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 74/1000
2023-09-09 10:40:43.769 
Epoch 74/1000 
	 loss: 28.4700, MinusLogProbMetric: 28.4700, val_loss: 28.7665, val_MinusLogProbMetric: 28.7665

Epoch 74: val_loss did not improve from 28.45849
196/196 - 10s - loss: 28.4700 - MinusLogProbMetric: 28.4700 - val_loss: 28.7665 - val_MinusLogProbMetric: 28.7665 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 75/1000
2023-09-09 10:40:54.139 
Epoch 75/1000 
	 loss: 28.4441, MinusLogProbMetric: 28.4441, val_loss: 28.8487, val_MinusLogProbMetric: 28.8487

Epoch 75: val_loss did not improve from 28.45849
196/196 - 10s - loss: 28.4441 - MinusLogProbMetric: 28.4441 - val_loss: 28.8487 - val_MinusLogProbMetric: 28.8487 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 76/1000
2023-09-09 10:41:04.699 
Epoch 76/1000 
	 loss: 28.4397, MinusLogProbMetric: 28.4397, val_loss: 28.4342, val_MinusLogProbMetric: 28.4342

Epoch 76: val_loss improved from 28.45849 to 28.43421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.4397 - MinusLogProbMetric: 28.4397 - val_loss: 28.4342 - val_MinusLogProbMetric: 28.4342 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 77/1000
2023-09-09 10:41:15.396 
Epoch 77/1000 
	 loss: 28.4177, MinusLogProbMetric: 28.4177, val_loss: 28.4546, val_MinusLogProbMetric: 28.4546

Epoch 77: val_loss did not improve from 28.43421
196/196 - 10s - loss: 28.4177 - MinusLogProbMetric: 28.4177 - val_loss: 28.4546 - val_MinusLogProbMetric: 28.4546 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 78/1000
2023-09-09 10:41:25.601 
Epoch 78/1000 
	 loss: 28.4454, MinusLogProbMetric: 28.4454, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 78: val_loss improved from 28.43421 to 28.35981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 28.4454 - MinusLogProbMetric: 28.4454 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 79/1000
2023-09-09 10:41:36.048 
Epoch 79/1000 
	 loss: 28.3971, MinusLogProbMetric: 28.3971, val_loss: 28.6176, val_MinusLogProbMetric: 28.6176

Epoch 79: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.3971 - MinusLogProbMetric: 28.3971 - val_loss: 28.6176 - val_MinusLogProbMetric: 28.6176 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 80/1000
2023-09-09 10:41:46.499 
Epoch 80/1000 
	 loss: 28.4454, MinusLogProbMetric: 28.4454, val_loss: 29.1973, val_MinusLogProbMetric: 29.1973

Epoch 80: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.4454 - MinusLogProbMetric: 28.4454 - val_loss: 29.1973 - val_MinusLogProbMetric: 29.1973 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 81/1000
2023-09-09 10:41:56.892 
Epoch 81/1000 
	 loss: 28.3767, MinusLogProbMetric: 28.3767, val_loss: 28.7044, val_MinusLogProbMetric: 28.7044

Epoch 81: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.3767 - MinusLogProbMetric: 28.3767 - val_loss: 28.7044 - val_MinusLogProbMetric: 28.7044 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 82/1000
2023-09-09 10:42:07.439 
Epoch 82/1000 
	 loss: 28.4749, MinusLogProbMetric: 28.4749, val_loss: 28.6490, val_MinusLogProbMetric: 28.6490

Epoch 82: val_loss did not improve from 28.35981
196/196 - 11s - loss: 28.4749 - MinusLogProbMetric: 28.4749 - val_loss: 28.6490 - val_MinusLogProbMetric: 28.6490 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 83/1000
2023-09-09 10:42:17.867 
Epoch 83/1000 
	 loss: 28.2985, MinusLogProbMetric: 28.2985, val_loss: 28.4011, val_MinusLogProbMetric: 28.4011

Epoch 83: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.2985 - MinusLogProbMetric: 28.2985 - val_loss: 28.4011 - val_MinusLogProbMetric: 28.4011 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 84/1000
2023-09-09 10:42:27.459 
Epoch 84/1000 
	 loss: 28.4497, MinusLogProbMetric: 28.4497, val_loss: 28.4483, val_MinusLogProbMetric: 28.4483

Epoch 84: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.4497 - MinusLogProbMetric: 28.4497 - val_loss: 28.4483 - val_MinusLogProbMetric: 28.4483 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 85/1000
2023-09-09 10:42:38.122 
Epoch 85/1000 
	 loss: 28.3468, MinusLogProbMetric: 28.3468, val_loss: 28.7509, val_MinusLogProbMetric: 28.7509

Epoch 85: val_loss did not improve from 28.35981
196/196 - 11s - loss: 28.3468 - MinusLogProbMetric: 28.3468 - val_loss: 28.7509 - val_MinusLogProbMetric: 28.7509 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 86/1000
2023-09-09 10:42:47.886 
Epoch 86/1000 
	 loss: 28.3261, MinusLogProbMetric: 28.3261, val_loss: 28.4524, val_MinusLogProbMetric: 28.4524

Epoch 86: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.3261 - MinusLogProbMetric: 28.3261 - val_loss: 28.4524 - val_MinusLogProbMetric: 28.4524 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 87/1000
2023-09-09 10:42:57.863 
Epoch 87/1000 
	 loss: 28.5105, MinusLogProbMetric: 28.5105, val_loss: 30.5776, val_MinusLogProbMetric: 30.5776

Epoch 87: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.5105 - MinusLogProbMetric: 28.5105 - val_loss: 30.5776 - val_MinusLogProbMetric: 30.5776 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 88/1000
2023-09-09 10:43:08.002 
Epoch 88/1000 
	 loss: 28.3519, MinusLogProbMetric: 28.3519, val_loss: 28.3900, val_MinusLogProbMetric: 28.3900

Epoch 88: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.3519 - MinusLogProbMetric: 28.3519 - val_loss: 28.3900 - val_MinusLogProbMetric: 28.3900 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 89/1000
2023-09-09 10:43:17.965 
Epoch 89/1000 
	 loss: 28.2875, MinusLogProbMetric: 28.2875, val_loss: 28.4675, val_MinusLogProbMetric: 28.4675

Epoch 89: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.2875 - MinusLogProbMetric: 28.2875 - val_loss: 28.4675 - val_MinusLogProbMetric: 28.4675 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 90/1000
2023-09-09 10:43:27.465 
Epoch 90/1000 
	 loss: 28.2630, MinusLogProbMetric: 28.2630, val_loss: 28.9072, val_MinusLogProbMetric: 28.9072

Epoch 90: val_loss did not improve from 28.35981
196/196 - 9s - loss: 28.2630 - MinusLogProbMetric: 28.2630 - val_loss: 28.9072 - val_MinusLogProbMetric: 28.9072 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 91/1000
2023-09-09 10:43:37.520 
Epoch 91/1000 
	 loss: 28.2840, MinusLogProbMetric: 28.2840, val_loss: 28.4892, val_MinusLogProbMetric: 28.4892

Epoch 91: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.2840 - MinusLogProbMetric: 28.2840 - val_loss: 28.4892 - val_MinusLogProbMetric: 28.4892 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 92/1000
2023-09-09 10:43:47.211 
Epoch 92/1000 
	 loss: 28.2811, MinusLogProbMetric: 28.2811, val_loss: 28.4258, val_MinusLogProbMetric: 28.4258

Epoch 92: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.2811 - MinusLogProbMetric: 28.2811 - val_loss: 28.4258 - val_MinusLogProbMetric: 28.4258 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 93/1000
2023-09-09 10:43:57.626 
Epoch 93/1000 
	 loss: 28.2616, MinusLogProbMetric: 28.2616, val_loss: 28.3850, val_MinusLogProbMetric: 28.3850

Epoch 93: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.2616 - MinusLogProbMetric: 28.2616 - val_loss: 28.3850 - val_MinusLogProbMetric: 28.3850 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 94/1000
2023-09-09 10:44:08.170 
Epoch 94/1000 
	 loss: 28.2094, MinusLogProbMetric: 28.2094, val_loss: 28.4253, val_MinusLogProbMetric: 28.4253

Epoch 94: val_loss did not improve from 28.35981
196/196 - 11s - loss: 28.2094 - MinusLogProbMetric: 28.2094 - val_loss: 28.4253 - val_MinusLogProbMetric: 28.4253 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 95/1000
2023-09-09 10:44:18.590 
Epoch 95/1000 
	 loss: 28.2149, MinusLogProbMetric: 28.2149, val_loss: 28.5602, val_MinusLogProbMetric: 28.5602

Epoch 95: val_loss did not improve from 28.35981
196/196 - 10s - loss: 28.2149 - MinusLogProbMetric: 28.2149 - val_loss: 28.5602 - val_MinusLogProbMetric: 28.5602 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 96/1000
2023-09-09 10:44:29.075 
Epoch 96/1000 
	 loss: 28.2668, MinusLogProbMetric: 28.2668, val_loss: 28.3072, val_MinusLogProbMetric: 28.3072

Epoch 96: val_loss improved from 28.35981 to 28.30723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.2668 - MinusLogProbMetric: 28.2668 - val_loss: 28.3072 - val_MinusLogProbMetric: 28.3072 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 97/1000
2023-09-09 10:44:39.894 
Epoch 97/1000 
	 loss: 28.2155, MinusLogProbMetric: 28.2155, val_loss: 28.2223, val_MinusLogProbMetric: 28.2223

Epoch 97: val_loss improved from 28.30723 to 28.22231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 28.2155 - MinusLogProbMetric: 28.2155 - val_loss: 28.2223 - val_MinusLogProbMetric: 28.2223 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 98/1000
2023-09-09 10:44:50.344 
Epoch 98/1000 
	 loss: 28.2317, MinusLogProbMetric: 28.2317, val_loss: 28.2425, val_MinusLogProbMetric: 28.2425

Epoch 98: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.2317 - MinusLogProbMetric: 28.2317 - val_loss: 28.2425 - val_MinusLogProbMetric: 28.2425 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 99/1000
2023-09-09 10:45:00.746 
Epoch 99/1000 
	 loss: 28.2027, MinusLogProbMetric: 28.2027, val_loss: 28.4236, val_MinusLogProbMetric: 28.4236

Epoch 99: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.2027 - MinusLogProbMetric: 28.2027 - val_loss: 28.4236 - val_MinusLogProbMetric: 28.4236 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 100/1000
2023-09-09 10:45:10.911 
Epoch 100/1000 
	 loss: 28.1484, MinusLogProbMetric: 28.1484, val_loss: 28.3696, val_MinusLogProbMetric: 28.3696

Epoch 100: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.1484 - MinusLogProbMetric: 28.1484 - val_loss: 28.3696 - val_MinusLogProbMetric: 28.3696 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 101/1000
2023-09-09 10:45:21.049 
Epoch 101/1000 
	 loss: 28.2493, MinusLogProbMetric: 28.2493, val_loss: 28.3072, val_MinusLogProbMetric: 28.3072

Epoch 101: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.2493 - MinusLogProbMetric: 28.2493 - val_loss: 28.3072 - val_MinusLogProbMetric: 28.3072 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 102/1000
2023-09-09 10:45:31.527 
Epoch 102/1000 
	 loss: 28.1606, MinusLogProbMetric: 28.1606, val_loss: 28.3806, val_MinusLogProbMetric: 28.3806

Epoch 102: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.1606 - MinusLogProbMetric: 28.1606 - val_loss: 28.3806 - val_MinusLogProbMetric: 28.3806 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 103/1000
2023-09-09 10:45:42.131 
Epoch 103/1000 
	 loss: 28.2120, MinusLogProbMetric: 28.2120, val_loss: 28.8360, val_MinusLogProbMetric: 28.8360

Epoch 103: val_loss did not improve from 28.22231
196/196 - 11s - loss: 28.2120 - MinusLogProbMetric: 28.2120 - val_loss: 28.8360 - val_MinusLogProbMetric: 28.8360 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 104/1000
2023-09-09 10:45:52.726 
Epoch 104/1000 
	 loss: 28.1780, MinusLogProbMetric: 28.1780, val_loss: 28.6076, val_MinusLogProbMetric: 28.6076

Epoch 104: val_loss did not improve from 28.22231
196/196 - 11s - loss: 28.1780 - MinusLogProbMetric: 28.1780 - val_loss: 28.6076 - val_MinusLogProbMetric: 28.6076 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 105/1000
2023-09-09 10:46:03.246 
Epoch 105/1000 
	 loss: 28.1614, MinusLogProbMetric: 28.1614, val_loss: 28.4026, val_MinusLogProbMetric: 28.4026

Epoch 105: val_loss did not improve from 28.22231
196/196 - 11s - loss: 28.1614 - MinusLogProbMetric: 28.1614 - val_loss: 28.4026 - val_MinusLogProbMetric: 28.4026 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 106/1000
2023-09-09 10:46:13.947 
Epoch 106/1000 
	 loss: 28.1610, MinusLogProbMetric: 28.1610, val_loss: 28.8749, val_MinusLogProbMetric: 28.8749

Epoch 106: val_loss did not improve from 28.22231
196/196 - 11s - loss: 28.1610 - MinusLogProbMetric: 28.1610 - val_loss: 28.8749 - val_MinusLogProbMetric: 28.8749 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 107/1000
2023-09-09 10:46:24.324 
Epoch 107/1000 
	 loss: 28.1381, MinusLogProbMetric: 28.1381, val_loss: 28.4977, val_MinusLogProbMetric: 28.4977

Epoch 107: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.1381 - MinusLogProbMetric: 28.1381 - val_loss: 28.4977 - val_MinusLogProbMetric: 28.4977 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 108/1000
2023-09-09 10:46:34.347 
Epoch 108/1000 
	 loss: 28.2461, MinusLogProbMetric: 28.2461, val_loss: 28.2683, val_MinusLogProbMetric: 28.2683

Epoch 108: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.2461 - MinusLogProbMetric: 28.2461 - val_loss: 28.2683 - val_MinusLogProbMetric: 28.2683 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 109/1000
2023-09-09 10:46:44.318 
Epoch 109/1000 
	 loss: 28.1699, MinusLogProbMetric: 28.1699, val_loss: 28.5881, val_MinusLogProbMetric: 28.5881

Epoch 109: val_loss did not improve from 28.22231
196/196 - 10s - loss: 28.1699 - MinusLogProbMetric: 28.1699 - val_loss: 28.5881 - val_MinusLogProbMetric: 28.5881 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 110/1000
2023-09-09 10:46:54.263 
Epoch 110/1000 
	 loss: 28.0759, MinusLogProbMetric: 28.0759, val_loss: 28.1324, val_MinusLogProbMetric: 28.1324

Epoch 110: val_loss improved from 28.22231 to 28.13242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 28.0759 - MinusLogProbMetric: 28.0759 - val_loss: 28.1324 - val_MinusLogProbMetric: 28.1324 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 111/1000
2023-09-09 10:47:05.089 
Epoch 111/1000 
	 loss: 28.1530, MinusLogProbMetric: 28.1530, val_loss: 28.2994, val_MinusLogProbMetric: 28.2994

Epoch 111: val_loss did not improve from 28.13242
196/196 - 11s - loss: 28.1530 - MinusLogProbMetric: 28.1530 - val_loss: 28.2994 - val_MinusLogProbMetric: 28.2994 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 112/1000
2023-09-09 10:47:15.299 
Epoch 112/1000 
	 loss: 28.0960, MinusLogProbMetric: 28.0960, val_loss: 28.4363, val_MinusLogProbMetric: 28.4363

Epoch 112: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0960 - MinusLogProbMetric: 28.0960 - val_loss: 28.4363 - val_MinusLogProbMetric: 28.4363 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 113/1000
2023-09-09 10:47:26.048 
Epoch 113/1000 
	 loss: 28.1139, MinusLogProbMetric: 28.1139, val_loss: 28.5975, val_MinusLogProbMetric: 28.5975

Epoch 113: val_loss did not improve from 28.13242
196/196 - 11s - loss: 28.1139 - MinusLogProbMetric: 28.1139 - val_loss: 28.5975 - val_MinusLogProbMetric: 28.5975 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 114/1000
2023-09-09 10:47:36.451 
Epoch 114/1000 
	 loss: 28.1141, MinusLogProbMetric: 28.1141, val_loss: 28.2291, val_MinusLogProbMetric: 28.2291

Epoch 114: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.1141 - MinusLogProbMetric: 28.1141 - val_loss: 28.2291 - val_MinusLogProbMetric: 28.2291 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 115/1000
2023-09-09 10:47:46.533 
Epoch 115/1000 
	 loss: 28.0659, MinusLogProbMetric: 28.0659, val_loss: 28.4231, val_MinusLogProbMetric: 28.4231

Epoch 115: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0659 - MinusLogProbMetric: 28.0659 - val_loss: 28.4231 - val_MinusLogProbMetric: 28.4231 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 116/1000
2023-09-09 10:47:57.076 
Epoch 116/1000 
	 loss: 28.0666, MinusLogProbMetric: 28.0666, val_loss: 28.4821, val_MinusLogProbMetric: 28.4821

Epoch 116: val_loss did not improve from 28.13242
196/196 - 11s - loss: 28.0666 - MinusLogProbMetric: 28.0666 - val_loss: 28.4821 - val_MinusLogProbMetric: 28.4821 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 117/1000
2023-09-09 10:48:07.201 
Epoch 117/1000 
	 loss: 28.0483, MinusLogProbMetric: 28.0483, val_loss: 28.3489, val_MinusLogProbMetric: 28.3489

Epoch 117: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0483 - MinusLogProbMetric: 28.0483 - val_loss: 28.3489 - val_MinusLogProbMetric: 28.3489 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 118/1000
2023-09-09 10:48:17.900 
Epoch 118/1000 
	 loss: 28.0858, MinusLogProbMetric: 28.0858, val_loss: 28.4197, val_MinusLogProbMetric: 28.4197

Epoch 118: val_loss did not improve from 28.13242
196/196 - 11s - loss: 28.0858 - MinusLogProbMetric: 28.0858 - val_loss: 28.4197 - val_MinusLogProbMetric: 28.4197 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 119/1000
2023-09-09 10:48:27.874 
Epoch 119/1000 
	 loss: 28.0838, MinusLogProbMetric: 28.0838, val_loss: 29.5930, val_MinusLogProbMetric: 29.5930

Epoch 119: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0838 - MinusLogProbMetric: 28.0838 - val_loss: 29.5930 - val_MinusLogProbMetric: 29.5930 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 120/1000
2023-09-09 10:48:38.399 
Epoch 120/1000 
	 loss: 28.0016, MinusLogProbMetric: 28.0016, val_loss: 28.8239, val_MinusLogProbMetric: 28.8239

Epoch 120: val_loss did not improve from 28.13242
196/196 - 11s - loss: 28.0016 - MinusLogProbMetric: 28.0016 - val_loss: 28.8239 - val_MinusLogProbMetric: 28.8239 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 121/1000
2023-09-09 10:48:48.637 
Epoch 121/1000 
	 loss: 28.0974, MinusLogProbMetric: 28.0974, val_loss: 28.4646, val_MinusLogProbMetric: 28.4646

Epoch 121: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0974 - MinusLogProbMetric: 28.0974 - val_loss: 28.4646 - val_MinusLogProbMetric: 28.4646 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 122/1000
2023-09-09 10:48:59.006 
Epoch 122/1000 
	 loss: 28.0279, MinusLogProbMetric: 28.0279, val_loss: 28.1390, val_MinusLogProbMetric: 28.1390

Epoch 122: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0279 - MinusLogProbMetric: 28.0279 - val_loss: 28.1390 - val_MinusLogProbMetric: 28.1390 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 123/1000
2023-09-09 10:49:09.232 
Epoch 123/1000 
	 loss: 28.0355, MinusLogProbMetric: 28.0355, val_loss: 28.2398, val_MinusLogProbMetric: 28.2398

Epoch 123: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0355 - MinusLogProbMetric: 28.0355 - val_loss: 28.2398 - val_MinusLogProbMetric: 28.2398 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 124/1000
2023-09-09 10:49:19.840 
Epoch 124/1000 
	 loss: 28.0316, MinusLogProbMetric: 28.0316, val_loss: 28.5500, val_MinusLogProbMetric: 28.5500

Epoch 124: val_loss did not improve from 28.13242
196/196 - 11s - loss: 28.0316 - MinusLogProbMetric: 28.0316 - val_loss: 28.5500 - val_MinusLogProbMetric: 28.5500 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 125/1000
2023-09-09 10:49:30.076 
Epoch 125/1000 
	 loss: 28.0319, MinusLogProbMetric: 28.0319, val_loss: 28.6514, val_MinusLogProbMetric: 28.6514

Epoch 125: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0319 - MinusLogProbMetric: 28.0319 - val_loss: 28.6514 - val_MinusLogProbMetric: 28.6514 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 126/1000
2023-09-09 10:49:40.505 
Epoch 126/1000 
	 loss: 27.9917, MinusLogProbMetric: 27.9917, val_loss: 28.2249, val_MinusLogProbMetric: 28.2249

Epoch 126: val_loss did not improve from 28.13242
196/196 - 10s - loss: 27.9917 - MinusLogProbMetric: 27.9917 - val_loss: 28.2249 - val_MinusLogProbMetric: 28.2249 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 127/1000
2023-09-09 10:49:49.901 
Epoch 127/1000 
	 loss: 28.0063, MinusLogProbMetric: 28.0063, val_loss: 28.1999, val_MinusLogProbMetric: 28.1999

Epoch 127: val_loss did not improve from 28.13242
196/196 - 9s - loss: 28.0063 - MinusLogProbMetric: 28.0063 - val_loss: 28.1999 - val_MinusLogProbMetric: 28.1999 - lr: 0.0010 - 9s/epoch - 48ms/step
Epoch 128/1000
2023-09-09 10:49:59.056 
Epoch 128/1000 
	 loss: 28.0203, MinusLogProbMetric: 28.0203, val_loss: 28.2421, val_MinusLogProbMetric: 28.2421

Epoch 128: val_loss did not improve from 28.13242
196/196 - 9s - loss: 28.0203 - MinusLogProbMetric: 28.0203 - val_loss: 28.2421 - val_MinusLogProbMetric: 28.2421 - lr: 0.0010 - 9s/epoch - 47ms/step
Epoch 129/1000
2023-09-09 10:50:08.264 
Epoch 129/1000 
	 loss: 27.9611, MinusLogProbMetric: 27.9611, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 129: val_loss did not improve from 28.13242
196/196 - 9s - loss: 27.9611 - MinusLogProbMetric: 27.9611 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 0.0010 - 9s/epoch - 47ms/step
Epoch 130/1000
2023-09-09 10:50:17.775 
Epoch 130/1000 
	 loss: 28.0125, MinusLogProbMetric: 28.0125, val_loss: 28.2272, val_MinusLogProbMetric: 28.2272

Epoch 130: val_loss did not improve from 28.13242
196/196 - 10s - loss: 28.0125 - MinusLogProbMetric: 28.0125 - val_loss: 28.2272 - val_MinusLogProbMetric: 28.2272 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 131/1000
2023-09-09 10:50:27.437 
Epoch 131/1000 
	 loss: 27.9967, MinusLogProbMetric: 27.9967, val_loss: 28.3272, val_MinusLogProbMetric: 28.3272

Epoch 131: val_loss did not improve from 28.13242
196/196 - 10s - loss: 27.9967 - MinusLogProbMetric: 27.9967 - val_loss: 28.3272 - val_MinusLogProbMetric: 28.3272 - lr: 0.0010 - 10s/epoch - 49ms/step
Epoch 132/1000
2023-09-09 10:50:36.918 
Epoch 132/1000 
	 loss: 27.9850, MinusLogProbMetric: 27.9850, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 132: val_loss improved from 28.13242 to 28.13142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 27.9850 - MinusLogProbMetric: 27.9850 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 133/1000
2023-09-09 10:50:47.972 
Epoch 133/1000 
	 loss: 27.9614, MinusLogProbMetric: 27.9614, val_loss: 28.3531, val_MinusLogProbMetric: 28.3531

Epoch 133: val_loss did not improve from 28.13142
196/196 - 11s - loss: 27.9614 - MinusLogProbMetric: 27.9614 - val_loss: 28.3531 - val_MinusLogProbMetric: 28.3531 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 134/1000
2023-09-09 10:50:58.454 
Epoch 134/1000 
	 loss: 27.9498, MinusLogProbMetric: 27.9498, val_loss: 28.2561, val_MinusLogProbMetric: 28.2561

Epoch 134: val_loss did not improve from 28.13142
196/196 - 10s - loss: 27.9498 - MinusLogProbMetric: 27.9498 - val_loss: 28.2561 - val_MinusLogProbMetric: 28.2561 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 135/1000
2023-09-09 10:51:08.713 
Epoch 135/1000 
	 loss: 27.9682, MinusLogProbMetric: 27.9682, val_loss: 28.2072, val_MinusLogProbMetric: 28.2072

Epoch 135: val_loss did not improve from 28.13142
196/196 - 10s - loss: 27.9682 - MinusLogProbMetric: 27.9682 - val_loss: 28.2072 - val_MinusLogProbMetric: 28.2072 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 136/1000
2023-09-09 10:51:18.941 
Epoch 136/1000 
	 loss: 27.9838, MinusLogProbMetric: 27.9838, val_loss: 28.1705, val_MinusLogProbMetric: 28.1705

Epoch 136: val_loss did not improve from 28.13142
196/196 - 10s - loss: 27.9838 - MinusLogProbMetric: 27.9838 - val_loss: 28.1705 - val_MinusLogProbMetric: 28.1705 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 137/1000
2023-09-09 10:51:29.009 
Epoch 137/1000 
	 loss: 27.9834, MinusLogProbMetric: 27.9834, val_loss: 28.2499, val_MinusLogProbMetric: 28.2499

Epoch 137: val_loss did not improve from 28.13142
196/196 - 10s - loss: 27.9834 - MinusLogProbMetric: 27.9834 - val_loss: 28.2499 - val_MinusLogProbMetric: 28.2499 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 138/1000
2023-09-09 10:51:39.291 
Epoch 138/1000 
	 loss: 27.9283, MinusLogProbMetric: 27.9283, val_loss: 28.0637, val_MinusLogProbMetric: 28.0637

Epoch 138: val_loss improved from 28.13142 to 28.06372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 27.9283 - MinusLogProbMetric: 27.9283 - val_loss: 28.0637 - val_MinusLogProbMetric: 28.0637 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 139/1000
2023-09-09 10:51:49.881 
Epoch 139/1000 
	 loss: 27.9542, MinusLogProbMetric: 27.9542, val_loss: 28.0114, val_MinusLogProbMetric: 28.0114

Epoch 139: val_loss improved from 28.06372 to 28.01143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 27.9542 - MinusLogProbMetric: 27.9542 - val_loss: 28.0114 - val_MinusLogProbMetric: 28.0114 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 140/1000
2023-09-09 10:52:00.860 
Epoch 140/1000 
	 loss: 27.9155, MinusLogProbMetric: 27.9155, val_loss: 28.2722, val_MinusLogProbMetric: 28.2722

Epoch 140: val_loss did not improve from 28.01143
196/196 - 11s - loss: 27.9155 - MinusLogProbMetric: 27.9155 - val_loss: 28.2722 - val_MinusLogProbMetric: 28.2722 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 141/1000
2023-09-09 10:52:11.306 
Epoch 141/1000 
	 loss: 27.9119, MinusLogProbMetric: 27.9119, val_loss: 28.3761, val_MinusLogProbMetric: 28.3761

Epoch 141: val_loss did not improve from 28.01143
196/196 - 10s - loss: 27.9119 - MinusLogProbMetric: 27.9119 - val_loss: 28.3761 - val_MinusLogProbMetric: 28.3761 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 142/1000
2023-09-09 10:52:21.824 
Epoch 142/1000 
	 loss: 27.9831, MinusLogProbMetric: 27.9831, val_loss: 28.2545, val_MinusLogProbMetric: 28.2545

Epoch 142: val_loss did not improve from 28.01143
196/196 - 11s - loss: 27.9831 - MinusLogProbMetric: 27.9831 - val_loss: 28.2545 - val_MinusLogProbMetric: 28.2545 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 143/1000
2023-09-09 10:52:32.226 
Epoch 143/1000 
	 loss: 27.9010, MinusLogProbMetric: 27.9010, val_loss: 28.2588, val_MinusLogProbMetric: 28.2588

Epoch 143: val_loss did not improve from 28.01143
196/196 - 10s - loss: 27.9010 - MinusLogProbMetric: 27.9010 - val_loss: 28.2588 - val_MinusLogProbMetric: 28.2588 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 144/1000
2023-09-09 10:52:43.219 
Epoch 144/1000 
	 loss: 27.9123, MinusLogProbMetric: 27.9123, val_loss: 27.9753, val_MinusLogProbMetric: 27.9753

Epoch 144: val_loss improved from 28.01143 to 27.97532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 27.9123 - MinusLogProbMetric: 27.9123 - val_loss: 27.9753 - val_MinusLogProbMetric: 27.9753 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 145/1000
2023-09-09 10:52:53.792 
Epoch 145/1000 
	 loss: 27.9159, MinusLogProbMetric: 27.9159, val_loss: 28.4250, val_MinusLogProbMetric: 28.4250

Epoch 145: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.9159 - MinusLogProbMetric: 27.9159 - val_loss: 28.4250 - val_MinusLogProbMetric: 28.4250 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 146/1000
2023-09-09 10:53:04.009 
Epoch 146/1000 
	 loss: 27.8835, MinusLogProbMetric: 27.8835, val_loss: 29.3092, val_MinusLogProbMetric: 29.3092

Epoch 146: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8835 - MinusLogProbMetric: 27.8835 - val_loss: 29.3092 - val_MinusLogProbMetric: 29.3092 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 147/1000
2023-09-09 10:53:14.287 
Epoch 147/1000 
	 loss: 27.9259, MinusLogProbMetric: 27.9259, val_loss: 28.0660, val_MinusLogProbMetric: 28.0660

Epoch 147: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.9259 - MinusLogProbMetric: 27.9259 - val_loss: 28.0660 - val_MinusLogProbMetric: 28.0660 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 148/1000
2023-09-09 10:53:24.732 
Epoch 148/1000 
	 loss: 27.8952, MinusLogProbMetric: 27.8952, val_loss: 28.1011, val_MinusLogProbMetric: 28.1011

Epoch 148: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8952 - MinusLogProbMetric: 27.8952 - val_loss: 28.1011 - val_MinusLogProbMetric: 28.1011 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 149/1000
2023-09-09 10:53:34.831 
Epoch 149/1000 
	 loss: 27.9427, MinusLogProbMetric: 27.9427, val_loss: 28.2051, val_MinusLogProbMetric: 28.2051

Epoch 149: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.9427 - MinusLogProbMetric: 27.9427 - val_loss: 28.2051 - val_MinusLogProbMetric: 28.2051 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 150/1000
2023-09-09 10:53:45.136 
Epoch 150/1000 
	 loss: 27.8927, MinusLogProbMetric: 27.8927, val_loss: 29.2575, val_MinusLogProbMetric: 29.2575

Epoch 150: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8927 - MinusLogProbMetric: 27.8927 - val_loss: 29.2575 - val_MinusLogProbMetric: 29.2575 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 151/1000
2023-09-09 10:53:55.616 
Epoch 151/1000 
	 loss: 27.9757, MinusLogProbMetric: 27.9757, val_loss: 28.3516, val_MinusLogProbMetric: 28.3516

Epoch 151: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.9757 - MinusLogProbMetric: 27.9757 - val_loss: 28.3516 - val_MinusLogProbMetric: 28.3516 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 152/1000
2023-09-09 10:54:06.134 
Epoch 152/1000 
	 loss: 27.8983, MinusLogProbMetric: 27.8983, val_loss: 28.2341, val_MinusLogProbMetric: 28.2341

Epoch 152: val_loss did not improve from 27.97532
196/196 - 11s - loss: 27.8983 - MinusLogProbMetric: 27.8983 - val_loss: 28.2341 - val_MinusLogProbMetric: 28.2341 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 153/1000
2023-09-09 10:54:16.907 
Epoch 153/1000 
	 loss: 27.8603, MinusLogProbMetric: 27.8603, val_loss: 28.0827, val_MinusLogProbMetric: 28.0827

Epoch 153: val_loss did not improve from 27.97532
196/196 - 11s - loss: 27.8603 - MinusLogProbMetric: 27.8603 - val_loss: 28.0827 - val_MinusLogProbMetric: 28.0827 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 154/1000
2023-09-09 10:54:27.375 
Epoch 154/1000 
	 loss: 27.9324, MinusLogProbMetric: 27.9324, val_loss: 28.0505, val_MinusLogProbMetric: 28.0505

Epoch 154: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.9324 - MinusLogProbMetric: 27.9324 - val_loss: 28.0505 - val_MinusLogProbMetric: 28.0505 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 155/1000
2023-09-09 10:54:37.778 
Epoch 155/1000 
	 loss: 27.8802, MinusLogProbMetric: 27.8802, val_loss: 27.9989, val_MinusLogProbMetric: 27.9989

Epoch 155: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8802 - MinusLogProbMetric: 27.8802 - val_loss: 27.9989 - val_MinusLogProbMetric: 27.9989 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 156/1000
2023-09-09 10:54:48.171 
Epoch 156/1000 
	 loss: 27.8716, MinusLogProbMetric: 27.8716, val_loss: 28.1373, val_MinusLogProbMetric: 28.1373

Epoch 156: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8716 - MinusLogProbMetric: 27.8716 - val_loss: 28.1373 - val_MinusLogProbMetric: 28.1373 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 157/1000
2023-09-09 10:54:58.552 
Epoch 157/1000 
	 loss: 27.8498, MinusLogProbMetric: 27.8498, val_loss: 28.0286, val_MinusLogProbMetric: 28.0286

Epoch 157: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8498 - MinusLogProbMetric: 27.8498 - val_loss: 28.0286 - val_MinusLogProbMetric: 28.0286 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 158/1000
2023-09-09 10:55:08.865 
Epoch 158/1000 
	 loss: 27.8770, MinusLogProbMetric: 27.8770, val_loss: 28.1883, val_MinusLogProbMetric: 28.1883

Epoch 158: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8770 - MinusLogProbMetric: 27.8770 - val_loss: 28.1883 - val_MinusLogProbMetric: 28.1883 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 159/1000
2023-09-09 10:55:19.184 
Epoch 159/1000 
	 loss: 27.8663, MinusLogProbMetric: 27.8663, val_loss: 28.3165, val_MinusLogProbMetric: 28.3165

Epoch 159: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8663 - MinusLogProbMetric: 27.8663 - val_loss: 28.3165 - val_MinusLogProbMetric: 28.3165 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 160/1000
2023-09-09 10:55:29.762 
Epoch 160/1000 
	 loss: 27.9026, MinusLogProbMetric: 27.9026, val_loss: 28.2611, val_MinusLogProbMetric: 28.2611

Epoch 160: val_loss did not improve from 27.97532
196/196 - 11s - loss: 27.9026 - MinusLogProbMetric: 27.9026 - val_loss: 28.2611 - val_MinusLogProbMetric: 28.2611 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 161/1000
2023-09-09 10:55:40.301 
Epoch 161/1000 
	 loss: 27.8682, MinusLogProbMetric: 27.8682, val_loss: 28.0375, val_MinusLogProbMetric: 28.0375

Epoch 161: val_loss did not improve from 27.97532
196/196 - 11s - loss: 27.8682 - MinusLogProbMetric: 27.8682 - val_loss: 28.0375 - val_MinusLogProbMetric: 28.0375 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 162/1000
2023-09-09 10:55:50.761 
Epoch 162/1000 
	 loss: 27.8480, MinusLogProbMetric: 27.8480, val_loss: 28.0928, val_MinusLogProbMetric: 28.0928

Epoch 162: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8480 - MinusLogProbMetric: 27.8480 - val_loss: 28.0928 - val_MinusLogProbMetric: 28.0928 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 163/1000
2023-09-09 10:56:01.098 
Epoch 163/1000 
	 loss: 27.8321, MinusLogProbMetric: 27.8321, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 163: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8321 - MinusLogProbMetric: 27.8321 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 164/1000
2023-09-09 10:56:11.295 
Epoch 164/1000 
	 loss: 27.9090, MinusLogProbMetric: 27.9090, val_loss: 28.3212, val_MinusLogProbMetric: 28.3212

Epoch 164: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.9090 - MinusLogProbMetric: 27.9090 - val_loss: 28.3212 - val_MinusLogProbMetric: 28.3212 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 165/1000
2023-09-09 10:56:21.612 
Epoch 165/1000 
	 loss: 27.8107, MinusLogProbMetric: 27.8107, val_loss: 28.2578, val_MinusLogProbMetric: 28.2578

Epoch 165: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8107 - MinusLogProbMetric: 27.8107 - val_loss: 28.2578 - val_MinusLogProbMetric: 28.2578 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 166/1000
2023-09-09 10:56:31.850 
Epoch 166/1000 
	 loss: 27.8503, MinusLogProbMetric: 27.8503, val_loss: 28.1439, val_MinusLogProbMetric: 28.1439

Epoch 166: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8503 - MinusLogProbMetric: 27.8503 - val_loss: 28.1439 - val_MinusLogProbMetric: 28.1439 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 167/1000
2023-09-09 10:56:42.146 
Epoch 167/1000 
	 loss: 27.8466, MinusLogProbMetric: 27.8466, val_loss: 29.1065, val_MinusLogProbMetric: 29.1065

Epoch 167: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8466 - MinusLogProbMetric: 27.8466 - val_loss: 29.1065 - val_MinusLogProbMetric: 29.1065 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 168/1000
2023-09-09 10:56:52.519 
Epoch 168/1000 
	 loss: 27.8087, MinusLogProbMetric: 27.8087, val_loss: 28.5088, val_MinusLogProbMetric: 28.5088

Epoch 168: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8087 - MinusLogProbMetric: 27.8087 - val_loss: 28.5088 - val_MinusLogProbMetric: 28.5088 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 169/1000
2023-09-09 10:57:02.961 
Epoch 169/1000 
	 loss: 27.8132, MinusLogProbMetric: 27.8132, val_loss: 28.2802, val_MinusLogProbMetric: 28.2802

Epoch 169: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8132 - MinusLogProbMetric: 27.8132 - val_loss: 28.2802 - val_MinusLogProbMetric: 28.2802 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 170/1000
2023-09-09 10:57:13.474 
Epoch 170/1000 
	 loss: 27.8260, MinusLogProbMetric: 27.8260, val_loss: 28.2215, val_MinusLogProbMetric: 28.2215

Epoch 170: val_loss did not improve from 27.97532
196/196 - 11s - loss: 27.8260 - MinusLogProbMetric: 27.8260 - val_loss: 28.2215 - val_MinusLogProbMetric: 28.2215 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 171/1000
2023-09-09 10:57:23.928 
Epoch 171/1000 
	 loss: 27.8032, MinusLogProbMetric: 27.8032, val_loss: 28.2698, val_MinusLogProbMetric: 28.2698

Epoch 171: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8032 - MinusLogProbMetric: 27.8032 - val_loss: 28.2698 - val_MinusLogProbMetric: 28.2698 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 172/1000
2023-09-09 10:57:34.286 
Epoch 172/1000 
	 loss: 27.8355, MinusLogProbMetric: 27.8355, val_loss: 28.7015, val_MinusLogProbMetric: 28.7015

Epoch 172: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8355 - MinusLogProbMetric: 27.8355 - val_loss: 28.7015 - val_MinusLogProbMetric: 28.7015 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 173/1000
2023-09-09 10:57:44.587 
Epoch 173/1000 
	 loss: 27.8678, MinusLogProbMetric: 27.8678, val_loss: 27.9811, val_MinusLogProbMetric: 27.9811

Epoch 173: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8678 - MinusLogProbMetric: 27.8678 - val_loss: 27.9811 - val_MinusLogProbMetric: 27.9811 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 174/1000
2023-09-09 10:57:54.904 
Epoch 174/1000 
	 loss: 27.8541, MinusLogProbMetric: 27.8541, val_loss: 28.5530, val_MinusLogProbMetric: 28.5530

Epoch 174: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8541 - MinusLogProbMetric: 27.8541 - val_loss: 28.5530 - val_MinusLogProbMetric: 28.5530 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 175/1000
2023-09-09 10:58:05.260 
Epoch 175/1000 
	 loss: 27.7910, MinusLogProbMetric: 27.7910, val_loss: 28.1309, val_MinusLogProbMetric: 28.1309

Epoch 175: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7910 - MinusLogProbMetric: 27.7910 - val_loss: 28.1309 - val_MinusLogProbMetric: 28.1309 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 176/1000
2023-09-09 10:58:15.823 
Epoch 176/1000 
	 loss: 27.8091, MinusLogProbMetric: 27.8091, val_loss: 28.0286, val_MinusLogProbMetric: 28.0286

Epoch 176: val_loss did not improve from 27.97532
196/196 - 11s - loss: 27.8091 - MinusLogProbMetric: 27.8091 - val_loss: 28.0286 - val_MinusLogProbMetric: 28.0286 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 177/1000
2023-09-09 10:58:26.192 
Epoch 177/1000 
	 loss: 27.7918, MinusLogProbMetric: 27.7918, val_loss: 28.2893, val_MinusLogProbMetric: 28.2893

Epoch 177: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7918 - MinusLogProbMetric: 27.7918 - val_loss: 28.2893 - val_MinusLogProbMetric: 28.2893 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 178/1000
2023-09-09 10:58:36.255 
Epoch 178/1000 
	 loss: 27.8048, MinusLogProbMetric: 27.8048, val_loss: 28.1691, val_MinusLogProbMetric: 28.1691

Epoch 178: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8048 - MinusLogProbMetric: 27.8048 - val_loss: 28.1691 - val_MinusLogProbMetric: 28.1691 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 179/1000
2023-09-09 10:58:46.581 
Epoch 179/1000 
	 loss: 27.7980, MinusLogProbMetric: 27.7980, val_loss: 27.9993, val_MinusLogProbMetric: 27.9993

Epoch 179: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7980 - MinusLogProbMetric: 27.7980 - val_loss: 27.9993 - val_MinusLogProbMetric: 27.9993 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 180/1000
2023-09-09 10:58:56.801 
Epoch 180/1000 
	 loss: 27.8428, MinusLogProbMetric: 27.8428, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 180: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8428 - MinusLogProbMetric: 27.8428 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 181/1000
2023-09-09 10:59:06.862 
Epoch 181/1000 
	 loss: 27.8009, MinusLogProbMetric: 27.8009, val_loss: 28.0154, val_MinusLogProbMetric: 28.0154

Epoch 181: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8009 - MinusLogProbMetric: 27.8009 - val_loss: 28.0154 - val_MinusLogProbMetric: 28.0154 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 182/1000
2023-09-09 10:59:17.125 
Epoch 182/1000 
	 loss: 27.7809, MinusLogProbMetric: 27.7809, val_loss: 27.9985, val_MinusLogProbMetric: 27.9985

Epoch 182: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7809 - MinusLogProbMetric: 27.7809 - val_loss: 27.9985 - val_MinusLogProbMetric: 27.9985 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 183/1000
2023-09-09 10:59:27.555 
Epoch 183/1000 
	 loss: 27.7531, MinusLogProbMetric: 27.7531, val_loss: 28.5628, val_MinusLogProbMetric: 28.5628

Epoch 183: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7531 - MinusLogProbMetric: 27.7531 - val_loss: 28.5628 - val_MinusLogProbMetric: 28.5628 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 184/1000
2023-09-09 10:59:37.952 
Epoch 184/1000 
	 loss: 27.8140, MinusLogProbMetric: 27.8140, val_loss: 28.1232, val_MinusLogProbMetric: 28.1232

Epoch 184: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8140 - MinusLogProbMetric: 27.8140 - val_loss: 28.1232 - val_MinusLogProbMetric: 28.1232 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 185/1000
2023-09-09 10:59:47.969 
Epoch 185/1000 
	 loss: 27.7649, MinusLogProbMetric: 27.7649, val_loss: 28.1160, val_MinusLogProbMetric: 28.1160

Epoch 185: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7649 - MinusLogProbMetric: 27.7649 - val_loss: 28.1160 - val_MinusLogProbMetric: 28.1160 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 186/1000
2023-09-09 10:59:58.193 
Epoch 186/1000 
	 loss: 27.8231, MinusLogProbMetric: 27.8231, val_loss: 28.2852, val_MinusLogProbMetric: 28.2852

Epoch 186: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.8231 - MinusLogProbMetric: 27.8231 - val_loss: 28.2852 - val_MinusLogProbMetric: 28.2852 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 187/1000
2023-09-09 11:00:08.478 
Epoch 187/1000 
	 loss: 27.7704, MinusLogProbMetric: 27.7704, val_loss: 28.1204, val_MinusLogProbMetric: 28.1204

Epoch 187: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7704 - MinusLogProbMetric: 27.7704 - val_loss: 28.1204 - val_MinusLogProbMetric: 28.1204 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 188/1000
2023-09-09 11:00:18.762 
Epoch 188/1000 
	 loss: 27.7686, MinusLogProbMetric: 27.7686, val_loss: 28.1802, val_MinusLogProbMetric: 28.1802

Epoch 188: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7686 - MinusLogProbMetric: 27.7686 - val_loss: 28.1802 - val_MinusLogProbMetric: 28.1802 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 189/1000
2023-09-09 11:00:28.644 
Epoch 189/1000 
	 loss: 27.7671, MinusLogProbMetric: 27.7671, val_loss: 28.2349, val_MinusLogProbMetric: 28.2349

Epoch 189: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7671 - MinusLogProbMetric: 27.7671 - val_loss: 28.2349 - val_MinusLogProbMetric: 28.2349 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 190/1000
2023-09-09 11:00:39.120 
Epoch 190/1000 
	 loss: 27.7371, MinusLogProbMetric: 27.7371, val_loss: 28.3080, val_MinusLogProbMetric: 28.3080

Epoch 190: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7371 - MinusLogProbMetric: 27.7371 - val_loss: 28.3080 - val_MinusLogProbMetric: 28.3080 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 191/1000
2023-09-09 11:00:49.571 
Epoch 191/1000 
	 loss: 27.7793, MinusLogProbMetric: 27.7793, val_loss: 28.0698, val_MinusLogProbMetric: 28.0698

Epoch 191: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7793 - MinusLogProbMetric: 27.7793 - val_loss: 28.0698 - val_MinusLogProbMetric: 28.0698 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 192/1000
2023-09-09 11:00:59.732 
Epoch 192/1000 
	 loss: 27.7580, MinusLogProbMetric: 27.7580, val_loss: 28.2333, val_MinusLogProbMetric: 28.2333

Epoch 192: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7580 - MinusLogProbMetric: 27.7580 - val_loss: 28.2333 - val_MinusLogProbMetric: 28.2333 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 193/1000
2023-09-09 11:01:10.006 
Epoch 193/1000 
	 loss: 27.7856, MinusLogProbMetric: 27.7856, val_loss: 28.2229, val_MinusLogProbMetric: 28.2229

Epoch 193: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7856 - MinusLogProbMetric: 27.7856 - val_loss: 28.2229 - val_MinusLogProbMetric: 28.2229 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 194/1000
2023-09-09 11:01:20.149 
Epoch 194/1000 
	 loss: 27.7516, MinusLogProbMetric: 27.7516, val_loss: 28.0538, val_MinusLogProbMetric: 28.0538

Epoch 194: val_loss did not improve from 27.97532
196/196 - 10s - loss: 27.7516 - MinusLogProbMetric: 27.7516 - val_loss: 28.0538 - val_MinusLogProbMetric: 28.0538 - lr: 0.0010 - 10s/epoch - 52ms/step
Epoch 195/1000
2023-09-09 11:01:30.414 
Epoch 195/1000 
	 loss: 27.4937, MinusLogProbMetric: 27.4937, val_loss: 27.7927, val_MinusLogProbMetric: 27.7927

Epoch 195: val_loss improved from 27.97532 to 27.79267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 27.4937 - MinusLogProbMetric: 27.4937 - val_loss: 27.7927 - val_MinusLogProbMetric: 27.7927 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 196/1000
2023-09-09 11:01:41.191 
Epoch 196/1000 
	 loss: 27.4894, MinusLogProbMetric: 27.4894, val_loss: 27.8698, val_MinusLogProbMetric: 27.8698

Epoch 196: val_loss did not improve from 27.79267
196/196 - 10s - loss: 27.4894 - MinusLogProbMetric: 27.4894 - val_loss: 27.8698 - val_MinusLogProbMetric: 27.8698 - lr: 5.0000e-04 - 10s/epoch - 54ms/step
Epoch 197/1000
2023-09-09 11:01:51.382 
Epoch 197/1000 
	 loss: 27.4981, MinusLogProbMetric: 27.4981, val_loss: 27.8285, val_MinusLogProbMetric: 27.8285

Epoch 197: val_loss did not improve from 27.79267
196/196 - 10s - loss: 27.4981 - MinusLogProbMetric: 27.4981 - val_loss: 27.8285 - val_MinusLogProbMetric: 27.8285 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 198/1000
2023-09-09 11:02:01.898 
Epoch 198/1000 
	 loss: 27.4972, MinusLogProbMetric: 27.4972, val_loss: 27.8427, val_MinusLogProbMetric: 27.8427

Epoch 198: val_loss did not improve from 27.79267
196/196 - 11s - loss: 27.4972 - MinusLogProbMetric: 27.4972 - val_loss: 27.8427 - val_MinusLogProbMetric: 27.8427 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 199/1000
2023-09-09 11:02:11.933 
Epoch 199/1000 
	 loss: 27.4851, MinusLogProbMetric: 27.4851, val_loss: 27.9554, val_MinusLogProbMetric: 27.9554

Epoch 199: val_loss did not improve from 27.79267
196/196 - 10s - loss: 27.4851 - MinusLogProbMetric: 27.4851 - val_loss: 27.9554 - val_MinusLogProbMetric: 27.9554 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 200/1000
2023-09-09 11:02:22.188 
Epoch 200/1000 
	 loss: 27.5068, MinusLogProbMetric: 27.5068, val_loss: 27.7779, val_MinusLogProbMetric: 27.7779

Epoch 200: val_loss improved from 27.79267 to 27.77791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 27.5068 - MinusLogProbMetric: 27.5068 - val_loss: 27.7779 - val_MinusLogProbMetric: 27.7779 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 201/1000
2023-09-09 11:02:32.890 
Epoch 201/1000 
	 loss: 27.4840, MinusLogProbMetric: 27.4840, val_loss: 27.8072, val_MinusLogProbMetric: 27.8072

Epoch 201: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4840 - MinusLogProbMetric: 27.4840 - val_loss: 27.8072 - val_MinusLogProbMetric: 27.8072 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 202/1000
2023-09-09 11:02:43.249 
Epoch 202/1000 
	 loss: 27.4710, MinusLogProbMetric: 27.4710, val_loss: 27.8736, val_MinusLogProbMetric: 27.8736

Epoch 202: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4710 - MinusLogProbMetric: 27.4710 - val_loss: 27.8736 - val_MinusLogProbMetric: 27.8736 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 203/1000
2023-09-09 11:02:53.478 
Epoch 203/1000 
	 loss: 27.4836, MinusLogProbMetric: 27.4836, val_loss: 27.8481, val_MinusLogProbMetric: 27.8481

Epoch 203: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4836 - MinusLogProbMetric: 27.4836 - val_loss: 27.8481 - val_MinusLogProbMetric: 27.8481 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 204/1000
2023-09-09 11:03:03.896 
Epoch 204/1000 
	 loss: 27.4890, MinusLogProbMetric: 27.4890, val_loss: 27.8220, val_MinusLogProbMetric: 27.8220

Epoch 204: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4890 - MinusLogProbMetric: 27.4890 - val_loss: 27.8220 - val_MinusLogProbMetric: 27.8220 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 205/1000
2023-09-09 11:03:14.352 
Epoch 205/1000 
	 loss: 27.4812, MinusLogProbMetric: 27.4812, val_loss: 27.7975, val_MinusLogProbMetric: 27.7975

Epoch 205: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4812 - MinusLogProbMetric: 27.4812 - val_loss: 27.7975 - val_MinusLogProbMetric: 27.7975 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 206/1000
2023-09-09 11:03:24.625 
Epoch 206/1000 
	 loss: 27.4813, MinusLogProbMetric: 27.4813, val_loss: 27.9136, val_MinusLogProbMetric: 27.9136

Epoch 206: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4813 - MinusLogProbMetric: 27.4813 - val_loss: 27.9136 - val_MinusLogProbMetric: 27.9136 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 207/1000
2023-09-09 11:03:34.556 
Epoch 207/1000 
	 loss: 27.4956, MinusLogProbMetric: 27.4956, val_loss: 27.8251, val_MinusLogProbMetric: 27.8251

Epoch 207: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4956 - MinusLogProbMetric: 27.4956 - val_loss: 27.8251 - val_MinusLogProbMetric: 27.8251 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 208/1000
2023-09-09 11:03:44.597 
Epoch 208/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 27.8829, val_MinusLogProbMetric: 27.8829

Epoch 208: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 27.8829 - val_MinusLogProbMetric: 27.8829 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 209/1000
2023-09-09 11:03:54.837 
Epoch 209/1000 
	 loss: 27.4698, MinusLogProbMetric: 27.4698, val_loss: 28.0180, val_MinusLogProbMetric: 28.0180

Epoch 209: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4698 - MinusLogProbMetric: 27.4698 - val_loss: 28.0180 - val_MinusLogProbMetric: 28.0180 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 210/1000
2023-09-09 11:04:05.103 
Epoch 210/1000 
	 loss: 27.4680, MinusLogProbMetric: 27.4680, val_loss: 27.8198, val_MinusLogProbMetric: 27.8198

Epoch 210: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4680 - MinusLogProbMetric: 27.4680 - val_loss: 27.8198 - val_MinusLogProbMetric: 27.8198 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 211/1000
2023-09-09 11:04:15.447 
Epoch 211/1000 
	 loss: 27.4773, MinusLogProbMetric: 27.4773, val_loss: 27.8655, val_MinusLogProbMetric: 27.8655

Epoch 211: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4773 - MinusLogProbMetric: 27.4773 - val_loss: 27.8655 - val_MinusLogProbMetric: 27.8655 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 212/1000
2023-09-09 11:04:25.682 
Epoch 212/1000 
	 loss: 27.4701, MinusLogProbMetric: 27.4701, val_loss: 27.8797, val_MinusLogProbMetric: 27.8797

Epoch 212: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4701 - MinusLogProbMetric: 27.4701 - val_loss: 27.8797 - val_MinusLogProbMetric: 27.8797 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 213/1000
2023-09-09 11:04:35.584 
Epoch 213/1000 
	 loss: 27.4679, MinusLogProbMetric: 27.4679, val_loss: 27.8053, val_MinusLogProbMetric: 27.8053

Epoch 213: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4679 - MinusLogProbMetric: 27.4679 - val_loss: 27.8053 - val_MinusLogProbMetric: 27.8053 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 214/1000
2023-09-09 11:04:45.786 
Epoch 214/1000 
	 loss: 27.4604, MinusLogProbMetric: 27.4604, val_loss: 27.7809, val_MinusLogProbMetric: 27.7809

Epoch 214: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4604 - MinusLogProbMetric: 27.4604 - val_loss: 27.7809 - val_MinusLogProbMetric: 27.7809 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 215/1000
2023-09-09 11:04:55.929 
Epoch 215/1000 
	 loss: 27.4685, MinusLogProbMetric: 27.4685, val_loss: 27.8647, val_MinusLogProbMetric: 27.8647

Epoch 215: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4685 - MinusLogProbMetric: 27.4685 - val_loss: 27.8647 - val_MinusLogProbMetric: 27.8647 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 216/1000
2023-09-09 11:05:06.511 
Epoch 216/1000 
	 loss: 27.4669, MinusLogProbMetric: 27.4669, val_loss: 27.8066, val_MinusLogProbMetric: 27.8066

Epoch 216: val_loss did not improve from 27.77791
196/196 - 11s - loss: 27.4669 - MinusLogProbMetric: 27.4669 - val_loss: 27.8066 - val_MinusLogProbMetric: 27.8066 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 217/1000
2023-09-09 11:05:16.934 
Epoch 217/1000 
	 loss: 27.4629, MinusLogProbMetric: 27.4629, val_loss: 27.7859, val_MinusLogProbMetric: 27.7859

Epoch 217: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4629 - MinusLogProbMetric: 27.4629 - val_loss: 27.7859 - val_MinusLogProbMetric: 27.7859 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 218/1000
2023-09-09 11:05:27.099 
Epoch 218/1000 
	 loss: 27.4652, MinusLogProbMetric: 27.4652, val_loss: 27.8561, val_MinusLogProbMetric: 27.8561

Epoch 218: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4652 - MinusLogProbMetric: 27.4652 - val_loss: 27.8561 - val_MinusLogProbMetric: 27.8561 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 219/1000
2023-09-09 11:05:37.135 
Epoch 219/1000 
	 loss: 27.4633, MinusLogProbMetric: 27.4633, val_loss: 27.8486, val_MinusLogProbMetric: 27.8486

Epoch 219: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4633 - MinusLogProbMetric: 27.4633 - val_loss: 27.8486 - val_MinusLogProbMetric: 27.8486 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 220/1000
2023-09-09 11:05:47.469 
Epoch 220/1000 
	 loss: 27.4978, MinusLogProbMetric: 27.4978, val_loss: 27.7953, val_MinusLogProbMetric: 27.7953

Epoch 220: val_loss did not improve from 27.77791
196/196 - 10s - loss: 27.4978 - MinusLogProbMetric: 27.4978 - val_loss: 27.7953 - val_MinusLogProbMetric: 27.7953 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 221/1000
2023-09-09 11:05:58.075 
Epoch 221/1000 
	 loss: 27.4687, MinusLogProbMetric: 27.4687, val_loss: 27.8060, val_MinusLogProbMetric: 27.8060

Epoch 221: val_loss did not improve from 27.77791
196/196 - 11s - loss: 27.4687 - MinusLogProbMetric: 27.4687 - val_loss: 27.8060 - val_MinusLogProbMetric: 27.8060 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 222/1000
2023-09-09 11:06:08.170 
Epoch 222/1000 
	 loss: 27.4579, MinusLogProbMetric: 27.4579, val_loss: 27.7735, val_MinusLogProbMetric: 27.7735

Epoch 222: val_loss improved from 27.77791 to 27.77350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 27.4579 - MinusLogProbMetric: 27.4579 - val_loss: 27.7735 - val_MinusLogProbMetric: 27.7735 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 223/1000
2023-09-09 11:06:20.447 
Epoch 223/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 27.8939, val_MinusLogProbMetric: 27.8939

Epoch 223: val_loss did not improve from 27.77350
196/196 - 12s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 27.8939 - val_MinusLogProbMetric: 27.8939 - lr: 5.0000e-04 - 12s/epoch - 61ms/step
Epoch 224/1000
2023-09-09 11:06:30.499 
Epoch 224/1000 
	 loss: 27.4730, MinusLogProbMetric: 27.4730, val_loss: 27.8378, val_MinusLogProbMetric: 27.8378

Epoch 224: val_loss did not improve from 27.77350
196/196 - 10s - loss: 27.4730 - MinusLogProbMetric: 27.4730 - val_loss: 27.8378 - val_MinusLogProbMetric: 27.8378 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 225/1000
2023-09-09 11:06:40.565 
Epoch 225/1000 
	 loss: 27.4659, MinusLogProbMetric: 27.4659, val_loss: 27.8692, val_MinusLogProbMetric: 27.8692

Epoch 225: val_loss did not improve from 27.77350
196/196 - 10s - loss: 27.4659 - MinusLogProbMetric: 27.4659 - val_loss: 27.8692 - val_MinusLogProbMetric: 27.8692 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 226/1000
2023-09-09 11:06:50.655 
Epoch 226/1000 
	 loss: 27.4734, MinusLogProbMetric: 27.4734, val_loss: 27.7708, val_MinusLogProbMetric: 27.7708

Epoch 226: val_loss improved from 27.77350 to 27.77079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 27.4734 - MinusLogProbMetric: 27.4734 - val_loss: 27.7708 - val_MinusLogProbMetric: 27.7708 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 227/1000
2023-09-09 11:07:01.214 
Epoch 227/1000 
	 loss: 27.4668, MinusLogProbMetric: 27.4668, val_loss: 27.8951, val_MinusLogProbMetric: 27.8951

Epoch 227: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4668 - MinusLogProbMetric: 27.4668 - val_loss: 27.8951 - val_MinusLogProbMetric: 27.8951 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 228/1000
2023-09-09 11:07:11.782 
Epoch 228/1000 
	 loss: 27.4816, MinusLogProbMetric: 27.4816, val_loss: 27.8127, val_MinusLogProbMetric: 27.8127

Epoch 228: val_loss did not improve from 27.77079
196/196 - 11s - loss: 27.4816 - MinusLogProbMetric: 27.4816 - val_loss: 27.8127 - val_MinusLogProbMetric: 27.8127 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 229/1000
2023-09-09 11:07:22.055 
Epoch 229/1000 
	 loss: 27.4701, MinusLogProbMetric: 27.4701, val_loss: 27.8543, val_MinusLogProbMetric: 27.8543

Epoch 229: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4701 - MinusLogProbMetric: 27.4701 - val_loss: 27.8543 - val_MinusLogProbMetric: 27.8543 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 230/1000
2023-09-09 11:07:32.112 
Epoch 230/1000 
	 loss: 27.4583, MinusLogProbMetric: 27.4583, val_loss: 27.8328, val_MinusLogProbMetric: 27.8328

Epoch 230: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4583 - MinusLogProbMetric: 27.4583 - val_loss: 27.8328 - val_MinusLogProbMetric: 27.8328 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 231/1000
2023-09-09 11:07:42.256 
Epoch 231/1000 
	 loss: 27.4686, MinusLogProbMetric: 27.4686, val_loss: 27.7730, val_MinusLogProbMetric: 27.7730

Epoch 231: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4686 - MinusLogProbMetric: 27.4686 - val_loss: 27.7730 - val_MinusLogProbMetric: 27.7730 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 232/1000
2023-09-09 11:07:52.150 
Epoch 232/1000 
	 loss: 27.4543, MinusLogProbMetric: 27.4543, val_loss: 27.7770, val_MinusLogProbMetric: 27.7770

Epoch 232: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4543 - MinusLogProbMetric: 27.4543 - val_loss: 27.7770 - val_MinusLogProbMetric: 27.7770 - lr: 5.0000e-04 - 10s/epoch - 50ms/step
Epoch 233/1000
2023-09-09 11:08:02.218 
Epoch 233/1000 
	 loss: 27.4640, MinusLogProbMetric: 27.4640, val_loss: 27.9087, val_MinusLogProbMetric: 27.9087

Epoch 233: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4640 - MinusLogProbMetric: 27.4640 - val_loss: 27.9087 - val_MinusLogProbMetric: 27.9087 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 234/1000
2023-09-09 11:08:12.428 
Epoch 234/1000 
	 loss: 27.4561, MinusLogProbMetric: 27.4561, val_loss: 27.8933, val_MinusLogProbMetric: 27.8933

Epoch 234: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4561 - MinusLogProbMetric: 27.4561 - val_loss: 27.8933 - val_MinusLogProbMetric: 27.8933 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 235/1000
2023-09-09 11:08:22.578 
Epoch 235/1000 
	 loss: 27.4737, MinusLogProbMetric: 27.4737, val_loss: 27.8340, val_MinusLogProbMetric: 27.8340

Epoch 235: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4737 - MinusLogProbMetric: 27.4737 - val_loss: 27.8340 - val_MinusLogProbMetric: 27.8340 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 236/1000
2023-09-09 11:08:33.021 
Epoch 236/1000 
	 loss: 27.4531, MinusLogProbMetric: 27.4531, val_loss: 27.8264, val_MinusLogProbMetric: 27.8264

Epoch 236: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4531 - MinusLogProbMetric: 27.4531 - val_loss: 27.8264 - val_MinusLogProbMetric: 27.8264 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 237/1000
2023-09-09 11:08:43.147 
Epoch 237/1000 
	 loss: 27.4483, MinusLogProbMetric: 27.4483, val_loss: 27.8513, val_MinusLogProbMetric: 27.8513

Epoch 237: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4483 - MinusLogProbMetric: 27.4483 - val_loss: 27.8513 - val_MinusLogProbMetric: 27.8513 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 238/1000
2023-09-09 11:08:53.066 
Epoch 238/1000 
	 loss: 27.4482, MinusLogProbMetric: 27.4482, val_loss: 27.7907, val_MinusLogProbMetric: 27.7907

Epoch 238: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4482 - MinusLogProbMetric: 27.4482 - val_loss: 27.7907 - val_MinusLogProbMetric: 27.7907 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 239/1000
2023-09-09 11:09:03.321 
Epoch 239/1000 
	 loss: 27.4574, MinusLogProbMetric: 27.4574, val_loss: 27.8447, val_MinusLogProbMetric: 27.8447

Epoch 239: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4574 - MinusLogProbMetric: 27.4574 - val_loss: 27.8447 - val_MinusLogProbMetric: 27.8447 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 240/1000
2023-09-09 11:09:13.549 
Epoch 240/1000 
	 loss: 27.4543, MinusLogProbMetric: 27.4543, val_loss: 27.8495, val_MinusLogProbMetric: 27.8495

Epoch 240: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4543 - MinusLogProbMetric: 27.4543 - val_loss: 27.8495 - val_MinusLogProbMetric: 27.8495 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 241/1000
2023-09-09 11:09:23.873 
Epoch 241/1000 
	 loss: 27.4697, MinusLogProbMetric: 27.4697, val_loss: 27.8619, val_MinusLogProbMetric: 27.8619

Epoch 241: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4697 - MinusLogProbMetric: 27.4697 - val_loss: 27.8619 - val_MinusLogProbMetric: 27.8619 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 242/1000
2023-09-09 11:09:34.185 
Epoch 242/1000 
	 loss: 27.4549, MinusLogProbMetric: 27.4549, val_loss: 27.8282, val_MinusLogProbMetric: 27.8282

Epoch 242: val_loss did not improve from 27.77079
196/196 - 10s - loss: 27.4549 - MinusLogProbMetric: 27.4549 - val_loss: 27.8282 - val_MinusLogProbMetric: 27.8282 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 243/1000
2023-09-09 11:09:44.489 
Epoch 243/1000 
	 loss: 27.4299, MinusLogProbMetric: 27.4299, val_loss: 27.7661, val_MinusLogProbMetric: 27.7661

Epoch 243: val_loss improved from 27.77079 to 27.76611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 27.4299 - MinusLogProbMetric: 27.4299 - val_loss: 27.7661 - val_MinusLogProbMetric: 27.7661 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 244/1000
2023-09-09 11:09:54.898 
Epoch 244/1000 
	 loss: 27.4418, MinusLogProbMetric: 27.4418, val_loss: 27.8277, val_MinusLogProbMetric: 27.8277

Epoch 244: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4418 - MinusLogProbMetric: 27.4418 - val_loss: 27.8277 - val_MinusLogProbMetric: 27.8277 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 245/1000
2023-09-09 11:10:05.102 
Epoch 245/1000 
	 loss: 27.4550, MinusLogProbMetric: 27.4550, val_loss: 27.8213, val_MinusLogProbMetric: 27.8213

Epoch 245: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4550 - MinusLogProbMetric: 27.4550 - val_loss: 27.8213 - val_MinusLogProbMetric: 27.8213 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 246/1000
2023-09-09 11:10:15.180 
Epoch 246/1000 
	 loss: 27.4757, MinusLogProbMetric: 27.4757, val_loss: 27.9563, val_MinusLogProbMetric: 27.9563

Epoch 246: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4757 - MinusLogProbMetric: 27.4757 - val_loss: 27.9563 - val_MinusLogProbMetric: 27.9563 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 247/1000
2023-09-09 11:10:25.244 
Epoch 247/1000 
	 loss: 27.4416, MinusLogProbMetric: 27.4416, val_loss: 27.8293, val_MinusLogProbMetric: 27.8293

Epoch 247: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4416 - MinusLogProbMetric: 27.4416 - val_loss: 27.8293 - val_MinusLogProbMetric: 27.8293 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 248/1000
2023-09-09 11:10:35.251 
Epoch 248/1000 
	 loss: 27.4599, MinusLogProbMetric: 27.4599, val_loss: 27.8278, val_MinusLogProbMetric: 27.8278

Epoch 248: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4599 - MinusLogProbMetric: 27.4599 - val_loss: 27.8278 - val_MinusLogProbMetric: 27.8278 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 249/1000
2023-09-09 11:10:45.398 
Epoch 249/1000 
	 loss: 27.4462, MinusLogProbMetric: 27.4462, val_loss: 27.8053, val_MinusLogProbMetric: 27.8053

Epoch 249: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4462 - MinusLogProbMetric: 27.4462 - val_loss: 27.8053 - val_MinusLogProbMetric: 27.8053 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 250/1000
2023-09-09 11:10:55.624 
Epoch 250/1000 
	 loss: 27.4483, MinusLogProbMetric: 27.4483, val_loss: 27.8273, val_MinusLogProbMetric: 27.8273

Epoch 250: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4483 - MinusLogProbMetric: 27.4483 - val_loss: 27.8273 - val_MinusLogProbMetric: 27.8273 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 251/1000
2023-09-09 11:11:05.864 
Epoch 251/1000 
	 loss: 27.4605, MinusLogProbMetric: 27.4605, val_loss: 27.8064, val_MinusLogProbMetric: 27.8064

Epoch 251: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4605 - MinusLogProbMetric: 27.4605 - val_loss: 27.8064 - val_MinusLogProbMetric: 27.8064 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 252/1000
2023-09-09 11:11:16.190 
Epoch 252/1000 
	 loss: 27.4411, MinusLogProbMetric: 27.4411, val_loss: 27.8244, val_MinusLogProbMetric: 27.8244

Epoch 252: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4411 - MinusLogProbMetric: 27.4411 - val_loss: 27.8244 - val_MinusLogProbMetric: 27.8244 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 253/1000
2023-09-09 11:11:26.579 
Epoch 253/1000 
	 loss: 27.4377, MinusLogProbMetric: 27.4377, val_loss: 27.7985, val_MinusLogProbMetric: 27.7985

Epoch 253: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4377 - MinusLogProbMetric: 27.4377 - val_loss: 27.7985 - val_MinusLogProbMetric: 27.7985 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 254/1000
2023-09-09 11:11:36.586 
Epoch 254/1000 
	 loss: 27.4402, MinusLogProbMetric: 27.4402, val_loss: 27.8178, val_MinusLogProbMetric: 27.8178

Epoch 254: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4402 - MinusLogProbMetric: 27.4402 - val_loss: 27.8178 - val_MinusLogProbMetric: 27.8178 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 255/1000
2023-09-09 11:11:46.954 
Epoch 255/1000 
	 loss: 27.4389, MinusLogProbMetric: 27.4389, val_loss: 27.8369, val_MinusLogProbMetric: 27.8369

Epoch 255: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4389 - MinusLogProbMetric: 27.4389 - val_loss: 27.8369 - val_MinusLogProbMetric: 27.8369 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 256/1000
2023-09-09 11:11:57.379 
Epoch 256/1000 
	 loss: 27.4559, MinusLogProbMetric: 27.4559, val_loss: 27.8366, val_MinusLogProbMetric: 27.8366

Epoch 256: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4559 - MinusLogProbMetric: 27.4559 - val_loss: 27.8366 - val_MinusLogProbMetric: 27.8366 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 257/1000
2023-09-09 11:12:07.829 
Epoch 257/1000 
	 loss: 27.4369, MinusLogProbMetric: 27.4369, val_loss: 27.8042, val_MinusLogProbMetric: 27.8042

Epoch 257: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4369 - MinusLogProbMetric: 27.4369 - val_loss: 27.8042 - val_MinusLogProbMetric: 27.8042 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 258/1000
2023-09-09 11:12:18.712 
Epoch 258/1000 
	 loss: 27.4518, MinusLogProbMetric: 27.4518, val_loss: 27.8556, val_MinusLogProbMetric: 27.8556

Epoch 258: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4518 - MinusLogProbMetric: 27.4518 - val_loss: 27.8556 - val_MinusLogProbMetric: 27.8556 - lr: 5.0000e-04 - 11s/epoch - 55ms/step
Epoch 259/1000
2023-09-09 11:12:29.114 
Epoch 259/1000 
	 loss: 27.4449, MinusLogProbMetric: 27.4449, val_loss: 27.9229, val_MinusLogProbMetric: 27.9229

Epoch 259: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4449 - MinusLogProbMetric: 27.4449 - val_loss: 27.9229 - val_MinusLogProbMetric: 27.9229 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 260/1000
2023-09-09 11:12:39.253 
Epoch 260/1000 
	 loss: 27.4318, MinusLogProbMetric: 27.4318, val_loss: 27.8515, val_MinusLogProbMetric: 27.8515

Epoch 260: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4318 - MinusLogProbMetric: 27.4318 - val_loss: 27.8515 - val_MinusLogProbMetric: 27.8515 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 261/1000
2023-09-09 11:12:49.362 
Epoch 261/1000 
	 loss: 27.4454, MinusLogProbMetric: 27.4454, val_loss: 27.7956, val_MinusLogProbMetric: 27.7956

Epoch 261: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4454 - MinusLogProbMetric: 27.4454 - val_loss: 27.7956 - val_MinusLogProbMetric: 27.7956 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 262/1000
2023-09-09 11:12:59.580 
Epoch 262/1000 
	 loss: 27.4378, MinusLogProbMetric: 27.4378, val_loss: 27.9791, val_MinusLogProbMetric: 27.9791

Epoch 262: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4378 - MinusLogProbMetric: 27.4378 - val_loss: 27.9791 - val_MinusLogProbMetric: 27.9791 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 263/1000
2023-09-09 11:13:09.425 
Epoch 263/1000 
	 loss: 27.4309, MinusLogProbMetric: 27.4309, val_loss: 27.8260, val_MinusLogProbMetric: 27.8260

Epoch 263: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4309 - MinusLogProbMetric: 27.4309 - val_loss: 27.8260 - val_MinusLogProbMetric: 27.8260 - lr: 5.0000e-04 - 10s/epoch - 50ms/step
Epoch 264/1000
2023-09-09 11:13:19.556 
Epoch 264/1000 
	 loss: 27.4422, MinusLogProbMetric: 27.4422, val_loss: 27.8543, val_MinusLogProbMetric: 27.8543

Epoch 264: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4422 - MinusLogProbMetric: 27.4422 - val_loss: 27.8543 - val_MinusLogProbMetric: 27.8543 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 265/1000
2023-09-09 11:13:30.044 
Epoch 265/1000 
	 loss: 27.4631, MinusLogProbMetric: 27.4631, val_loss: 27.7782, val_MinusLogProbMetric: 27.7782

Epoch 265: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4631 - MinusLogProbMetric: 27.4631 - val_loss: 27.7782 - val_MinusLogProbMetric: 27.7782 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 266/1000
2023-09-09 11:13:40.407 
Epoch 266/1000 
	 loss: 27.4360, MinusLogProbMetric: 27.4360, val_loss: 27.8490, val_MinusLogProbMetric: 27.8490

Epoch 266: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4360 - MinusLogProbMetric: 27.4360 - val_loss: 27.8490 - val_MinusLogProbMetric: 27.8490 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 267/1000
2023-09-09 11:13:50.467 
Epoch 267/1000 
	 loss: 27.4391, MinusLogProbMetric: 27.4391, val_loss: 27.9371, val_MinusLogProbMetric: 27.9371

Epoch 267: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4391 - MinusLogProbMetric: 27.4391 - val_loss: 27.9371 - val_MinusLogProbMetric: 27.9371 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 268/1000
2023-09-09 11:14:00.992 
Epoch 268/1000 
	 loss: 27.4306, MinusLogProbMetric: 27.4306, val_loss: 27.8472, val_MinusLogProbMetric: 27.8472

Epoch 268: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4306 - MinusLogProbMetric: 27.4306 - val_loss: 27.8472 - val_MinusLogProbMetric: 27.8472 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 269/1000
2023-09-09 11:14:11.534 
Epoch 269/1000 
	 loss: 27.4338, MinusLogProbMetric: 27.4338, val_loss: 27.8647, val_MinusLogProbMetric: 27.8647

Epoch 269: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4338 - MinusLogProbMetric: 27.4338 - val_loss: 27.8647 - val_MinusLogProbMetric: 27.8647 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 270/1000
2023-09-09 11:14:21.871 
Epoch 270/1000 
	 loss: 27.4390, MinusLogProbMetric: 27.4390, val_loss: 27.8324, val_MinusLogProbMetric: 27.8324

Epoch 270: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4390 - MinusLogProbMetric: 27.4390 - val_loss: 27.8324 - val_MinusLogProbMetric: 27.8324 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 271/1000
2023-09-09 11:14:32.324 
Epoch 271/1000 
	 loss: 27.4545, MinusLogProbMetric: 27.4545, val_loss: 27.8671, val_MinusLogProbMetric: 27.8671

Epoch 271: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4545 - MinusLogProbMetric: 27.4545 - val_loss: 27.8671 - val_MinusLogProbMetric: 27.8671 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 272/1000
2023-09-09 11:14:42.442 
Epoch 272/1000 
	 loss: 27.4405, MinusLogProbMetric: 27.4405, val_loss: 27.8411, val_MinusLogProbMetric: 27.8411

Epoch 272: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4405 - MinusLogProbMetric: 27.4405 - val_loss: 27.8411 - val_MinusLogProbMetric: 27.8411 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 273/1000
2023-09-09 11:14:52.437 
Epoch 273/1000 
	 loss: 27.4200, MinusLogProbMetric: 27.4200, val_loss: 27.7954, val_MinusLogProbMetric: 27.7954

Epoch 273: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4200 - MinusLogProbMetric: 27.4200 - val_loss: 27.7954 - val_MinusLogProbMetric: 27.7954 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 274/1000
2023-09-09 11:15:02.684 
Epoch 274/1000 
	 loss: 27.4512, MinusLogProbMetric: 27.4512, val_loss: 27.7768, val_MinusLogProbMetric: 27.7768

Epoch 274: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4512 - MinusLogProbMetric: 27.4512 - val_loss: 27.7768 - val_MinusLogProbMetric: 27.7768 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 275/1000
2023-09-09 11:15:13.132 
Epoch 275/1000 
	 loss: 27.4403, MinusLogProbMetric: 27.4403, val_loss: 27.9346, val_MinusLogProbMetric: 27.9346

Epoch 275: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4403 - MinusLogProbMetric: 27.4403 - val_loss: 27.9346 - val_MinusLogProbMetric: 27.9346 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 276/1000
2023-09-09 11:15:23.483 
Epoch 276/1000 
	 loss: 27.4251, MinusLogProbMetric: 27.4251, val_loss: 27.7700, val_MinusLogProbMetric: 27.7700

Epoch 276: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4251 - MinusLogProbMetric: 27.4251 - val_loss: 27.7700 - val_MinusLogProbMetric: 27.7700 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 277/1000
2023-09-09 11:15:34.178 
Epoch 277/1000 
	 loss: 27.4343, MinusLogProbMetric: 27.4343, val_loss: 27.8559, val_MinusLogProbMetric: 27.8559

Epoch 277: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4343 - MinusLogProbMetric: 27.4343 - val_loss: 27.8559 - val_MinusLogProbMetric: 27.8559 - lr: 5.0000e-04 - 11s/epoch - 55ms/step
Epoch 278/1000
2023-09-09 11:15:44.422 
Epoch 278/1000 
	 loss: 27.4360, MinusLogProbMetric: 27.4360, val_loss: 27.9904, val_MinusLogProbMetric: 27.9904

Epoch 278: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4360 - MinusLogProbMetric: 27.4360 - val_loss: 27.9904 - val_MinusLogProbMetric: 27.9904 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 279/1000
2023-09-09 11:15:54.616 
Epoch 279/1000 
	 loss: 27.4156, MinusLogProbMetric: 27.4156, val_loss: 27.8599, val_MinusLogProbMetric: 27.8599

Epoch 279: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4156 - MinusLogProbMetric: 27.4156 - val_loss: 27.8599 - val_MinusLogProbMetric: 27.8599 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 280/1000
2023-09-09 11:16:04.872 
Epoch 280/1000 
	 loss: 27.4374, MinusLogProbMetric: 27.4374, val_loss: 27.8797, val_MinusLogProbMetric: 27.8797

Epoch 280: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4374 - MinusLogProbMetric: 27.4374 - val_loss: 27.8797 - val_MinusLogProbMetric: 27.8797 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 281/1000
2023-09-09 11:16:14.917 
Epoch 281/1000 
	 loss: 27.4323, MinusLogProbMetric: 27.4323, val_loss: 27.8475, val_MinusLogProbMetric: 27.8475

Epoch 281: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4323 - MinusLogProbMetric: 27.4323 - val_loss: 27.8475 - val_MinusLogProbMetric: 27.8475 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 282/1000
2023-09-09 11:16:24.858 
Epoch 282/1000 
	 loss: 27.4242, MinusLogProbMetric: 27.4242, val_loss: 27.8092, val_MinusLogProbMetric: 27.8092

Epoch 282: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4242 - MinusLogProbMetric: 27.4242 - val_loss: 27.8092 - val_MinusLogProbMetric: 27.8092 - lr: 5.0000e-04 - 10s/epoch - 51ms/step
Epoch 283/1000
2023-09-09 11:16:34.970 
Epoch 283/1000 
	 loss: 27.4240, MinusLogProbMetric: 27.4240, val_loss: 27.7807, val_MinusLogProbMetric: 27.7807

Epoch 283: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4240 - MinusLogProbMetric: 27.4240 - val_loss: 27.7807 - val_MinusLogProbMetric: 27.7807 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 284/1000
2023-09-09 11:16:45.234 
Epoch 284/1000 
	 loss: 27.4349, MinusLogProbMetric: 27.4349, val_loss: 27.8669, val_MinusLogProbMetric: 27.8669

Epoch 284: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4349 - MinusLogProbMetric: 27.4349 - val_loss: 27.8669 - val_MinusLogProbMetric: 27.8669 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 285/1000
2023-09-09 11:16:56.428 
Epoch 285/1000 
	 loss: 27.4138, MinusLogProbMetric: 27.4138, val_loss: 27.9262, val_MinusLogProbMetric: 27.9262

Epoch 285: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4138 - MinusLogProbMetric: 27.4138 - val_loss: 27.9262 - val_MinusLogProbMetric: 27.9262 - lr: 5.0000e-04 - 11s/epoch - 57ms/step
Epoch 286/1000
2023-09-09 11:17:07.744 
Epoch 286/1000 
	 loss: 27.4205, MinusLogProbMetric: 27.4205, val_loss: 27.8545, val_MinusLogProbMetric: 27.8545

Epoch 286: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4205 - MinusLogProbMetric: 27.4205 - val_loss: 27.8545 - val_MinusLogProbMetric: 27.8545 - lr: 5.0000e-04 - 11s/epoch - 58ms/step
Epoch 287/1000
2023-09-09 11:17:17.925 
Epoch 287/1000 
	 loss: 27.4257, MinusLogProbMetric: 27.4257, val_loss: 27.7861, val_MinusLogProbMetric: 27.7861

Epoch 287: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4257 - MinusLogProbMetric: 27.4257 - val_loss: 27.7861 - val_MinusLogProbMetric: 27.7861 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 288/1000
2023-09-09 11:17:28.727 
Epoch 288/1000 
	 loss: 27.4170, MinusLogProbMetric: 27.4170, val_loss: 27.8074, val_MinusLogProbMetric: 27.8074

Epoch 288: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4170 - MinusLogProbMetric: 27.4170 - val_loss: 27.8074 - val_MinusLogProbMetric: 27.8074 - lr: 5.0000e-04 - 11s/epoch - 55ms/step
Epoch 289/1000
2023-09-09 11:17:39.132 
Epoch 289/1000 
	 loss: 27.4232, MinusLogProbMetric: 27.4232, val_loss: 27.7947, val_MinusLogProbMetric: 27.7947

Epoch 289: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4232 - MinusLogProbMetric: 27.4232 - val_loss: 27.7947 - val_MinusLogProbMetric: 27.7947 - lr: 5.0000e-04 - 10s/epoch - 53ms/step
Epoch 290/1000
2023-09-09 11:17:49.336 
Epoch 290/1000 
	 loss: 27.4152, MinusLogProbMetric: 27.4152, val_loss: 27.8332, val_MinusLogProbMetric: 27.8332

Epoch 290: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4152 - MinusLogProbMetric: 27.4152 - val_loss: 27.8332 - val_MinusLogProbMetric: 27.8332 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 291/1000
2023-09-09 11:17:59.866 
Epoch 291/1000 
	 loss: 27.4135, MinusLogProbMetric: 27.4135, val_loss: 27.8004, val_MinusLogProbMetric: 27.8004

Epoch 291: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4135 - MinusLogProbMetric: 27.4135 - val_loss: 27.8004 - val_MinusLogProbMetric: 27.8004 - lr: 5.0000e-04 - 11s/epoch - 54ms/step
Epoch 292/1000
2023-09-09 11:18:10.058 
Epoch 292/1000 
	 loss: 27.4068, MinusLogProbMetric: 27.4068, val_loss: 27.9103, val_MinusLogProbMetric: 27.9103

Epoch 292: val_loss did not improve from 27.76611
196/196 - 10s - loss: 27.4068 - MinusLogProbMetric: 27.4068 - val_loss: 27.9103 - val_MinusLogProbMetric: 27.9103 - lr: 5.0000e-04 - 10s/epoch - 52ms/step
Epoch 293/1000
2023-09-09 11:18:20.937 
Epoch 293/1000 
	 loss: 27.4273, MinusLogProbMetric: 27.4273, val_loss: 27.8742, val_MinusLogProbMetric: 27.8742

Epoch 293: val_loss did not improve from 27.76611
196/196 - 11s - loss: 27.4273 - MinusLogProbMetric: 27.4273 - val_loss: 27.8742 - val_MinusLogProbMetric: 27.8742 - lr: 5.0000e-04 - 11s/epoch - 55ms/step
Epoch 294/1000
2023-09-09 11:18:32.014 
Epoch 294/1000 
	 loss: 27.3303, MinusLogProbMetric: 27.3303, val_loss: 27.7413, val_MinusLogProbMetric: 27.7413

Epoch 294: val_loss improved from 27.76611 to 27.74129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 27.3303 - MinusLogProbMetric: 27.3303 - val_loss: 27.7413 - val_MinusLogProbMetric: 27.7413 - lr: 2.5000e-04 - 11s/epoch - 58ms/step
Epoch 295/1000
2023-09-09 11:18:42.511 
Epoch 295/1000 
	 loss: 27.3308, MinusLogProbMetric: 27.3308, val_loss: 27.8355, val_MinusLogProbMetric: 27.8355

Epoch 295: val_loss did not improve from 27.74129
196/196 - 10s - loss: 27.3308 - MinusLogProbMetric: 27.3308 - val_loss: 27.8355 - val_MinusLogProbMetric: 27.8355 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 296/1000
2023-09-09 11:18:52.798 
Epoch 296/1000 
	 loss: 27.3278, MinusLogProbMetric: 27.3278, val_loss: 27.7332, val_MinusLogProbMetric: 27.7332

Epoch 296: val_loss improved from 27.74129 to 27.73320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 11s - loss: 27.3278 - MinusLogProbMetric: 27.3278 - val_loss: 27.7332 - val_MinusLogProbMetric: 27.7332 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 297/1000
2023-09-09 11:19:03.873 
Epoch 297/1000 
	 loss: 27.3279, MinusLogProbMetric: 27.3279, val_loss: 27.8149, val_MinusLogProbMetric: 27.8149

Epoch 297: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3279 - MinusLogProbMetric: 27.3279 - val_loss: 27.8149 - val_MinusLogProbMetric: 27.8149 - lr: 2.5000e-04 - 11s/epoch - 55ms/step
Epoch 298/1000
2023-09-09 11:19:14.220 
Epoch 298/1000 
	 loss: 27.3294, MinusLogProbMetric: 27.3294, val_loss: 27.7376, val_MinusLogProbMetric: 27.7376

Epoch 298: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3294 - MinusLogProbMetric: 27.3294 - val_loss: 27.7376 - val_MinusLogProbMetric: 27.7376 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 299/1000
2023-09-09 11:19:25.169 
Epoch 299/1000 
	 loss: 27.3274, MinusLogProbMetric: 27.3274, val_loss: 27.7591, val_MinusLogProbMetric: 27.7591

Epoch 299: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3274 - MinusLogProbMetric: 27.3274 - val_loss: 27.7591 - val_MinusLogProbMetric: 27.7591 - lr: 2.5000e-04 - 11s/epoch - 56ms/step
Epoch 300/1000
2023-09-09 11:19:35.465 
Epoch 300/1000 
	 loss: 27.3208, MinusLogProbMetric: 27.3208, val_loss: 27.7581, val_MinusLogProbMetric: 27.7581

Epoch 300: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3208 - MinusLogProbMetric: 27.3208 - val_loss: 27.7581 - val_MinusLogProbMetric: 27.7581 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 301/1000
2023-09-09 11:19:45.977 
Epoch 301/1000 
	 loss: 27.3238, MinusLogProbMetric: 27.3238, val_loss: 27.7412, val_MinusLogProbMetric: 27.7412

Epoch 301: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3238 - MinusLogProbMetric: 27.3238 - val_loss: 27.7412 - val_MinusLogProbMetric: 27.7412 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 302/1000
2023-09-09 11:19:56.771 
Epoch 302/1000 
	 loss: 27.3241, MinusLogProbMetric: 27.3241, val_loss: 27.7531, val_MinusLogProbMetric: 27.7531

Epoch 302: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3241 - MinusLogProbMetric: 27.3241 - val_loss: 27.7531 - val_MinusLogProbMetric: 27.7531 - lr: 2.5000e-04 - 11s/epoch - 55ms/step
Epoch 303/1000
2023-09-09 11:20:07.261 
Epoch 303/1000 
	 loss: 27.3311, MinusLogProbMetric: 27.3311, val_loss: 27.7604, val_MinusLogProbMetric: 27.7604

Epoch 303: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3311 - MinusLogProbMetric: 27.3311 - val_loss: 27.7604 - val_MinusLogProbMetric: 27.7604 - lr: 2.5000e-04 - 10s/epoch - 54ms/step
Epoch 304/1000
2023-09-09 11:20:17.333 
Epoch 304/1000 
	 loss: 27.3240, MinusLogProbMetric: 27.3240, val_loss: 27.7623, val_MinusLogProbMetric: 27.7623

Epoch 304: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3240 - MinusLogProbMetric: 27.3240 - val_loss: 27.7623 - val_MinusLogProbMetric: 27.7623 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 305/1000
2023-09-09 11:20:27.479 
Epoch 305/1000 
	 loss: 27.3231, MinusLogProbMetric: 27.3231, val_loss: 27.7631, val_MinusLogProbMetric: 27.7631

Epoch 305: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3231 - MinusLogProbMetric: 27.3231 - val_loss: 27.7631 - val_MinusLogProbMetric: 27.7631 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 306/1000
2023-09-09 11:20:37.558 
Epoch 306/1000 
	 loss: 27.3317, MinusLogProbMetric: 27.3317, val_loss: 27.7556, val_MinusLogProbMetric: 27.7556

Epoch 306: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3317 - MinusLogProbMetric: 27.3317 - val_loss: 27.7556 - val_MinusLogProbMetric: 27.7556 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 307/1000
2023-09-09 11:20:47.702 
Epoch 307/1000 
	 loss: 27.3236, MinusLogProbMetric: 27.3236, val_loss: 27.8020, val_MinusLogProbMetric: 27.8020

Epoch 307: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3236 - MinusLogProbMetric: 27.3236 - val_loss: 27.8020 - val_MinusLogProbMetric: 27.8020 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 308/1000
2023-09-09 11:20:57.921 
Epoch 308/1000 
	 loss: 27.3262, MinusLogProbMetric: 27.3262, val_loss: 27.7442, val_MinusLogProbMetric: 27.7442

Epoch 308: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3262 - MinusLogProbMetric: 27.3262 - val_loss: 27.7442 - val_MinusLogProbMetric: 27.7442 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 309/1000
2023-09-09 11:21:08.335 
Epoch 309/1000 
	 loss: 27.3264, MinusLogProbMetric: 27.3264, val_loss: 27.8197, val_MinusLogProbMetric: 27.8197

Epoch 309: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3264 - MinusLogProbMetric: 27.3264 - val_loss: 27.8197 - val_MinusLogProbMetric: 27.8197 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 310/1000
2023-09-09 11:21:18.475 
Epoch 310/1000 
	 loss: 27.3223, MinusLogProbMetric: 27.3223, val_loss: 27.7439, val_MinusLogProbMetric: 27.7439

Epoch 310: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3223 - MinusLogProbMetric: 27.3223 - val_loss: 27.7439 - val_MinusLogProbMetric: 27.7439 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 311/1000
2023-09-09 11:21:28.580 
Epoch 311/1000 
	 loss: 27.3244, MinusLogProbMetric: 27.3244, val_loss: 27.7677, val_MinusLogProbMetric: 27.7677

Epoch 311: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3244 - MinusLogProbMetric: 27.3244 - val_loss: 27.7677 - val_MinusLogProbMetric: 27.7677 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 312/1000
2023-09-09 11:21:38.787 
Epoch 312/1000 
	 loss: 27.3283, MinusLogProbMetric: 27.3283, val_loss: 27.7531, val_MinusLogProbMetric: 27.7531

Epoch 312: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3283 - MinusLogProbMetric: 27.3283 - val_loss: 27.7531 - val_MinusLogProbMetric: 27.7531 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 313/1000
2023-09-09 11:21:49.242 
Epoch 313/1000 
	 loss: 27.3214, MinusLogProbMetric: 27.3214, val_loss: 27.7551, val_MinusLogProbMetric: 27.7551

Epoch 313: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3214 - MinusLogProbMetric: 27.3214 - val_loss: 27.7551 - val_MinusLogProbMetric: 27.7551 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 314/1000
2023-09-09 11:21:59.276 
Epoch 314/1000 
	 loss: 27.3194, MinusLogProbMetric: 27.3194, val_loss: 27.7601, val_MinusLogProbMetric: 27.7601

Epoch 314: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3194 - MinusLogProbMetric: 27.3194 - val_loss: 27.7601 - val_MinusLogProbMetric: 27.7601 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 315/1000
2023-09-09 11:22:09.337 
Epoch 315/1000 
	 loss: 27.3187, MinusLogProbMetric: 27.3187, val_loss: 27.7472, val_MinusLogProbMetric: 27.7472

Epoch 315: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3187 - MinusLogProbMetric: 27.3187 - val_loss: 27.7472 - val_MinusLogProbMetric: 27.7472 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 316/1000
2023-09-09 11:22:20.242 
Epoch 316/1000 
	 loss: 27.3237, MinusLogProbMetric: 27.3237, val_loss: 27.7971, val_MinusLogProbMetric: 27.7971

Epoch 316: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3237 - MinusLogProbMetric: 27.3237 - val_loss: 27.7971 - val_MinusLogProbMetric: 27.7971 - lr: 2.5000e-04 - 11s/epoch - 56ms/step
Epoch 317/1000
2023-09-09 11:22:30.294 
Epoch 317/1000 
	 loss: 27.3232, MinusLogProbMetric: 27.3232, val_loss: 27.7988, val_MinusLogProbMetric: 27.7988

Epoch 317: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3232 - MinusLogProbMetric: 27.3232 - val_loss: 27.7988 - val_MinusLogProbMetric: 27.7988 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 318/1000
2023-09-09 11:22:40.795 
Epoch 318/1000 
	 loss: 27.3145, MinusLogProbMetric: 27.3145, val_loss: 27.7546, val_MinusLogProbMetric: 27.7546

Epoch 318: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3145 - MinusLogProbMetric: 27.3145 - val_loss: 27.7546 - val_MinusLogProbMetric: 27.7546 - lr: 2.5000e-04 - 10s/epoch - 54ms/step
Epoch 319/1000
2023-09-09 11:22:50.551 
Epoch 319/1000 
	 loss: 27.3138, MinusLogProbMetric: 27.3138, val_loss: 27.7597, val_MinusLogProbMetric: 27.7597

Epoch 319: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3138 - MinusLogProbMetric: 27.3138 - val_loss: 27.7597 - val_MinusLogProbMetric: 27.7597 - lr: 2.5000e-04 - 10s/epoch - 50ms/step
Epoch 320/1000
2023-09-09 11:23:01.338 
Epoch 320/1000 
	 loss: 27.3279, MinusLogProbMetric: 27.3279, val_loss: 27.7770, val_MinusLogProbMetric: 27.7770

Epoch 320: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3279 - MinusLogProbMetric: 27.3279 - val_loss: 27.7770 - val_MinusLogProbMetric: 27.7770 - lr: 2.5000e-04 - 11s/epoch - 55ms/step
Epoch 321/1000
2023-09-09 11:23:11.631 
Epoch 321/1000 
	 loss: 27.3187, MinusLogProbMetric: 27.3187, val_loss: 27.7566, val_MinusLogProbMetric: 27.7566

Epoch 321: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3187 - MinusLogProbMetric: 27.3187 - val_loss: 27.7566 - val_MinusLogProbMetric: 27.7566 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 322/1000
2023-09-09 11:23:21.932 
Epoch 322/1000 
	 loss: 27.3294, MinusLogProbMetric: 27.3294, val_loss: 27.7781, val_MinusLogProbMetric: 27.7781

Epoch 322: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3294 - MinusLogProbMetric: 27.3294 - val_loss: 27.7781 - val_MinusLogProbMetric: 27.7781 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 323/1000
2023-09-09 11:23:32.451 
Epoch 323/1000 
	 loss: 27.3206, MinusLogProbMetric: 27.3206, val_loss: 27.7442, val_MinusLogProbMetric: 27.7442

Epoch 323: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3206 - MinusLogProbMetric: 27.3206 - val_loss: 27.7442 - val_MinusLogProbMetric: 27.7442 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 324/1000
2023-09-09 11:23:43.049 
Epoch 324/1000 
	 loss: 27.3197, MinusLogProbMetric: 27.3197, val_loss: 27.7586, val_MinusLogProbMetric: 27.7586

Epoch 324: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3197 - MinusLogProbMetric: 27.3197 - val_loss: 27.7586 - val_MinusLogProbMetric: 27.7586 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 325/1000
2023-09-09 11:23:53.313 
Epoch 325/1000 
	 loss: 27.3138, MinusLogProbMetric: 27.3138, val_loss: 27.7356, val_MinusLogProbMetric: 27.7356

Epoch 325: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3138 - MinusLogProbMetric: 27.3138 - val_loss: 27.7356 - val_MinusLogProbMetric: 27.7356 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 326/1000
2023-09-09 11:24:03.334 
Epoch 326/1000 
	 loss: 27.3159, MinusLogProbMetric: 27.3159, val_loss: 27.7459, val_MinusLogProbMetric: 27.7459

Epoch 326: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3159 - MinusLogProbMetric: 27.3159 - val_loss: 27.7459 - val_MinusLogProbMetric: 27.7459 - lr: 2.5000e-04 - 10s/epoch - 51ms/step
Epoch 327/1000
2023-09-09 11:24:13.487 
Epoch 327/1000 
	 loss: 27.3206, MinusLogProbMetric: 27.3206, val_loss: 27.7618, val_MinusLogProbMetric: 27.7618

Epoch 327: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3206 - MinusLogProbMetric: 27.3206 - val_loss: 27.7618 - val_MinusLogProbMetric: 27.7618 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 328/1000
2023-09-09 11:24:24.092 
Epoch 328/1000 
	 loss: 27.3149, MinusLogProbMetric: 27.3149, val_loss: 27.7383, val_MinusLogProbMetric: 27.7383

Epoch 328: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3149 - MinusLogProbMetric: 27.3149 - val_loss: 27.7383 - val_MinusLogProbMetric: 27.7383 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 329/1000
2023-09-09 11:24:34.447 
Epoch 329/1000 
	 loss: 27.3150, MinusLogProbMetric: 27.3150, val_loss: 27.7537, val_MinusLogProbMetric: 27.7537

Epoch 329: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3150 - MinusLogProbMetric: 27.3150 - val_loss: 27.7537 - val_MinusLogProbMetric: 27.7537 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 330/1000
2023-09-09 11:24:44.805 
Epoch 330/1000 
	 loss: 27.3209, MinusLogProbMetric: 27.3209, val_loss: 27.8215, val_MinusLogProbMetric: 27.8215

Epoch 330: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3209 - MinusLogProbMetric: 27.3209 - val_loss: 27.8215 - val_MinusLogProbMetric: 27.8215 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 331/1000
2023-09-09 11:24:55.427 
Epoch 331/1000 
	 loss: 27.3131, MinusLogProbMetric: 27.3131, val_loss: 27.7498, val_MinusLogProbMetric: 27.7498

Epoch 331: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3131 - MinusLogProbMetric: 27.3131 - val_loss: 27.7498 - val_MinusLogProbMetric: 27.7498 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 332/1000
2023-09-09 11:25:05.945 
Epoch 332/1000 
	 loss: 27.3121, MinusLogProbMetric: 27.3121, val_loss: 27.7551, val_MinusLogProbMetric: 27.7551

Epoch 332: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3121 - MinusLogProbMetric: 27.3121 - val_loss: 27.7551 - val_MinusLogProbMetric: 27.7551 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 333/1000
2023-09-09 11:25:16.413 
Epoch 333/1000 
	 loss: 27.3204, MinusLogProbMetric: 27.3204, val_loss: 27.7377, val_MinusLogProbMetric: 27.7377

Epoch 333: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3204 - MinusLogProbMetric: 27.3204 - val_loss: 27.7377 - val_MinusLogProbMetric: 27.7377 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 334/1000
2023-09-09 11:25:26.827 
Epoch 334/1000 
	 loss: 27.3121, MinusLogProbMetric: 27.3121, val_loss: 27.7551, val_MinusLogProbMetric: 27.7551

Epoch 334: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3121 - MinusLogProbMetric: 27.3121 - val_loss: 27.7551 - val_MinusLogProbMetric: 27.7551 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 335/1000
2023-09-09 11:25:37.154 
Epoch 335/1000 
	 loss: 27.3157, MinusLogProbMetric: 27.3157, val_loss: 27.7635, val_MinusLogProbMetric: 27.7635

Epoch 335: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3157 - MinusLogProbMetric: 27.3157 - val_loss: 27.7635 - val_MinusLogProbMetric: 27.7635 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 336/1000
2023-09-09 11:25:47.281 
Epoch 336/1000 
	 loss: 27.3176, MinusLogProbMetric: 27.3176, val_loss: 27.7583, val_MinusLogProbMetric: 27.7583

Epoch 336: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3176 - MinusLogProbMetric: 27.3176 - val_loss: 27.7583 - val_MinusLogProbMetric: 27.7583 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 337/1000
2023-09-09 11:25:57.607 
Epoch 337/1000 
	 loss: 27.3125, MinusLogProbMetric: 27.3125, val_loss: 27.7482, val_MinusLogProbMetric: 27.7482

Epoch 337: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3125 - MinusLogProbMetric: 27.3125 - val_loss: 27.7482 - val_MinusLogProbMetric: 27.7482 - lr: 2.5000e-04 - 10s/epoch - 53ms/step
Epoch 338/1000
2023-09-09 11:26:07.878 
Epoch 338/1000 
	 loss: 27.3086, MinusLogProbMetric: 27.3086, val_loss: 27.8766, val_MinusLogProbMetric: 27.8766

Epoch 338: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3086 - MinusLogProbMetric: 27.3086 - val_loss: 27.8766 - val_MinusLogProbMetric: 27.8766 - lr: 2.5000e-04 - 10s/epoch - 52ms/step
Epoch 339/1000
2023-09-09 11:26:18.420 
Epoch 339/1000 
	 loss: 27.3221, MinusLogProbMetric: 27.3221, val_loss: 27.7855, val_MinusLogProbMetric: 27.7855

Epoch 339: val_loss did not improve from 27.73320
196/196 - 11s - loss: 27.3221 - MinusLogProbMetric: 27.3221 - val_loss: 27.7855 - val_MinusLogProbMetric: 27.7855 - lr: 2.5000e-04 - 11s/epoch - 54ms/step
Epoch 340/1000
2023-09-09 11:26:27.729 
Epoch 340/1000 
	 loss: 27.3141, MinusLogProbMetric: 27.3141, val_loss: 27.7699, val_MinusLogProbMetric: 27.7699

Epoch 340: val_loss did not improve from 27.73320
196/196 - 9s - loss: 27.3141 - MinusLogProbMetric: 27.3141 - val_loss: 27.7699 - val_MinusLogProbMetric: 27.7699 - lr: 2.5000e-04 - 9s/epoch - 47ms/step
Epoch 341/1000
2023-09-09 11:26:37.519 
Epoch 341/1000 
	 loss: 27.3147, MinusLogProbMetric: 27.3147, val_loss: 27.7632, val_MinusLogProbMetric: 27.7632

Epoch 341: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3147 - MinusLogProbMetric: 27.3147 - val_loss: 27.7632 - val_MinusLogProbMetric: 27.7632 - lr: 2.5000e-04 - 10s/epoch - 50ms/step
Epoch 342/1000
2023-09-09 11:26:47.109 
Epoch 342/1000 
	 loss: 27.3166, MinusLogProbMetric: 27.3166, val_loss: 27.7637, val_MinusLogProbMetric: 27.7637

Epoch 342: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3166 - MinusLogProbMetric: 27.3166 - val_loss: 27.7637 - val_MinusLogProbMetric: 27.7637 - lr: 2.5000e-04 - 10s/epoch - 49ms/step
Epoch 343/1000
2023-09-09 11:26:56.845 
Epoch 343/1000 
	 loss: 27.3098, MinusLogProbMetric: 27.3098, val_loss: 27.7984, val_MinusLogProbMetric: 27.7984

Epoch 343: val_loss did not improve from 27.73320
196/196 - 10s - loss: 27.3098 - MinusLogProbMetric: 27.3098 - val_loss: 27.7984 - val_MinusLogProbMetric: 27.7984 - lr: 2.5000e-04 - 10s/epoch - 50ms/step
Epoch 344/1000
2023-09-09 11:27:05.825 
Epoch 344/1000 
	 loss: 27.3102, MinusLogProbMetric: 27.3102, val_loss: 27.7576, val_MinusLogProbMetric: 27.7576

Epoch 344: val_loss did not improve from 27.73320
196/196 - 9s - loss: 27.3102 - MinusLogProbMetric: 27.3102 - val_loss: 27.7576 - val_MinusLogProbMetric: 27.7576 - lr: 2.5000e-04 - 9s/epoch - 46ms/step
Epoch 345/1000
2023-09-09 11:27:14.928 
Epoch 345/1000 
	 loss: 27.3126, MinusLogProbMetric: 27.3126, val_loss: 27.7607, val_MinusLogProbMetric: 27.7607

Epoch 345: val_loss did not improve from 27.73320
196/196 - 9s - loss: 27.3126 - MinusLogProbMetric: 27.3126 - val_loss: 27.7607 - val_MinusLogProbMetric: 27.7607 - lr: 2.5000e-04 - 9s/epoch - 46ms/step
Epoch 346/1000
2023-09-09 11:27:24.174 
Epoch 346/1000 
	 loss: 27.3159, MinusLogProbMetric: 27.3159, val_loss: 27.7875, val_MinusLogProbMetric: 27.7875

Epoch 346: val_loss did not improve from 27.73320
196/196 - 9s - loss: 27.3159 - MinusLogProbMetric: 27.3159 - val_loss: 27.7875 - val_MinusLogProbMetric: 27.7875 - lr: 2.5000e-04 - 9s/epoch - 47ms/step
Epoch 347/1000
2023-09-09 11:27:33.634 
Epoch 347/1000 
	 loss: 27.2717, MinusLogProbMetric: 27.2717, val_loss: 27.7344, val_MinusLogProbMetric: 27.7344

Epoch 347: val_loss did not improve from 27.73320
196/196 - 9s - loss: 27.2717 - MinusLogProbMetric: 27.2717 - val_loss: 27.7344 - val_MinusLogProbMetric: 27.7344 - lr: 1.2500e-04 - 9s/epoch - 48ms/step
Epoch 348/1000
2023-09-09 11:27:43.668 
Epoch 348/1000 
	 loss: 27.2675, MinusLogProbMetric: 27.2675, val_loss: 27.7192, val_MinusLogProbMetric: 27.7192

Epoch 348: val_loss improved from 27.73320 to 27.71923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 10s - loss: 27.2675 - MinusLogProbMetric: 27.2675 - val_loss: 27.7192 - val_MinusLogProbMetric: 27.7192 - lr: 1.2500e-04 - 10s/epoch - 53ms/step
Epoch 349/1000
2023-09-09 11:27:53.202 
Epoch 349/1000 
	 loss: 27.2693, MinusLogProbMetric: 27.2693, val_loss: 27.7230, val_MinusLogProbMetric: 27.7230

Epoch 349: val_loss did not improve from 27.71923
196/196 - 9s - loss: 27.2693 - MinusLogProbMetric: 27.2693 - val_loss: 27.7230 - val_MinusLogProbMetric: 27.7230 - lr: 1.2500e-04 - 9s/epoch - 47ms/step
Epoch 350/1000
2023-09-09 11:28:03.689 
Epoch 350/1000 
	 loss: 27.2684, MinusLogProbMetric: 27.2684, val_loss: 27.7280, val_MinusLogProbMetric: 27.7280

Epoch 350: val_loss did not improve from 27.71923
196/196 - 11s - loss: 27.2684 - MinusLogProbMetric: 27.2684 - val_loss: 27.7280 - val_MinusLogProbMetric: 27.7280 - lr: 1.2500e-04 - 11s/epoch - 54ms/step
Epoch 351/1000
2023-09-09 11:28:20.374 
Epoch 351/1000 
	 loss: 27.2671, MinusLogProbMetric: 27.2671, val_loss: 27.7358, val_MinusLogProbMetric: 27.7358

Epoch 351: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2671 - MinusLogProbMetric: 27.2671 - val_loss: 27.7358 - val_MinusLogProbMetric: 27.7358 - lr: 1.2500e-04 - 17s/epoch - 85ms/step
Epoch 352/1000
2023-09-09 11:28:36.130 
Epoch 352/1000 
	 loss: 27.2733, MinusLogProbMetric: 27.2733, val_loss: 27.7329, val_MinusLogProbMetric: 27.7329

Epoch 352: val_loss did not improve from 27.71923
196/196 - 16s - loss: 27.2733 - MinusLogProbMetric: 27.2733 - val_loss: 27.7329 - val_MinusLogProbMetric: 27.7329 - lr: 1.2500e-04 - 16s/epoch - 80ms/step
Epoch 353/1000
2023-09-09 11:28:53.918 
Epoch 353/1000 
	 loss: 27.2680, MinusLogProbMetric: 27.2680, val_loss: 27.7412, val_MinusLogProbMetric: 27.7412

Epoch 353: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2680 - MinusLogProbMetric: 27.2680 - val_loss: 27.7412 - val_MinusLogProbMetric: 27.7412 - lr: 1.2500e-04 - 18s/epoch - 91ms/step
Epoch 354/1000
2023-09-09 11:29:12.844 
Epoch 354/1000 
	 loss: 27.2713, MinusLogProbMetric: 27.2713, val_loss: 27.7390, val_MinusLogProbMetric: 27.7390

Epoch 354: val_loss did not improve from 27.71923
196/196 - 19s - loss: 27.2713 - MinusLogProbMetric: 27.2713 - val_loss: 27.7390 - val_MinusLogProbMetric: 27.7390 - lr: 1.2500e-04 - 19s/epoch - 97ms/step
Epoch 355/1000
2023-09-09 11:29:30.418 
Epoch 355/1000 
	 loss: 27.2707, MinusLogProbMetric: 27.2707, val_loss: 27.7415, val_MinusLogProbMetric: 27.7415

Epoch 355: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2707 - MinusLogProbMetric: 27.2707 - val_loss: 27.7415 - val_MinusLogProbMetric: 27.7415 - lr: 1.2500e-04 - 18s/epoch - 90ms/step
Epoch 356/1000
2023-09-09 11:29:47.949 
Epoch 356/1000 
	 loss: 27.2723, MinusLogProbMetric: 27.2723, val_loss: 27.7309, val_MinusLogProbMetric: 27.7309

Epoch 356: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2723 - MinusLogProbMetric: 27.2723 - val_loss: 27.7309 - val_MinusLogProbMetric: 27.7309 - lr: 1.2500e-04 - 17s/epoch - 89ms/step
Epoch 357/1000
2023-09-09 11:30:06.222 
Epoch 357/1000 
	 loss: 27.2678, MinusLogProbMetric: 27.2678, val_loss: 27.7234, val_MinusLogProbMetric: 27.7234

Epoch 357: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2678 - MinusLogProbMetric: 27.2678 - val_loss: 27.7234 - val_MinusLogProbMetric: 27.7234 - lr: 1.2500e-04 - 18s/epoch - 93ms/step
Epoch 358/1000
2023-09-09 11:30:23.981 
Epoch 358/1000 
	 loss: 27.2690, MinusLogProbMetric: 27.2690, val_loss: 27.7340, val_MinusLogProbMetric: 27.7340

Epoch 358: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2690 - MinusLogProbMetric: 27.2690 - val_loss: 27.7340 - val_MinusLogProbMetric: 27.7340 - lr: 1.2500e-04 - 18s/epoch - 91ms/step
Epoch 359/1000
2023-09-09 11:30:41.826 
Epoch 359/1000 
	 loss: 27.2689, MinusLogProbMetric: 27.2689, val_loss: 27.7466, val_MinusLogProbMetric: 27.7466

Epoch 359: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2689 - MinusLogProbMetric: 27.2689 - val_loss: 27.7466 - val_MinusLogProbMetric: 27.7466 - lr: 1.2500e-04 - 18s/epoch - 91ms/step
Epoch 360/1000
2023-09-09 11:30:58.862 
Epoch 360/1000 
	 loss: 27.2656, MinusLogProbMetric: 27.2656, val_loss: 27.7361, val_MinusLogProbMetric: 27.7361

Epoch 360: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2656 - MinusLogProbMetric: 27.2656 - val_loss: 27.7361 - val_MinusLogProbMetric: 27.7361 - lr: 1.2500e-04 - 17s/epoch - 87ms/step
Epoch 361/1000
2023-09-09 11:31:15.745 
Epoch 361/1000 
	 loss: 27.2683, MinusLogProbMetric: 27.2683, val_loss: 27.7232, val_MinusLogProbMetric: 27.7232

Epoch 361: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2683 - MinusLogProbMetric: 27.2683 - val_loss: 27.7232 - val_MinusLogProbMetric: 27.7232 - lr: 1.2500e-04 - 17s/epoch - 86ms/step
Epoch 362/1000
2023-09-09 11:31:32.934 
Epoch 362/1000 
	 loss: 27.2700, MinusLogProbMetric: 27.2700, val_loss: 27.7327, val_MinusLogProbMetric: 27.7327

Epoch 362: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2700 - MinusLogProbMetric: 27.2700 - val_loss: 27.7327 - val_MinusLogProbMetric: 27.7327 - lr: 1.2500e-04 - 17s/epoch - 88ms/step
Epoch 363/1000
2023-09-09 11:31:50.428 
Epoch 363/1000 
	 loss: 27.2687, MinusLogProbMetric: 27.2687, val_loss: 27.7354, val_MinusLogProbMetric: 27.7354

Epoch 363: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2687 - MinusLogProbMetric: 27.2687 - val_loss: 27.7354 - val_MinusLogProbMetric: 27.7354 - lr: 1.2500e-04 - 17s/epoch - 89ms/step
Epoch 364/1000
2023-09-09 11:32:06.754 
Epoch 364/1000 
	 loss: 27.2689, MinusLogProbMetric: 27.2689, val_loss: 27.7315, val_MinusLogProbMetric: 27.7315

Epoch 364: val_loss did not improve from 27.71923
196/196 - 16s - loss: 27.2689 - MinusLogProbMetric: 27.2689 - val_loss: 27.7315 - val_MinusLogProbMetric: 27.7315 - lr: 1.2500e-04 - 16s/epoch - 83ms/step
Epoch 365/1000
2023-09-09 11:32:24.794 
Epoch 365/1000 
	 loss: 27.2700, MinusLogProbMetric: 27.2700, val_loss: 27.7266, val_MinusLogProbMetric: 27.7266

Epoch 365: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2700 - MinusLogProbMetric: 27.2700 - val_loss: 27.7266 - val_MinusLogProbMetric: 27.7266 - lr: 1.2500e-04 - 18s/epoch - 92ms/step
Epoch 366/1000
2023-09-09 11:32:41.838 
Epoch 366/1000 
	 loss: 27.2655, MinusLogProbMetric: 27.2655, val_loss: 27.7300, val_MinusLogProbMetric: 27.7300

Epoch 366: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2655 - MinusLogProbMetric: 27.2655 - val_loss: 27.7300 - val_MinusLogProbMetric: 27.7300 - lr: 1.2500e-04 - 17s/epoch - 87ms/step
Epoch 367/1000
2023-09-09 11:32:59.380 
Epoch 367/1000 
	 loss: 27.2663, MinusLogProbMetric: 27.2663, val_loss: 27.7214, val_MinusLogProbMetric: 27.7214

Epoch 367: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2663 - MinusLogProbMetric: 27.2663 - val_loss: 27.7214 - val_MinusLogProbMetric: 27.7214 - lr: 1.2500e-04 - 18s/epoch - 89ms/step
Epoch 368/1000
2023-09-09 11:33:16.842 
Epoch 368/1000 
	 loss: 27.2693, MinusLogProbMetric: 27.2693, val_loss: 27.7405, val_MinusLogProbMetric: 27.7405

Epoch 368: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2693 - MinusLogProbMetric: 27.2693 - val_loss: 27.7405 - val_MinusLogProbMetric: 27.7405 - lr: 1.2500e-04 - 17s/epoch - 89ms/step
Epoch 369/1000
2023-09-09 11:33:34.971 
Epoch 369/1000 
	 loss: 27.2677, MinusLogProbMetric: 27.2677, val_loss: 27.7261, val_MinusLogProbMetric: 27.7261

Epoch 369: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2677 - MinusLogProbMetric: 27.2677 - val_loss: 27.7261 - val_MinusLogProbMetric: 27.7261 - lr: 1.2500e-04 - 18s/epoch - 92ms/step
Epoch 370/1000
2023-09-09 11:33:53.239 
Epoch 370/1000 
	 loss: 27.2705, MinusLogProbMetric: 27.2705, val_loss: 27.7385, val_MinusLogProbMetric: 27.7385

Epoch 370: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2705 - MinusLogProbMetric: 27.2705 - val_loss: 27.7385 - val_MinusLogProbMetric: 27.7385 - lr: 1.2500e-04 - 18s/epoch - 93ms/step
Epoch 371/1000
2023-09-09 11:34:10.201 
Epoch 371/1000 
	 loss: 27.2674, MinusLogProbMetric: 27.2674, val_loss: 27.7543, val_MinusLogProbMetric: 27.7543

Epoch 371: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2674 - MinusLogProbMetric: 27.2674 - val_loss: 27.7543 - val_MinusLogProbMetric: 27.7543 - lr: 1.2500e-04 - 17s/epoch - 87ms/step
Epoch 372/1000
2023-09-09 11:34:28.330 
Epoch 372/1000 
	 loss: 27.2657, MinusLogProbMetric: 27.2657, val_loss: 27.7304, val_MinusLogProbMetric: 27.7304

Epoch 372: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2657 - MinusLogProbMetric: 27.2657 - val_loss: 27.7304 - val_MinusLogProbMetric: 27.7304 - lr: 1.2500e-04 - 18s/epoch - 92ms/step
Epoch 373/1000
2023-09-09 11:34:42.916 
Epoch 373/1000 
	 loss: 27.2677, MinusLogProbMetric: 27.2677, val_loss: 27.7464, val_MinusLogProbMetric: 27.7464

Epoch 373: val_loss did not improve from 27.71923
196/196 - 15s - loss: 27.2677 - MinusLogProbMetric: 27.2677 - val_loss: 27.7464 - val_MinusLogProbMetric: 27.7464 - lr: 1.2500e-04 - 15s/epoch - 74ms/step
Epoch 374/1000
2023-09-09 11:34:57.472 
Epoch 374/1000 
	 loss: 27.2650, MinusLogProbMetric: 27.2650, val_loss: 27.7419, val_MinusLogProbMetric: 27.7419

Epoch 374: val_loss did not improve from 27.71923
196/196 - 15s - loss: 27.2650 - MinusLogProbMetric: 27.2650 - val_loss: 27.7419 - val_MinusLogProbMetric: 27.7419 - lr: 1.2500e-04 - 15s/epoch - 74ms/step
Epoch 375/1000
2023-09-09 11:35:10.938 
Epoch 375/1000 
	 loss: 27.2638, MinusLogProbMetric: 27.2638, val_loss: 27.7374, val_MinusLogProbMetric: 27.7374

Epoch 375: val_loss did not improve from 27.71923
196/196 - 13s - loss: 27.2638 - MinusLogProbMetric: 27.2638 - val_loss: 27.7374 - val_MinusLogProbMetric: 27.7374 - lr: 1.2500e-04 - 13s/epoch - 69ms/step
Epoch 376/1000
2023-09-09 11:35:25.242 
Epoch 376/1000 
	 loss: 27.2656, MinusLogProbMetric: 27.2656, val_loss: 27.7286, val_MinusLogProbMetric: 27.7286

Epoch 376: val_loss did not improve from 27.71923
196/196 - 14s - loss: 27.2656 - MinusLogProbMetric: 27.2656 - val_loss: 27.7286 - val_MinusLogProbMetric: 27.7286 - lr: 1.2500e-04 - 14s/epoch - 73ms/step
Epoch 377/1000
2023-09-09 11:35:42.440 
Epoch 377/1000 
	 loss: 27.2612, MinusLogProbMetric: 27.2612, val_loss: 27.7483, val_MinusLogProbMetric: 27.7483

Epoch 377: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2612 - MinusLogProbMetric: 27.2612 - val_loss: 27.7483 - val_MinusLogProbMetric: 27.7483 - lr: 1.2500e-04 - 17s/epoch - 88ms/step
Epoch 378/1000
2023-09-09 11:35:59.444 
Epoch 378/1000 
	 loss: 27.2656, MinusLogProbMetric: 27.2656, val_loss: 27.7291, val_MinusLogProbMetric: 27.7291

Epoch 378: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2656 - MinusLogProbMetric: 27.2656 - val_loss: 27.7291 - val_MinusLogProbMetric: 27.7291 - lr: 1.2500e-04 - 17s/epoch - 87ms/step
Epoch 379/1000
2023-09-09 11:36:16.624 
Epoch 379/1000 
	 loss: 27.2616, MinusLogProbMetric: 27.2616, val_loss: 27.7238, val_MinusLogProbMetric: 27.7238

Epoch 379: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2616 - MinusLogProbMetric: 27.2616 - val_loss: 27.7238 - val_MinusLogProbMetric: 27.7238 - lr: 1.2500e-04 - 17s/epoch - 88ms/step
Epoch 380/1000
2023-09-09 11:36:33.696 
Epoch 380/1000 
	 loss: 27.2673, MinusLogProbMetric: 27.2673, val_loss: 27.7286, val_MinusLogProbMetric: 27.7286

Epoch 380: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2673 - MinusLogProbMetric: 27.2673 - val_loss: 27.7286 - val_MinusLogProbMetric: 27.7286 - lr: 1.2500e-04 - 17s/epoch - 87ms/step
Epoch 381/1000
2023-09-09 11:36:50.640 
Epoch 381/1000 
	 loss: 27.2652, MinusLogProbMetric: 27.2652, val_loss: 27.7359, val_MinusLogProbMetric: 27.7359

Epoch 381: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2652 - MinusLogProbMetric: 27.2652 - val_loss: 27.7359 - val_MinusLogProbMetric: 27.7359 - lr: 1.2500e-04 - 17s/epoch - 86ms/step
Epoch 382/1000
2023-09-09 11:37:06.968 
Epoch 382/1000 
	 loss: 27.2615, MinusLogProbMetric: 27.2615, val_loss: 27.7417, val_MinusLogProbMetric: 27.7417

Epoch 382: val_loss did not improve from 27.71923
196/196 - 16s - loss: 27.2615 - MinusLogProbMetric: 27.2615 - val_loss: 27.7417 - val_MinusLogProbMetric: 27.7417 - lr: 1.2500e-04 - 16s/epoch - 83ms/step
Epoch 383/1000
2023-09-09 11:37:23.624 
Epoch 383/1000 
	 loss: 27.2650, MinusLogProbMetric: 27.2650, val_loss: 27.7684, val_MinusLogProbMetric: 27.7684

Epoch 383: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2650 - MinusLogProbMetric: 27.2650 - val_loss: 27.7684 - val_MinusLogProbMetric: 27.7684 - lr: 1.2500e-04 - 17s/epoch - 85ms/step
Epoch 384/1000
2023-09-09 11:37:40.489 
Epoch 384/1000 
	 loss: 27.2651, MinusLogProbMetric: 27.2651, val_loss: 27.7263, val_MinusLogProbMetric: 27.7263

Epoch 384: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2651 - MinusLogProbMetric: 27.2651 - val_loss: 27.7263 - val_MinusLogProbMetric: 27.7263 - lr: 1.2500e-04 - 17s/epoch - 86ms/step
Epoch 385/1000
2023-09-09 11:37:58.166 
Epoch 385/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 27.7231, val_MinusLogProbMetric: 27.7231

Epoch 385: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 27.7231 - val_MinusLogProbMetric: 27.7231 - lr: 1.2500e-04 - 18s/epoch - 90ms/step
Epoch 386/1000
2023-09-09 11:38:15.647 
Epoch 386/1000 
	 loss: 27.2621, MinusLogProbMetric: 27.2621, val_loss: 27.7295, val_MinusLogProbMetric: 27.7295

Epoch 386: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2621 - MinusLogProbMetric: 27.2621 - val_loss: 27.7295 - val_MinusLogProbMetric: 27.7295 - lr: 1.2500e-04 - 17s/epoch - 89ms/step
Epoch 387/1000
2023-09-09 11:38:33.544 
Epoch 387/1000 
	 loss: 27.2642, MinusLogProbMetric: 27.2642, val_loss: 27.7425, val_MinusLogProbMetric: 27.7425

Epoch 387: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2642 - MinusLogProbMetric: 27.2642 - val_loss: 27.7425 - val_MinusLogProbMetric: 27.7425 - lr: 1.2500e-04 - 18s/epoch - 91ms/step
Epoch 388/1000
2023-09-09 11:38:50.884 
Epoch 388/1000 
	 loss: 27.2669, MinusLogProbMetric: 27.2669, val_loss: 27.7390, val_MinusLogProbMetric: 27.7390

Epoch 388: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2669 - MinusLogProbMetric: 27.2669 - val_loss: 27.7390 - val_MinusLogProbMetric: 27.7390 - lr: 1.2500e-04 - 17s/epoch - 88ms/step
Epoch 389/1000
2023-09-09 11:39:08.460 
Epoch 389/1000 
	 loss: 27.2636, MinusLogProbMetric: 27.2636, val_loss: 27.7261, val_MinusLogProbMetric: 27.7261

Epoch 389: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2636 - MinusLogProbMetric: 27.2636 - val_loss: 27.7261 - val_MinusLogProbMetric: 27.7261 - lr: 1.2500e-04 - 18s/epoch - 90ms/step
Epoch 390/1000
2023-09-09 11:39:26.509 
Epoch 390/1000 
	 loss: 27.2633, MinusLogProbMetric: 27.2633, val_loss: 27.7486, val_MinusLogProbMetric: 27.7486

Epoch 390: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2633 - MinusLogProbMetric: 27.2633 - val_loss: 27.7486 - val_MinusLogProbMetric: 27.7486 - lr: 1.2500e-04 - 18s/epoch - 92ms/step
Epoch 391/1000
2023-09-09 11:39:43.820 
Epoch 391/1000 
	 loss: 27.2643, MinusLogProbMetric: 27.2643, val_loss: 27.7348, val_MinusLogProbMetric: 27.7348

Epoch 391: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2643 - MinusLogProbMetric: 27.2643 - val_loss: 27.7348 - val_MinusLogProbMetric: 27.7348 - lr: 1.2500e-04 - 17s/epoch - 88ms/step
Epoch 392/1000
2023-09-09 11:40:01.554 
Epoch 392/1000 
	 loss: 27.2612, MinusLogProbMetric: 27.2612, val_loss: 27.7547, val_MinusLogProbMetric: 27.7547

Epoch 392: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2612 - MinusLogProbMetric: 27.2612 - val_loss: 27.7547 - val_MinusLogProbMetric: 27.7547 - lr: 1.2500e-04 - 18s/epoch - 90ms/step
Epoch 393/1000
2023-09-09 11:40:19.001 
Epoch 393/1000 
	 loss: 27.2616, MinusLogProbMetric: 27.2616, val_loss: 27.7306, val_MinusLogProbMetric: 27.7306

Epoch 393: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2616 - MinusLogProbMetric: 27.2616 - val_loss: 27.7306 - val_MinusLogProbMetric: 27.7306 - lr: 1.2500e-04 - 17s/epoch - 89ms/step
Epoch 394/1000
2023-09-09 11:40:37.119 
Epoch 394/1000 
	 loss: 27.2611, MinusLogProbMetric: 27.2611, val_loss: 27.7553, val_MinusLogProbMetric: 27.7553

Epoch 394: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2611 - MinusLogProbMetric: 27.2611 - val_loss: 27.7553 - val_MinusLogProbMetric: 27.7553 - lr: 1.2500e-04 - 18s/epoch - 92ms/step
Epoch 395/1000
2023-09-09 11:40:54.790 
Epoch 395/1000 
	 loss: 27.2614, MinusLogProbMetric: 27.2614, val_loss: 27.7424, val_MinusLogProbMetric: 27.7424

Epoch 395: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2614 - MinusLogProbMetric: 27.2614 - val_loss: 27.7424 - val_MinusLogProbMetric: 27.7424 - lr: 1.2500e-04 - 18s/epoch - 90ms/step
Epoch 396/1000
2023-09-09 11:41:11.733 
Epoch 396/1000 
	 loss: 27.2620, MinusLogProbMetric: 27.2620, val_loss: 27.7341, val_MinusLogProbMetric: 27.7341

Epoch 396: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2620 - MinusLogProbMetric: 27.2620 - val_loss: 27.7341 - val_MinusLogProbMetric: 27.7341 - lr: 1.2500e-04 - 17s/epoch - 86ms/step
Epoch 397/1000
2023-09-09 11:41:29.151 
Epoch 397/1000 
	 loss: 27.2603, MinusLogProbMetric: 27.2603, val_loss: 27.7419, val_MinusLogProbMetric: 27.7419

Epoch 397: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2603 - MinusLogProbMetric: 27.2603 - val_loss: 27.7419 - val_MinusLogProbMetric: 27.7419 - lr: 1.2500e-04 - 17s/epoch - 89ms/step
Epoch 398/1000
2023-09-09 11:41:46.914 
Epoch 398/1000 
	 loss: 27.2624, MinusLogProbMetric: 27.2624, val_loss: 27.7451, val_MinusLogProbMetric: 27.7451

Epoch 398: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2624 - MinusLogProbMetric: 27.2624 - val_loss: 27.7451 - val_MinusLogProbMetric: 27.7451 - lr: 1.2500e-04 - 18s/epoch - 91ms/step
Epoch 399/1000
2023-09-09 11:42:05.082 
Epoch 399/1000 
	 loss: 27.2436, MinusLogProbMetric: 27.2436, val_loss: 27.7219, val_MinusLogProbMetric: 27.7219

Epoch 399: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2436 - MinusLogProbMetric: 27.2436 - val_loss: 27.7219 - val_MinusLogProbMetric: 27.7219 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 400/1000
2023-09-09 11:42:23.158 
Epoch 400/1000 
	 loss: 27.2416, MinusLogProbMetric: 27.2416, val_loss: 27.7310, val_MinusLogProbMetric: 27.7310

Epoch 400: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2416 - MinusLogProbMetric: 27.2416 - val_loss: 27.7310 - val_MinusLogProbMetric: 27.7310 - lr: 6.2500e-05 - 18s/epoch - 92ms/step
Epoch 401/1000
2023-09-09 11:42:41.378 
Epoch 401/1000 
	 loss: 27.2431, MinusLogProbMetric: 27.2431, val_loss: 27.7316, val_MinusLogProbMetric: 27.7316

Epoch 401: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2431 - MinusLogProbMetric: 27.2431 - val_loss: 27.7316 - val_MinusLogProbMetric: 27.7316 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 402/1000
2023-09-09 11:42:59.796 
Epoch 402/1000 
	 loss: 27.2424, MinusLogProbMetric: 27.2424, val_loss: 27.7313, val_MinusLogProbMetric: 27.7313

Epoch 402: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2424 - MinusLogProbMetric: 27.2424 - val_loss: 27.7313 - val_MinusLogProbMetric: 27.7313 - lr: 6.2500e-05 - 18s/epoch - 94ms/step
Epoch 403/1000
2023-09-09 11:43:18.269 
Epoch 403/1000 
	 loss: 27.2414, MinusLogProbMetric: 27.2414, val_loss: 27.7233, val_MinusLogProbMetric: 27.7233

Epoch 403: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2414 - MinusLogProbMetric: 27.2414 - val_loss: 27.7233 - val_MinusLogProbMetric: 27.7233 - lr: 6.2500e-05 - 18s/epoch - 94ms/step
Epoch 404/1000
2023-09-09 11:43:35.878 
Epoch 404/1000 
	 loss: 27.2432, MinusLogProbMetric: 27.2432, val_loss: 27.7228, val_MinusLogProbMetric: 27.7228

Epoch 404: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2432 - MinusLogProbMetric: 27.2432 - val_loss: 27.7228 - val_MinusLogProbMetric: 27.7228 - lr: 6.2500e-05 - 18s/epoch - 90ms/step
Epoch 405/1000
2023-09-09 11:43:52.967 
Epoch 405/1000 
	 loss: 27.2429, MinusLogProbMetric: 27.2429, val_loss: 27.7252, val_MinusLogProbMetric: 27.7252

Epoch 405: val_loss did not improve from 27.71923
196/196 - 17s - loss: 27.2429 - MinusLogProbMetric: 27.2429 - val_loss: 27.7252 - val_MinusLogProbMetric: 27.7252 - lr: 6.2500e-05 - 17s/epoch - 87ms/step
Epoch 406/1000
2023-09-09 11:44:10.537 
Epoch 406/1000 
	 loss: 27.2433, MinusLogProbMetric: 27.2433, val_loss: 27.7218, val_MinusLogProbMetric: 27.7218

Epoch 406: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2433 - MinusLogProbMetric: 27.2433 - val_loss: 27.7218 - val_MinusLogProbMetric: 27.7218 - lr: 6.2500e-05 - 18s/epoch - 90ms/step
Epoch 407/1000
2023-09-09 11:44:28.587 
Epoch 407/1000 
	 loss: 27.2414, MinusLogProbMetric: 27.2414, val_loss: 27.7246, val_MinusLogProbMetric: 27.7246

Epoch 407: val_loss did not improve from 27.71923
196/196 - 18s - loss: 27.2414 - MinusLogProbMetric: 27.2414 - val_loss: 27.7246 - val_MinusLogProbMetric: 27.7246 - lr: 6.2500e-05 - 18s/epoch - 92ms/step
Epoch 408/1000
2023-09-09 11:44:46.148 
Epoch 408/1000 
	 loss: 27.2431, MinusLogProbMetric: 27.2431, val_loss: 27.7174, val_MinusLogProbMetric: 27.7174

Epoch 408: val_loss improved from 27.71923 to 27.71742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 18s - loss: 27.2431 - MinusLogProbMetric: 27.2431 - val_loss: 27.7174 - val_MinusLogProbMetric: 27.7174 - lr: 6.2500e-05 - 18s/epoch - 92ms/step
Epoch 409/1000
2023-09-09 11:45:04.828 
Epoch 409/1000 
	 loss: 27.2434, MinusLogProbMetric: 27.2434, val_loss: 27.7262, val_MinusLogProbMetric: 27.7262

Epoch 409: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2434 - MinusLogProbMetric: 27.2434 - val_loss: 27.7262 - val_MinusLogProbMetric: 27.7262 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 410/1000
2023-09-09 11:45:22.938 
Epoch 410/1000 
	 loss: 27.2428, MinusLogProbMetric: 27.2428, val_loss: 27.7277, val_MinusLogProbMetric: 27.7277

Epoch 410: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2428 - MinusLogProbMetric: 27.2428 - val_loss: 27.7277 - val_MinusLogProbMetric: 27.7277 - lr: 6.2500e-05 - 18s/epoch - 92ms/step
Epoch 411/1000
2023-09-09 11:45:40.823 
Epoch 411/1000 
	 loss: 27.2415, MinusLogProbMetric: 27.2415, val_loss: 27.7320, val_MinusLogProbMetric: 27.7320

Epoch 411: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2415 - MinusLogProbMetric: 27.2415 - val_loss: 27.7320 - val_MinusLogProbMetric: 27.7320 - lr: 6.2500e-05 - 18s/epoch - 91ms/step
Epoch 412/1000
2023-09-09 11:45:59.560 
Epoch 412/1000 
	 loss: 27.2433, MinusLogProbMetric: 27.2433, val_loss: 27.7249, val_MinusLogProbMetric: 27.7249

Epoch 412: val_loss did not improve from 27.71742
196/196 - 19s - loss: 27.2433 - MinusLogProbMetric: 27.2433 - val_loss: 27.7249 - val_MinusLogProbMetric: 27.7249 - lr: 6.2500e-05 - 19s/epoch - 96ms/step
Epoch 413/1000
2023-09-09 11:46:17.710 
Epoch 413/1000 
	 loss: 27.2405, MinusLogProbMetric: 27.2405, val_loss: 27.7259, val_MinusLogProbMetric: 27.7259

Epoch 413: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2405 - MinusLogProbMetric: 27.2405 - val_loss: 27.7259 - val_MinusLogProbMetric: 27.7259 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 414/1000
2023-09-09 11:46:35.907 
Epoch 414/1000 
	 loss: 27.2406, MinusLogProbMetric: 27.2406, val_loss: 27.7276, val_MinusLogProbMetric: 27.7276

Epoch 414: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2406 - MinusLogProbMetric: 27.2406 - val_loss: 27.7276 - val_MinusLogProbMetric: 27.7276 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 415/1000
2023-09-09 11:46:53.946 
Epoch 415/1000 
	 loss: 27.2413, MinusLogProbMetric: 27.2413, val_loss: 27.7336, val_MinusLogProbMetric: 27.7336

Epoch 415: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2413 - MinusLogProbMetric: 27.2413 - val_loss: 27.7336 - val_MinusLogProbMetric: 27.7336 - lr: 6.2500e-05 - 18s/epoch - 92ms/step
Epoch 416/1000
2023-09-09 11:47:12.641 
Epoch 416/1000 
	 loss: 27.2414, MinusLogProbMetric: 27.2414, val_loss: 27.7242, val_MinusLogProbMetric: 27.7242

Epoch 416: val_loss did not improve from 27.71742
196/196 - 19s - loss: 27.2414 - MinusLogProbMetric: 27.2414 - val_loss: 27.7242 - val_MinusLogProbMetric: 27.7242 - lr: 6.2500e-05 - 19s/epoch - 95ms/step
Epoch 417/1000
2023-09-09 11:47:32.659 
Epoch 417/1000 
	 loss: 27.2417, MinusLogProbMetric: 27.2417, val_loss: 27.7339, val_MinusLogProbMetric: 27.7339

Epoch 417: val_loss did not improve from 27.71742
196/196 - 20s - loss: 27.2417 - MinusLogProbMetric: 27.2417 - val_loss: 27.7339 - val_MinusLogProbMetric: 27.7339 - lr: 6.2500e-05 - 20s/epoch - 102ms/step
Epoch 418/1000
2023-09-09 11:47:50.368 
Epoch 418/1000 
	 loss: 27.2408, MinusLogProbMetric: 27.2408, val_loss: 27.7370, val_MinusLogProbMetric: 27.7370

Epoch 418: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2408 - MinusLogProbMetric: 27.2408 - val_loss: 27.7370 - val_MinusLogProbMetric: 27.7370 - lr: 6.2500e-05 - 18s/epoch - 90ms/step
Epoch 419/1000
2023-09-09 11:48:08.368 
Epoch 419/1000 
	 loss: 27.2421, MinusLogProbMetric: 27.2421, val_loss: 27.7189, val_MinusLogProbMetric: 27.7189

Epoch 419: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2421 - MinusLogProbMetric: 27.2421 - val_loss: 27.7189 - val_MinusLogProbMetric: 27.7189 - lr: 6.2500e-05 - 18s/epoch - 92ms/step
Epoch 420/1000
2023-09-09 11:48:26.230 
Epoch 420/1000 
	 loss: 27.2411, MinusLogProbMetric: 27.2411, val_loss: 27.7202, val_MinusLogProbMetric: 27.7202

Epoch 420: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2411 - MinusLogProbMetric: 27.2411 - val_loss: 27.7202 - val_MinusLogProbMetric: 27.7202 - lr: 6.2500e-05 - 18s/epoch - 91ms/step
Epoch 421/1000
2023-09-09 11:48:43.346 
Epoch 421/1000 
	 loss: 27.2393, MinusLogProbMetric: 27.2393, val_loss: 27.7300, val_MinusLogProbMetric: 27.7300

Epoch 421: val_loss did not improve from 27.71742
196/196 - 17s - loss: 27.2393 - MinusLogProbMetric: 27.2393 - val_loss: 27.7300 - val_MinusLogProbMetric: 27.7300 - lr: 6.2500e-05 - 17s/epoch - 87ms/step
Epoch 422/1000
2023-09-09 11:49:01.368 
Epoch 422/1000 
	 loss: 27.2411, MinusLogProbMetric: 27.2411, val_loss: 27.7247, val_MinusLogProbMetric: 27.7247

Epoch 422: val_loss did not improve from 27.71742
196/196 - 18s - loss: 27.2411 - MinusLogProbMetric: 27.2411 - val_loss: 27.7247 - val_MinusLogProbMetric: 27.7247 - lr: 6.2500e-05 - 18s/epoch - 92ms/step
Epoch 423/1000
2023-09-09 11:49:17.136 
Epoch 423/1000 
	 loss: 27.2426, MinusLogProbMetric: 27.2426, val_loss: 27.7283, val_MinusLogProbMetric: 27.7283

Epoch 423: val_loss did not improve from 27.71742
196/196 - 16s - loss: 27.2426 - MinusLogProbMetric: 27.2426 - val_loss: 27.7283 - val_MinusLogProbMetric: 27.7283 - lr: 6.2500e-05 - 16s/epoch - 80ms/step
Epoch 424/1000
2023-09-09 11:49:32.285 
Epoch 424/1000 
	 loss: 27.2423, MinusLogProbMetric: 27.2423, val_loss: 27.7160, val_MinusLogProbMetric: 27.7160

Epoch 424: val_loss improved from 27.71742 to 27.71598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 16s - loss: 27.2423 - MinusLogProbMetric: 27.2423 - val_loss: 27.7160 - val_MinusLogProbMetric: 27.7160 - lr: 6.2500e-05 - 16s/epoch - 80ms/step
Epoch 425/1000
2023-09-09 11:49:47.482 
Epoch 425/1000 
	 loss: 27.2414, MinusLogProbMetric: 27.2414, val_loss: 27.7399, val_MinusLogProbMetric: 27.7399

Epoch 425: val_loss did not improve from 27.71598
196/196 - 15s - loss: 27.2414 - MinusLogProbMetric: 27.2414 - val_loss: 27.7399 - val_MinusLogProbMetric: 27.7399 - lr: 6.2500e-05 - 15s/epoch - 75ms/step
Epoch 426/1000
2023-09-09 11:50:02.394 
Epoch 426/1000 
	 loss: 27.2399, MinusLogProbMetric: 27.2399, val_loss: 27.7300, val_MinusLogProbMetric: 27.7300

Epoch 426: val_loss did not improve from 27.71598
196/196 - 15s - loss: 27.2399 - MinusLogProbMetric: 27.2399 - val_loss: 27.7300 - val_MinusLogProbMetric: 27.7300 - lr: 6.2500e-05 - 15s/epoch - 76ms/step
Epoch 427/1000
2023-09-09 11:50:16.984 
Epoch 427/1000 
	 loss: 27.2411, MinusLogProbMetric: 27.2411, val_loss: 27.7320, val_MinusLogProbMetric: 27.7320

Epoch 427: val_loss did not improve from 27.71598
196/196 - 15s - loss: 27.2411 - MinusLogProbMetric: 27.2411 - val_loss: 27.7320 - val_MinusLogProbMetric: 27.7320 - lr: 6.2500e-05 - 15s/epoch - 74ms/step
Epoch 428/1000
2023-09-09 11:50:32.893 
Epoch 428/1000 
	 loss: 27.2416, MinusLogProbMetric: 27.2416, val_loss: 27.7252, val_MinusLogProbMetric: 27.7252

Epoch 428: val_loss did not improve from 27.71598
196/196 - 16s - loss: 27.2416 - MinusLogProbMetric: 27.2416 - val_loss: 27.7252 - val_MinusLogProbMetric: 27.7252 - lr: 6.2500e-05 - 16s/epoch - 81ms/step
Epoch 429/1000
2023-09-09 11:50:49.469 
Epoch 429/1000 
	 loss: 27.2402, MinusLogProbMetric: 27.2402, val_loss: 27.7187, val_MinusLogProbMetric: 27.7187

Epoch 429: val_loss did not improve from 27.71598
196/196 - 17s - loss: 27.2402 - MinusLogProbMetric: 27.2402 - val_loss: 27.7187 - val_MinusLogProbMetric: 27.7187 - lr: 6.2500e-05 - 17s/epoch - 84ms/step
Epoch 430/1000
2023-09-09 11:51:05.041 
Epoch 430/1000 
	 loss: 27.2409, MinusLogProbMetric: 27.2409, val_loss: 27.7298, val_MinusLogProbMetric: 27.7298

Epoch 430: val_loss did not improve from 27.71598
196/196 - 16s - loss: 27.2409 - MinusLogProbMetric: 27.2409 - val_loss: 27.7298 - val_MinusLogProbMetric: 27.7298 - lr: 6.2500e-05 - 16s/epoch - 79ms/step
Epoch 431/1000
2023-09-09 11:51:21.186 
Epoch 431/1000 
	 loss: 27.2404, MinusLogProbMetric: 27.2404, val_loss: 27.7289, val_MinusLogProbMetric: 27.7289

Epoch 431: val_loss did not improve from 27.71598
196/196 - 16s - loss: 27.2404 - MinusLogProbMetric: 27.2404 - val_loss: 27.7289 - val_MinusLogProbMetric: 27.7289 - lr: 6.2500e-05 - 16s/epoch - 82ms/step
Epoch 432/1000
2023-09-09 11:51:37.537 
Epoch 432/1000 
	 loss: 27.2391, MinusLogProbMetric: 27.2391, val_loss: 27.7235, val_MinusLogProbMetric: 27.7235

Epoch 432: val_loss did not improve from 27.71598
196/196 - 16s - loss: 27.2391 - MinusLogProbMetric: 27.2391 - val_loss: 27.7235 - val_MinusLogProbMetric: 27.7235 - lr: 6.2500e-05 - 16s/epoch - 83ms/step
Epoch 433/1000
2023-09-09 11:51:54.221 
Epoch 433/1000 
	 loss: 27.2397, MinusLogProbMetric: 27.2397, val_loss: 27.7233, val_MinusLogProbMetric: 27.7233

Epoch 433: val_loss did not improve from 27.71598
196/196 - 17s - loss: 27.2397 - MinusLogProbMetric: 27.2397 - val_loss: 27.7233 - val_MinusLogProbMetric: 27.7233 - lr: 6.2500e-05 - 17s/epoch - 85ms/step
Epoch 434/1000
2023-09-09 11:52:10.780 
Epoch 434/1000 
	 loss: 27.2396, MinusLogProbMetric: 27.2396, val_loss: 27.7135, val_MinusLogProbMetric: 27.7135

Epoch 434: val_loss improved from 27.71598 to 27.71353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_179/weights/best_weights.h5
196/196 - 17s - loss: 27.2396 - MinusLogProbMetric: 27.2396 - val_loss: 27.7135 - val_MinusLogProbMetric: 27.7135 - lr: 6.2500e-05 - 17s/epoch - 87ms/step
Epoch 435/1000
2023-09-09 11:52:28.356 
Epoch 435/1000 
	 loss: 27.2396, MinusLogProbMetric: 27.2396, val_loss: 27.7351, val_MinusLogProbMetric: 27.7351

Epoch 435: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2396 - MinusLogProbMetric: 27.2396 - val_loss: 27.7351 - val_MinusLogProbMetric: 27.7351 - lr: 6.2500e-05 - 17s/epoch - 87ms/step
Epoch 436/1000
2023-09-09 11:52:45.669 
Epoch 436/1000 
	 loss: 27.2402, MinusLogProbMetric: 27.2402, val_loss: 27.7301, val_MinusLogProbMetric: 27.7301

Epoch 436: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2402 - MinusLogProbMetric: 27.2402 - val_loss: 27.7301 - val_MinusLogProbMetric: 27.7301 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 437/1000
2023-09-09 11:53:01.927 
Epoch 437/1000 
	 loss: 27.2409, MinusLogProbMetric: 27.2409, val_loss: 27.7181, val_MinusLogProbMetric: 27.7181

Epoch 437: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2409 - MinusLogProbMetric: 27.2409 - val_loss: 27.7181 - val_MinusLogProbMetric: 27.7181 - lr: 6.2500e-05 - 16s/epoch - 83ms/step
Epoch 438/1000
2023-09-09 11:53:19.183 
Epoch 438/1000 
	 loss: 27.2406, MinusLogProbMetric: 27.2406, val_loss: 27.7200, val_MinusLogProbMetric: 27.7200

Epoch 438: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2406 - MinusLogProbMetric: 27.2406 - val_loss: 27.7200 - val_MinusLogProbMetric: 27.7200 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 439/1000
2023-09-09 11:53:36.800 
Epoch 439/1000 
	 loss: 27.2400, MinusLogProbMetric: 27.2400, val_loss: 27.7230, val_MinusLogProbMetric: 27.7230

Epoch 439: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2400 - MinusLogProbMetric: 27.2400 - val_loss: 27.7230 - val_MinusLogProbMetric: 27.7230 - lr: 6.2500e-05 - 18s/epoch - 90ms/step
Epoch 440/1000
2023-09-09 11:53:53.954 
Epoch 440/1000 
	 loss: 27.2402, MinusLogProbMetric: 27.2402, val_loss: 27.7336, val_MinusLogProbMetric: 27.7336

Epoch 440: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2402 - MinusLogProbMetric: 27.2402 - val_loss: 27.7336 - val_MinusLogProbMetric: 27.7336 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 441/1000
2023-09-09 11:54:11.488 
Epoch 441/1000 
	 loss: 27.2402, MinusLogProbMetric: 27.2402, val_loss: 27.7242, val_MinusLogProbMetric: 27.7242

Epoch 441: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2402 - MinusLogProbMetric: 27.2402 - val_loss: 27.7242 - val_MinusLogProbMetric: 27.7242 - lr: 6.2500e-05 - 18s/epoch - 89ms/step
Epoch 442/1000
2023-09-09 11:54:29.752 
Epoch 442/1000 
	 loss: 27.2389, MinusLogProbMetric: 27.2389, val_loss: 27.7222, val_MinusLogProbMetric: 27.7222

Epoch 442: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2389 - MinusLogProbMetric: 27.2389 - val_loss: 27.7222 - val_MinusLogProbMetric: 27.7222 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 443/1000
2023-09-09 11:54:47.103 
Epoch 443/1000 
	 loss: 27.2400, MinusLogProbMetric: 27.2400, val_loss: 27.7338, val_MinusLogProbMetric: 27.7338

Epoch 443: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2400 - MinusLogProbMetric: 27.2400 - val_loss: 27.7338 - val_MinusLogProbMetric: 27.7338 - lr: 6.2500e-05 - 17s/epoch - 89ms/step
Epoch 444/1000
2023-09-09 11:55:04.644 
Epoch 444/1000 
	 loss: 27.2379, MinusLogProbMetric: 27.2379, val_loss: 27.7337, val_MinusLogProbMetric: 27.7337

Epoch 444: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2379 - MinusLogProbMetric: 27.2379 - val_loss: 27.7337 - val_MinusLogProbMetric: 27.7337 - lr: 6.2500e-05 - 18s/epoch - 90ms/step
Epoch 445/1000
2023-09-09 11:55:22.504 
Epoch 445/1000 
	 loss: 27.2416, MinusLogProbMetric: 27.2416, val_loss: 27.7341, val_MinusLogProbMetric: 27.7341

Epoch 445: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2416 - MinusLogProbMetric: 27.2416 - val_loss: 27.7341 - val_MinusLogProbMetric: 27.7341 - lr: 6.2500e-05 - 18s/epoch - 91ms/step
Epoch 446/1000
2023-09-09 11:55:41.145 
Epoch 446/1000 
	 loss: 27.2385, MinusLogProbMetric: 27.2385, val_loss: 27.7481, val_MinusLogProbMetric: 27.7481

Epoch 446: val_loss did not improve from 27.71353
196/196 - 19s - loss: 27.2385 - MinusLogProbMetric: 27.2385 - val_loss: 27.7481 - val_MinusLogProbMetric: 27.7481 - lr: 6.2500e-05 - 19s/epoch - 95ms/step
Epoch 447/1000
2023-09-09 11:55:59.309 
Epoch 447/1000 
	 loss: 27.2401, MinusLogProbMetric: 27.2401, val_loss: 27.7367, val_MinusLogProbMetric: 27.7367

Epoch 447: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2401 - MinusLogProbMetric: 27.2401 - val_loss: 27.7367 - val_MinusLogProbMetric: 27.7367 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 448/1000
2023-09-09 11:56:19.070 
Epoch 448/1000 
	 loss: 27.2392, MinusLogProbMetric: 27.2392, val_loss: 27.7256, val_MinusLogProbMetric: 27.7256

Epoch 448: val_loss did not improve from 27.71353
196/196 - 20s - loss: 27.2392 - MinusLogProbMetric: 27.2392 - val_loss: 27.7256 - val_MinusLogProbMetric: 27.7256 - lr: 6.2500e-05 - 20s/epoch - 101ms/step
Epoch 449/1000
2023-09-09 11:56:37.289 
Epoch 449/1000 
	 loss: 27.2395, MinusLogProbMetric: 27.2395, val_loss: 27.7273, val_MinusLogProbMetric: 27.7273

Epoch 449: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2395 - MinusLogProbMetric: 27.2395 - val_loss: 27.7273 - val_MinusLogProbMetric: 27.7273 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 450/1000
2023-09-09 11:56:55.460 
Epoch 450/1000 
	 loss: 27.2400, MinusLogProbMetric: 27.2400, val_loss: 27.7254, val_MinusLogProbMetric: 27.7254

Epoch 450: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2400 - MinusLogProbMetric: 27.2400 - val_loss: 27.7254 - val_MinusLogProbMetric: 27.7254 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 451/1000
2023-09-09 11:57:13.942 
Epoch 451/1000 
	 loss: 27.2387, MinusLogProbMetric: 27.2387, val_loss: 27.7224, val_MinusLogProbMetric: 27.7224

Epoch 451: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2387 - MinusLogProbMetric: 27.2387 - val_loss: 27.7224 - val_MinusLogProbMetric: 27.7224 - lr: 6.2500e-05 - 18s/epoch - 94ms/step
Epoch 452/1000
2023-09-09 11:57:31.199 
Epoch 452/1000 
	 loss: 27.2382, MinusLogProbMetric: 27.2382, val_loss: 27.7280, val_MinusLogProbMetric: 27.7280

Epoch 452: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2382 - MinusLogProbMetric: 27.2382 - val_loss: 27.7280 - val_MinusLogProbMetric: 27.7280 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 453/1000
2023-09-09 11:57:51.469 
Epoch 453/1000 
	 loss: 27.2389, MinusLogProbMetric: 27.2389, val_loss: 27.7270, val_MinusLogProbMetric: 27.7270

Epoch 453: val_loss did not improve from 27.71353
196/196 - 20s - loss: 27.2389 - MinusLogProbMetric: 27.2389 - val_loss: 27.7270 - val_MinusLogProbMetric: 27.7270 - lr: 6.2500e-05 - 20s/epoch - 103ms/step
Epoch 454/1000
2023-09-09 11:58:10.030 
Epoch 454/1000 
	 loss: 27.2380, MinusLogProbMetric: 27.2380, val_loss: 27.7308, val_MinusLogProbMetric: 27.7308

Epoch 454: val_loss did not improve from 27.71353
196/196 - 19s - loss: 27.2380 - MinusLogProbMetric: 27.2380 - val_loss: 27.7308 - val_MinusLogProbMetric: 27.7308 - lr: 6.2500e-05 - 19s/epoch - 95ms/step
Epoch 455/1000
2023-09-09 11:58:28.611 
Epoch 455/1000 
	 loss: 27.2395, MinusLogProbMetric: 27.2395, val_loss: 27.7302, val_MinusLogProbMetric: 27.7302

Epoch 455: val_loss did not improve from 27.71353
196/196 - 19s - loss: 27.2395 - MinusLogProbMetric: 27.2395 - val_loss: 27.7302 - val_MinusLogProbMetric: 27.7302 - lr: 6.2500e-05 - 19s/epoch - 95ms/step
Epoch 456/1000
2023-09-09 11:58:46.883 
Epoch 456/1000 
	 loss: 27.2379, MinusLogProbMetric: 27.2379, val_loss: 27.7201, val_MinusLogProbMetric: 27.7201

Epoch 456: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2379 - MinusLogProbMetric: 27.2379 - val_loss: 27.7201 - val_MinusLogProbMetric: 27.7201 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 457/1000
2023-09-09 11:59:05.223 
Epoch 457/1000 
	 loss: 27.2381, MinusLogProbMetric: 27.2381, val_loss: 27.7308, val_MinusLogProbMetric: 27.7308

Epoch 457: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2381 - MinusLogProbMetric: 27.2381 - val_loss: 27.7308 - val_MinusLogProbMetric: 27.7308 - lr: 6.2500e-05 - 18s/epoch - 94ms/step
Epoch 458/1000
2023-09-09 11:59:24.363 
Epoch 458/1000 
	 loss: 27.2381, MinusLogProbMetric: 27.2381, val_loss: 27.7276, val_MinusLogProbMetric: 27.7276

Epoch 458: val_loss did not improve from 27.71353
196/196 - 19s - loss: 27.2381 - MinusLogProbMetric: 27.2381 - val_loss: 27.7276 - val_MinusLogProbMetric: 27.7276 - lr: 6.2500e-05 - 19s/epoch - 98ms/step
Epoch 459/1000
2023-09-09 11:59:44.031 
Epoch 459/1000 
	 loss: 27.2372, MinusLogProbMetric: 27.2372, val_loss: 27.7260, val_MinusLogProbMetric: 27.7260

Epoch 459: val_loss did not improve from 27.71353
196/196 - 20s - loss: 27.2372 - MinusLogProbMetric: 27.2372 - val_loss: 27.7260 - val_MinusLogProbMetric: 27.7260 - lr: 6.2500e-05 - 20s/epoch - 100ms/step
Epoch 460/1000
2023-09-09 12:00:03.467 
Epoch 460/1000 
	 loss: 27.2376, MinusLogProbMetric: 27.2376, val_loss: 27.7323, val_MinusLogProbMetric: 27.7323

Epoch 460: val_loss did not improve from 27.71353
196/196 - 19s - loss: 27.2376 - MinusLogProbMetric: 27.2376 - val_loss: 27.7323 - val_MinusLogProbMetric: 27.7323 - lr: 6.2500e-05 - 19s/epoch - 99ms/step
Epoch 461/1000
2023-09-09 12:00:20.188 
Epoch 461/1000 
	 loss: 27.2385, MinusLogProbMetric: 27.2385, val_loss: 27.7310, val_MinusLogProbMetric: 27.7310

Epoch 461: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2385 - MinusLogProbMetric: 27.2385 - val_loss: 27.7310 - val_MinusLogProbMetric: 27.7310 - lr: 6.2500e-05 - 17s/epoch - 85ms/step
Epoch 462/1000
2023-09-09 12:00:34.629 
Epoch 462/1000 
	 loss: 27.2386, MinusLogProbMetric: 27.2386, val_loss: 27.7258, val_MinusLogProbMetric: 27.7258

Epoch 462: val_loss did not improve from 27.71353
196/196 - 14s - loss: 27.2386 - MinusLogProbMetric: 27.2386 - val_loss: 27.7258 - val_MinusLogProbMetric: 27.7258 - lr: 6.2500e-05 - 14s/epoch - 74ms/step
Epoch 463/1000
2023-09-09 12:00:50.412 
Epoch 463/1000 
	 loss: 27.2389, MinusLogProbMetric: 27.2389, val_loss: 27.7415, val_MinusLogProbMetric: 27.7415

Epoch 463: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2389 - MinusLogProbMetric: 27.2389 - val_loss: 27.7415 - val_MinusLogProbMetric: 27.7415 - lr: 6.2500e-05 - 16s/epoch - 80ms/step
Epoch 464/1000
2023-09-09 12:01:06.366 
Epoch 464/1000 
	 loss: 27.2383, MinusLogProbMetric: 27.2383, val_loss: 27.7215, val_MinusLogProbMetric: 27.7215

Epoch 464: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2383 - MinusLogProbMetric: 27.2383 - val_loss: 27.7215 - val_MinusLogProbMetric: 27.7215 - lr: 6.2500e-05 - 16s/epoch - 81ms/step
Epoch 465/1000
2023-09-09 12:01:23.155 
Epoch 465/1000 
	 loss: 27.2378, MinusLogProbMetric: 27.2378, val_loss: 27.7335, val_MinusLogProbMetric: 27.7335

Epoch 465: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2378 - MinusLogProbMetric: 27.2378 - val_loss: 27.7335 - val_MinusLogProbMetric: 27.7335 - lr: 6.2500e-05 - 17s/epoch - 86ms/step
Epoch 466/1000
2023-09-09 12:01:40.589 
Epoch 466/1000 
	 loss: 27.2394, MinusLogProbMetric: 27.2394, val_loss: 27.7429, val_MinusLogProbMetric: 27.7429

Epoch 466: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2394 - MinusLogProbMetric: 27.2394 - val_loss: 27.7429 - val_MinusLogProbMetric: 27.7429 - lr: 6.2500e-05 - 17s/epoch - 89ms/step
Epoch 467/1000
2023-09-09 12:01:57.302 
Epoch 467/1000 
	 loss: 27.2382, MinusLogProbMetric: 27.2382, val_loss: 27.7355, val_MinusLogProbMetric: 27.7355

Epoch 467: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2382 - MinusLogProbMetric: 27.2382 - val_loss: 27.7355 - val_MinusLogProbMetric: 27.7355 - lr: 6.2500e-05 - 17s/epoch - 85ms/step
Epoch 468/1000
2023-09-09 12:02:15.044 
Epoch 468/1000 
	 loss: 27.2395, MinusLogProbMetric: 27.2395, val_loss: 27.7341, val_MinusLogProbMetric: 27.7341

Epoch 468: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2395 - MinusLogProbMetric: 27.2395 - val_loss: 27.7341 - val_MinusLogProbMetric: 27.7341 - lr: 6.2500e-05 - 18s/epoch - 90ms/step
Epoch 469/1000
2023-09-09 12:02:32.379 
Epoch 469/1000 
	 loss: 27.2374, MinusLogProbMetric: 27.2374, val_loss: 27.7286, val_MinusLogProbMetric: 27.7286

Epoch 469: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2374 - MinusLogProbMetric: 27.2374 - val_loss: 27.7286 - val_MinusLogProbMetric: 27.7286 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 470/1000
2023-09-09 12:02:49.472 
Epoch 470/1000 
	 loss: 27.2401, MinusLogProbMetric: 27.2401, val_loss: 27.7258, val_MinusLogProbMetric: 27.7258

Epoch 470: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2401 - MinusLogProbMetric: 27.2401 - val_loss: 27.7258 - val_MinusLogProbMetric: 27.7258 - lr: 6.2500e-05 - 17s/epoch - 87ms/step
Epoch 471/1000
2023-09-09 12:03:06.640 
Epoch 471/1000 
	 loss: 27.2390, MinusLogProbMetric: 27.2390, val_loss: 27.7315, val_MinusLogProbMetric: 27.7315

Epoch 471: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2390 - MinusLogProbMetric: 27.2390 - val_loss: 27.7315 - val_MinusLogProbMetric: 27.7315 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 472/1000
2023-09-09 12:03:23.360 
Epoch 472/1000 
	 loss: 27.2380, MinusLogProbMetric: 27.2380, val_loss: 27.7226, val_MinusLogProbMetric: 27.7226

Epoch 472: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2380 - MinusLogProbMetric: 27.2380 - val_loss: 27.7226 - val_MinusLogProbMetric: 27.7226 - lr: 6.2500e-05 - 17s/epoch - 85ms/step
Epoch 473/1000
2023-09-09 12:03:41.578 
Epoch 473/1000 
	 loss: 27.2376, MinusLogProbMetric: 27.2376, val_loss: 27.7331, val_MinusLogProbMetric: 27.7331

Epoch 473: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2376 - MinusLogProbMetric: 27.2376 - val_loss: 27.7331 - val_MinusLogProbMetric: 27.7331 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 474/1000
2023-09-09 12:03:59.809 
Epoch 474/1000 
	 loss: 27.2369, MinusLogProbMetric: 27.2369, val_loss: 27.7230, val_MinusLogProbMetric: 27.7230

Epoch 474: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2369 - MinusLogProbMetric: 27.2369 - val_loss: 27.7230 - val_MinusLogProbMetric: 27.7230 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 475/1000
2023-09-09 12:04:17.203 
Epoch 475/1000 
	 loss: 27.2365, MinusLogProbMetric: 27.2365, val_loss: 27.7365, val_MinusLogProbMetric: 27.7365

Epoch 475: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2365 - MinusLogProbMetric: 27.2365 - val_loss: 27.7365 - val_MinusLogProbMetric: 27.7365 - lr: 6.2500e-05 - 17s/epoch - 89ms/step
Epoch 476/1000
2023-09-09 12:04:34.267 
Epoch 476/1000 
	 loss: 27.2377, MinusLogProbMetric: 27.2377, val_loss: 27.7236, val_MinusLogProbMetric: 27.7236

Epoch 476: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2377 - MinusLogProbMetric: 27.2377 - val_loss: 27.7236 - val_MinusLogProbMetric: 27.7236 - lr: 6.2500e-05 - 17s/epoch - 87ms/step
Epoch 477/1000
2023-09-09 12:04:51.296 
Epoch 477/1000 
	 loss: 27.2370, MinusLogProbMetric: 27.2370, val_loss: 27.7321, val_MinusLogProbMetric: 27.7321

Epoch 477: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2370 - MinusLogProbMetric: 27.2370 - val_loss: 27.7321 - val_MinusLogProbMetric: 27.7321 - lr: 6.2500e-05 - 17s/epoch - 87ms/step
Epoch 478/1000
2023-09-09 12:05:08.756 
Epoch 478/1000 
	 loss: 27.2371, MinusLogProbMetric: 27.2371, val_loss: 27.7215, val_MinusLogProbMetric: 27.7215

Epoch 478: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2371 - MinusLogProbMetric: 27.2371 - val_loss: 27.7215 - val_MinusLogProbMetric: 27.7215 - lr: 6.2500e-05 - 17s/epoch - 89ms/step
Epoch 479/1000
2023-09-09 12:05:26.200 
Epoch 479/1000 
	 loss: 27.2359, MinusLogProbMetric: 27.2359, val_loss: 27.7351, val_MinusLogProbMetric: 27.7351

Epoch 479: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2359 - MinusLogProbMetric: 27.2359 - val_loss: 27.7351 - val_MinusLogProbMetric: 27.7351 - lr: 6.2500e-05 - 17s/epoch - 89ms/step
Epoch 480/1000
2023-09-09 12:05:43.432 
Epoch 480/1000 
	 loss: 27.2374, MinusLogProbMetric: 27.2374, val_loss: 27.7197, val_MinusLogProbMetric: 27.7197

Epoch 480: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2374 - MinusLogProbMetric: 27.2374 - val_loss: 27.7197 - val_MinusLogProbMetric: 27.7197 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 481/1000
2023-09-09 12:06:00.668 
Epoch 481/1000 
	 loss: 27.2374, MinusLogProbMetric: 27.2374, val_loss: 27.7322, val_MinusLogProbMetric: 27.7322

Epoch 481: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2374 - MinusLogProbMetric: 27.2374 - val_loss: 27.7322 - val_MinusLogProbMetric: 27.7322 - lr: 6.2500e-05 - 17s/epoch - 88ms/step
Epoch 482/1000
2023-09-09 12:06:18.866 
Epoch 482/1000 
	 loss: 27.2370, MinusLogProbMetric: 27.2370, val_loss: 27.7355, val_MinusLogProbMetric: 27.7355

Epoch 482: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2370 - MinusLogProbMetric: 27.2370 - val_loss: 27.7355 - val_MinusLogProbMetric: 27.7355 - lr: 6.2500e-05 - 18s/epoch - 93ms/step
Epoch 483/1000
2023-09-09 12:06:37.751 
Epoch 483/1000 
	 loss: 27.2380, MinusLogProbMetric: 27.2380, val_loss: 27.7457, val_MinusLogProbMetric: 27.7457

Epoch 483: val_loss did not improve from 27.71353
196/196 - 19s - loss: 27.2380 - MinusLogProbMetric: 27.2380 - val_loss: 27.7457 - val_MinusLogProbMetric: 27.7457 - lr: 6.2500e-05 - 19s/epoch - 96ms/step
Epoch 484/1000
2023-09-09 12:06:55.395 
Epoch 484/1000 
	 loss: 27.2368, MinusLogProbMetric: 27.2368, val_loss: 27.7265, val_MinusLogProbMetric: 27.7265

Epoch 484: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2368 - MinusLogProbMetric: 27.2368 - val_loss: 27.7265 - val_MinusLogProbMetric: 27.7265 - lr: 6.2500e-05 - 18s/epoch - 90ms/step
Epoch 485/1000
2023-09-09 12:07:12.513 
Epoch 485/1000 
	 loss: 27.2277, MinusLogProbMetric: 27.2277, val_loss: 27.7286, val_MinusLogProbMetric: 27.7286

Epoch 485: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2277 - MinusLogProbMetric: 27.2277 - val_loss: 27.7286 - val_MinusLogProbMetric: 27.7286 - lr: 3.1250e-05 - 17s/epoch - 87ms/step
Epoch 486/1000
2023-09-09 12:07:30.282 
Epoch 486/1000 
	 loss: 27.2270, MinusLogProbMetric: 27.2270, val_loss: 27.7230, val_MinusLogProbMetric: 27.7230

Epoch 486: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2270 - MinusLogProbMetric: 27.2270 - val_loss: 27.7230 - val_MinusLogProbMetric: 27.7230 - lr: 3.1250e-05 - 18s/epoch - 91ms/step
Epoch 487/1000
2023-09-09 12:07:47.764 
Epoch 487/1000 
	 loss: 27.2280, MinusLogProbMetric: 27.2280, val_loss: 27.7199, val_MinusLogProbMetric: 27.7199

Epoch 487: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2280 - MinusLogProbMetric: 27.2280 - val_loss: 27.7199 - val_MinusLogProbMetric: 27.7199 - lr: 3.1250e-05 - 17s/epoch - 89ms/step
Epoch 488/1000
2023-09-09 12:08:06.068 
Epoch 488/1000 
	 loss: 27.2262, MinusLogProbMetric: 27.2262, val_loss: 27.7238, val_MinusLogProbMetric: 27.7238

Epoch 488: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2262 - MinusLogProbMetric: 27.2262 - val_loss: 27.7238 - val_MinusLogProbMetric: 27.7238 - lr: 3.1250e-05 - 18s/epoch - 93ms/step
Epoch 489/1000
2023-09-09 12:08:23.201 
Epoch 489/1000 
	 loss: 27.2264, MinusLogProbMetric: 27.2264, val_loss: 27.7266, val_MinusLogProbMetric: 27.7266

Epoch 489: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2264 - MinusLogProbMetric: 27.2264 - val_loss: 27.7266 - val_MinusLogProbMetric: 27.7266 - lr: 3.1250e-05 - 17s/epoch - 87ms/step
Epoch 490/1000
2023-09-09 12:08:41.358 
Epoch 490/1000 
	 loss: 27.2273, MinusLogProbMetric: 27.2273, val_loss: 27.7235, val_MinusLogProbMetric: 27.7235

Epoch 490: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2273 - MinusLogProbMetric: 27.2273 - val_loss: 27.7235 - val_MinusLogProbMetric: 27.7235 - lr: 3.1250e-05 - 18s/epoch - 93ms/step
Epoch 491/1000
2023-09-09 12:08:58.539 
Epoch 491/1000 
	 loss: 27.2270, MinusLogProbMetric: 27.2270, val_loss: 27.7252, val_MinusLogProbMetric: 27.7252

Epoch 491: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2270 - MinusLogProbMetric: 27.2270 - val_loss: 27.7252 - val_MinusLogProbMetric: 27.7252 - lr: 3.1250e-05 - 17s/epoch - 88ms/step
Epoch 492/1000
2023-09-09 12:09:15.979 
Epoch 492/1000 
	 loss: 27.2271, MinusLogProbMetric: 27.2271, val_loss: 27.7250, val_MinusLogProbMetric: 27.7250

Epoch 492: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2271 - MinusLogProbMetric: 27.2271 - val_loss: 27.7250 - val_MinusLogProbMetric: 27.7250 - lr: 3.1250e-05 - 17s/epoch - 89ms/step
Epoch 493/1000
2023-09-09 12:09:33.629 
Epoch 493/1000 
	 loss: 27.2269, MinusLogProbMetric: 27.2269, val_loss: 27.7251, val_MinusLogProbMetric: 27.7251

Epoch 493: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2269 - MinusLogProbMetric: 27.2269 - val_loss: 27.7251 - val_MinusLogProbMetric: 27.7251 - lr: 3.1250e-05 - 18s/epoch - 90ms/step
Epoch 494/1000
2023-09-09 12:09:48.131 
Epoch 494/1000 
	 loss: 27.2271, MinusLogProbMetric: 27.2271, val_loss: 27.7217, val_MinusLogProbMetric: 27.7217

Epoch 494: val_loss did not improve from 27.71353
196/196 - 14s - loss: 27.2271 - MinusLogProbMetric: 27.2271 - val_loss: 27.7217 - val_MinusLogProbMetric: 27.7217 - lr: 3.1250e-05 - 14s/epoch - 74ms/step
Epoch 495/1000
2023-09-09 12:10:04.667 
Epoch 495/1000 
	 loss: 27.2271, MinusLogProbMetric: 27.2271, val_loss: 27.7347, val_MinusLogProbMetric: 27.7347

Epoch 495: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2271 - MinusLogProbMetric: 27.2271 - val_loss: 27.7347 - val_MinusLogProbMetric: 27.7347 - lr: 3.1250e-05 - 17s/epoch - 84ms/step
Epoch 496/1000
2023-09-09 12:10:20.526 
Epoch 496/1000 
	 loss: 27.2277, MinusLogProbMetric: 27.2277, val_loss: 27.7285, val_MinusLogProbMetric: 27.7285

Epoch 496: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2277 - MinusLogProbMetric: 27.2277 - val_loss: 27.7285 - val_MinusLogProbMetric: 27.7285 - lr: 3.1250e-05 - 16s/epoch - 81ms/step
Epoch 497/1000
2023-09-09 12:10:36.976 
Epoch 497/1000 
	 loss: 27.2264, MinusLogProbMetric: 27.2264, val_loss: 27.7268, val_MinusLogProbMetric: 27.7268

Epoch 497: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2264 - MinusLogProbMetric: 27.2264 - val_loss: 27.7268 - val_MinusLogProbMetric: 27.7268 - lr: 3.1250e-05 - 16s/epoch - 84ms/step
Epoch 498/1000
2023-09-09 12:10:52.762 
Epoch 498/1000 
	 loss: 27.2277, MinusLogProbMetric: 27.2277, val_loss: 27.7259, val_MinusLogProbMetric: 27.7259

Epoch 498: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2277 - MinusLogProbMetric: 27.2277 - val_loss: 27.7259 - val_MinusLogProbMetric: 27.7259 - lr: 3.1250e-05 - 16s/epoch - 81ms/step
Epoch 499/1000
2023-09-09 12:11:08.270 
Epoch 499/1000 
	 loss: 27.2262, MinusLogProbMetric: 27.2262, val_loss: 27.7268, val_MinusLogProbMetric: 27.7268

Epoch 499: val_loss did not improve from 27.71353
196/196 - 15s - loss: 27.2262 - MinusLogProbMetric: 27.2262 - val_loss: 27.7268 - val_MinusLogProbMetric: 27.7268 - lr: 3.1250e-05 - 15s/epoch - 79ms/step
Epoch 500/1000
2023-09-09 12:11:22.999 
Epoch 500/1000 
	 loss: 27.2262, MinusLogProbMetric: 27.2262, val_loss: 27.7261, val_MinusLogProbMetric: 27.7261

Epoch 500: val_loss did not improve from 27.71353
196/196 - 15s - loss: 27.2262 - MinusLogProbMetric: 27.2262 - val_loss: 27.7261 - val_MinusLogProbMetric: 27.7261 - lr: 3.1250e-05 - 15s/epoch - 75ms/step
Epoch 501/1000
2023-09-09 12:11:38.188 
Epoch 501/1000 
	 loss: 27.2266, MinusLogProbMetric: 27.2266, val_loss: 27.7283, val_MinusLogProbMetric: 27.7283

Epoch 501: val_loss did not improve from 27.71353
196/196 - 15s - loss: 27.2266 - MinusLogProbMetric: 27.2266 - val_loss: 27.7283 - val_MinusLogProbMetric: 27.7283 - lr: 3.1250e-05 - 15s/epoch - 77ms/step
Epoch 502/1000
2023-09-09 12:11:54.183 
Epoch 502/1000 
	 loss: 27.2268, MinusLogProbMetric: 27.2268, val_loss: 27.7252, val_MinusLogProbMetric: 27.7252

Epoch 502: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2268 - MinusLogProbMetric: 27.2268 - val_loss: 27.7252 - val_MinusLogProbMetric: 27.7252 - lr: 3.1250e-05 - 16s/epoch - 82ms/step
Epoch 503/1000
2023-09-09 12:12:10.458 
Epoch 503/1000 
	 loss: 27.2262, MinusLogProbMetric: 27.2262, val_loss: 27.7251, val_MinusLogProbMetric: 27.7251

Epoch 503: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2262 - MinusLogProbMetric: 27.2262 - val_loss: 27.7251 - val_MinusLogProbMetric: 27.7251 - lr: 3.1250e-05 - 16s/epoch - 83ms/step
Epoch 504/1000
2023-09-09 12:12:27.515 
Epoch 504/1000 
	 loss: 27.2261, MinusLogProbMetric: 27.2261, val_loss: 27.7230, val_MinusLogProbMetric: 27.7230

Epoch 504: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2261 - MinusLogProbMetric: 27.2261 - val_loss: 27.7230 - val_MinusLogProbMetric: 27.7230 - lr: 3.1250e-05 - 17s/epoch - 87ms/step
Epoch 505/1000
2023-09-09 12:12:44.309 
Epoch 505/1000 
	 loss: 27.2258, MinusLogProbMetric: 27.2258, val_loss: 27.7252, val_MinusLogProbMetric: 27.7252

Epoch 505: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2258 - MinusLogProbMetric: 27.2258 - val_loss: 27.7252 - val_MinusLogProbMetric: 27.7252 - lr: 3.1250e-05 - 17s/epoch - 86ms/step
Epoch 506/1000
2023-09-09 12:13:01.355 
Epoch 506/1000 
	 loss: 27.2265, MinusLogProbMetric: 27.2265, val_loss: 27.7234, val_MinusLogProbMetric: 27.7234

Epoch 506: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2265 - MinusLogProbMetric: 27.2265 - val_loss: 27.7234 - val_MinusLogProbMetric: 27.7234 - lr: 3.1250e-05 - 17s/epoch - 87ms/step
Epoch 507/1000
2023-09-09 12:13:19.504 
Epoch 507/1000 
	 loss: 27.2261, MinusLogProbMetric: 27.2261, val_loss: 27.7313, val_MinusLogProbMetric: 27.7313

Epoch 507: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2261 - MinusLogProbMetric: 27.2261 - val_loss: 27.7313 - val_MinusLogProbMetric: 27.7313 - lr: 3.1250e-05 - 18s/epoch - 93ms/step
Epoch 508/1000
2023-09-09 12:13:36.866 
Epoch 508/1000 
	 loss: 27.2256, MinusLogProbMetric: 27.2256, val_loss: 27.7212, val_MinusLogProbMetric: 27.7212

Epoch 508: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2256 - MinusLogProbMetric: 27.2256 - val_loss: 27.7212 - val_MinusLogProbMetric: 27.7212 - lr: 3.1250e-05 - 17s/epoch - 89ms/step
Epoch 509/1000
2023-09-09 12:13:53.184 
Epoch 509/1000 
	 loss: 27.2262, MinusLogProbMetric: 27.2262, val_loss: 27.7288, val_MinusLogProbMetric: 27.7288

Epoch 509: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2262 - MinusLogProbMetric: 27.2262 - val_loss: 27.7288 - val_MinusLogProbMetric: 27.7288 - lr: 3.1250e-05 - 16s/epoch - 83ms/step
Epoch 510/1000
2023-09-09 12:14:10.351 
Epoch 510/1000 
	 loss: 27.2257, MinusLogProbMetric: 27.2257, val_loss: 27.7265, val_MinusLogProbMetric: 27.7265

Epoch 510: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2257 - MinusLogProbMetric: 27.2257 - val_loss: 27.7265 - val_MinusLogProbMetric: 27.7265 - lr: 3.1250e-05 - 17s/epoch - 88ms/step
Epoch 511/1000
2023-09-09 12:14:26.801 
Epoch 511/1000 
	 loss: 27.2261, MinusLogProbMetric: 27.2261, val_loss: 27.7269, val_MinusLogProbMetric: 27.7269

Epoch 511: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2261 - MinusLogProbMetric: 27.2261 - val_loss: 27.7269 - val_MinusLogProbMetric: 27.7269 - lr: 3.1250e-05 - 16s/epoch - 84ms/step
Epoch 512/1000
2023-09-09 12:14:43.960 
Epoch 512/1000 
	 loss: 27.2266, MinusLogProbMetric: 27.2266, val_loss: 27.7352, val_MinusLogProbMetric: 27.7352

Epoch 512: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2266 - MinusLogProbMetric: 27.2266 - val_loss: 27.7352 - val_MinusLogProbMetric: 27.7352 - lr: 3.1250e-05 - 17s/epoch - 87ms/step
Epoch 513/1000
2023-09-09 12:15:02.265 
Epoch 513/1000 
	 loss: 27.2262, MinusLogProbMetric: 27.2262, val_loss: 27.7292, val_MinusLogProbMetric: 27.7292

Epoch 513: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2262 - MinusLogProbMetric: 27.2262 - val_loss: 27.7292 - val_MinusLogProbMetric: 27.7292 - lr: 3.1250e-05 - 18s/epoch - 93ms/step
Epoch 514/1000
2023-09-09 12:15:19.718 
Epoch 514/1000 
	 loss: 27.2264, MinusLogProbMetric: 27.2264, val_loss: 27.7245, val_MinusLogProbMetric: 27.7245

Epoch 514: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2264 - MinusLogProbMetric: 27.2264 - val_loss: 27.7245 - val_MinusLogProbMetric: 27.7245 - lr: 3.1250e-05 - 17s/epoch - 89ms/step
Epoch 515/1000
2023-09-09 12:15:37.024 
Epoch 515/1000 
	 loss: 27.2260, MinusLogProbMetric: 27.2260, val_loss: 27.7253, val_MinusLogProbMetric: 27.7253

Epoch 515: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2260 - MinusLogProbMetric: 27.2260 - val_loss: 27.7253 - val_MinusLogProbMetric: 27.7253 - lr: 3.1250e-05 - 17s/epoch - 88ms/step
Epoch 516/1000
2023-09-09 12:15:53.845 
Epoch 516/1000 
	 loss: 27.2258, MinusLogProbMetric: 27.2258, val_loss: 27.7273, val_MinusLogProbMetric: 27.7273

Epoch 516: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2258 - MinusLogProbMetric: 27.2258 - val_loss: 27.7273 - val_MinusLogProbMetric: 27.7273 - lr: 3.1250e-05 - 17s/epoch - 86ms/step
Epoch 517/1000
2023-09-09 12:16:10.756 
Epoch 517/1000 
	 loss: 27.2256, MinusLogProbMetric: 27.2256, val_loss: 27.7197, val_MinusLogProbMetric: 27.7197

Epoch 517: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2256 - MinusLogProbMetric: 27.2256 - val_loss: 27.7197 - val_MinusLogProbMetric: 27.7197 - lr: 3.1250e-05 - 17s/epoch - 86ms/step
Epoch 518/1000
2023-09-09 12:16:28.252 
Epoch 518/1000 
	 loss: 27.2254, MinusLogProbMetric: 27.2254, val_loss: 27.7190, val_MinusLogProbMetric: 27.7190

Epoch 518: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2254 - MinusLogProbMetric: 27.2254 - val_loss: 27.7190 - val_MinusLogProbMetric: 27.7190 - lr: 3.1250e-05 - 17s/epoch - 89ms/step
Epoch 519/1000
2023-09-09 12:16:44.877 
Epoch 519/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 27.7275, val_MinusLogProbMetric: 27.7275

Epoch 519: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 27.7275 - val_MinusLogProbMetric: 27.7275 - lr: 3.1250e-05 - 17s/epoch - 85ms/step
Epoch 520/1000
2023-09-09 12:17:00.853 
Epoch 520/1000 
	 loss: 27.2256, MinusLogProbMetric: 27.2256, val_loss: 27.7280, val_MinusLogProbMetric: 27.7280

Epoch 520: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2256 - MinusLogProbMetric: 27.2256 - val_loss: 27.7280 - val_MinusLogProbMetric: 27.7280 - lr: 3.1250e-05 - 16s/epoch - 81ms/step
Epoch 521/1000
2023-09-09 12:17:19.215 
Epoch 521/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 27.7242, val_MinusLogProbMetric: 27.7242

Epoch 521: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 27.7242 - val_MinusLogProbMetric: 27.7242 - lr: 3.1250e-05 - 18s/epoch - 94ms/step
Epoch 522/1000
2023-09-09 12:17:35.458 
Epoch 522/1000 
	 loss: 27.2251, MinusLogProbMetric: 27.2251, val_loss: 27.7338, val_MinusLogProbMetric: 27.7338

Epoch 522: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2251 - MinusLogProbMetric: 27.2251 - val_loss: 27.7338 - val_MinusLogProbMetric: 27.7338 - lr: 3.1250e-05 - 16s/epoch - 83ms/step
Epoch 523/1000
2023-09-09 12:17:50.934 
Epoch 523/1000 
	 loss: 27.2257, MinusLogProbMetric: 27.2257, val_loss: 27.7261, val_MinusLogProbMetric: 27.7261

Epoch 523: val_loss did not improve from 27.71353
196/196 - 15s - loss: 27.2257 - MinusLogProbMetric: 27.2257 - val_loss: 27.7261 - val_MinusLogProbMetric: 27.7261 - lr: 3.1250e-05 - 15s/epoch - 79ms/step
Epoch 524/1000
2023-09-09 12:18:07.139 
Epoch 524/1000 
	 loss: 27.2252, MinusLogProbMetric: 27.2252, val_loss: 27.7225, val_MinusLogProbMetric: 27.7225

Epoch 524: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2252 - MinusLogProbMetric: 27.2252 - val_loss: 27.7225 - val_MinusLogProbMetric: 27.7225 - lr: 3.1250e-05 - 16s/epoch - 83ms/step
Epoch 525/1000
2023-09-09 12:18:23.626 
Epoch 525/1000 
	 loss: 27.2254, MinusLogProbMetric: 27.2254, val_loss: 27.7249, val_MinusLogProbMetric: 27.7249

Epoch 525: val_loss did not improve from 27.71353
196/196 - 16s - loss: 27.2254 - MinusLogProbMetric: 27.2254 - val_loss: 27.7249 - val_MinusLogProbMetric: 27.7249 - lr: 3.1250e-05 - 16s/epoch - 84ms/step
Epoch 526/1000
2023-09-09 12:18:40.162 
Epoch 526/1000 
	 loss: 27.2252, MinusLogProbMetric: 27.2252, val_loss: 27.7368, val_MinusLogProbMetric: 27.7368

Epoch 526: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2252 - MinusLogProbMetric: 27.2252 - val_loss: 27.7368 - val_MinusLogProbMetric: 27.7368 - lr: 3.1250e-05 - 17s/epoch - 84ms/step
Epoch 527/1000
2023-09-09 12:18:58.143 
Epoch 527/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 27.7251, val_MinusLogProbMetric: 27.7251

Epoch 527: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 27.7251 - val_MinusLogProbMetric: 27.7251 - lr: 3.1250e-05 - 18s/epoch - 92ms/step
Epoch 528/1000
2023-09-09 12:19:15.153 
Epoch 528/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 27.7247, val_MinusLogProbMetric: 27.7247

Epoch 528: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 27.7247 - val_MinusLogProbMetric: 27.7247 - lr: 3.1250e-05 - 17s/epoch - 87ms/step
Epoch 529/1000
2023-09-09 12:19:32.411 
Epoch 529/1000 
	 loss: 27.2259, MinusLogProbMetric: 27.2259, val_loss: 27.7308, val_MinusLogProbMetric: 27.7308

Epoch 529: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2259 - MinusLogProbMetric: 27.2259 - val_loss: 27.7308 - val_MinusLogProbMetric: 27.7308 - lr: 3.1250e-05 - 17s/epoch - 88ms/step
Epoch 530/1000
2023-09-09 12:19:50.585 
Epoch 530/1000 
	 loss: 27.2248, MinusLogProbMetric: 27.2248, val_loss: 27.7241, val_MinusLogProbMetric: 27.7241

Epoch 530: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2248 - MinusLogProbMetric: 27.2248 - val_loss: 27.7241 - val_MinusLogProbMetric: 27.7241 - lr: 3.1250e-05 - 18s/epoch - 93ms/step
Epoch 531/1000
2023-09-09 12:20:08.158 
Epoch 531/1000 
	 loss: 27.2259, MinusLogProbMetric: 27.2259, val_loss: 27.7258, val_MinusLogProbMetric: 27.7258

Epoch 531: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2259 - MinusLogProbMetric: 27.2259 - val_loss: 27.7258 - val_MinusLogProbMetric: 27.7258 - lr: 3.1250e-05 - 18s/epoch - 90ms/step
Epoch 532/1000
2023-09-09 12:20:25.935 
Epoch 532/1000 
	 loss: 27.2253, MinusLogProbMetric: 27.2253, val_loss: 27.7243, val_MinusLogProbMetric: 27.7243

Epoch 532: val_loss did not improve from 27.71353
196/196 - 18s - loss: 27.2253 - MinusLogProbMetric: 27.2253 - val_loss: 27.7243 - val_MinusLogProbMetric: 27.7243 - lr: 3.1250e-05 - 18s/epoch - 91ms/step
Epoch 533/1000
2023-09-09 12:20:43.350 
Epoch 533/1000 
	 loss: 27.2255, MinusLogProbMetric: 27.2255, val_loss: 27.7268, val_MinusLogProbMetric: 27.7268

Epoch 533: val_loss did not improve from 27.71353
196/196 - 17s - loss: 27.2255 - MinusLogProbMetric: 27.2255 - val_loss: 27.7268 - val_MinusLogProbMetric: 27.7268 - lr: 3.1250e-05 - 17s/epoch - 89ms/step
Epoch 534/1000
2023-09-09 12:21:00.669 
Epoch 534/1000 
	 loss: 27.2248, MinusLogProbMetric: 27.2248, val_loss: 27.7284, val_MinusLogProbMetric: 27.7284

Epoch 534: val_loss did not improve from 27.71353
Restoring model weights from the end of the best epoch: 434.
196/196 - 18s - loss: 27.2248 - MinusLogProbMetric: 27.2248 - val_loss: 27.7284 - val_MinusLogProbMetric: 27.7284 - lr: 3.1250e-05 - 18s/epoch - 90ms/step
Epoch 534: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 17.637330709025264 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
KS tests calculation completed in 15.867392990039662 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
SWD metric calculation completed in 8.28553745499812 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
FN metric calculation completed in 10.610440842923708 seconds.
WARNING:root:Too few points to create valid contours
Training succeeded with seed 520.
Model trained in 6794.74 s.

===========
Computing predictions
===========

Computing metrics...
Warning: Batch size too large. Halving batch size to 500000 and retrying.
Warning: Batch size too large. Halving batch size to 250000 and retrying.
Warning: Batch size too large. Halving batch size to 125000 and retrying.
Warning: Batch size too large. Halving batch size to 62500 and retrying.
Warning: Batch size too large. Halving batch size to 31250 and retrying.
Warning: Batch size too large. Halving batch size to 15625 and retrying.
Warning: Batch size too large. Halving batch size to 7812 and retrying.
Warning: Batch size too large. Halving batch size to 500000 and retrying.
Warning: Batch size too large. Halving batch size to 250000 and retrying.
Warning: Batch size too large. Halving batch size to 125000 and retrying.
Warning: Batch size too large. Halving batch size to 62500 and retrying.
Warning: Batch size too large. Halving batch size to 31250 and retrying.
Warning: Batch size too large. Halving batch size to 15625 and retrying.
Warning: Batch size too large. Halving batch size to 7812 and retrying.
Metrics computed in 3881.68 s.
Plots done in 164.68 s.
results.txt saved
results.json saved
Results log saved
Model predictions computed in 4046.35 s.
===========
Run 179/360 done in 10842.90 s.
===========

Directory ../../results/MAFN_new/run_180/ already exists.
Skipping it.
===========
Run 180/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_181/ already exists.
Skipping it.
===========
Run 181/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_182/ already exists.
Skipping it.
===========
Run 182/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_183/ already exists.
Skipping it.
===========
Run 183/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_184/ already exists.
Skipping it.
===========
Run 184/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_185/ already exists.
Skipping it.
===========
Run 185/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_186/ already exists.
Skipping it.
===========
Run 186/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_187/ already exists.
Skipping it.
===========
Run 187/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_188/ already exists.
Skipping it.
===========
Run 188/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_189/ already exists.
Skipping it.
===========
Run 189/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_190/ already exists.
Skipping it.
===========
Run 190/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_191/ already exists.
Skipping it.
===========
Run 191/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_192/ already exists.
Skipping it.
===========
Run 192/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_193/ already exists.
Skipping it.
===========
Run 193/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_194/ already exists.
Skipping it.
===========
Run 194/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_195/ already exists.
Skipping it.
===========
Run 195/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_196/ already exists.
Skipping it.
===========
Run 196/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_197/ already exists.
Skipping it.
===========
Run 197/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_198/ already exists.
Skipping it.
===========
Run 198/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_199/ already exists.
Skipping it.
===========
Run 199/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_200/ already exists.
Skipping it.
===========
Run 200/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_201/ already exists.
Skipping it.
===========
Run 201/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_202/ already exists.
Skipping it.
===========
Run 202/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_203/ already exists.
Skipping it.
===========
Run 203/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_204/ already exists.
Skipping it.
===========
Run 204/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_205/ already exists.
Skipping it.
===========
Run 205/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_206/ already exists.
Skipping it.
===========
Run 206/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_207/ already exists.
Skipping it.
===========
Run 207/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_208/ already exists.
Skipping it.
===========
Run 208/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_209/ already exists.
Skipping it.
===========
Run 209/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_210/ already exists.
Skipping it.
===========
Run 210/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_211/ already exists.
Skipping it.
===========
Run 211/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_212/ already exists.
Skipping it.
===========
Run 212/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_213/ already exists.
Skipping it.
===========
Run 213/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_214/ already exists.
Skipping it.
===========
Run 214/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_215/ already exists.
Skipping it.
===========
Run 215/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_216/ already exists.
Skipping it.
===========
Run 216/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_217/ already exists.
Skipping it.
===========
Run 217/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_218/ already exists.
Skipping it.
===========
Run 218/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_219/ already exists.
Skipping it.
===========
Run 219/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_220/ already exists.
Skipping it.
===========
Run 220/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_221/ already exists.
Skipping it.
===========
Run 221/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_222/ already exists.
Skipping it.
===========
Run 222/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_223/ already exists.
Skipping it.
===========
Run 223/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_224/ already exists.
Skipping it.
===========
Run 224/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_225/ already exists.
Skipping it.
===========
Run 225/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_226/ already exists.
Skipping it.
===========
Run 226/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_227/ already exists.
Skipping it.
===========
Run 227/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_228/ already exists.
Skipping it.
===========
Run 228/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_229/ already exists.
Skipping it.
===========
Run 229/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_230/ already exists.
Skipping it.
===========
Run 230/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_231/ already exists.
Skipping it.
===========
Run 231/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_232/ already exists.
Skipping it.
===========
Run 232/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_233/ already exists.
Skipping it.
===========
Run 233/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_234/ already exists.
Skipping it.
===========
Run 234/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_235/ already exists.
Skipping it.
===========
Run 235/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_236/ already exists.
Skipping it.
===========
Run 236/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_237/ already exists.
Skipping it.
===========
Run 237/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_238/ already exists.
Skipping it.
===========
Run 238/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_239/ already exists.
Skipping it.
===========
Run 239/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_240/ already exists.
Skipping it.
===========
Run 240/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_241/ already exists.
Skipping it.
===========
Run 241/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_242/ already exists.
Skipping it.
===========
Run 242/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_243/ already exists.
Skipping it.
===========
Run 243/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_244/ already exists.
Skipping it.
===========
Run 244/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_245/ already exists.
Skipping it.
===========
Run 245/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_246/ already exists.
Skipping it.
===========
Run 246/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_247/ already exists.
Skipping it.
===========
Run 247/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_248/ already exists.
Skipping it.
===========
Run 248/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_249/ already exists.
Skipping it.
===========
Run 249/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_250/ already exists.
Skipping it.
===========
Run 250/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_251/ already exists.
Skipping it.
===========
Run 251/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_252/ already exists.
Skipping it.
===========
Run 252/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_253/ already exists.
Skipping it.
===========
Run 253/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_254/ already exists.
Skipping it.
===========
Run 254/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_255/ already exists.
Skipping it.
===========
Run 255/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_256/ already exists.
Skipping it.
===========
Run 256/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_257/ already exists.
Skipping it.
===========
Run 257/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_258/ already exists.
Skipping it.
===========
Run 258/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_259/ already exists.
Skipping it.
===========
Run 259/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_260/ already exists.
Skipping it.
===========
Run 260/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_261/ already exists.
Skipping it.
===========
Run 261/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_262/ already exists.
Skipping it.
===========
Run 262/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_263/ already exists.
Skipping it.
===========
Run 263/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_264/ already exists.
Skipping it.
===========
Run 264/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_265/ already exists.
Skipping it.
===========
Run 265/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_266/ already exists.
Skipping it.
===========
Run 266/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_267/ already exists.
Skipping it.
===========
Run 267/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_268/ already exists.
Skipping it.
===========
Run 268/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_269/ already exists.
Skipping it.
===========
Run 269/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_270/ already exists.
Skipping it.
===========
Run 270/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_271/ already exists.
Skipping it.
===========
Run 271/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_272/ already exists.
Skipping it.
===========
Run 272/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_273/ already exists.
Skipping it.
===========
Run 273/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_274/ already exists.
Skipping it.
===========
Run 274/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_275/ already exists.
Skipping it.
===========
Run 275/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_276/ already exists.
Skipping it.
===========
Run 276/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_277/ already exists.
Skipping it.
===========
Run 277/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_278/ already exists.
Skipping it.
===========
Run 278/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_279/ already exists.
Skipping it.
===========
Run 279/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_280/ already exists.
Skipping it.
===========
Run 280/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_281/ already exists.
Skipping it.
===========
Run 281/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_282/ already exists.
Skipping it.
===========
Run 282/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_283/ already exists.
Skipping it.
===========
Run 283/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_284/ already exists.
Skipping it.
===========
Run 284/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_285/ already exists.
Skipping it.
===========
Run 285/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_286/ already exists.
Skipping it.
===========
Run 286/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_287/ already exists.
Skipping it.
===========
Run 287/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_288/ already exists.
Skipping it.
===========
Run 288/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_289/ already exists.
Skipping it.
===========
Run 289/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_290/ already exists.
Skipping it.
===========
Run 290/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_291/ already exists.
Skipping it.
===========
Run 291/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_292/ already exists.
Skipping it.
===========
Run 292/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_293/ already exists.
Skipping it.
===========
Run 293/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_294/ already exists.
Skipping it.
===========
Run 294/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_295/ already exists.
Skipping it.
===========
Run 295/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_296/ already exists.
Skipping it.
===========
Run 296/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_297/ already exists.
Skipping it.
===========
Run 297/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_298/ already exists.
Skipping it.
===========
Run 298/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_299/ already exists.
Skipping it.
===========
Run 299/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_300/ already exists.
Skipping it.
===========
Run 300/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_301/ already exists.
Skipping it.
===========
Run 301/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_302/ already exists.
Skipping it.
===========
Run 302/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_303/ already exists.
Skipping it.
===========
Run 303/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_304/ already exists.
Skipping it.
===========
Run 304/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_305/ already exists.
Skipping it.
===========
Run 305/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_306/ already exists.
Skipping it.
===========
Run 306/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_307/ already exists.
Skipping it.
===========
Run 307/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_308/ already exists.
Skipping it.
===========
Run 308/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_309/ already exists.
Skipping it.
===========
Run 309/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_310/ already exists.
Skipping it.
===========
Run 310/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_311/ already exists.
Skipping it.
===========
Run 311/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_312/ already exists.
Skipping it.
===========
Run 312/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_313/ already exists.
Skipping it.
===========
Run 313/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_314/ already exists.
Skipping it.
===========
Run 314/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_315/ already exists.
Skipping it.
===========
Run 315/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_316/ already exists.
Skipping it.
===========
Run 316/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_317/ already exists.
Skipping it.
===========
Run 317/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_318/ already exists.
Skipping it.
===========
Run 318/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_319/ already exists.
Skipping it.
===========
Run 319/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_320/ already exists.
Skipping it.
===========
Run 320/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_321/ already exists.
Skipping it.
===========
Run 321/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_322/ already exists.
Skipping it.
===========
Run 322/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_323/ already exists.
Skipping it.
===========
Run 323/360 already exists. Skipping it.
===========

===========
Generating train data for run 324.
===========
Train data generated in 2.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_324/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_324/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_324/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_324
self.data_kwargs: {'seed': 0}
self.x_data: [[8.210789   4.3589015  5.1949253  ... 2.1186912  8.28666    6.44192   ]
 [5.776202   0.0413513  4.6274076  ... 4.7782993  6.688914   5.570342  ]
 [5.7263374  6.2561994  5.7377024  ... 8.820506   0.58727944 6.515802  ]
 ...
 [5.7884197  0.56533414 4.7867274  ... 4.878848   6.0022035  4.9900055 ]
 [5.9892163  0.222366   4.8566475  ... 4.5662537  6.433208   4.1020565 ]
 [7.8653784  4.6189337  5.157348   ... 2.6189103  7.542449   6.328924  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f9f542b0580>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f9f542b0b20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9f542b0b20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fa024d21de0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fa04401c5b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fa04401cb50>, <keras.callbacks.ModelCheckpoint object at 0x7fa04401c9a0>, <keras.callbacks.EarlyStopping object at 0x7fa04401cc10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fa04401cc40>, <keras.callbacks.TerminateOnNaN object at 0x7fa04401c9d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_324/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 324/360 with hyperparameters:
timestamp = 2023-09-09 13:28:36.245923
ndims = 1000
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 8.21078873e+00  4.35890150e+00  5.19492531e+00  2.84795880e+00
  6.23694658e+00  2.48332572e+00  6.01452780e+00  1.49544990e+00
  1.56305087e+00  4.33542109e+00  3.63919592e+00  2.33469009e+00
  7.12970674e-01  4.46248204e-01  1.55621916e-01  3.05101776e+00
  4.20721769e+00  7.49161625e+00  3.96869850e+00  9.05481625e+00
  4.47121114e-01  2.05655861e+00  3.52404070e+00  5.90453529e+00
  8.73787975e+00  3.53096032e+00  2.76253247e+00  6.50554955e-01
  8.03491688e+00  3.65929008e+00  4.93096781e+00  1.00386086e+01
  5.29845119e-01  1.22730446e+00  7.25914860e+00  7.58729315e+00
  7.71210337e+00  5.14168453e+00  9.58227444e+00  4.79759741e+00
  6.05360174e+00 -3.22920084e-03  4.30121565e+00  7.28324509e+00
  2.30840397e+00  9.72805214e+00  7.01551247e+00  3.15825868e+00
  7.77309942e+00  1.27091324e+00  9.88113785e+00  5.76226175e-01
  9.93493366e+00  6.19022250e-01  7.72215176e+00  1.45727849e+00
  2.63241100e+00  7.56394577e+00  4.26764488e+00  1.30306756e+00
  4.47696209e+00  4.10884666e+00  6.25260830e+00  7.17153311e+00
  7.79660606e+00  4.86149597e+00  4.01030636e+00  1.48931491e+00
  8.25993347e+00  6.99169493e+00  4.15396929e+00  5.40362930e+00
  6.69921494e+00  1.80113685e+00 -2.28963286e-01  3.53704309e+00
  5.97519684e+00  5.03495884e+00  5.96093559e+00  8.27530861e+00
  5.11110401e+00  1.71916723e-01  1.00485516e+01  5.39971399e+00
  6.97965956e+00  6.67698717e+00  9.53224850e+00  2.60424829e+00
  6.92656088e+00  2.48836160e+00  1.50950837e+00  3.11806774e+00
  3.62086797e+00  9.48242664e+00  2.35715413e+00  1.47683930e+00
  6.57032585e+00  2.31288910e+00  5.07415724e+00  8.22896194e+00
  2.67307043e+00  4.70531940e+00  5.81045723e+00  3.97613907e+00
  1.00836468e+01  6.80917358e+00  7.91102648e+00  9.83973694e+00
  3.97975469e+00  2.81087971e+00  6.61194921e-01  2.09042239e+00
  1.80242181e+00  2.45311332e+00  7.75794411e+00  5.35263777e+00
  6.36038160e+00  8.03104973e+00  2.23086929e+00  3.26186609e+00
  7.10611820e+00  4.36860800e+00  5.38674593e-01  5.03258133e+00
  8.79059315e+00  4.12663126e+00  2.96597481e+00  9.92143154e+00
  6.63155603e+00  2.29087067e+00  9.67418194e-01  8.54019165e+00
  7.32748413e+00  9.67831326e+00  3.08930302e+00  1.43350685e+00
  7.23957300e+00  2.06572795e+00  1.31994998e+00  2.38906145e+00
  8.15470123e+00  4.92610788e+00  4.60461289e-01  4.69264269e+00
  7.11180496e+00  6.15540504e+00  5.58532810e+00  1.97622848e+00
  8.55810928e+00  5.60682297e+00  6.50732088e+00  9.74595356e+00
  3.70203424e+00  8.65994740e+00  3.66820455e-01 -1.19887441e-01
  5.62859249e+00  3.21332264e+00  2.41353798e+00  2.97626567e+00
  3.33519387e+00  7.60388327e+00  5.94638872e+00  7.90076303e+00
  8.14614868e+00  8.43794918e+00  5.11629820e+00  4.45509529e+00
  8.96594524e+00  6.83408117e+00  5.29020429e-01  5.47313118e+00
  3.56114149e+00  9.86122417e+00 -8.47060025e-01  5.97660685e+00
  7.56903505e+00  3.27315211e+00  9.39664459e+00  9.18616390e+00
  9.16251087e+00  3.12576860e-01  5.81769562e+00  2.36430216e+00
  8.91152668e+00  6.80723286e+00  5.49286032e+00  1.25512540e+00
  6.51518726e+00  6.32031965e+00  9.03301430e+00  5.99218893e+00
  9.68340588e+00  9.75206280e+00  6.48233700e+00  5.02287769e+00
  4.40520668e+00  8.71606636e+00  1.15936518e-01  1.15061617e+00
  1.13465822e+00  7.72926044e+00  3.59013176e+00  6.88610840e+00
  6.62146950e+00  3.82466817e+00  5.06336927e+00  6.39645910e+00
  9.77296638e+00  6.23589993e+00  6.13958120e+00  6.46353066e-01
  7.98168039e+00  5.24744987e+00  8.84934807e+00  7.04309762e-01
  4.59585285e+00  2.57471061e+00  2.76045990e+00  7.58923578e+00
  6.31161690e+00  3.93680930e+00  3.41281295e-03  9.66006947e+00
  9.57943439e+00  2.44953036e+00  7.60993719e-01  5.62422276e+00
  9.45851040e+00  9.12294960e+00  5.39434481e+00  2.33969688e+00
  3.87983751e+00  8.94526768e+00  4.94882011e+00  7.46908426e-01
  3.23506546e+00  2.77883267e+00  1.57590222e+00  5.10829020e+00
  9.52500534e+00  6.17966223e+00  1.10281193e+00  5.18856943e-01
  2.46926522e+00  3.78143835e+00  6.96128178e+00  1.00144138e+01
  3.66667557e+00  5.97449923e+00  9.95713520e+00  7.44061518e+00
  6.02692795e+00  7.26248920e-01  3.43354082e+00  1.11553822e+01
  1.09044552e+00  8.32242012e+00  7.46725512e+00  3.50707126e+00
  4.66837549e+00  6.27135944e+00  7.01602077e+00  4.99376917e+00
  1.22947979e+00  6.21938133e+00  4.57749987e+00  9.84504700e+00
  3.30335212e+00  2.10937715e+00  8.59310150e-01  5.95748758e+00
  7.51813459e+00  1.18187451e+00  3.30510187e+00  6.36148548e+00
  4.50776958e+00  4.50498581e+00  1.05220985e+01  9.05682564e+00
  6.02789402e+00  7.13881588e+00  7.25975084e+00  6.13174152e+00
  2.50386310e+00  6.21227741e+00  6.38945484e+00  8.56712151e+00
  3.63186836e-01  9.32480907e+00  5.54315186e+00 -3.98739636e-01
  3.37429929e+00  7.95210361e+00  5.01406813e+00  9.63213921e+00
  3.99637389e+00  2.97415352e+00  8.29203606e+00  6.00829792e+00
  6.21376610e+00  7.23727989e+00  6.71069527e+00  5.01448536e+00
  9.13507938e-01  6.87569332e+00  9.86827469e+00  4.45279694e+00
  8.80540848e+00  5.50182295e+00  6.56988764e+00  8.22664070e+00
  6.11226082e+00  9.90539372e-01  8.70905399e+00  2.35227823e+00
  6.95939922e+00  4.88893890e+00  4.05508137e+00  1.27094746e+00
  7.41610336e+00  9.57014084e-01  1.03208637e+01  8.77536011e+00
  5.40437126e+00  3.66490221e+00  1.92103791e+00  5.05432844e+00
  6.88084650e+00  2.36994123e+00  4.34519386e+00  1.71688604e+00
  3.39162683e+00  1.85334861e+00  9.56473351e+00  6.55594063e+00
  7.24569368e+00  9.57989931e-01  3.30405354e+00  8.91251564e+00
  9.43958402e-01  2.91835928e+00  6.76908159e+00  6.41434669e+00
  8.02711487e+00  8.37068748e+00  1.76286387e+00  2.83487058e+00
  4.26155806e+00 -1.64624956e-02  5.80787063e-02  2.90215087e+00
  4.82958937e+00  2.67308688e+00  8.74459743e+00  3.35418940e+00
  9.50176620e+00  9.24923992e+00  5.82722712e+00  3.35397267e+00
  3.11244607e+00  7.65114069e+00  2.58552289e+00  3.75996971e+00
  5.17049551e+00  5.17774963e+00  9.58946896e+00  2.46264791e+00
  4.03688717e+00  2.72447777e+00  2.75106120e+00  1.24247396e+00
  2.18880701e+00  8.80450439e+00  2.16815996e+00  4.50869846e+00
  9.47779369e+00  6.92755127e+00  5.21426487e+00  7.63384438e+00
  1.33627713e+00  5.05314064e+00  8.01588058e-01  2.33411765e+00
  2.04597259e+00  3.71735334e+00  3.99399781e+00  9.83961201e+00
  2.91163659e+00  3.73780441e+00  7.86999273e+00  7.62870979e+00
  7.85087109e+00  2.87814641e+00  5.46488953e+00  1.44017196e+00
  2.16978312e+00  4.34651899e+00  7.40160513e+00  5.01842690e+00
  8.79978561e+00  8.01273918e+00  6.23900414e-01  5.34396315e+00
  5.84303570e+00  9.49287987e+00 -2.03253716e-01  5.55819988e+00
  2.56452465e+00  5.52358687e-01  5.80285740e+00  8.33415508e+00
  8.18197632e+00 -8.59678984e-02  3.97874522e+00  4.08348382e-01
  2.77977419e+00  2.01314735e+00  2.23359156e+00  3.15110850e+00
  3.16415262e+00  9.69314194e+00  3.46537042e+00  8.90620518e+00
  1.80000591e+00  6.60689974e+00 -7.20865309e-01  4.78756142e+00
 -1.09752685e-01  9.03241920e+00  8.91885853e+00  6.74625039e-01
  9.64687729e+00  5.28304863e+00  3.44711399e+00  9.08611012e+00
  3.02275133e+00  4.11477327e+00  1.01315606e+00  7.55140066e+00
  7.66734123e-01  4.76552486e+00  6.70099640e+00  7.79399300e+00
  1.63697755e+00  6.94904470e+00  6.56399965e+00  4.96204281e+00
  5.89378309e+00  5.83356380e-01  6.12774992e+00  4.05969334e+00
  8.60876942e+00  3.77930760e+00  5.77260780e+00  4.28877592e+00
  9.17067528e+00  2.55027103e+00  1.33500242e+00  8.23266697e+00
  4.27657557e+00  5.92882109e+00  3.65143597e-01  1.05127277e+01
  9.12497520e+00  4.73808956e+00  1.15415120e+00  4.24439526e+00
  9.80254364e+00  7.13764381e+00  2.21075535e+00  4.46259594e+00
  9.60248947e+00  6.37608194e+00  3.89600515e-01  6.30370569e+00
  2.47006059e+00  9.89494145e-01  4.10520649e+00  5.44608831e+00
  5.12012959e-01  4.04597473e+00  5.92461109e+00  8.09785938e+00
  5.32709646e+00  5.15376234e+00  7.13395691e+00  8.32365990e+00
  4.20349646e+00  6.05286407e+00  6.26060724e+00  6.60055876e-03
  8.09364128e+00  3.49219418e+00  7.10890245e+00  4.91867685e+00
  7.30661583e+00  6.67151976e+00  2.00603342e+00  6.53424406e+00
  8.17309189e+00  2.37493563e+00  7.19742537e+00  6.21698093e+00
  1.02654848e+01  4.02459478e+00  2.00096679e+00  3.60146332e+00
  2.72641587e+00  7.55267096e+00  7.33097744e+00  6.48247480e+00
  1.84161115e+00  8.67273045e+00  1.03891554e+01  5.11711931e+00
  6.14349842e+00  4.63130045e+00  1.04338384e+00  3.16097307e+00
  2.48957181e+00  2.08219433e+00  9.02428818e+00  7.58692598e+00
  3.53382182e+00  2.87967825e+00  9.43597221e+00  1.85656917e+00
  5.57112312e+00  9.00374889e-01  1.43457806e+00  5.70213127e+00
  6.18463993e+00  5.21287870e+00  8.30662155e+00  9.78884792e+00
  8.07082462e+00  4.94385624e+00  9.57595062e+00  4.81561422e-01
  5.93677044e+00  6.03051233e+00  1.27146268e+00  9.59977341e+00
  3.53188229e+00  5.92599010e+00  8.90297794e+00  5.73115826e+00
  8.29036653e-01  6.28442287e+00  4.10012245e+00  8.16020679e+00
  9.57242489e+00  8.73237801e+00  3.82743692e+00  7.33982182e+00
  9.39437389e+00  5.32039928e+00  4.49351358e+00  7.41679072e-01
  7.33104944e+00  8.68616486e+00  2.54391122e+00  2.88136458e+00
  5.05854321e+00  4.12581384e-01  7.20838428e-01  4.39249325e+00
  2.12579203e+00  5.69973421e+00  3.94032979e+00  1.01283722e+01
  9.79717064e+00  2.52009940e+00  8.86107635e+00  8.79512429e-01
  4.84236622e+00  1.72291732e+00  3.82897258e+00  6.94665861e+00
  4.18651247e+00  8.54575062e+00  7.19269276e+00  9.72444630e+00
  5.51988173e+00  8.01771069e+00  3.28205919e+00  3.97813463e+00
  4.18136644e+00  5.16412115e+00  1.75697100e+00  5.15417719e+00
  1.50469279e+00  9.25226021e+00  4.35976595e-01  4.19854736e+00
  3.64054823e+00  1.06782198e-01  8.30288601e+00  7.06305122e+00
  1.87979758e+00  6.35672998e+00  8.16109753e+00  6.51096630e+00
  5.61127961e-01  6.76316595e+00  6.57550716e+00  7.76742983e+00
  5.40051842e+00  4.99695921e+00  1.05247574e+01  7.60159779e+00
  5.60209799e+00  3.34395909e+00  9.19912910e+00  4.68194294e+00
  1.00466835e+00  5.51672554e+00  8.06357193e+00  4.60604143e+00
  7.99039245e-01  6.11217165e+00  9.98776340e+00  7.64210176e+00
  6.11796856e+00  9.89960003e+00  7.96584511e+00  6.26075697e+00
  4.63107634e+00  7.38753414e+00  6.49842548e+00  7.23403335e-01
  7.84123564e+00  3.43258095e+00  5.87750053e+00  8.16937256e+00
  4.84820747e+00  1.59829724e+00  4.18775225e+00  1.30482543e+00
  6.89824677e+00  9.70094967e+00  7.83443594e+00  4.79529858e+00
  6.26568747e+00  3.08977580e+00  5.72115278e+00  2.92952847e+00
  7.45006084e+00  2.38916183e+00  9.71957111e+00  6.54768562e+00
  6.25139415e-01 -2.21325979e-02  1.01162004e+01  1.64097524e+00
  3.30466270e+00  2.32712507e+00  9.82605267e+00  5.42485619e+00
  4.57557726e+00  5.65397930e+00  2.95226574e+00  6.00716829e+00
  4.69566488e+00  1.86493647e+00  5.55787706e+00  4.29599905e+00
  8.97355652e+00  4.49551582e+00  5.21416712e+00  7.87060261e+00
  4.67125940e+00  9.76075363e+00  8.19973755e+00  8.41541100e+00
  9.90537739e+00  2.93996453e+00  5.85271358e+00  7.30432892e+00
  7.22648525e+00  9.17044544e+00  2.83920431e+00  3.20150447e+00
  8.31029034e+00  7.78128242e+00  2.25734663e+00  5.91900253e+00
  7.87056684e+00  3.05571938e+00  3.93086529e+00  5.76315022e+00
  5.30861235e+00  7.55313301e+00 -1.47460684e-01  4.09288597e+00
  1.74171937e+00  4.78674221e+00  7.88887119e+00  7.54741371e-01
  1.54648781e+00  7.28928089e+00  7.57703447e+00  5.61207056e+00
  9.85756111e+00  4.10112953e+00  1.57129598e+00  6.85865021e+00
  6.33887672e+00  6.33726215e+00  5.35512924e+00  8.78589249e+00
  8.93958569e-01 -1.22548640e-01  1.54779208e+00  2.83668208e+00
  5.92615891e+00  5.92532015e+00  2.29801345e+00  4.49938583e+00
  4.36498499e+00  5.37602377e+00  8.03752041e+00  5.66677046e+00
  8.83142376e+00  1.65858328e+00  5.42761660e+00  8.26859760e+00
  1.79626775e+00  4.40248251e+00  5.96399355e+00  2.07387352e+00
  7.58685493e+00  6.19924068e-01  9.63695049e+00  4.80245113e+00
  2.84362292e+00  1.11408031e+00  4.65407324e+00  6.60547924e+00
  1.22691751e+00  3.05578947e+00  1.09281607e+01  6.40130091e+00
  2.10371280e+00  1.27200794e+00  8.89848232e+00  2.80182910e+00
  2.19280529e+00  1.05606747e+01  2.94065785e+00  1.69049454e+00
  5.48146868e+00  4.31994724e+00  9.06744099e+00  1.15592277e+00
 -7.79310703e-01  1.04373407e+00  8.86706924e+00  4.28514481e+00
  1.81781006e+00  5.78969526e+00  5.54994345e+00  2.30405951e+00
  5.54053879e+00  2.74468279e+00  2.27062321e+00  4.07878542e+00
  2.84625888e+00  2.04214787e+00  8.94879723e+00  6.88817441e-01
  2.71495771e+00  3.84625363e+00  2.55016398e+00  2.09636283e+00
  7.99209976e+00  3.07564521e+00  1.11745620e+00  5.30355263e+00
  9.93744850e-01  2.74349833e+00  8.41612911e+00  4.34795761e+00
  6.55439472e+00  6.15104818e+00  5.19303381e-01  6.36637783e+00
  8.49066544e+00  8.24014544e-01  6.90668011e+00  7.08179665e+00
  9.01312637e+00  3.36554408e+00  2.87336349e+00  6.86589861e+00
 -2.01100767e-01  7.31349230e+00  8.66450596e+00  4.29815626e+00
  3.38823771e+00  3.01338983e+00  8.08154404e-01  1.78297210e+00
  9.61242580e+00  8.81327534e+00  2.97019339e+00  8.63016319e+00
  3.02107668e+00  2.72670150e+00  2.13139439e+00  8.94742870e+00
  9.83511925e+00  3.19685555e+00  6.81030130e+00  3.50014639e+00
  1.05297625e+00  4.07330465e+00  5.53509521e+00  1.18256664e+00
  8.23826122e+00  5.53470898e+00  2.24809480e+00  5.40734196e+00
  7.84483671e+00  2.75392199e+00  7.62425852e+00  4.27786589e+00
  9.72611332e+00  8.83753872e+00  3.43726850e+00  1.27960294e-01
  7.29834032e+00  2.04023504e+00  1.30689442e+00  9.04135704e+00
  1.00866823e+01  2.58009624e+00  3.01804805e+00  1.68803823e+00
  4.80738878e+00  8.68211079e+00  6.73749733e+00  8.63989162e+00
  2.21114421e+00  7.04848576e+00  9.34753704e+00  2.76184368e+00
  6.90854836e+00  4.05415773e+00  3.81649780e+00  9.40583229e+00
  4.05083704e+00  8.92635822e-01  3.82358503e+00  4.43289518e+00
  7.26910877e+00  8.82122993e+00  8.22476578e+00  9.33980370e+00
  4.71105146e+00  6.95373154e+00  3.06833911e+00 -4.22824502e-01
  3.65492988e+00  6.86570597e+00  2.05108023e+00  8.74809170e+00
  8.31052685e+00  1.87380075e+00  2.81262565e+00  8.66123104e+00
  3.31760716e+00  3.31471944e+00  3.48811197e+00  7.28613329e+00
 -9.66772735e-02 -7.69796014e-01  6.32836771e+00  3.86802888e+00
  5.99920893e+00  2.40654874e+00  9.12174988e+00  8.34914875e+00
  7.07096219e-01  7.57762194e+00  9.58280849e+00  8.65084171e+00
  6.67687321e+00  1.21811235e+00  5.86707783e+00  3.30112028e+00
  1.01402960e+01  7.17766523e+00  5.74374008e+00  6.99432039e+00
  4.40368319e+00  2.86542940e+00  9.42987251e+00  3.30265784e+00
  3.83230895e-01  4.20491457e+00  8.71614361e+00  7.57816601e+00
  9.77553654e+00  6.46106720e-01  9.44217777e+00  4.46305180e+00
  4.83198547e+00  3.06845570e+00  1.06641746e+00  4.49923372e+00
  7.57343435e+00  5.86748123e-03  1.58664107e+00  5.31323195e-01
  8.55088329e+00  6.48872995e+00  1.54820132e+00  7.71537733e+00
  8.20388412e+00  9.80402565e+00  9.79537868e+00 -8.38677704e-01
  5.94942427e+00  2.88661289e+00  5.26643085e+00  3.18921828e+00
  7.79481411e-01  4.42821693e+00  5.33531380e+00  9.32881355e+00
  2.65765786e-02  3.08747840e+00  8.52437019e+00  6.85948515e+00
  8.21984768e+00  6.24451685e+00  7.30141687e+00  4.90936804e+00
  2.20824790e+00  8.48395348e+00  5.67844057e+00  1.24840343e+00
  8.05260468e+00  9.64081573e+00  3.07099938e+00  3.18240905e+00
  9.12044704e-01  5.96499348e+00  9.19813156e+00  3.36040854e-01
  4.73263264e+00  1.30067658e+00  5.19609153e-01  6.70743704e+00
  8.23411751e+00  3.52812862e+00  8.22581482e+00  1.19572031e+00
  2.63071656e+00  9.79853821e+00  4.98861170e+00  6.00117397e+00
  3.19664389e-01  1.32244027e+00  6.05848253e-01  7.35753870e+00
  1.13696325e+00  8.69066429e+00  3.58892393e+00  7.88068628e+00
  3.04448676e+00  2.65313506e+00  9.56735551e-01  2.75976801e+00
  4.98727465e+00  7.08365965e+00  5.05365181e+00  9.17652988e+00
  5.92350817e+00  1.26345801e+00  3.95679140e+00  7.46081781e+00
  2.64605379e+00  6.97033882e+00  5.93741179e+00  1.01776525e-01
  7.47330189e+00  5.93837404e+00  8.12254488e-01  3.60206747e+00
  3.23938537e+00  8.24193764e+00  3.56294060e+00  9.41426849e+00
  5.64243269e+00  2.11869121e+00  8.28666019e+00  6.44191980e+00]
Epoch 1/1000
2023-09-09 13:30:16.562 
Epoch 1/1000 
	 loss: 1553.1256, MinusLogProbMetric: 1553.1256, val_loss: 667.2691, val_MinusLogProbMetric: 667.2691

Epoch 1: val_loss improved from inf to 667.26910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 101s - loss: 1553.1256 - MinusLogProbMetric: 1553.1256 - val_loss: 667.2691 - val_MinusLogProbMetric: 667.2691 - lr: 0.0010 - 101s/epoch - 514ms/step
Epoch 2/1000
2023-09-09 13:30:35.109 
Epoch 2/1000 
	 loss: 575.3978, MinusLogProbMetric: 575.3978, val_loss: 533.0767, val_MinusLogProbMetric: 533.0767

Epoch 2: val_loss improved from 667.26910 to 533.07672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 575.3978 - MinusLogProbMetric: 575.3978 - val_loss: 533.0767 - val_MinusLogProbMetric: 533.0767 - lr: 0.0010 - 18s/epoch - 93ms/step
Epoch 3/1000
2023-09-09 13:30:52.899 
Epoch 3/1000 
	 loss: 513.0021, MinusLogProbMetric: 513.0021, val_loss: 500.8269, val_MinusLogProbMetric: 500.8269

Epoch 3: val_loss improved from 533.07672 to 500.82693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 513.0021 - MinusLogProbMetric: 513.0021 - val_loss: 500.8269 - val_MinusLogProbMetric: 500.8269 - lr: 0.0010 - 18s/epoch - 92ms/step
Epoch 4/1000
2023-09-09 13:31:11.059 
Epoch 4/1000 
	 loss: 494.7870, MinusLogProbMetric: 494.7870, val_loss: 481.9696, val_MinusLogProbMetric: 481.9696

Epoch 4: val_loss improved from 500.82693 to 481.96957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 494.7870 - MinusLogProbMetric: 494.7870 - val_loss: 481.9696 - val_MinusLogProbMetric: 481.9696 - lr: 0.0010 - 18s/epoch - 91ms/step
Epoch 5/1000
2023-09-09 13:31:29.827 
Epoch 5/1000 
	 loss: 482.5584, MinusLogProbMetric: 482.5584, val_loss: 470.2126, val_MinusLogProbMetric: 470.2126

Epoch 5: val_loss improved from 481.96957 to 470.21255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 19s - loss: 482.5584 - MinusLogProbMetric: 482.5584 - val_loss: 470.2126 - val_MinusLogProbMetric: 470.2126 - lr: 0.0010 - 19s/epoch - 95ms/step
Epoch 6/1000
2023-09-09 13:31:50.480 
Epoch 6/1000 
	 loss: 474.6929, MinusLogProbMetric: 474.6929, val_loss: 470.9580, val_MinusLogProbMetric: 470.9580

Epoch 6: val_loss did not improve from 470.21255
196/196 - 20s - loss: 474.6929 - MinusLogProbMetric: 474.6929 - val_loss: 470.9580 - val_MinusLogProbMetric: 470.9580 - lr: 0.0010 - 20s/epoch - 102ms/step
Epoch 7/1000
2023-09-09 13:32:10.303 
Epoch 7/1000 
	 loss: 467.8078, MinusLogProbMetric: 467.8078, val_loss: 461.6903, val_MinusLogProbMetric: 461.6903

Epoch 7: val_loss improved from 470.21255 to 461.69028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 21s - loss: 467.8078 - MinusLogProbMetric: 467.8078 - val_loss: 461.6903 - val_MinusLogProbMetric: 461.6903 - lr: 0.0010 - 21s/epoch - 106ms/step
Epoch 8/1000
2023-09-09 13:32:31.138 
Epoch 8/1000 
	 loss: 460.3419, MinusLogProbMetric: 460.3419, val_loss: 459.5035, val_MinusLogProbMetric: 459.5035

Epoch 8: val_loss improved from 461.69028 to 459.50351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 21s - loss: 460.3419 - MinusLogProbMetric: 460.3419 - val_loss: 459.5035 - val_MinusLogProbMetric: 459.5035 - lr: 0.0010 - 21s/epoch - 107ms/step
Epoch 9/1000
2023-09-09 13:32:51.050 
Epoch 9/1000 
	 loss: 450.5688, MinusLogProbMetric: 450.5688, val_loss: 476.7940, val_MinusLogProbMetric: 476.7940

Epoch 9: val_loss did not improve from 459.50351
196/196 - 19s - loss: 450.5688 - MinusLogProbMetric: 450.5688 - val_loss: 476.7940 - val_MinusLogProbMetric: 476.7940 - lr: 0.0010 - 19s/epoch - 97ms/step
Epoch 10/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 36: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-09 13:32:56.068 
Epoch 10/1000 
	 loss: inf, MinusLogProbMetric: 3534741259198166820855808985661440.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 10: val_loss did not improve from 459.50351
196/196 - 5s - loss: inf - MinusLogProbMetric: 3534741259198166820855808985661440.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 5s/epoch - 25ms/step
The loss history contains Inf values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 324.
===========
Train data generated in 2.53 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_324/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_324/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_324/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_324
self.data_kwargs: {'seed': 0}
self.x_data: [[8.210789   4.3589015  5.1949253  ... 2.1186912  8.28666    6.44192   ]
 [5.776202   0.0413513  4.6274076  ... 4.7782993  6.688914   5.570342  ]
 [5.7263374  6.2561994  5.7377024  ... 8.820506   0.58727944 6.515802  ]
 ...
 [5.7884197  0.56533414 4.7867274  ... 4.878848   6.0022035  4.9900055 ]
 [5.9892163  0.222366   4.8566475  ... 4.5662537  6.433208   4.1020565 ]
 [7.8653784  4.6189337  5.157348   ... 2.6189103  7.542449   6.328924  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  9018400   
 yer)                                                            
                                                                 
=================================================================
Total params: 9,018,400
Trainable params: 9,018,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7fa025808250>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fa38028f520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fa38028f520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f9f194e5ff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f9f194b62c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f9f194b67a0>, <keras.callbacks.ModelCheckpoint object at 0x7f9f194b6860>, <keras.callbacks.EarlyStopping object at 0x7f9f194b6ad0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f9f194b6b00>, <keras.callbacks.TerminateOnNaN object at 0x7f9f194b6740>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 324/360 with hyperparameters:
timestamp = 2023-09-09 13:33:05.211417
ndims = 1000
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 256-256-256
trainable_parameters = 9018400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 8.21078873e+00  4.35890150e+00  5.19492531e+00  2.84795880e+00
  6.23694658e+00  2.48332572e+00  6.01452780e+00  1.49544990e+00
  1.56305087e+00  4.33542109e+00  3.63919592e+00  2.33469009e+00
  7.12970674e-01  4.46248204e-01  1.55621916e-01  3.05101776e+00
  4.20721769e+00  7.49161625e+00  3.96869850e+00  9.05481625e+00
  4.47121114e-01  2.05655861e+00  3.52404070e+00  5.90453529e+00
  8.73787975e+00  3.53096032e+00  2.76253247e+00  6.50554955e-01
  8.03491688e+00  3.65929008e+00  4.93096781e+00  1.00386086e+01
  5.29845119e-01  1.22730446e+00  7.25914860e+00  7.58729315e+00
  7.71210337e+00  5.14168453e+00  9.58227444e+00  4.79759741e+00
  6.05360174e+00 -3.22920084e-03  4.30121565e+00  7.28324509e+00
  2.30840397e+00  9.72805214e+00  7.01551247e+00  3.15825868e+00
  7.77309942e+00  1.27091324e+00  9.88113785e+00  5.76226175e-01
  9.93493366e+00  6.19022250e-01  7.72215176e+00  1.45727849e+00
  2.63241100e+00  7.56394577e+00  4.26764488e+00  1.30306756e+00
  4.47696209e+00  4.10884666e+00  6.25260830e+00  7.17153311e+00
  7.79660606e+00  4.86149597e+00  4.01030636e+00  1.48931491e+00
  8.25993347e+00  6.99169493e+00  4.15396929e+00  5.40362930e+00
  6.69921494e+00  1.80113685e+00 -2.28963286e-01  3.53704309e+00
  5.97519684e+00  5.03495884e+00  5.96093559e+00  8.27530861e+00
  5.11110401e+00  1.71916723e-01  1.00485516e+01  5.39971399e+00
  6.97965956e+00  6.67698717e+00  9.53224850e+00  2.60424829e+00
  6.92656088e+00  2.48836160e+00  1.50950837e+00  3.11806774e+00
  3.62086797e+00  9.48242664e+00  2.35715413e+00  1.47683930e+00
  6.57032585e+00  2.31288910e+00  5.07415724e+00  8.22896194e+00
  2.67307043e+00  4.70531940e+00  5.81045723e+00  3.97613907e+00
  1.00836468e+01  6.80917358e+00  7.91102648e+00  9.83973694e+00
  3.97975469e+00  2.81087971e+00  6.61194921e-01  2.09042239e+00
  1.80242181e+00  2.45311332e+00  7.75794411e+00  5.35263777e+00
  6.36038160e+00  8.03104973e+00  2.23086929e+00  3.26186609e+00
  7.10611820e+00  4.36860800e+00  5.38674593e-01  5.03258133e+00
  8.79059315e+00  4.12663126e+00  2.96597481e+00  9.92143154e+00
  6.63155603e+00  2.29087067e+00  9.67418194e-01  8.54019165e+00
  7.32748413e+00  9.67831326e+00  3.08930302e+00  1.43350685e+00
  7.23957300e+00  2.06572795e+00  1.31994998e+00  2.38906145e+00
  8.15470123e+00  4.92610788e+00  4.60461289e-01  4.69264269e+00
  7.11180496e+00  6.15540504e+00  5.58532810e+00  1.97622848e+00
  8.55810928e+00  5.60682297e+00  6.50732088e+00  9.74595356e+00
  3.70203424e+00  8.65994740e+00  3.66820455e-01 -1.19887441e-01
  5.62859249e+00  3.21332264e+00  2.41353798e+00  2.97626567e+00
  3.33519387e+00  7.60388327e+00  5.94638872e+00  7.90076303e+00
  8.14614868e+00  8.43794918e+00  5.11629820e+00  4.45509529e+00
  8.96594524e+00  6.83408117e+00  5.29020429e-01  5.47313118e+00
  3.56114149e+00  9.86122417e+00 -8.47060025e-01  5.97660685e+00
  7.56903505e+00  3.27315211e+00  9.39664459e+00  9.18616390e+00
  9.16251087e+00  3.12576860e-01  5.81769562e+00  2.36430216e+00
  8.91152668e+00  6.80723286e+00  5.49286032e+00  1.25512540e+00
  6.51518726e+00  6.32031965e+00  9.03301430e+00  5.99218893e+00
  9.68340588e+00  9.75206280e+00  6.48233700e+00  5.02287769e+00
  4.40520668e+00  8.71606636e+00  1.15936518e-01  1.15061617e+00
  1.13465822e+00  7.72926044e+00  3.59013176e+00  6.88610840e+00
  6.62146950e+00  3.82466817e+00  5.06336927e+00  6.39645910e+00
  9.77296638e+00  6.23589993e+00  6.13958120e+00  6.46353066e-01
  7.98168039e+00  5.24744987e+00  8.84934807e+00  7.04309762e-01
  4.59585285e+00  2.57471061e+00  2.76045990e+00  7.58923578e+00
  6.31161690e+00  3.93680930e+00  3.41281295e-03  9.66006947e+00
  9.57943439e+00  2.44953036e+00  7.60993719e-01  5.62422276e+00
  9.45851040e+00  9.12294960e+00  5.39434481e+00  2.33969688e+00
  3.87983751e+00  8.94526768e+00  4.94882011e+00  7.46908426e-01
  3.23506546e+00  2.77883267e+00  1.57590222e+00  5.10829020e+00
  9.52500534e+00  6.17966223e+00  1.10281193e+00  5.18856943e-01
  2.46926522e+00  3.78143835e+00  6.96128178e+00  1.00144138e+01
  3.66667557e+00  5.97449923e+00  9.95713520e+00  7.44061518e+00
  6.02692795e+00  7.26248920e-01  3.43354082e+00  1.11553822e+01
  1.09044552e+00  8.32242012e+00  7.46725512e+00  3.50707126e+00
  4.66837549e+00  6.27135944e+00  7.01602077e+00  4.99376917e+00
  1.22947979e+00  6.21938133e+00  4.57749987e+00  9.84504700e+00
  3.30335212e+00  2.10937715e+00  8.59310150e-01  5.95748758e+00
  7.51813459e+00  1.18187451e+00  3.30510187e+00  6.36148548e+00
  4.50776958e+00  4.50498581e+00  1.05220985e+01  9.05682564e+00
  6.02789402e+00  7.13881588e+00  7.25975084e+00  6.13174152e+00
  2.50386310e+00  6.21227741e+00  6.38945484e+00  8.56712151e+00
  3.63186836e-01  9.32480907e+00  5.54315186e+00 -3.98739636e-01
  3.37429929e+00  7.95210361e+00  5.01406813e+00  9.63213921e+00
  3.99637389e+00  2.97415352e+00  8.29203606e+00  6.00829792e+00
  6.21376610e+00  7.23727989e+00  6.71069527e+00  5.01448536e+00
  9.13507938e-01  6.87569332e+00  9.86827469e+00  4.45279694e+00
  8.80540848e+00  5.50182295e+00  6.56988764e+00  8.22664070e+00
  6.11226082e+00  9.90539372e-01  8.70905399e+00  2.35227823e+00
  6.95939922e+00  4.88893890e+00  4.05508137e+00  1.27094746e+00
  7.41610336e+00  9.57014084e-01  1.03208637e+01  8.77536011e+00
  5.40437126e+00  3.66490221e+00  1.92103791e+00  5.05432844e+00
  6.88084650e+00  2.36994123e+00  4.34519386e+00  1.71688604e+00
  3.39162683e+00  1.85334861e+00  9.56473351e+00  6.55594063e+00
  7.24569368e+00  9.57989931e-01  3.30405354e+00  8.91251564e+00
  9.43958402e-01  2.91835928e+00  6.76908159e+00  6.41434669e+00
  8.02711487e+00  8.37068748e+00  1.76286387e+00  2.83487058e+00
  4.26155806e+00 -1.64624956e-02  5.80787063e-02  2.90215087e+00
  4.82958937e+00  2.67308688e+00  8.74459743e+00  3.35418940e+00
  9.50176620e+00  9.24923992e+00  5.82722712e+00  3.35397267e+00
  3.11244607e+00  7.65114069e+00  2.58552289e+00  3.75996971e+00
  5.17049551e+00  5.17774963e+00  9.58946896e+00  2.46264791e+00
  4.03688717e+00  2.72447777e+00  2.75106120e+00  1.24247396e+00
  2.18880701e+00  8.80450439e+00  2.16815996e+00  4.50869846e+00
  9.47779369e+00  6.92755127e+00  5.21426487e+00  7.63384438e+00
  1.33627713e+00  5.05314064e+00  8.01588058e-01  2.33411765e+00
  2.04597259e+00  3.71735334e+00  3.99399781e+00  9.83961201e+00
  2.91163659e+00  3.73780441e+00  7.86999273e+00  7.62870979e+00
  7.85087109e+00  2.87814641e+00  5.46488953e+00  1.44017196e+00
  2.16978312e+00  4.34651899e+00  7.40160513e+00  5.01842690e+00
  8.79978561e+00  8.01273918e+00  6.23900414e-01  5.34396315e+00
  5.84303570e+00  9.49287987e+00 -2.03253716e-01  5.55819988e+00
  2.56452465e+00  5.52358687e-01  5.80285740e+00  8.33415508e+00
  8.18197632e+00 -8.59678984e-02  3.97874522e+00  4.08348382e-01
  2.77977419e+00  2.01314735e+00  2.23359156e+00  3.15110850e+00
  3.16415262e+00  9.69314194e+00  3.46537042e+00  8.90620518e+00
  1.80000591e+00  6.60689974e+00 -7.20865309e-01  4.78756142e+00
 -1.09752685e-01  9.03241920e+00  8.91885853e+00  6.74625039e-01
  9.64687729e+00  5.28304863e+00  3.44711399e+00  9.08611012e+00
  3.02275133e+00  4.11477327e+00  1.01315606e+00  7.55140066e+00
  7.66734123e-01  4.76552486e+00  6.70099640e+00  7.79399300e+00
  1.63697755e+00  6.94904470e+00  6.56399965e+00  4.96204281e+00
  5.89378309e+00  5.83356380e-01  6.12774992e+00  4.05969334e+00
  8.60876942e+00  3.77930760e+00  5.77260780e+00  4.28877592e+00
  9.17067528e+00  2.55027103e+00  1.33500242e+00  8.23266697e+00
  4.27657557e+00  5.92882109e+00  3.65143597e-01  1.05127277e+01
  9.12497520e+00  4.73808956e+00  1.15415120e+00  4.24439526e+00
  9.80254364e+00  7.13764381e+00  2.21075535e+00  4.46259594e+00
  9.60248947e+00  6.37608194e+00  3.89600515e-01  6.30370569e+00
  2.47006059e+00  9.89494145e-01  4.10520649e+00  5.44608831e+00
  5.12012959e-01  4.04597473e+00  5.92461109e+00  8.09785938e+00
  5.32709646e+00  5.15376234e+00  7.13395691e+00  8.32365990e+00
  4.20349646e+00  6.05286407e+00  6.26060724e+00  6.60055876e-03
  8.09364128e+00  3.49219418e+00  7.10890245e+00  4.91867685e+00
  7.30661583e+00  6.67151976e+00  2.00603342e+00  6.53424406e+00
  8.17309189e+00  2.37493563e+00  7.19742537e+00  6.21698093e+00
  1.02654848e+01  4.02459478e+00  2.00096679e+00  3.60146332e+00
  2.72641587e+00  7.55267096e+00  7.33097744e+00  6.48247480e+00
  1.84161115e+00  8.67273045e+00  1.03891554e+01  5.11711931e+00
  6.14349842e+00  4.63130045e+00  1.04338384e+00  3.16097307e+00
  2.48957181e+00  2.08219433e+00  9.02428818e+00  7.58692598e+00
  3.53382182e+00  2.87967825e+00  9.43597221e+00  1.85656917e+00
  5.57112312e+00  9.00374889e-01  1.43457806e+00  5.70213127e+00
  6.18463993e+00  5.21287870e+00  8.30662155e+00  9.78884792e+00
  8.07082462e+00  4.94385624e+00  9.57595062e+00  4.81561422e-01
  5.93677044e+00  6.03051233e+00  1.27146268e+00  9.59977341e+00
  3.53188229e+00  5.92599010e+00  8.90297794e+00  5.73115826e+00
  8.29036653e-01  6.28442287e+00  4.10012245e+00  8.16020679e+00
  9.57242489e+00  8.73237801e+00  3.82743692e+00  7.33982182e+00
  9.39437389e+00  5.32039928e+00  4.49351358e+00  7.41679072e-01
  7.33104944e+00  8.68616486e+00  2.54391122e+00  2.88136458e+00
  5.05854321e+00  4.12581384e-01  7.20838428e-01  4.39249325e+00
  2.12579203e+00  5.69973421e+00  3.94032979e+00  1.01283722e+01
  9.79717064e+00  2.52009940e+00  8.86107635e+00  8.79512429e-01
  4.84236622e+00  1.72291732e+00  3.82897258e+00  6.94665861e+00
  4.18651247e+00  8.54575062e+00  7.19269276e+00  9.72444630e+00
  5.51988173e+00  8.01771069e+00  3.28205919e+00  3.97813463e+00
  4.18136644e+00  5.16412115e+00  1.75697100e+00  5.15417719e+00
  1.50469279e+00  9.25226021e+00  4.35976595e-01  4.19854736e+00
  3.64054823e+00  1.06782198e-01  8.30288601e+00  7.06305122e+00
  1.87979758e+00  6.35672998e+00  8.16109753e+00  6.51096630e+00
  5.61127961e-01  6.76316595e+00  6.57550716e+00  7.76742983e+00
  5.40051842e+00  4.99695921e+00  1.05247574e+01  7.60159779e+00
  5.60209799e+00  3.34395909e+00  9.19912910e+00  4.68194294e+00
  1.00466835e+00  5.51672554e+00  8.06357193e+00  4.60604143e+00
  7.99039245e-01  6.11217165e+00  9.98776340e+00  7.64210176e+00
  6.11796856e+00  9.89960003e+00  7.96584511e+00  6.26075697e+00
  4.63107634e+00  7.38753414e+00  6.49842548e+00  7.23403335e-01
  7.84123564e+00  3.43258095e+00  5.87750053e+00  8.16937256e+00
  4.84820747e+00  1.59829724e+00  4.18775225e+00  1.30482543e+00
  6.89824677e+00  9.70094967e+00  7.83443594e+00  4.79529858e+00
  6.26568747e+00  3.08977580e+00  5.72115278e+00  2.92952847e+00
  7.45006084e+00  2.38916183e+00  9.71957111e+00  6.54768562e+00
  6.25139415e-01 -2.21325979e-02  1.01162004e+01  1.64097524e+00
  3.30466270e+00  2.32712507e+00  9.82605267e+00  5.42485619e+00
  4.57557726e+00  5.65397930e+00  2.95226574e+00  6.00716829e+00
  4.69566488e+00  1.86493647e+00  5.55787706e+00  4.29599905e+00
  8.97355652e+00  4.49551582e+00  5.21416712e+00  7.87060261e+00
  4.67125940e+00  9.76075363e+00  8.19973755e+00  8.41541100e+00
  9.90537739e+00  2.93996453e+00  5.85271358e+00  7.30432892e+00
  7.22648525e+00  9.17044544e+00  2.83920431e+00  3.20150447e+00
  8.31029034e+00  7.78128242e+00  2.25734663e+00  5.91900253e+00
  7.87056684e+00  3.05571938e+00  3.93086529e+00  5.76315022e+00
  5.30861235e+00  7.55313301e+00 -1.47460684e-01  4.09288597e+00
  1.74171937e+00  4.78674221e+00  7.88887119e+00  7.54741371e-01
  1.54648781e+00  7.28928089e+00  7.57703447e+00  5.61207056e+00
  9.85756111e+00  4.10112953e+00  1.57129598e+00  6.85865021e+00
  6.33887672e+00  6.33726215e+00  5.35512924e+00  8.78589249e+00
  8.93958569e-01 -1.22548640e-01  1.54779208e+00  2.83668208e+00
  5.92615891e+00  5.92532015e+00  2.29801345e+00  4.49938583e+00
  4.36498499e+00  5.37602377e+00  8.03752041e+00  5.66677046e+00
  8.83142376e+00  1.65858328e+00  5.42761660e+00  8.26859760e+00
  1.79626775e+00  4.40248251e+00  5.96399355e+00  2.07387352e+00
  7.58685493e+00  6.19924068e-01  9.63695049e+00  4.80245113e+00
  2.84362292e+00  1.11408031e+00  4.65407324e+00  6.60547924e+00
  1.22691751e+00  3.05578947e+00  1.09281607e+01  6.40130091e+00
  2.10371280e+00  1.27200794e+00  8.89848232e+00  2.80182910e+00
  2.19280529e+00  1.05606747e+01  2.94065785e+00  1.69049454e+00
  5.48146868e+00  4.31994724e+00  9.06744099e+00  1.15592277e+00
 -7.79310703e-01  1.04373407e+00  8.86706924e+00  4.28514481e+00
  1.81781006e+00  5.78969526e+00  5.54994345e+00  2.30405951e+00
  5.54053879e+00  2.74468279e+00  2.27062321e+00  4.07878542e+00
  2.84625888e+00  2.04214787e+00  8.94879723e+00  6.88817441e-01
  2.71495771e+00  3.84625363e+00  2.55016398e+00  2.09636283e+00
  7.99209976e+00  3.07564521e+00  1.11745620e+00  5.30355263e+00
  9.93744850e-01  2.74349833e+00  8.41612911e+00  4.34795761e+00
  6.55439472e+00  6.15104818e+00  5.19303381e-01  6.36637783e+00
  8.49066544e+00  8.24014544e-01  6.90668011e+00  7.08179665e+00
  9.01312637e+00  3.36554408e+00  2.87336349e+00  6.86589861e+00
 -2.01100767e-01  7.31349230e+00  8.66450596e+00  4.29815626e+00
  3.38823771e+00  3.01338983e+00  8.08154404e-01  1.78297210e+00
  9.61242580e+00  8.81327534e+00  2.97019339e+00  8.63016319e+00
  3.02107668e+00  2.72670150e+00  2.13139439e+00  8.94742870e+00
  9.83511925e+00  3.19685555e+00  6.81030130e+00  3.50014639e+00
  1.05297625e+00  4.07330465e+00  5.53509521e+00  1.18256664e+00
  8.23826122e+00  5.53470898e+00  2.24809480e+00  5.40734196e+00
  7.84483671e+00  2.75392199e+00  7.62425852e+00  4.27786589e+00
  9.72611332e+00  8.83753872e+00  3.43726850e+00  1.27960294e-01
  7.29834032e+00  2.04023504e+00  1.30689442e+00  9.04135704e+00
  1.00866823e+01  2.58009624e+00  3.01804805e+00  1.68803823e+00
  4.80738878e+00  8.68211079e+00  6.73749733e+00  8.63989162e+00
  2.21114421e+00  7.04848576e+00  9.34753704e+00  2.76184368e+00
  6.90854836e+00  4.05415773e+00  3.81649780e+00  9.40583229e+00
  4.05083704e+00  8.92635822e-01  3.82358503e+00  4.43289518e+00
  7.26910877e+00  8.82122993e+00  8.22476578e+00  9.33980370e+00
  4.71105146e+00  6.95373154e+00  3.06833911e+00 -4.22824502e-01
  3.65492988e+00  6.86570597e+00  2.05108023e+00  8.74809170e+00
  8.31052685e+00  1.87380075e+00  2.81262565e+00  8.66123104e+00
  3.31760716e+00  3.31471944e+00  3.48811197e+00  7.28613329e+00
 -9.66772735e-02 -7.69796014e-01  6.32836771e+00  3.86802888e+00
  5.99920893e+00  2.40654874e+00  9.12174988e+00  8.34914875e+00
  7.07096219e-01  7.57762194e+00  9.58280849e+00  8.65084171e+00
  6.67687321e+00  1.21811235e+00  5.86707783e+00  3.30112028e+00
  1.01402960e+01  7.17766523e+00  5.74374008e+00  6.99432039e+00
  4.40368319e+00  2.86542940e+00  9.42987251e+00  3.30265784e+00
  3.83230895e-01  4.20491457e+00  8.71614361e+00  7.57816601e+00
  9.77553654e+00  6.46106720e-01  9.44217777e+00  4.46305180e+00
  4.83198547e+00  3.06845570e+00  1.06641746e+00  4.49923372e+00
  7.57343435e+00  5.86748123e-03  1.58664107e+00  5.31323195e-01
  8.55088329e+00  6.48872995e+00  1.54820132e+00  7.71537733e+00
  8.20388412e+00  9.80402565e+00  9.79537868e+00 -8.38677704e-01
  5.94942427e+00  2.88661289e+00  5.26643085e+00  3.18921828e+00
  7.79481411e-01  4.42821693e+00  5.33531380e+00  9.32881355e+00
  2.65765786e-02  3.08747840e+00  8.52437019e+00  6.85948515e+00
  8.21984768e+00  6.24451685e+00  7.30141687e+00  4.90936804e+00
  2.20824790e+00  8.48395348e+00  5.67844057e+00  1.24840343e+00
  8.05260468e+00  9.64081573e+00  3.07099938e+00  3.18240905e+00
  9.12044704e-01  5.96499348e+00  9.19813156e+00  3.36040854e-01
  4.73263264e+00  1.30067658e+00  5.19609153e-01  6.70743704e+00
  8.23411751e+00  3.52812862e+00  8.22581482e+00  1.19572031e+00
  2.63071656e+00  9.79853821e+00  4.98861170e+00  6.00117397e+00
  3.19664389e-01  1.32244027e+00  6.05848253e-01  7.35753870e+00
  1.13696325e+00  8.69066429e+00  3.58892393e+00  7.88068628e+00
  3.04448676e+00  2.65313506e+00  9.56735551e-01  2.75976801e+00
  4.98727465e+00  7.08365965e+00  5.05365181e+00  9.17652988e+00
  5.92350817e+00  1.26345801e+00  3.95679140e+00  7.46081781e+00
  2.64605379e+00  6.97033882e+00  5.93741179e+00  1.01776525e-01
  7.47330189e+00  5.93837404e+00  8.12254488e-01  3.60206747e+00
  3.23938537e+00  8.24193764e+00  3.56294060e+00  9.41426849e+00
  5.64243269e+00  2.11869121e+00  8.28666019e+00  6.44191980e+00]
Epoch 1/1000
2023-09-09 13:34:49.729 
Epoch 1/1000 
	 loss: 509.6606, MinusLogProbMetric: 509.6606, val_loss: 430.8166, val_MinusLogProbMetric: 430.8166

Epoch 1: val_loss improved from inf to 430.81662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 105s - loss: 509.6606 - MinusLogProbMetric: 509.6606 - val_loss: 430.8166 - val_MinusLogProbMetric: 430.8166 - lr: 3.3333e-04 - 105s/epoch - 535ms/step
Epoch 2/1000
2023-09-09 13:35:06.740 
Epoch 2/1000 
	 loss: 426.8765, MinusLogProbMetric: 426.8765, val_loss: 426.9691, val_MinusLogProbMetric: 426.9691

Epoch 2: val_loss improved from 430.81662 to 426.96915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 17s - loss: 426.8765 - MinusLogProbMetric: 426.8765 - val_loss: 426.9691 - val_MinusLogProbMetric: 426.9691 - lr: 3.3333e-04 - 17s/epoch - 84ms/step
Epoch 3/1000
2023-09-09 13:35:25.392 
Epoch 3/1000 
	 loss: 425.2451, MinusLogProbMetric: 425.2451, val_loss: 425.4106, val_MinusLogProbMetric: 425.4106

Epoch 3: val_loss improved from 426.96915 to 425.41064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 19s - loss: 425.2451 - MinusLogProbMetric: 425.2451 - val_loss: 425.4106 - val_MinusLogProbMetric: 425.4106 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 4/1000
2023-09-09 13:35:46.089 
Epoch 4/1000 
	 loss: 423.5574, MinusLogProbMetric: 423.5574, val_loss: 422.8382, val_MinusLogProbMetric: 422.8382

Epoch 4: val_loss improved from 425.41064 to 422.83820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 21s - loss: 423.5574 - MinusLogProbMetric: 423.5574 - val_loss: 422.8382 - val_MinusLogProbMetric: 422.8382 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 5/1000
2023-09-09 13:36:07.797 
Epoch 5/1000 
	 loss: 422.4836, MinusLogProbMetric: 422.4836, val_loss: 421.0707, val_MinusLogProbMetric: 421.0707

Epoch 5: val_loss improved from 422.83820 to 421.07071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 21s - loss: 422.4836 - MinusLogProbMetric: 422.4836 - val_loss: 421.0707 - val_MinusLogProbMetric: 421.0707 - lr: 3.3333e-04 - 21s/epoch - 110ms/step
Epoch 6/1000
2023-09-09 13:36:28.430 
Epoch 6/1000 
	 loss: 420.7856, MinusLogProbMetric: 420.7856, val_loss: 421.7432, val_MinusLogProbMetric: 421.7432

Epoch 6: val_loss did not improve from 421.07071
196/196 - 20s - loss: 420.7856 - MinusLogProbMetric: 420.7856 - val_loss: 421.7432 - val_MinusLogProbMetric: 421.7432 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 7/1000
2023-09-09 13:36:48.086 
Epoch 7/1000 
	 loss: 421.5324, MinusLogProbMetric: 421.5324, val_loss: 425.2146, val_MinusLogProbMetric: 425.2146

Epoch 7: val_loss did not improve from 421.07071
196/196 - 20s - loss: 421.5324 - MinusLogProbMetric: 421.5324 - val_loss: 425.2146 - val_MinusLogProbMetric: 425.2146 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 8/1000
2023-09-09 13:37:07.249 
Epoch 8/1000 
	 loss: 421.3118, MinusLogProbMetric: 421.3118, val_loss: 418.4578, val_MinusLogProbMetric: 418.4578

Epoch 8: val_loss improved from 421.07071 to 418.45782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 20s - loss: 421.3118 - MinusLogProbMetric: 421.3118 - val_loss: 418.4578 - val_MinusLogProbMetric: 418.4578 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 9/1000
2023-09-09 13:37:28.164 
Epoch 9/1000 
	 loss: 417.8023, MinusLogProbMetric: 417.8023, val_loss: 419.5374, val_MinusLogProbMetric: 419.5374

Epoch 9: val_loss did not improve from 418.45782
196/196 - 20s - loss: 417.8023 - MinusLogProbMetric: 417.8023 - val_loss: 419.5374 - val_MinusLogProbMetric: 419.5374 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 10/1000
2023-09-09 13:37:47.658 
Epoch 10/1000 
	 loss: 417.8284, MinusLogProbMetric: 417.8284, val_loss: 418.2403, val_MinusLogProbMetric: 418.2403

Epoch 10: val_loss improved from 418.45782 to 418.24033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 20s - loss: 417.8284 - MinusLogProbMetric: 417.8284 - val_loss: 418.2403 - val_MinusLogProbMetric: 418.2403 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 11/1000
2023-09-09 13:38:04.513 
Epoch 11/1000 
	 loss: 416.4509, MinusLogProbMetric: 416.4509, val_loss: 437.1809, val_MinusLogProbMetric: 437.1809

Epoch 11: val_loss did not improve from 418.24033
196/196 - 16s - loss: 416.4509 - MinusLogProbMetric: 416.4509 - val_loss: 437.1809 - val_MinusLogProbMetric: 437.1809 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 12/1000
2023-09-09 13:38:19.529 
Epoch 12/1000 
	 loss: 416.4889, MinusLogProbMetric: 416.4889, val_loss: 413.3506, val_MinusLogProbMetric: 413.3506

Epoch 12: val_loss improved from 418.24033 to 413.35065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 16s - loss: 416.4889 - MinusLogProbMetric: 416.4889 - val_loss: 413.3506 - val_MinusLogProbMetric: 413.3506 - lr: 3.3333e-04 - 16s/epoch - 80ms/step
Epoch 13/1000
2023-09-09 13:38:35.614 
Epoch 13/1000 
	 loss: 416.5457, MinusLogProbMetric: 416.5457, val_loss: 416.9203, val_MinusLogProbMetric: 416.9203

Epoch 13: val_loss did not improve from 413.35065
196/196 - 15s - loss: 416.5457 - MinusLogProbMetric: 416.5457 - val_loss: 416.9203 - val_MinusLogProbMetric: 416.9203 - lr: 3.3333e-04 - 15s/epoch - 79ms/step
Epoch 14/1000
2023-09-09 13:38:50.909 
Epoch 14/1000 
	 loss: 415.0139, MinusLogProbMetric: 415.0139, val_loss: 426.5849, val_MinusLogProbMetric: 426.5849

Epoch 14: val_loss did not improve from 413.35065
196/196 - 15s - loss: 415.0139 - MinusLogProbMetric: 415.0139 - val_loss: 426.5849 - val_MinusLogProbMetric: 426.5849 - lr: 3.3333e-04 - 15s/epoch - 78ms/step
Epoch 15/1000
2023-09-09 13:39:05.835 
Epoch 15/1000 
	 loss: 414.0616, MinusLogProbMetric: 414.0616, val_loss: 418.3760, val_MinusLogProbMetric: 418.3760

Epoch 15: val_loss did not improve from 413.35065
196/196 - 15s - loss: 414.0616 - MinusLogProbMetric: 414.0616 - val_loss: 418.3760 - val_MinusLogProbMetric: 418.3760 - lr: 3.3333e-04 - 15s/epoch - 76ms/step
Epoch 16/1000
2023-09-09 13:39:20.867 
Epoch 16/1000 
	 loss: 414.1870, MinusLogProbMetric: 414.1870, val_loss: 416.6638, val_MinusLogProbMetric: 416.6638

Epoch 16: val_loss did not improve from 413.35065
196/196 - 15s - loss: 414.1870 - MinusLogProbMetric: 414.1870 - val_loss: 416.6638 - val_MinusLogProbMetric: 416.6638 - lr: 3.3333e-04 - 15s/epoch - 77ms/step
Epoch 17/1000
2023-09-09 13:39:35.809 
Epoch 17/1000 
	 loss: 413.6914, MinusLogProbMetric: 413.6914, val_loss: 421.7352, val_MinusLogProbMetric: 421.7352

Epoch 17: val_loss did not improve from 413.35065
196/196 - 15s - loss: 413.6914 - MinusLogProbMetric: 413.6914 - val_loss: 421.7352 - val_MinusLogProbMetric: 421.7352 - lr: 3.3333e-04 - 15s/epoch - 76ms/step
Epoch 18/1000
2023-09-09 13:39:50.637 
Epoch 18/1000 
	 loss: 413.0050, MinusLogProbMetric: 413.0050, val_loss: 415.2505, val_MinusLogProbMetric: 415.2505

Epoch 18: val_loss did not improve from 413.35065
196/196 - 15s - loss: 413.0050 - MinusLogProbMetric: 413.0050 - val_loss: 415.2505 - val_MinusLogProbMetric: 415.2505 - lr: 3.3333e-04 - 15s/epoch - 76ms/step
Epoch 19/1000
2023-09-09 13:40:05.431 
Epoch 19/1000 
	 loss: 411.8427, MinusLogProbMetric: 411.8427, val_loss: 411.1615, val_MinusLogProbMetric: 411.1615

Epoch 19: val_loss improved from 413.35065 to 411.16153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 16s - loss: 411.8427 - MinusLogProbMetric: 411.8427 - val_loss: 411.1615 - val_MinusLogProbMetric: 411.1615 - lr: 3.3333e-04 - 16s/epoch - 80ms/step
Epoch 20/1000
2023-09-09 13:40:21.275 
Epoch 20/1000 
	 loss: 411.6169, MinusLogProbMetric: 411.6169, val_loss: 411.4864, val_MinusLogProbMetric: 411.4864

Epoch 20: val_loss did not improve from 411.16153
196/196 - 15s - loss: 411.6169 - MinusLogProbMetric: 411.6169 - val_loss: 411.4864 - val_MinusLogProbMetric: 411.4864 - lr: 3.3333e-04 - 15s/epoch - 77ms/step
Epoch 21/1000
2023-09-09 13:40:36.639 
Epoch 21/1000 
	 loss: 410.8375, MinusLogProbMetric: 410.8375, val_loss: 410.5773, val_MinusLogProbMetric: 410.5773

Epoch 21: val_loss improved from 411.16153 to 410.57730, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 16s - loss: 410.8375 - MinusLogProbMetric: 410.8375 - val_loss: 410.5773 - val_MinusLogProbMetric: 410.5773 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 22/1000
2023-09-09 13:40:51.917 
Epoch 22/1000 
	 loss: 411.7173, MinusLogProbMetric: 411.7173, val_loss: 415.0710, val_MinusLogProbMetric: 415.0710

Epoch 22: val_loss did not improve from 410.57730
196/196 - 15s - loss: 411.7173 - MinusLogProbMetric: 411.7173 - val_loss: 415.0710 - val_MinusLogProbMetric: 415.0710 - lr: 3.3333e-04 - 15s/epoch - 74ms/step
Epoch 23/1000
2023-09-09 13:41:06.055 
Epoch 23/1000 
	 loss: 410.0331, MinusLogProbMetric: 410.0331, val_loss: 413.3690, val_MinusLogProbMetric: 413.3690

Epoch 23: val_loss did not improve from 410.57730
196/196 - 14s - loss: 410.0331 - MinusLogProbMetric: 410.0331 - val_loss: 413.3690 - val_MinusLogProbMetric: 413.3690 - lr: 3.3333e-04 - 14s/epoch - 72ms/step
Epoch 24/1000
2023-09-09 13:41:20.658 
Epoch 24/1000 
	 loss: 409.7460, MinusLogProbMetric: 409.7460, val_loss: 410.5921, val_MinusLogProbMetric: 410.5921

Epoch 24: val_loss did not improve from 410.57730
196/196 - 15s - loss: 409.7460 - MinusLogProbMetric: 409.7460 - val_loss: 410.5921 - val_MinusLogProbMetric: 410.5921 - lr: 3.3333e-04 - 15s/epoch - 74ms/step
Epoch 25/1000
2023-09-09 13:41:35.723 
Epoch 25/1000 
	 loss: 409.3512, MinusLogProbMetric: 409.3512, val_loss: 411.7618, val_MinusLogProbMetric: 411.7618

Epoch 25: val_loss did not improve from 410.57730
196/196 - 15s - loss: 409.3512 - MinusLogProbMetric: 409.3512 - val_loss: 411.7618 - val_MinusLogProbMetric: 411.7618 - lr: 3.3333e-04 - 15s/epoch - 77ms/step
Epoch 26/1000
2023-09-09 13:41:50.605 
Epoch 26/1000 
	 loss: 410.3984, MinusLogProbMetric: 410.3984, val_loss: 409.1570, val_MinusLogProbMetric: 409.1570

Epoch 26: val_loss improved from 410.57730 to 409.15701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 16s - loss: 410.3984 - MinusLogProbMetric: 410.3984 - val_loss: 409.1570 - val_MinusLogProbMetric: 409.1570 - lr: 3.3333e-04 - 16s/epoch - 80ms/step
Epoch 27/1000
2023-09-09 13:42:06.867 
Epoch 27/1000 
	 loss: 409.0207, MinusLogProbMetric: 409.0207, val_loss: 411.9681, val_MinusLogProbMetric: 411.9681

Epoch 27: val_loss did not improve from 409.15701
196/196 - 16s - loss: 409.0207 - MinusLogProbMetric: 409.0207 - val_loss: 411.9681 - val_MinusLogProbMetric: 411.9681 - lr: 3.3333e-04 - 16s/epoch - 79ms/step
Epoch 28/1000
2023-09-09 13:42:22.949 
Epoch 28/1000 
	 loss: 409.6089, MinusLogProbMetric: 409.6089, val_loss: 407.0148, val_MinusLogProbMetric: 407.0148

Epoch 28: val_loss improved from 409.15701 to 407.01483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 17s - loss: 409.6089 - MinusLogProbMetric: 409.6089 - val_loss: 407.0148 - val_MinusLogProbMetric: 407.0148 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 29/1000
2023-09-09 13:42:39.075 
Epoch 29/1000 
	 loss: 407.7151, MinusLogProbMetric: 407.7151, val_loss: 409.0930, val_MinusLogProbMetric: 409.0930

Epoch 29: val_loss did not improve from 407.01483
196/196 - 15s - loss: 407.7151 - MinusLogProbMetric: 407.7151 - val_loss: 409.0930 - val_MinusLogProbMetric: 409.0930 - lr: 3.3333e-04 - 15s/epoch - 79ms/step
Epoch 30/1000
2023-09-09 13:42:54.274 
Epoch 30/1000 
	 loss: 977.7548, MinusLogProbMetric: 977.7548, val_loss: 535.7845, val_MinusLogProbMetric: 535.7845

Epoch 30: val_loss did not improve from 407.01483
196/196 - 15s - loss: 977.7548 - MinusLogProbMetric: 977.7548 - val_loss: 535.7845 - val_MinusLogProbMetric: 535.7845 - lr: 3.3333e-04 - 15s/epoch - 78ms/step
Epoch 31/1000
2023-09-09 13:43:10.040 
Epoch 31/1000 
	 loss: 462.4727, MinusLogProbMetric: 462.4727, val_loss: 445.1123, val_MinusLogProbMetric: 445.1123

Epoch 31: val_loss did not improve from 407.01483
196/196 - 16s - loss: 462.4727 - MinusLogProbMetric: 462.4727 - val_loss: 445.1123 - val_MinusLogProbMetric: 445.1123 - lr: 3.3333e-04 - 16s/epoch - 80ms/step
Epoch 32/1000
2023-09-09 13:43:26.045 
Epoch 32/1000 
	 loss: 435.9760, MinusLogProbMetric: 435.9760, val_loss: 433.2792, val_MinusLogProbMetric: 433.2792

Epoch 32: val_loss did not improve from 407.01483
196/196 - 16s - loss: 435.9760 - MinusLogProbMetric: 435.9760 - val_loss: 433.2792 - val_MinusLogProbMetric: 433.2792 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 33/1000
2023-09-09 13:43:41.484 
Epoch 33/1000 
	 loss: 428.5098, MinusLogProbMetric: 428.5098, val_loss: 426.9915, val_MinusLogProbMetric: 426.9915

Epoch 33: val_loss did not improve from 407.01483
196/196 - 15s - loss: 428.5098 - MinusLogProbMetric: 428.5098 - val_loss: 426.9915 - val_MinusLogProbMetric: 426.9915 - lr: 3.3333e-04 - 15s/epoch - 79ms/step
Epoch 34/1000
2023-09-09 13:43:57.004 
Epoch 34/1000 
	 loss: 424.1477, MinusLogProbMetric: 424.1477, val_loss: 425.4365, val_MinusLogProbMetric: 425.4365

Epoch 34: val_loss did not improve from 407.01483
196/196 - 16s - loss: 424.1477 - MinusLogProbMetric: 424.1477 - val_loss: 425.4365 - val_MinusLogProbMetric: 425.4365 - lr: 3.3333e-04 - 16s/epoch - 79ms/step
Epoch 35/1000
2023-09-09 13:44:14.028 
Epoch 35/1000 
	 loss: 421.3840, MinusLogProbMetric: 421.3840, val_loss: 421.3771, val_MinusLogProbMetric: 421.3771

Epoch 35: val_loss did not improve from 407.01483
196/196 - 17s - loss: 421.3840 - MinusLogProbMetric: 421.3840 - val_loss: 421.3771 - val_MinusLogProbMetric: 421.3771 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 36/1000
2023-09-09 13:44:32.409 
Epoch 36/1000 
	 loss: 419.2810, MinusLogProbMetric: 419.2810, val_loss: 419.8525, val_MinusLogProbMetric: 419.8525

Epoch 36: val_loss did not improve from 407.01483
196/196 - 18s - loss: 419.2810 - MinusLogProbMetric: 419.2810 - val_loss: 419.8525 - val_MinusLogProbMetric: 419.8525 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 37/1000
2023-09-09 13:44:52.861 
Epoch 37/1000 
	 loss: 417.7162, MinusLogProbMetric: 417.7162, val_loss: 418.9386, val_MinusLogProbMetric: 418.9386

Epoch 37: val_loss did not improve from 407.01483
196/196 - 20s - loss: 417.7162 - MinusLogProbMetric: 417.7162 - val_loss: 418.9386 - val_MinusLogProbMetric: 418.9386 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 38/1000
2023-09-09 13:45:12.873 
Epoch 38/1000 
	 loss: 416.5125, MinusLogProbMetric: 416.5125, val_loss: 417.9444, val_MinusLogProbMetric: 417.9444

Epoch 38: val_loss did not improve from 407.01483
196/196 - 20s - loss: 416.5125 - MinusLogProbMetric: 416.5125 - val_loss: 417.9444 - val_MinusLogProbMetric: 417.9444 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 39/1000
2023-09-09 13:45:32.609 
Epoch 39/1000 
	 loss: 415.1361, MinusLogProbMetric: 415.1361, val_loss: 415.7705, val_MinusLogProbMetric: 415.7705

Epoch 39: val_loss did not improve from 407.01483
196/196 - 20s - loss: 415.1361 - MinusLogProbMetric: 415.1361 - val_loss: 415.7705 - val_MinusLogProbMetric: 415.7705 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 40/1000
2023-09-09 13:45:52.130 
Epoch 40/1000 
	 loss: 413.9591, MinusLogProbMetric: 413.9591, val_loss: 414.5866, val_MinusLogProbMetric: 414.5866

Epoch 40: val_loss did not improve from 407.01483
196/196 - 20s - loss: 413.9591 - MinusLogProbMetric: 413.9591 - val_loss: 414.5866 - val_MinusLogProbMetric: 414.5866 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 41/1000
2023-09-09 13:46:11.248 
Epoch 41/1000 
	 loss: 413.3669, MinusLogProbMetric: 413.3669, val_loss: 415.7875, val_MinusLogProbMetric: 415.7875

Epoch 41: val_loss did not improve from 407.01483
196/196 - 19s - loss: 413.3669 - MinusLogProbMetric: 413.3669 - val_loss: 415.7875 - val_MinusLogProbMetric: 415.7875 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 42/1000
2023-09-09 13:46:31.866 
Epoch 42/1000 
	 loss: 412.3783, MinusLogProbMetric: 412.3783, val_loss: 412.5892, val_MinusLogProbMetric: 412.5892

Epoch 42: val_loss did not improve from 407.01483
196/196 - 21s - loss: 412.3783 - MinusLogProbMetric: 412.3783 - val_loss: 412.5892 - val_MinusLogProbMetric: 412.5892 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 43/1000
2023-09-09 13:46:51.856 
Epoch 43/1000 
	 loss: 411.9575, MinusLogProbMetric: 411.9575, val_loss: 414.9984, val_MinusLogProbMetric: 414.9984

Epoch 43: val_loss did not improve from 407.01483
196/196 - 20s - loss: 411.9575 - MinusLogProbMetric: 411.9575 - val_loss: 414.9984 - val_MinusLogProbMetric: 414.9984 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 44/1000
2023-09-09 13:47:11.991 
Epoch 44/1000 
	 loss: 410.9273, MinusLogProbMetric: 410.9273, val_loss: 412.7434, val_MinusLogProbMetric: 412.7434

Epoch 44: val_loss did not improve from 407.01483
196/196 - 20s - loss: 410.9273 - MinusLogProbMetric: 410.9273 - val_loss: 412.7434 - val_MinusLogProbMetric: 412.7434 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 45/1000
2023-09-09 13:47:31.683 
Epoch 45/1000 
	 loss: 411.0007, MinusLogProbMetric: 411.0007, val_loss: 411.6625, val_MinusLogProbMetric: 411.6625

Epoch 45: val_loss did not improve from 407.01483
196/196 - 20s - loss: 411.0007 - MinusLogProbMetric: 411.0007 - val_loss: 411.6625 - val_MinusLogProbMetric: 411.6625 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 46/1000
2023-09-09 13:47:51.849 
Epoch 46/1000 
	 loss: 410.5036, MinusLogProbMetric: 410.5036, val_loss: 412.2344, val_MinusLogProbMetric: 412.2344

Epoch 46: val_loss did not improve from 407.01483
196/196 - 20s - loss: 410.5036 - MinusLogProbMetric: 410.5036 - val_loss: 412.2344 - val_MinusLogProbMetric: 412.2344 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 47/1000
2023-09-09 13:48:11.368 
Epoch 47/1000 
	 loss: 409.6160, MinusLogProbMetric: 409.6160, val_loss: 413.3802, val_MinusLogProbMetric: 413.3802

Epoch 47: val_loss did not improve from 407.01483
196/196 - 20s - loss: 409.6160 - MinusLogProbMetric: 409.6160 - val_loss: 413.3802 - val_MinusLogProbMetric: 413.3802 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 48/1000
2023-09-09 13:48:31.048 
Epoch 48/1000 
	 loss: 408.9454, MinusLogProbMetric: 408.9454, val_loss: 409.8780, val_MinusLogProbMetric: 409.8780

Epoch 48: val_loss did not improve from 407.01483
196/196 - 20s - loss: 408.9454 - MinusLogProbMetric: 408.9454 - val_loss: 409.8780 - val_MinusLogProbMetric: 409.8780 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 49/1000
2023-09-09 13:48:51.278 
Epoch 49/1000 
	 loss: 408.8804, MinusLogProbMetric: 408.8804, val_loss: 410.4162, val_MinusLogProbMetric: 410.4162

Epoch 49: val_loss did not improve from 407.01483
196/196 - 20s - loss: 408.8804 - MinusLogProbMetric: 408.8804 - val_loss: 410.4162 - val_MinusLogProbMetric: 410.4162 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 50/1000
2023-09-09 13:49:10.708 
Epoch 50/1000 
	 loss: 409.0061, MinusLogProbMetric: 409.0061, val_loss: 414.1755, val_MinusLogProbMetric: 414.1755

Epoch 50: val_loss did not improve from 407.01483
196/196 - 19s - loss: 409.0061 - MinusLogProbMetric: 409.0061 - val_loss: 414.1755 - val_MinusLogProbMetric: 414.1755 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 51/1000
2023-09-09 13:49:30.757 
Epoch 51/1000 
	 loss: 408.6264, MinusLogProbMetric: 408.6264, val_loss: 410.6050, val_MinusLogProbMetric: 410.6050

Epoch 51: val_loss did not improve from 407.01483
196/196 - 20s - loss: 408.6264 - MinusLogProbMetric: 408.6264 - val_loss: 410.6050 - val_MinusLogProbMetric: 410.6050 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 52/1000
2023-09-09 13:49:50.161 
Epoch 52/1000 
	 loss: 408.0488, MinusLogProbMetric: 408.0488, val_loss: 413.1559, val_MinusLogProbMetric: 413.1559

Epoch 52: val_loss did not improve from 407.01483
196/196 - 19s - loss: 408.0488 - MinusLogProbMetric: 408.0488 - val_loss: 413.1559 - val_MinusLogProbMetric: 413.1559 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 53/1000
2023-09-09 13:50:10.735 
Epoch 53/1000 
	 loss: 407.3336, MinusLogProbMetric: 407.3336, val_loss: 411.2668, val_MinusLogProbMetric: 411.2668

Epoch 53: val_loss did not improve from 407.01483
196/196 - 21s - loss: 407.3336 - MinusLogProbMetric: 407.3336 - val_loss: 411.2668 - val_MinusLogProbMetric: 411.2668 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 54/1000
2023-09-09 13:50:29.393 
Epoch 54/1000 
	 loss: 407.6228, MinusLogProbMetric: 407.6228, val_loss: 409.5406, val_MinusLogProbMetric: 409.5406

Epoch 54: val_loss did not improve from 407.01483
196/196 - 19s - loss: 407.6228 - MinusLogProbMetric: 407.6228 - val_loss: 409.5406 - val_MinusLogProbMetric: 409.5406 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 55/1000
2023-09-09 13:50:47.862 
Epoch 55/1000 
	 loss: 407.6351, MinusLogProbMetric: 407.6351, val_loss: 408.6926, val_MinusLogProbMetric: 408.6926

Epoch 55: val_loss did not improve from 407.01483
196/196 - 18s - loss: 407.6351 - MinusLogProbMetric: 407.6351 - val_loss: 408.6926 - val_MinusLogProbMetric: 408.6926 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 56/1000
2023-09-09 13:51:05.984 
Epoch 56/1000 
	 loss: 406.4282, MinusLogProbMetric: 406.4282, val_loss: 406.7207, val_MinusLogProbMetric: 406.7207

Epoch 56: val_loss improved from 407.01483 to 406.72070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 19s - loss: 406.4282 - MinusLogProbMetric: 406.4282 - val_loss: 406.7207 - val_MinusLogProbMetric: 406.7207 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 57/1000
2023-09-09 13:51:25.483 
Epoch 57/1000 
	 loss: 407.5734, MinusLogProbMetric: 407.5734, val_loss: 407.4607, val_MinusLogProbMetric: 407.4607

Epoch 57: val_loss did not improve from 406.72070
196/196 - 19s - loss: 407.5734 - MinusLogProbMetric: 407.5734 - val_loss: 407.4607 - val_MinusLogProbMetric: 407.4607 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 58/1000
2023-09-09 13:51:44.744 
Epoch 58/1000 
	 loss: 406.3670, MinusLogProbMetric: 406.3670, val_loss: 406.8938, val_MinusLogProbMetric: 406.8938

Epoch 58: val_loss did not improve from 406.72070
196/196 - 19s - loss: 406.3670 - MinusLogProbMetric: 406.3670 - val_loss: 406.8938 - val_MinusLogProbMetric: 406.8938 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 59/1000
2023-09-09 13:52:04.826 
Epoch 59/1000 
	 loss: 406.0578, MinusLogProbMetric: 406.0578, val_loss: 406.9617, val_MinusLogProbMetric: 406.9617

Epoch 59: val_loss did not improve from 406.72070
196/196 - 20s - loss: 406.0578 - MinusLogProbMetric: 406.0578 - val_loss: 406.9617 - val_MinusLogProbMetric: 406.9617 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 60/1000
2023-09-09 13:52:25.630 
Epoch 60/1000 
	 loss: 405.7996, MinusLogProbMetric: 405.7996, val_loss: 408.4208, val_MinusLogProbMetric: 408.4208

Epoch 60: val_loss did not improve from 406.72070
196/196 - 21s - loss: 405.7996 - MinusLogProbMetric: 405.7996 - val_loss: 408.4208 - val_MinusLogProbMetric: 408.4208 - lr: 3.3333e-04 - 21s/epoch - 106ms/step
Epoch 61/1000
2023-09-09 13:52:45.268 
Epoch 61/1000 
	 loss: 405.2775, MinusLogProbMetric: 405.2775, val_loss: 407.0680, val_MinusLogProbMetric: 407.0680

Epoch 61: val_loss did not improve from 406.72070
196/196 - 20s - loss: 405.2775 - MinusLogProbMetric: 405.2775 - val_loss: 407.0680 - val_MinusLogProbMetric: 407.0680 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 62/1000
2023-09-09 13:53:04.695 
Epoch 62/1000 
	 loss: 405.1352, MinusLogProbMetric: 405.1352, val_loss: 410.2266, val_MinusLogProbMetric: 410.2266

Epoch 62: val_loss did not improve from 406.72070
196/196 - 19s - loss: 405.1352 - MinusLogProbMetric: 405.1352 - val_loss: 410.2266 - val_MinusLogProbMetric: 410.2266 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 63/1000
2023-09-09 13:53:23.303 
Epoch 63/1000 
	 loss: 405.3553, MinusLogProbMetric: 405.3553, val_loss: 406.0997, val_MinusLogProbMetric: 406.0997

Epoch 63: val_loss improved from 406.72070 to 406.09967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 19s - loss: 405.3553 - MinusLogProbMetric: 405.3553 - val_loss: 406.0997 - val_MinusLogProbMetric: 406.0997 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 64/1000
2023-09-09 13:53:43.830 
Epoch 64/1000 
	 loss: 404.6100, MinusLogProbMetric: 404.6100, val_loss: 407.1916, val_MinusLogProbMetric: 407.1916

Epoch 64: val_loss did not improve from 406.09967
196/196 - 20s - loss: 404.6100 - MinusLogProbMetric: 404.6100 - val_loss: 407.1916 - val_MinusLogProbMetric: 407.1916 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 65/1000
2023-09-09 13:54:02.684 
Epoch 65/1000 
	 loss: 404.6775, MinusLogProbMetric: 404.6775, val_loss: 404.6660, val_MinusLogProbMetric: 404.6660

Epoch 65: val_loss improved from 406.09967 to 404.66599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 20s - loss: 404.6775 - MinusLogProbMetric: 404.6775 - val_loss: 404.6660 - val_MinusLogProbMetric: 404.6660 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 66/1000
2023-09-09 13:54:24.055 
Epoch 66/1000 
	 loss: 404.4828, MinusLogProbMetric: 404.4828, val_loss: 406.4696, val_MinusLogProbMetric: 406.4696

Epoch 66: val_loss did not improve from 404.66599
196/196 - 21s - loss: 404.4828 - MinusLogProbMetric: 404.4828 - val_loss: 406.4696 - val_MinusLogProbMetric: 406.4696 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 67/1000
2023-09-09 13:54:43.285 
Epoch 67/1000 
	 loss: 404.2495, MinusLogProbMetric: 404.2495, val_loss: 410.8671, val_MinusLogProbMetric: 410.8671

Epoch 67: val_loss did not improve from 404.66599
196/196 - 19s - loss: 404.2495 - MinusLogProbMetric: 404.2495 - val_loss: 410.8671 - val_MinusLogProbMetric: 410.8671 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 68/1000
2023-09-09 13:55:02.734 
Epoch 68/1000 
	 loss: 404.3010, MinusLogProbMetric: 404.3010, val_loss: 405.5960, val_MinusLogProbMetric: 405.5960

Epoch 68: val_loss did not improve from 404.66599
196/196 - 19s - loss: 404.3010 - MinusLogProbMetric: 404.3010 - val_loss: 405.5960 - val_MinusLogProbMetric: 405.5960 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 69/1000
2023-09-09 13:55:22.202 
Epoch 69/1000 
	 loss: 404.0524, MinusLogProbMetric: 404.0524, val_loss: 405.6010, val_MinusLogProbMetric: 405.6010

Epoch 69: val_loss did not improve from 404.66599
196/196 - 19s - loss: 404.0524 - MinusLogProbMetric: 404.0524 - val_loss: 405.6010 - val_MinusLogProbMetric: 405.6010 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 70/1000
2023-09-09 13:55:39.927 
Epoch 70/1000 
	 loss: 404.7014, MinusLogProbMetric: 404.7014, val_loss: 411.2399, val_MinusLogProbMetric: 411.2399

Epoch 70: val_loss did not improve from 404.66599
196/196 - 18s - loss: 404.7014 - MinusLogProbMetric: 404.7014 - val_loss: 411.2399 - val_MinusLogProbMetric: 411.2399 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 71/1000
2023-09-09 13:55:59.774 
Epoch 71/1000 
	 loss: 403.5937, MinusLogProbMetric: 403.5937, val_loss: 405.0084, val_MinusLogProbMetric: 405.0084

Epoch 71: val_loss did not improve from 404.66599
196/196 - 20s - loss: 403.5937 - MinusLogProbMetric: 403.5937 - val_loss: 405.0084 - val_MinusLogProbMetric: 405.0084 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 72/1000
2023-09-09 13:56:20.215 
Epoch 72/1000 
	 loss: 403.3388, MinusLogProbMetric: 403.3388, val_loss: 405.5323, val_MinusLogProbMetric: 405.5323

Epoch 72: val_loss did not improve from 404.66599
196/196 - 20s - loss: 403.3388 - MinusLogProbMetric: 403.3388 - val_loss: 405.5323 - val_MinusLogProbMetric: 405.5323 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 73/1000
2023-09-09 13:56:39.089 
Epoch 73/1000 
	 loss: 403.8858, MinusLogProbMetric: 403.8858, val_loss: 409.5402, val_MinusLogProbMetric: 409.5402

Epoch 73: val_loss did not improve from 404.66599
196/196 - 19s - loss: 403.8858 - MinusLogProbMetric: 403.8858 - val_loss: 409.5402 - val_MinusLogProbMetric: 409.5402 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 74/1000
2023-09-09 13:56:55.775 
Epoch 74/1000 
	 loss: 402.9782, MinusLogProbMetric: 402.9782, val_loss: 410.8256, val_MinusLogProbMetric: 410.8256

Epoch 74: val_loss did not improve from 404.66599
196/196 - 17s - loss: 402.9782 - MinusLogProbMetric: 402.9782 - val_loss: 410.8256 - val_MinusLogProbMetric: 410.8256 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 75/1000
2023-09-09 13:57:15.013 
Epoch 75/1000 
	 loss: 403.6075, MinusLogProbMetric: 403.6075, val_loss: 406.7258, val_MinusLogProbMetric: 406.7258

Epoch 75: val_loss did not improve from 404.66599
196/196 - 19s - loss: 403.6075 - MinusLogProbMetric: 403.6075 - val_loss: 406.7258 - val_MinusLogProbMetric: 406.7258 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 76/1000
2023-09-09 13:57:31.764 
Epoch 76/1000 
	 loss: 402.9290, MinusLogProbMetric: 402.9290, val_loss: 413.9080, val_MinusLogProbMetric: 413.9080

Epoch 76: val_loss did not improve from 404.66599
196/196 - 17s - loss: 402.9290 - MinusLogProbMetric: 402.9290 - val_loss: 413.9080 - val_MinusLogProbMetric: 413.9080 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 77/1000
2023-09-09 13:57:48.465 
Epoch 77/1000 
	 loss: 403.5626, MinusLogProbMetric: 403.5626, val_loss: 404.8530, val_MinusLogProbMetric: 404.8530

Epoch 77: val_loss did not improve from 404.66599
196/196 - 17s - loss: 403.5626 - MinusLogProbMetric: 403.5626 - val_loss: 404.8530 - val_MinusLogProbMetric: 404.8530 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 78/1000
2023-09-09 13:58:05.997 
Epoch 78/1000 
	 loss: 402.7876, MinusLogProbMetric: 402.7876, val_loss: 405.0634, val_MinusLogProbMetric: 405.0634

Epoch 78: val_loss did not improve from 404.66599
196/196 - 17s - loss: 402.7876 - MinusLogProbMetric: 402.7876 - val_loss: 405.0634 - val_MinusLogProbMetric: 405.0634 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 79/1000
2023-09-09 13:58:23.093 
Epoch 79/1000 
	 loss: 402.4347, MinusLogProbMetric: 402.4347, val_loss: 403.9183, val_MinusLogProbMetric: 403.9183

Epoch 79: val_loss improved from 404.66599 to 403.91833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 402.4347 - MinusLogProbMetric: 402.4347 - val_loss: 403.9183 - val_MinusLogProbMetric: 403.9183 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 80/1000
2023-09-09 13:58:41.389 
Epoch 80/1000 
	 loss: 402.0947, MinusLogProbMetric: 402.0947, val_loss: 404.8189, val_MinusLogProbMetric: 404.8189

Epoch 80: val_loss did not improve from 403.91833
196/196 - 17s - loss: 402.0947 - MinusLogProbMetric: 402.0947 - val_loss: 404.8189 - val_MinusLogProbMetric: 404.8189 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 81/1000
2023-09-09 13:58:58.773 
Epoch 81/1000 
	 loss: 402.4773, MinusLogProbMetric: 402.4773, val_loss: 403.8516, val_MinusLogProbMetric: 403.8516

Epoch 81: val_loss improved from 403.91833 to 403.85156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 402.4773 - MinusLogProbMetric: 402.4773 - val_loss: 403.8516 - val_MinusLogProbMetric: 403.8516 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 82/1000
2023-09-09 13:59:16.871 
Epoch 82/1000 
	 loss: 402.9154, MinusLogProbMetric: 402.9154, val_loss: 405.8480, val_MinusLogProbMetric: 405.8480

Epoch 82: val_loss did not improve from 403.85156
196/196 - 17s - loss: 402.9154 - MinusLogProbMetric: 402.9154 - val_loss: 405.8480 - val_MinusLogProbMetric: 405.8480 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 83/1000
2023-09-09 13:59:33.803 
Epoch 83/1000 
	 loss: 402.4441, MinusLogProbMetric: 402.4441, val_loss: 402.7935, val_MinusLogProbMetric: 402.7935

Epoch 83: val_loss improved from 403.85156 to 402.79346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 402.4441 - MinusLogProbMetric: 402.4441 - val_loss: 402.7935 - val_MinusLogProbMetric: 402.7935 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 84/1000
2023-09-09 13:59:53.112 
Epoch 84/1000 
	 loss: 401.7611, MinusLogProbMetric: 401.7611, val_loss: 409.1738, val_MinusLogProbMetric: 409.1738

Epoch 84: val_loss did not improve from 402.79346
196/196 - 18s - loss: 401.7611 - MinusLogProbMetric: 401.7611 - val_loss: 409.1738 - val_MinusLogProbMetric: 409.1738 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 85/1000
2023-09-09 14:00:09.295 
Epoch 85/1000 
	 loss: 401.7501, MinusLogProbMetric: 401.7501, val_loss: 402.9205, val_MinusLogProbMetric: 402.9205

Epoch 85: val_loss did not improve from 402.79346
196/196 - 16s - loss: 401.7501 - MinusLogProbMetric: 401.7501 - val_loss: 402.9205 - val_MinusLogProbMetric: 402.9205 - lr: 3.3333e-04 - 16s/epoch - 83ms/step
Epoch 86/1000
2023-09-09 14:00:26.231 
Epoch 86/1000 
	 loss: 401.6192, MinusLogProbMetric: 401.6192, val_loss: 403.3084, val_MinusLogProbMetric: 403.3084

Epoch 86: val_loss did not improve from 402.79346
196/196 - 17s - loss: 401.6192 - MinusLogProbMetric: 401.6192 - val_loss: 403.3084 - val_MinusLogProbMetric: 403.3084 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 87/1000
2023-09-09 14:00:42.334 
Epoch 87/1000 
	 loss: 401.4904, MinusLogProbMetric: 401.4904, val_loss: 401.8514, val_MinusLogProbMetric: 401.8514

Epoch 87: val_loss improved from 402.79346 to 401.85141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 17s - loss: 401.4904 - MinusLogProbMetric: 401.4904 - val_loss: 401.8514 - val_MinusLogProbMetric: 401.8514 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 88/1000
2023-09-09 14:01:00.117 
Epoch 88/1000 
	 loss: 402.1523, MinusLogProbMetric: 402.1523, val_loss: 402.3458, val_MinusLogProbMetric: 402.3458

Epoch 88: val_loss did not improve from 401.85141
196/196 - 17s - loss: 402.1523 - MinusLogProbMetric: 402.1523 - val_loss: 402.3458 - val_MinusLogProbMetric: 402.3458 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 89/1000
2023-09-09 14:01:17.619 
Epoch 89/1000 
	 loss: 401.5480, MinusLogProbMetric: 401.5480, val_loss: 402.9483, val_MinusLogProbMetric: 402.9483

Epoch 89: val_loss did not improve from 401.85141
196/196 - 17s - loss: 401.5480 - MinusLogProbMetric: 401.5480 - val_loss: 402.9483 - val_MinusLogProbMetric: 402.9483 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 90/1000
2023-09-09 14:01:34.947 
Epoch 90/1000 
	 loss: 401.4923, MinusLogProbMetric: 401.4923, val_loss: 404.1952, val_MinusLogProbMetric: 404.1952

Epoch 90: val_loss did not improve from 401.85141
196/196 - 17s - loss: 401.4923 - MinusLogProbMetric: 401.4923 - val_loss: 404.1952 - val_MinusLogProbMetric: 404.1952 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 91/1000
2023-09-09 14:01:52.830 
Epoch 91/1000 
	 loss: 401.1298, MinusLogProbMetric: 401.1298, val_loss: 404.2772, val_MinusLogProbMetric: 404.2772

Epoch 91: val_loss did not improve from 401.85141
196/196 - 18s - loss: 401.1298 - MinusLogProbMetric: 401.1298 - val_loss: 404.2772 - val_MinusLogProbMetric: 404.2772 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 92/1000
2023-09-09 14:02:09.716 
Epoch 92/1000 
	 loss: 401.5168, MinusLogProbMetric: 401.5168, val_loss: 402.8062, val_MinusLogProbMetric: 402.8062

Epoch 92: val_loss did not improve from 401.85141
196/196 - 17s - loss: 401.5168 - MinusLogProbMetric: 401.5168 - val_loss: 402.8062 - val_MinusLogProbMetric: 402.8062 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 93/1000
2023-09-09 14:02:26.511 
Epoch 93/1000 
	 loss: 401.3545, MinusLogProbMetric: 401.3545, val_loss: 402.7880, val_MinusLogProbMetric: 402.7880

Epoch 93: val_loss did not improve from 401.85141
196/196 - 17s - loss: 401.3545 - MinusLogProbMetric: 401.3545 - val_loss: 402.7880 - val_MinusLogProbMetric: 402.7880 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 94/1000
2023-09-09 14:02:44.230 
Epoch 94/1000 
	 loss: 400.8050, MinusLogProbMetric: 400.8050, val_loss: 403.9919, val_MinusLogProbMetric: 403.9919

Epoch 94: val_loss did not improve from 401.85141
196/196 - 18s - loss: 400.8050 - MinusLogProbMetric: 400.8050 - val_loss: 403.9919 - val_MinusLogProbMetric: 403.9919 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 95/1000
2023-09-09 14:03:00.268 
Epoch 95/1000 
	 loss: 400.6463, MinusLogProbMetric: 400.6463, val_loss: 402.2976, val_MinusLogProbMetric: 402.2976

Epoch 95: val_loss did not improve from 401.85141
196/196 - 16s - loss: 400.6463 - MinusLogProbMetric: 400.6463 - val_loss: 402.2976 - val_MinusLogProbMetric: 402.2976 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 96/1000
2023-09-09 14:03:14.745 
Epoch 96/1000 
	 loss: 401.3080, MinusLogProbMetric: 401.3080, val_loss: 402.8775, val_MinusLogProbMetric: 402.8775

Epoch 96: val_loss did not improve from 401.85141
196/196 - 14s - loss: 401.3080 - MinusLogProbMetric: 401.3080 - val_loss: 402.8775 - val_MinusLogProbMetric: 402.8775 - lr: 3.3333e-04 - 14s/epoch - 74ms/step
Epoch 97/1000
2023-09-09 14:03:29.080 
Epoch 97/1000 
	 loss: 400.3733, MinusLogProbMetric: 400.3733, val_loss: 402.2284, val_MinusLogProbMetric: 402.2284

Epoch 97: val_loss did not improve from 401.85141
196/196 - 14s - loss: 400.3733 - MinusLogProbMetric: 400.3733 - val_loss: 402.2284 - val_MinusLogProbMetric: 402.2284 - lr: 3.3333e-04 - 14s/epoch - 73ms/step
Epoch 98/1000
2023-09-09 14:03:43.765 
Epoch 98/1000 
	 loss: 400.9484, MinusLogProbMetric: 400.9484, val_loss: 402.3799, val_MinusLogProbMetric: 402.3799

Epoch 98: val_loss did not improve from 401.85141
196/196 - 15s - loss: 400.9484 - MinusLogProbMetric: 400.9484 - val_loss: 402.3799 - val_MinusLogProbMetric: 402.3799 - lr: 3.3333e-04 - 15s/epoch - 75ms/step
Epoch 99/1000
2023-09-09 14:03:58.241 
Epoch 99/1000 
	 loss: 400.3569, MinusLogProbMetric: 400.3569, val_loss: 402.0784, val_MinusLogProbMetric: 402.0784

Epoch 99: val_loss did not improve from 401.85141
196/196 - 14s - loss: 400.3569 - MinusLogProbMetric: 400.3569 - val_loss: 402.0784 - val_MinusLogProbMetric: 402.0784 - lr: 3.3333e-04 - 14s/epoch - 74ms/step
Epoch 100/1000
2023-09-09 14:04:13.195 
Epoch 100/1000 
	 loss: 400.2263, MinusLogProbMetric: 400.2263, val_loss: 401.4415, val_MinusLogProbMetric: 401.4415

Epoch 100: val_loss improved from 401.85141 to 401.44147, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 16s - loss: 400.2263 - MinusLogProbMetric: 400.2263 - val_loss: 401.4415 - val_MinusLogProbMetric: 401.4415 - lr: 3.3333e-04 - 16s/epoch - 80ms/step
Epoch 101/1000
2023-09-09 14:04:28.682 
Epoch 101/1000 
	 loss: 401.8320, MinusLogProbMetric: 401.8320, val_loss: 402.7538, val_MinusLogProbMetric: 402.7538

Epoch 101: val_loss did not improve from 401.44147
196/196 - 15s - loss: 401.8320 - MinusLogProbMetric: 401.8320 - val_loss: 402.7538 - val_MinusLogProbMetric: 402.7538 - lr: 3.3333e-04 - 15s/epoch - 75ms/step
Epoch 102/1000
2023-09-09 14:04:43.936 
Epoch 102/1000 
	 loss: 400.0859, MinusLogProbMetric: 400.0859, val_loss: 401.3996, val_MinusLogProbMetric: 401.3996

Epoch 102: val_loss improved from 401.44147 to 401.39957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 16s - loss: 400.0859 - MinusLogProbMetric: 400.0859 - val_loss: 401.3996 - val_MinusLogProbMetric: 401.3996 - lr: 3.3333e-04 - 16s/epoch - 81ms/step
Epoch 103/1000
2023-09-09 14:04:59.510 
Epoch 103/1000 
	 loss: 399.9810, MinusLogProbMetric: 399.9810, val_loss: 402.9768, val_MinusLogProbMetric: 402.9768

Epoch 103: val_loss did not improve from 401.39957
196/196 - 15s - loss: 399.9810 - MinusLogProbMetric: 399.9810 - val_loss: 402.9768 - val_MinusLogProbMetric: 402.9768 - lr: 3.3333e-04 - 15s/epoch - 76ms/step
Epoch 104/1000
2023-09-09 14:05:14.809 
Epoch 104/1000 
	 loss: 400.0066, MinusLogProbMetric: 400.0066, val_loss: 402.2069, val_MinusLogProbMetric: 402.2069

Epoch 104: val_loss did not improve from 401.39957
196/196 - 15s - loss: 400.0066 - MinusLogProbMetric: 400.0066 - val_loss: 402.2069 - val_MinusLogProbMetric: 402.2069 - lr: 3.3333e-04 - 15s/epoch - 78ms/step
Epoch 105/1000
2023-09-09 14:05:29.924 
Epoch 105/1000 
	 loss: 401.2890, MinusLogProbMetric: 401.2890, val_loss: 403.2694, val_MinusLogProbMetric: 403.2694

Epoch 105: val_loss did not improve from 401.39957
196/196 - 15s - loss: 401.2890 - MinusLogProbMetric: 401.2890 - val_loss: 403.2694 - val_MinusLogProbMetric: 403.2694 - lr: 3.3333e-04 - 15s/epoch - 77ms/step
Epoch 106/1000
2023-09-09 14:05:44.981 
Epoch 106/1000 
	 loss: 399.5709, MinusLogProbMetric: 399.5709, val_loss: 403.0618, val_MinusLogProbMetric: 403.0618

Epoch 106: val_loss did not improve from 401.39957
196/196 - 15s - loss: 399.5709 - MinusLogProbMetric: 399.5709 - val_loss: 403.0618 - val_MinusLogProbMetric: 403.0618 - lr: 3.3333e-04 - 15s/epoch - 77ms/step
Epoch 107/1000
2023-09-09 14:06:02.514 
Epoch 107/1000 
	 loss: 399.7156, MinusLogProbMetric: 399.7156, val_loss: 402.4438, val_MinusLogProbMetric: 402.4438

Epoch 107: val_loss did not improve from 401.39957
196/196 - 18s - loss: 399.7156 - MinusLogProbMetric: 399.7156 - val_loss: 402.4438 - val_MinusLogProbMetric: 402.4438 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 108/1000
2023-09-09 14:06:18.735 
Epoch 108/1000 
	 loss: 399.5484, MinusLogProbMetric: 399.5484, val_loss: 403.9454, val_MinusLogProbMetric: 403.9454

Epoch 108: val_loss did not improve from 401.39957
196/196 - 16s - loss: 399.5484 - MinusLogProbMetric: 399.5484 - val_loss: 403.9454 - val_MinusLogProbMetric: 403.9454 - lr: 3.3333e-04 - 16s/epoch - 83ms/step
Epoch 109/1000
2023-09-09 14:06:34.406 
Epoch 109/1000 
	 loss: 400.1987, MinusLogProbMetric: 400.1987, val_loss: 402.6625, val_MinusLogProbMetric: 402.6625

Epoch 109: val_loss did not improve from 401.39957
196/196 - 16s - loss: 400.1987 - MinusLogProbMetric: 400.1987 - val_loss: 402.6625 - val_MinusLogProbMetric: 402.6625 - lr: 3.3333e-04 - 16s/epoch - 80ms/step
Epoch 110/1000
2023-09-09 14:06:49.889 
Epoch 110/1000 
	 loss: 399.3630, MinusLogProbMetric: 399.3630, val_loss: 402.7303, val_MinusLogProbMetric: 402.7303

Epoch 110: val_loss did not improve from 401.39957
196/196 - 16s - loss: 399.3630 - MinusLogProbMetric: 399.3630 - val_loss: 402.7303 - val_MinusLogProbMetric: 402.7303 - lr: 3.3333e-04 - 16s/epoch - 79ms/step
Epoch 111/1000
2023-09-09 14:07:06.311 
Epoch 111/1000 
	 loss: 401.0278, MinusLogProbMetric: 401.0278, val_loss: 403.7804, val_MinusLogProbMetric: 403.7804

Epoch 111: val_loss did not improve from 401.39957
196/196 - 16s - loss: 401.0278 - MinusLogProbMetric: 401.0278 - val_loss: 403.7804 - val_MinusLogProbMetric: 403.7804 - lr: 3.3333e-04 - 16s/epoch - 84ms/step
Epoch 112/1000
2023-09-09 14:07:22.987 
Epoch 112/1000 
	 loss: 399.4458, MinusLogProbMetric: 399.4458, val_loss: 401.9549, val_MinusLogProbMetric: 401.9549

Epoch 112: val_loss did not improve from 401.39957
196/196 - 17s - loss: 399.4458 - MinusLogProbMetric: 399.4458 - val_loss: 401.9549 - val_MinusLogProbMetric: 401.9549 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 113/1000
2023-09-09 14:07:40.348 
Epoch 113/1000 
	 loss: 399.1720, MinusLogProbMetric: 399.1720, val_loss: 403.5712, val_MinusLogProbMetric: 403.5712

Epoch 113: val_loss did not improve from 401.39957
196/196 - 17s - loss: 399.1720 - MinusLogProbMetric: 399.1720 - val_loss: 403.5712 - val_MinusLogProbMetric: 403.5712 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 114/1000
2023-09-09 14:07:56.783 
Epoch 114/1000 
	 loss: 400.5824, MinusLogProbMetric: 400.5824, val_loss: 400.9218, val_MinusLogProbMetric: 400.9218

Epoch 114: val_loss improved from 401.39957 to 400.92175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 17s - loss: 400.5824 - MinusLogProbMetric: 400.5824 - val_loss: 400.9218 - val_MinusLogProbMetric: 400.9218 - lr: 3.3333e-04 - 17s/epoch - 88ms/step
Epoch 115/1000
2023-09-09 14:08:14.098 
Epoch 115/1000 
	 loss: 399.3604, MinusLogProbMetric: 399.3604, val_loss: 402.4288, val_MinusLogProbMetric: 402.4288

Epoch 115: val_loss did not improve from 400.92175
196/196 - 17s - loss: 399.3604 - MinusLogProbMetric: 399.3604 - val_loss: 402.4288 - val_MinusLogProbMetric: 402.4288 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 116/1000
2023-09-09 14:08:30.239 
Epoch 116/1000 
	 loss: 399.1312, MinusLogProbMetric: 399.1312, val_loss: 404.4557, val_MinusLogProbMetric: 404.4557

Epoch 116: val_loss did not improve from 400.92175
196/196 - 16s - loss: 399.1312 - MinusLogProbMetric: 399.1312 - val_loss: 404.4557 - val_MinusLogProbMetric: 404.4557 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 117/1000
2023-09-09 14:08:46.905 
Epoch 117/1000 
	 loss: 399.0944, MinusLogProbMetric: 399.0944, val_loss: 404.9847, val_MinusLogProbMetric: 404.9847

Epoch 117: val_loss did not improve from 400.92175
196/196 - 17s - loss: 399.0944 - MinusLogProbMetric: 399.0944 - val_loss: 404.9847 - val_MinusLogProbMetric: 404.9847 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 118/1000
2023-09-09 14:09:03.327 
Epoch 118/1000 
	 loss: 399.0027, MinusLogProbMetric: 399.0027, val_loss: 401.2724, val_MinusLogProbMetric: 401.2724

Epoch 118: val_loss did not improve from 400.92175
196/196 - 16s - loss: 399.0027 - MinusLogProbMetric: 399.0027 - val_loss: 401.2724 - val_MinusLogProbMetric: 401.2724 - lr: 3.3333e-04 - 16s/epoch - 84ms/step
Epoch 119/1000
2023-09-09 14:09:20.341 
Epoch 119/1000 
	 loss: 399.2636, MinusLogProbMetric: 399.2636, val_loss: 402.5955, val_MinusLogProbMetric: 402.5955

Epoch 119: val_loss did not improve from 400.92175
196/196 - 17s - loss: 399.2636 - MinusLogProbMetric: 399.2636 - val_loss: 402.5955 - val_MinusLogProbMetric: 402.5955 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 120/1000
2023-09-09 14:09:38.015 
Epoch 120/1000 
	 loss: 398.7970, MinusLogProbMetric: 398.7970, val_loss: 403.8638, val_MinusLogProbMetric: 403.8638

Epoch 120: val_loss did not improve from 400.92175
196/196 - 18s - loss: 398.7970 - MinusLogProbMetric: 398.7970 - val_loss: 403.8638 - val_MinusLogProbMetric: 403.8638 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 121/1000
2023-09-09 14:09:57.402 
Epoch 121/1000 
	 loss: 399.3972, MinusLogProbMetric: 399.3972, val_loss: 401.7906, val_MinusLogProbMetric: 401.7906

Epoch 121: val_loss did not improve from 400.92175
196/196 - 19s - loss: 399.3972 - MinusLogProbMetric: 399.3972 - val_loss: 401.7906 - val_MinusLogProbMetric: 401.7906 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 122/1000
2023-09-09 14:10:15.389 
Epoch 122/1000 
	 loss: 399.0163, MinusLogProbMetric: 399.0163, val_loss: 405.0829, val_MinusLogProbMetric: 405.0829

Epoch 122: val_loss did not improve from 400.92175
196/196 - 18s - loss: 399.0163 - MinusLogProbMetric: 399.0163 - val_loss: 405.0829 - val_MinusLogProbMetric: 405.0829 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 123/1000
2023-09-09 14:10:34.556 
Epoch 123/1000 
	 loss: 399.3433, MinusLogProbMetric: 399.3433, val_loss: 400.9709, val_MinusLogProbMetric: 400.9709

Epoch 123: val_loss did not improve from 400.92175
196/196 - 19s - loss: 399.3433 - MinusLogProbMetric: 399.3433 - val_loss: 400.9709 - val_MinusLogProbMetric: 400.9709 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 124/1000
2023-09-09 14:10:53.017 
Epoch 124/1000 
	 loss: 398.7548, MinusLogProbMetric: 398.7548, val_loss: 405.0217, val_MinusLogProbMetric: 405.0217

Epoch 124: val_loss did not improve from 400.92175
196/196 - 18s - loss: 398.7548 - MinusLogProbMetric: 398.7548 - val_loss: 405.0217 - val_MinusLogProbMetric: 405.0217 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 125/1000
2023-09-09 14:11:10.968 
Epoch 125/1000 
	 loss: 399.1288, MinusLogProbMetric: 399.1288, val_loss: 399.8752, val_MinusLogProbMetric: 399.8752

Epoch 125: val_loss improved from 400.92175 to 399.87515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 19s - loss: 399.1288 - MinusLogProbMetric: 399.1288 - val_loss: 399.8752 - val_MinusLogProbMetric: 399.8752 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 126/1000
2023-09-09 14:11:31.369 
Epoch 126/1000 
	 loss: 398.3526, MinusLogProbMetric: 398.3526, val_loss: 399.6275, val_MinusLogProbMetric: 399.6275

Epoch 126: val_loss improved from 399.87515 to 399.62747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 20s - loss: 398.3526 - MinusLogProbMetric: 398.3526 - val_loss: 399.6275 - val_MinusLogProbMetric: 399.6275 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 127/1000
2023-09-09 14:11:50.414 
Epoch 127/1000 
	 loss: 398.5617, MinusLogProbMetric: 398.5617, val_loss: 401.2601, val_MinusLogProbMetric: 401.2601

Epoch 127: val_loss did not improve from 399.62747
196/196 - 18s - loss: 398.5617 - MinusLogProbMetric: 398.5617 - val_loss: 401.2601 - val_MinusLogProbMetric: 401.2601 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 128/1000
2023-09-09 14:12:08.521 
Epoch 128/1000 
	 loss: 398.4144, MinusLogProbMetric: 398.4144, val_loss: 400.7200, val_MinusLogProbMetric: 400.7200

Epoch 128: val_loss did not improve from 399.62747
196/196 - 18s - loss: 398.4144 - MinusLogProbMetric: 398.4144 - val_loss: 400.7200 - val_MinusLogProbMetric: 400.7200 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 129/1000
2023-09-09 14:12:26.615 
Epoch 129/1000 
	 loss: 398.2928, MinusLogProbMetric: 398.2928, val_loss: 401.9640, val_MinusLogProbMetric: 401.9640

Epoch 129: val_loss did not improve from 399.62747
196/196 - 18s - loss: 398.2928 - MinusLogProbMetric: 398.2928 - val_loss: 401.9640 - val_MinusLogProbMetric: 401.9640 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 130/1000
2023-09-09 14:12:43.665 
Epoch 130/1000 
	 loss: 398.6241, MinusLogProbMetric: 398.6241, val_loss: 400.7165, val_MinusLogProbMetric: 400.7165

Epoch 130: val_loss did not improve from 399.62747
196/196 - 17s - loss: 398.6241 - MinusLogProbMetric: 398.6241 - val_loss: 400.7165 - val_MinusLogProbMetric: 400.7165 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 131/1000
2023-09-09 14:13:00.698 
Epoch 131/1000 
	 loss: 398.2183, MinusLogProbMetric: 398.2183, val_loss: 405.4582, val_MinusLogProbMetric: 405.4582

Epoch 131: val_loss did not improve from 399.62747
196/196 - 17s - loss: 398.2183 - MinusLogProbMetric: 398.2183 - val_loss: 405.4582 - val_MinusLogProbMetric: 405.4582 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 132/1000
2023-09-09 14:13:17.095 
Epoch 132/1000 
	 loss: 398.1650, MinusLogProbMetric: 398.1650, val_loss: 400.2137, val_MinusLogProbMetric: 400.2137

Epoch 132: val_loss did not improve from 399.62747
196/196 - 16s - loss: 398.1650 - MinusLogProbMetric: 398.1650 - val_loss: 400.2137 - val_MinusLogProbMetric: 400.2137 - lr: 3.3333e-04 - 16s/epoch - 84ms/step
Epoch 133/1000
2023-09-09 14:13:34.474 
Epoch 133/1000 
	 loss: 398.6772, MinusLogProbMetric: 398.6772, val_loss: 400.3509, val_MinusLogProbMetric: 400.3509

Epoch 133: val_loss did not improve from 399.62747
196/196 - 17s - loss: 398.6772 - MinusLogProbMetric: 398.6772 - val_loss: 400.3509 - val_MinusLogProbMetric: 400.3509 - lr: 3.3333e-04 - 17s/epoch - 89ms/step
Epoch 134/1000
2023-09-09 14:13:51.267 
Epoch 134/1000 
	 loss: 398.1902, MinusLogProbMetric: 398.1902, val_loss: 399.9754, val_MinusLogProbMetric: 399.9754

Epoch 134: val_loss did not improve from 399.62747
196/196 - 17s - loss: 398.1902 - MinusLogProbMetric: 398.1902 - val_loss: 399.9754 - val_MinusLogProbMetric: 399.9754 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 135/1000
2023-09-09 14:14:08.223 
Epoch 135/1000 
	 loss: 398.1314, MinusLogProbMetric: 398.1314, val_loss: 400.6461, val_MinusLogProbMetric: 400.6461

Epoch 135: val_loss did not improve from 399.62747
196/196 - 17s - loss: 398.1314 - MinusLogProbMetric: 398.1314 - val_loss: 400.6461 - val_MinusLogProbMetric: 400.6461 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 136/1000
2023-09-09 14:14:24.759 
Epoch 136/1000 
	 loss: 397.9415, MinusLogProbMetric: 397.9415, val_loss: 400.1001, val_MinusLogProbMetric: 400.1001

Epoch 136: val_loss did not improve from 399.62747
196/196 - 17s - loss: 397.9415 - MinusLogProbMetric: 397.9415 - val_loss: 400.1001 - val_MinusLogProbMetric: 400.1001 - lr: 3.3333e-04 - 17s/epoch - 84ms/step
Epoch 137/1000
2023-09-09 14:14:41.280 
Epoch 137/1000 
	 loss: 398.0922, MinusLogProbMetric: 398.0922, val_loss: 401.1870, val_MinusLogProbMetric: 401.1870

Epoch 137: val_loss did not improve from 399.62747
196/196 - 17s - loss: 398.0922 - MinusLogProbMetric: 398.0922 - val_loss: 401.1870 - val_MinusLogProbMetric: 401.1870 - lr: 3.3333e-04 - 17s/epoch - 84ms/step
Epoch 138/1000
2023-09-09 14:14:57.169 
Epoch 138/1000 
	 loss: 397.9761, MinusLogProbMetric: 397.9761, val_loss: 399.6852, val_MinusLogProbMetric: 399.6852

Epoch 138: val_loss did not improve from 399.62747
196/196 - 16s - loss: 397.9761 - MinusLogProbMetric: 397.9761 - val_loss: 399.6852 - val_MinusLogProbMetric: 399.6852 - lr: 3.3333e-04 - 16s/epoch - 81ms/step
Epoch 139/1000
2023-09-09 14:15:14.036 
Epoch 139/1000 
	 loss: 398.1835, MinusLogProbMetric: 398.1835, val_loss: 399.4722, val_MinusLogProbMetric: 399.4722

Epoch 139: val_loss improved from 399.62747 to 399.47220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 398.1835 - MinusLogProbMetric: 398.1835 - val_loss: 399.4722 - val_MinusLogProbMetric: 399.4722 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 140/1000
2023-09-09 14:15:30.710 
Epoch 140/1000 
	 loss: 398.4121, MinusLogProbMetric: 398.4121, val_loss: 401.9243, val_MinusLogProbMetric: 401.9243

Epoch 140: val_loss did not improve from 399.47220
196/196 - 16s - loss: 398.4121 - MinusLogProbMetric: 398.4121 - val_loss: 401.9243 - val_MinusLogProbMetric: 401.9243 - lr: 3.3333e-04 - 16s/epoch - 81ms/step
Epoch 141/1000
2023-09-09 14:15:47.833 
Epoch 141/1000 
	 loss: 397.7484, MinusLogProbMetric: 397.7484, val_loss: 400.5808, val_MinusLogProbMetric: 400.5808

Epoch 141: val_loss did not improve from 399.47220
196/196 - 17s - loss: 397.7484 - MinusLogProbMetric: 397.7484 - val_loss: 400.5808 - val_MinusLogProbMetric: 400.5808 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 142/1000
2023-09-09 14:16:04.668 
Epoch 142/1000 
	 loss: 398.1660, MinusLogProbMetric: 398.1660, val_loss: 399.9706, val_MinusLogProbMetric: 399.9706

Epoch 142: val_loss did not improve from 399.47220
196/196 - 17s - loss: 398.1660 - MinusLogProbMetric: 398.1660 - val_loss: 399.9706 - val_MinusLogProbMetric: 399.9706 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 143/1000
2023-09-09 14:16:21.569 
Epoch 143/1000 
	 loss: 397.6561, MinusLogProbMetric: 397.6561, val_loss: 401.8837, val_MinusLogProbMetric: 401.8837

Epoch 143: val_loss did not improve from 399.47220
196/196 - 17s - loss: 397.6561 - MinusLogProbMetric: 397.6561 - val_loss: 401.8837 - val_MinusLogProbMetric: 401.8837 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 144/1000
2023-09-09 14:16:41.005 
Epoch 144/1000 
	 loss: 397.6573, MinusLogProbMetric: 397.6573, val_loss: 401.7251, val_MinusLogProbMetric: 401.7251

Epoch 144: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.6573 - MinusLogProbMetric: 397.6573 - val_loss: 401.7251 - val_MinusLogProbMetric: 401.7251 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 145/1000
2023-09-09 14:16:59.904 
Epoch 145/1000 
	 loss: 397.8779, MinusLogProbMetric: 397.8779, val_loss: 399.9457, val_MinusLogProbMetric: 399.9457

Epoch 145: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.8779 - MinusLogProbMetric: 397.8779 - val_loss: 399.9457 - val_MinusLogProbMetric: 399.9457 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 146/1000
2023-09-09 14:17:19.170 
Epoch 146/1000 
	 loss: 397.6523, MinusLogProbMetric: 397.6523, val_loss: 400.5960, val_MinusLogProbMetric: 400.5960

Epoch 146: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.6523 - MinusLogProbMetric: 397.6523 - val_loss: 400.5960 - val_MinusLogProbMetric: 400.5960 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 147/1000
2023-09-09 14:17:37.805 
Epoch 147/1000 
	 loss: 397.5978, MinusLogProbMetric: 397.5978, val_loss: 402.4681, val_MinusLogProbMetric: 402.4681

Epoch 147: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.5978 - MinusLogProbMetric: 397.5978 - val_loss: 402.4681 - val_MinusLogProbMetric: 402.4681 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 148/1000
2023-09-09 14:17:55.673 
Epoch 148/1000 
	 loss: 397.6210, MinusLogProbMetric: 397.6210, val_loss: 401.4428, val_MinusLogProbMetric: 401.4428

Epoch 148: val_loss did not improve from 399.47220
196/196 - 18s - loss: 397.6210 - MinusLogProbMetric: 397.6210 - val_loss: 401.4428 - val_MinusLogProbMetric: 401.4428 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 149/1000
2023-09-09 14:18:14.751 
Epoch 149/1000 
	 loss: 397.6606, MinusLogProbMetric: 397.6606, val_loss: 402.2092, val_MinusLogProbMetric: 402.2092

Epoch 149: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.6606 - MinusLogProbMetric: 397.6606 - val_loss: 402.2092 - val_MinusLogProbMetric: 402.2092 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 150/1000
2023-09-09 14:18:34.883 
Epoch 150/1000 
	 loss: 397.4502, MinusLogProbMetric: 397.4502, val_loss: 401.6099, val_MinusLogProbMetric: 401.6099

Epoch 150: val_loss did not improve from 399.47220
196/196 - 20s - loss: 397.4502 - MinusLogProbMetric: 397.4502 - val_loss: 401.6099 - val_MinusLogProbMetric: 401.6099 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 151/1000
2023-09-09 14:18:53.188 
Epoch 151/1000 
	 loss: 397.7569, MinusLogProbMetric: 397.7569, val_loss: 400.3573, val_MinusLogProbMetric: 400.3573

Epoch 151: val_loss did not improve from 399.47220
196/196 - 18s - loss: 397.7569 - MinusLogProbMetric: 397.7569 - val_loss: 400.3573 - val_MinusLogProbMetric: 400.3573 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 152/1000
2023-09-09 14:19:12.374 
Epoch 152/1000 
	 loss: 397.4995, MinusLogProbMetric: 397.4995, val_loss: 400.5346, val_MinusLogProbMetric: 400.5346

Epoch 152: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.4995 - MinusLogProbMetric: 397.4995 - val_loss: 400.5346 - val_MinusLogProbMetric: 400.5346 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 153/1000
2023-09-09 14:19:30.336 
Epoch 153/1000 
	 loss: 397.1711, MinusLogProbMetric: 397.1711, val_loss: 404.0117, val_MinusLogProbMetric: 404.0117

Epoch 153: val_loss did not improve from 399.47220
196/196 - 18s - loss: 397.1711 - MinusLogProbMetric: 397.1711 - val_loss: 404.0117 - val_MinusLogProbMetric: 404.0117 - lr: 3.3333e-04 - 18s/epoch - 91ms/step
Epoch 154/1000
2023-09-09 14:19:49.847 
Epoch 154/1000 
	 loss: 397.3359, MinusLogProbMetric: 397.3359, val_loss: 405.7472, val_MinusLogProbMetric: 405.7472

Epoch 154: val_loss did not improve from 399.47220
196/196 - 20s - loss: 397.3359 - MinusLogProbMetric: 397.3359 - val_loss: 405.7472 - val_MinusLogProbMetric: 405.7472 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 155/1000
2023-09-09 14:20:09.092 
Epoch 155/1000 
	 loss: 397.3508, MinusLogProbMetric: 397.3508, val_loss: 400.0589, val_MinusLogProbMetric: 400.0589

Epoch 155: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.3508 - MinusLogProbMetric: 397.3508 - val_loss: 400.0589 - val_MinusLogProbMetric: 400.0589 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 156/1000
2023-09-09 14:20:28.589 
Epoch 156/1000 
	 loss: 397.7474, MinusLogProbMetric: 397.7474, val_loss: 399.5924, val_MinusLogProbMetric: 399.5924

Epoch 156: val_loss did not improve from 399.47220
196/196 - 19s - loss: 397.7474 - MinusLogProbMetric: 397.7474 - val_loss: 399.5924 - val_MinusLogProbMetric: 399.5924 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 157/1000
2023-09-09 14:20:48.161 
Epoch 157/1000 
	 loss: 396.9857, MinusLogProbMetric: 396.9857, val_loss: 401.7729, val_MinusLogProbMetric: 401.7729

Epoch 157: val_loss did not improve from 399.47220
196/196 - 20s - loss: 396.9857 - MinusLogProbMetric: 396.9857 - val_loss: 401.7729 - val_MinusLogProbMetric: 401.7729 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 158/1000
2023-09-09 14:21:06.400 
Epoch 158/1000 
	 loss: 397.1548, MinusLogProbMetric: 397.1548, val_loss: 400.4271, val_MinusLogProbMetric: 400.4271

Epoch 158: val_loss did not improve from 399.47220
196/196 - 18s - loss: 397.1548 - MinusLogProbMetric: 397.1548 - val_loss: 400.4271 - val_MinusLogProbMetric: 400.4271 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 159/1000
2023-09-09 14:21:26.250 
Epoch 159/1000 
	 loss: 396.9586, MinusLogProbMetric: 396.9586, val_loss: 403.4988, val_MinusLogProbMetric: 403.4988

Epoch 159: val_loss did not improve from 399.47220
196/196 - 20s - loss: 396.9586 - MinusLogProbMetric: 396.9586 - val_loss: 403.4988 - val_MinusLogProbMetric: 403.4988 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 160/1000
2023-09-09 14:21:45.807 
Epoch 160/1000 
	 loss: 397.0612, MinusLogProbMetric: 397.0612, val_loss: 399.2551, val_MinusLogProbMetric: 399.2551

Epoch 160: val_loss improved from 399.47220 to 399.25513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 20s - loss: 397.0612 - MinusLogProbMetric: 397.0612 - val_loss: 399.2551 - val_MinusLogProbMetric: 399.2551 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 161/1000
2023-09-09 14:22:05.704 
Epoch 161/1000 
	 loss: 396.5723, MinusLogProbMetric: 396.5723, val_loss: 403.4104, val_MinusLogProbMetric: 403.4104

Epoch 161: val_loss did not improve from 399.25513
196/196 - 19s - loss: 396.5723 - MinusLogProbMetric: 396.5723 - val_loss: 403.4104 - val_MinusLogProbMetric: 403.4104 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 162/1000
2023-09-09 14:22:25.623 
Epoch 162/1000 
	 loss: 397.5619, MinusLogProbMetric: 397.5619, val_loss: 398.7520, val_MinusLogProbMetric: 398.7520

Epoch 162: val_loss improved from 399.25513 to 398.75201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 21s - loss: 397.5619 - MinusLogProbMetric: 397.5619 - val_loss: 398.7520 - val_MinusLogProbMetric: 398.7520 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 163/1000
2023-09-09 14:22:46.933 
Epoch 163/1000 
	 loss: 396.9257, MinusLogProbMetric: 396.9257, val_loss: 405.9297, val_MinusLogProbMetric: 405.9297

Epoch 163: val_loss did not improve from 398.75201
196/196 - 21s - loss: 396.9257 - MinusLogProbMetric: 396.9257 - val_loss: 405.9297 - val_MinusLogProbMetric: 405.9297 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 164/1000
2023-09-09 14:23:06.910 
Epoch 164/1000 
	 loss: 396.8168, MinusLogProbMetric: 396.8168, val_loss: 399.6842, val_MinusLogProbMetric: 399.6842

Epoch 164: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.8168 - MinusLogProbMetric: 396.8168 - val_loss: 399.6842 - val_MinusLogProbMetric: 399.6842 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 165/1000
2023-09-09 14:23:27.211 
Epoch 165/1000 
	 loss: 396.8141, MinusLogProbMetric: 396.8141, val_loss: 400.1545, val_MinusLogProbMetric: 400.1545

Epoch 165: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.8141 - MinusLogProbMetric: 396.8141 - val_loss: 400.1545 - val_MinusLogProbMetric: 400.1545 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 166/1000
2023-09-09 14:23:46.846 
Epoch 166/1000 
	 loss: 396.4588, MinusLogProbMetric: 396.4588, val_loss: 401.1665, val_MinusLogProbMetric: 401.1665

Epoch 166: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.4588 - MinusLogProbMetric: 396.4588 - val_loss: 401.1665 - val_MinusLogProbMetric: 401.1665 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 167/1000
2023-09-09 14:24:05.511 
Epoch 167/1000 
	 loss: 396.8191, MinusLogProbMetric: 396.8191, val_loss: 401.5244, val_MinusLogProbMetric: 401.5244

Epoch 167: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.8191 - MinusLogProbMetric: 396.8191 - val_loss: 401.5244 - val_MinusLogProbMetric: 401.5244 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 168/1000
2023-09-09 14:24:25.844 
Epoch 168/1000 
	 loss: 396.6078, MinusLogProbMetric: 396.6078, val_loss: 399.9886, val_MinusLogProbMetric: 399.9886

Epoch 168: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.6078 - MinusLogProbMetric: 396.6078 - val_loss: 399.9886 - val_MinusLogProbMetric: 399.9886 - lr: 3.3333e-04 - 20s/epoch - 104ms/step
Epoch 169/1000
2023-09-09 14:24:45.089 
Epoch 169/1000 
	 loss: 396.6186, MinusLogProbMetric: 396.6186, val_loss: 398.9081, val_MinusLogProbMetric: 398.9081

Epoch 169: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.6186 - MinusLogProbMetric: 396.6186 - val_loss: 398.9081 - val_MinusLogProbMetric: 398.9081 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 170/1000
2023-09-09 14:25:05.317 
Epoch 170/1000 
	 loss: 396.6236, MinusLogProbMetric: 396.6236, val_loss: 399.5418, val_MinusLogProbMetric: 399.5418

Epoch 170: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.6236 - MinusLogProbMetric: 396.6236 - val_loss: 399.5418 - val_MinusLogProbMetric: 399.5418 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 171/1000
2023-09-09 14:25:24.842 
Epoch 171/1000 
	 loss: 396.5161, MinusLogProbMetric: 396.5161, val_loss: 400.7751, val_MinusLogProbMetric: 400.7751

Epoch 171: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.5161 - MinusLogProbMetric: 396.5161 - val_loss: 400.7751 - val_MinusLogProbMetric: 400.7751 - lr: 3.3333e-04 - 20s/epoch - 100ms/step
Epoch 172/1000
2023-09-09 14:25:44.183 
Epoch 172/1000 
	 loss: 397.0134, MinusLogProbMetric: 397.0134, val_loss: 399.7870, val_MinusLogProbMetric: 399.7870

Epoch 172: val_loss did not improve from 398.75201
196/196 - 19s - loss: 397.0134 - MinusLogProbMetric: 397.0134 - val_loss: 399.7870 - val_MinusLogProbMetric: 399.7870 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 173/1000
2023-09-09 14:26:02.951 
Epoch 173/1000 
	 loss: 396.3912, MinusLogProbMetric: 396.3912, val_loss: 405.4534, val_MinusLogProbMetric: 405.4534

Epoch 173: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.3912 - MinusLogProbMetric: 396.3912 - val_loss: 405.4534 - val_MinusLogProbMetric: 405.4534 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 174/1000
2023-09-09 14:26:22.699 
Epoch 174/1000 
	 loss: 396.3423, MinusLogProbMetric: 396.3423, val_loss: 399.4312, val_MinusLogProbMetric: 399.4312

Epoch 174: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.3423 - MinusLogProbMetric: 396.3423 - val_loss: 399.4312 - val_MinusLogProbMetric: 399.4312 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 175/1000
2023-09-09 14:26:41.575 
Epoch 175/1000 
	 loss: 396.7126, MinusLogProbMetric: 396.7126, val_loss: 399.8707, val_MinusLogProbMetric: 399.8707

Epoch 175: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.7126 - MinusLogProbMetric: 396.7126 - val_loss: 399.8707 - val_MinusLogProbMetric: 399.8707 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 176/1000
2023-09-09 14:27:01.055 
Epoch 176/1000 
	 loss: 396.2318, MinusLogProbMetric: 396.2318, val_loss: 403.7011, val_MinusLogProbMetric: 403.7011

Epoch 176: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.2318 - MinusLogProbMetric: 396.2318 - val_loss: 403.7011 - val_MinusLogProbMetric: 403.7011 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 177/1000
2023-09-09 14:27:20.449 
Epoch 177/1000 
	 loss: 396.4140, MinusLogProbMetric: 396.4140, val_loss: 401.0942, val_MinusLogProbMetric: 401.0942

Epoch 177: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.4140 - MinusLogProbMetric: 396.4140 - val_loss: 401.0942 - val_MinusLogProbMetric: 401.0942 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 178/1000
2023-09-09 14:27:39.302 
Epoch 178/1000 
	 loss: 396.2025, MinusLogProbMetric: 396.2025, val_loss: 399.1060, val_MinusLogProbMetric: 399.1060

Epoch 178: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.2025 - MinusLogProbMetric: 396.2025 - val_loss: 399.1060 - val_MinusLogProbMetric: 399.1060 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 179/1000
2023-09-09 14:28:00.130 
Epoch 179/1000 
	 loss: 396.6449, MinusLogProbMetric: 396.6449, val_loss: 400.0665, val_MinusLogProbMetric: 400.0665

Epoch 179: val_loss did not improve from 398.75201
196/196 - 21s - loss: 396.6449 - MinusLogProbMetric: 396.6449 - val_loss: 400.0665 - val_MinusLogProbMetric: 400.0665 - lr: 3.3333e-04 - 21s/epoch - 106ms/step
Epoch 180/1000
2023-09-09 14:28:18.969 
Epoch 180/1000 
	 loss: 396.1844, MinusLogProbMetric: 396.1844, val_loss: 399.4374, val_MinusLogProbMetric: 399.4374

Epoch 180: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.1844 - MinusLogProbMetric: 396.1844 - val_loss: 399.4374 - val_MinusLogProbMetric: 399.4374 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 181/1000
2023-09-09 14:28:37.751 
Epoch 181/1000 
	 loss: 395.9877, MinusLogProbMetric: 395.9877, val_loss: 408.7959, val_MinusLogProbMetric: 408.7959

Epoch 181: val_loss did not improve from 398.75201
196/196 - 19s - loss: 395.9877 - MinusLogProbMetric: 395.9877 - val_loss: 408.7959 - val_MinusLogProbMetric: 408.7959 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 182/1000
2023-09-09 14:28:56.181 
Epoch 182/1000 
	 loss: 396.4502, MinusLogProbMetric: 396.4502, val_loss: 401.5988, val_MinusLogProbMetric: 401.5988

Epoch 182: val_loss did not improve from 398.75201
196/196 - 18s - loss: 396.4502 - MinusLogProbMetric: 396.4502 - val_loss: 401.5988 - val_MinusLogProbMetric: 401.5988 - lr: 3.3333e-04 - 18s/epoch - 94ms/step
Epoch 183/1000
2023-09-09 14:29:16.065 
Epoch 183/1000 
	 loss: 395.9754, MinusLogProbMetric: 395.9754, val_loss: 399.6697, val_MinusLogProbMetric: 399.6697

Epoch 183: val_loss did not improve from 398.75201
196/196 - 20s - loss: 395.9754 - MinusLogProbMetric: 395.9754 - val_loss: 399.6697 - val_MinusLogProbMetric: 399.6697 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 184/1000
2023-09-09 14:29:35.246 
Epoch 184/1000 
	 loss: 396.1407, MinusLogProbMetric: 396.1407, val_loss: 400.3234, val_MinusLogProbMetric: 400.3234

Epoch 184: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.1407 - MinusLogProbMetric: 396.1407 - val_loss: 400.3234 - val_MinusLogProbMetric: 400.3234 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 185/1000
2023-09-09 14:29:53.889 
Epoch 185/1000 
	 loss: 396.1607, MinusLogProbMetric: 396.1607, val_loss: 407.8484, val_MinusLogProbMetric: 407.8484

Epoch 185: val_loss did not improve from 398.75201
196/196 - 19s - loss: 396.1607 - MinusLogProbMetric: 396.1607 - val_loss: 407.8484 - val_MinusLogProbMetric: 407.8484 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 186/1000
2023-09-09 14:30:12.020 
Epoch 186/1000 
	 loss: 396.1534, MinusLogProbMetric: 396.1534, val_loss: 400.1291, val_MinusLogProbMetric: 400.1291

Epoch 186: val_loss did not improve from 398.75201
196/196 - 18s - loss: 396.1534 - MinusLogProbMetric: 396.1534 - val_loss: 400.1291 - val_MinusLogProbMetric: 400.1291 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 187/1000
2023-09-09 14:30:31.931 
Epoch 187/1000 
	 loss: 396.0979, MinusLogProbMetric: 396.0979, val_loss: 400.7419, val_MinusLogProbMetric: 400.7419

Epoch 187: val_loss did not improve from 398.75201
196/196 - 20s - loss: 396.0979 - MinusLogProbMetric: 396.0979 - val_loss: 400.7419 - val_MinusLogProbMetric: 400.7419 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 188/1000
2023-09-09 14:30:51.367 
Epoch 188/1000 
	 loss: 395.8822, MinusLogProbMetric: 395.8822, val_loss: 399.4731, val_MinusLogProbMetric: 399.4731

Epoch 188: val_loss did not improve from 398.75201
196/196 - 19s - loss: 395.8822 - MinusLogProbMetric: 395.8822 - val_loss: 399.4731 - val_MinusLogProbMetric: 399.4731 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 189/1000
2023-09-09 14:31:09.405 
Epoch 189/1000 
	 loss: 396.2175, MinusLogProbMetric: 396.2175, val_loss: 398.1127, val_MinusLogProbMetric: 398.1127

Epoch 189: val_loss improved from 398.75201 to 398.11273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 19s - loss: 396.2175 - MinusLogProbMetric: 396.2175 - val_loss: 398.1127 - val_MinusLogProbMetric: 398.1127 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 190/1000
2023-09-09 14:31:28.700 
Epoch 190/1000 
	 loss: 396.2704, MinusLogProbMetric: 396.2704, val_loss: 400.1256, val_MinusLogProbMetric: 400.1256

Epoch 190: val_loss did not improve from 398.11273
196/196 - 19s - loss: 396.2704 - MinusLogProbMetric: 396.2704 - val_loss: 400.1256 - val_MinusLogProbMetric: 400.1256 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 191/1000
2023-09-09 14:31:48.719 
Epoch 191/1000 
	 loss: 395.7583, MinusLogProbMetric: 395.7583, val_loss: 399.8579, val_MinusLogProbMetric: 399.8579

Epoch 191: val_loss did not improve from 398.11273
196/196 - 20s - loss: 395.7583 - MinusLogProbMetric: 395.7583 - val_loss: 399.8579 - val_MinusLogProbMetric: 399.8579 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 192/1000
2023-09-09 14:32:08.681 
Epoch 192/1000 
	 loss: 396.2165, MinusLogProbMetric: 396.2165, val_loss: 399.2141, val_MinusLogProbMetric: 399.2141

Epoch 192: val_loss did not improve from 398.11273
196/196 - 20s - loss: 396.2165 - MinusLogProbMetric: 396.2165 - val_loss: 399.2141 - val_MinusLogProbMetric: 399.2141 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 193/1000
2023-09-09 14:32:27.485 
Epoch 193/1000 
	 loss: 395.8607, MinusLogProbMetric: 395.8607, val_loss: 399.3420, val_MinusLogProbMetric: 399.3420

Epoch 193: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.8607 - MinusLogProbMetric: 395.8607 - val_loss: 399.3420 - val_MinusLogProbMetric: 399.3420 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 194/1000
2023-09-09 14:32:47.223 
Epoch 194/1000 
	 loss: 396.1300, MinusLogProbMetric: 396.1300, val_loss: 399.2515, val_MinusLogProbMetric: 399.2515

Epoch 194: val_loss did not improve from 398.11273
196/196 - 20s - loss: 396.1300 - MinusLogProbMetric: 396.1300 - val_loss: 399.2515 - val_MinusLogProbMetric: 399.2515 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 195/1000
2023-09-09 14:33:06.723 
Epoch 195/1000 
	 loss: 395.8058, MinusLogProbMetric: 395.8058, val_loss: 400.1765, val_MinusLogProbMetric: 400.1765

Epoch 195: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.8058 - MinusLogProbMetric: 395.8058 - val_loss: 400.1765 - val_MinusLogProbMetric: 400.1765 - lr: 3.3333e-04 - 19s/epoch - 99ms/step
Epoch 196/1000
2023-09-09 14:33:25.575 
Epoch 196/1000 
	 loss: 395.8930, MinusLogProbMetric: 395.8930, val_loss: 398.9554, val_MinusLogProbMetric: 398.9554

Epoch 196: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.8930 - MinusLogProbMetric: 395.8930 - val_loss: 398.9554 - val_MinusLogProbMetric: 398.9554 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 197/1000
2023-09-09 14:33:45.826 
Epoch 197/1000 
	 loss: 395.5785, MinusLogProbMetric: 395.5785, val_loss: 399.9772, val_MinusLogProbMetric: 399.9772

Epoch 197: val_loss did not improve from 398.11273
196/196 - 20s - loss: 395.5785 - MinusLogProbMetric: 395.5785 - val_loss: 399.9772 - val_MinusLogProbMetric: 399.9772 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 198/1000
2023-09-09 14:34:04.932 
Epoch 198/1000 
	 loss: 395.5048, MinusLogProbMetric: 395.5048, val_loss: 399.0862, val_MinusLogProbMetric: 399.0862

Epoch 198: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.5048 - MinusLogProbMetric: 395.5048 - val_loss: 399.0862 - val_MinusLogProbMetric: 399.0862 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 199/1000
2023-09-09 14:34:24.042 
Epoch 199/1000 
	 loss: 396.0036, MinusLogProbMetric: 396.0036, val_loss: 401.2163, val_MinusLogProbMetric: 401.2163

Epoch 199: val_loss did not improve from 398.11273
196/196 - 19s - loss: 396.0036 - MinusLogProbMetric: 396.0036 - val_loss: 401.2163 - val_MinusLogProbMetric: 401.2163 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 200/1000
2023-09-09 14:34:43.196 
Epoch 200/1000 
	 loss: 395.6732, MinusLogProbMetric: 395.6732, val_loss: 400.2504, val_MinusLogProbMetric: 400.2504

Epoch 200: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.6732 - MinusLogProbMetric: 395.6732 - val_loss: 400.2504 - val_MinusLogProbMetric: 400.2504 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 201/1000
2023-09-09 14:35:03.821 
Epoch 201/1000 
	 loss: 395.5697, MinusLogProbMetric: 395.5697, val_loss: 399.2217, val_MinusLogProbMetric: 399.2217

Epoch 201: val_loss did not improve from 398.11273
196/196 - 21s - loss: 395.5697 - MinusLogProbMetric: 395.5697 - val_loss: 399.2217 - val_MinusLogProbMetric: 399.2217 - lr: 3.3333e-04 - 21s/epoch - 105ms/step
Epoch 202/1000
2023-09-09 14:35:23.850 
Epoch 202/1000 
	 loss: 395.4812, MinusLogProbMetric: 395.4812, val_loss: 401.7707, val_MinusLogProbMetric: 401.7707

Epoch 202: val_loss did not improve from 398.11273
196/196 - 20s - loss: 395.4812 - MinusLogProbMetric: 395.4812 - val_loss: 401.7707 - val_MinusLogProbMetric: 401.7707 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 203/1000
2023-09-09 14:35:44.139 
Epoch 203/1000 
	 loss: 395.8144, MinusLogProbMetric: 395.8144, val_loss: 400.4094, val_MinusLogProbMetric: 400.4094

Epoch 203: val_loss did not improve from 398.11273
196/196 - 20s - loss: 395.8144 - MinusLogProbMetric: 395.8144 - val_loss: 400.4094 - val_MinusLogProbMetric: 400.4094 - lr: 3.3333e-04 - 20s/epoch - 103ms/step
Epoch 204/1000
2023-09-09 14:36:03.876 
Epoch 204/1000 
	 loss: 395.5139, MinusLogProbMetric: 395.5139, val_loss: 400.4003, val_MinusLogProbMetric: 400.4003

Epoch 204: val_loss did not improve from 398.11273
196/196 - 20s - loss: 395.5139 - MinusLogProbMetric: 395.5139 - val_loss: 400.4003 - val_MinusLogProbMetric: 400.4003 - lr: 3.3333e-04 - 20s/epoch - 101ms/step
Epoch 205/1000
2023-09-09 14:36:23.049 
Epoch 205/1000 
	 loss: 395.3562, MinusLogProbMetric: 395.3562, val_loss: 399.9470, val_MinusLogProbMetric: 399.9470

Epoch 205: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.3562 - MinusLogProbMetric: 395.3562 - val_loss: 399.9470 - val_MinusLogProbMetric: 399.9470 - lr: 3.3333e-04 - 19s/epoch - 98ms/step
Epoch 206/1000
2023-09-09 14:36:42.088 
Epoch 206/1000 
	 loss: 395.4644, MinusLogProbMetric: 395.4644, val_loss: 400.2281, val_MinusLogProbMetric: 400.2281

Epoch 206: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.4644 - MinusLogProbMetric: 395.4644 - val_loss: 400.2281 - val_MinusLogProbMetric: 400.2281 - lr: 3.3333e-04 - 19s/epoch - 97ms/step
Epoch 207/1000
2023-09-09 14:37:02.158 
Epoch 207/1000 
	 loss: 395.5414, MinusLogProbMetric: 395.5414, val_loss: 399.4781, val_MinusLogProbMetric: 399.4781

Epoch 207: val_loss did not improve from 398.11273
196/196 - 20s - loss: 395.5414 - MinusLogProbMetric: 395.5414 - val_loss: 399.4781 - val_MinusLogProbMetric: 399.4781 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 208/1000
2023-09-09 14:37:23.874 
Epoch 208/1000 
	 loss: 394.9471, MinusLogProbMetric: 394.9471, val_loss: 402.5864, val_MinusLogProbMetric: 402.5864

Epoch 208: val_loss did not improve from 398.11273
196/196 - 22s - loss: 394.9471 - MinusLogProbMetric: 394.9471 - val_loss: 402.5864 - val_MinusLogProbMetric: 402.5864 - lr: 3.3333e-04 - 22s/epoch - 111ms/step
Epoch 209/1000
2023-09-09 14:37:41.482 
Epoch 209/1000 
	 loss: 395.7444, MinusLogProbMetric: 395.7444, val_loss: 399.7106, val_MinusLogProbMetric: 399.7106

Epoch 209: val_loss did not improve from 398.11273
196/196 - 18s - loss: 395.7444 - MinusLogProbMetric: 395.7444 - val_loss: 399.7106 - val_MinusLogProbMetric: 399.7106 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 210/1000
2023-09-09 14:37:57.620 
Epoch 210/1000 
	 loss: 395.2988, MinusLogProbMetric: 395.2988, val_loss: 398.5694, val_MinusLogProbMetric: 398.5694

Epoch 210: val_loss did not improve from 398.11273
196/196 - 16s - loss: 395.2988 - MinusLogProbMetric: 395.2988 - val_loss: 398.5694 - val_MinusLogProbMetric: 398.5694 - lr: 3.3333e-04 - 16s/epoch - 82ms/step
Epoch 211/1000
2023-09-09 14:38:14.599 
Epoch 211/1000 
	 loss: 396.1240, MinusLogProbMetric: 396.1240, val_loss: 398.5672, val_MinusLogProbMetric: 398.5672

Epoch 211: val_loss did not improve from 398.11273
196/196 - 17s - loss: 396.1240 - MinusLogProbMetric: 396.1240 - val_loss: 398.5672 - val_MinusLogProbMetric: 398.5672 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 212/1000
2023-09-09 14:38:33.114 
Epoch 212/1000 
	 loss: 395.2716, MinusLogProbMetric: 395.2716, val_loss: 400.8254, val_MinusLogProbMetric: 400.8254

Epoch 212: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.2716 - MinusLogProbMetric: 395.2716 - val_loss: 400.8254 - val_MinusLogProbMetric: 400.8254 - lr: 3.3333e-04 - 19s/epoch - 94ms/step
Epoch 213/1000
2023-09-09 14:38:51.342 
Epoch 213/1000 
	 loss: 395.1605, MinusLogProbMetric: 395.1605, val_loss: 401.2737, val_MinusLogProbMetric: 401.2737

Epoch 213: val_loss did not improve from 398.11273
196/196 - 18s - loss: 395.1605 - MinusLogProbMetric: 395.1605 - val_loss: 401.2737 - val_MinusLogProbMetric: 401.2737 - lr: 3.3333e-04 - 18s/epoch - 93ms/step
Epoch 214/1000
2023-09-09 14:39:11.259 
Epoch 214/1000 
	 loss: 395.2643, MinusLogProbMetric: 395.2643, val_loss: 402.0663, val_MinusLogProbMetric: 402.0663

Epoch 214: val_loss did not improve from 398.11273
196/196 - 20s - loss: 395.2643 - MinusLogProbMetric: 395.2643 - val_loss: 402.0663 - val_MinusLogProbMetric: 402.0663 - lr: 3.3333e-04 - 20s/epoch - 102ms/step
Epoch 215/1000
2023-09-09 14:39:28.912 
Epoch 215/1000 
	 loss: 395.1137, MinusLogProbMetric: 395.1137, val_loss: 401.1343, val_MinusLogProbMetric: 401.1343

Epoch 215: val_loss did not improve from 398.11273
196/196 - 18s - loss: 395.1137 - MinusLogProbMetric: 395.1137 - val_loss: 401.1343 - val_MinusLogProbMetric: 401.1343 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 216/1000
2023-09-09 14:39:47.427 
Epoch 216/1000 
	 loss: 395.0345, MinusLogProbMetric: 395.0345, val_loss: 399.4484, val_MinusLogProbMetric: 399.4484

Epoch 216: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.0345 - MinusLogProbMetric: 395.0345 - val_loss: 399.4484 - val_MinusLogProbMetric: 399.4484 - lr: 3.3333e-04 - 19s/epoch - 94ms/step
Epoch 217/1000
2023-09-09 14:40:05.946 
Epoch 217/1000 
	 loss: 394.9820, MinusLogProbMetric: 394.9820, val_loss: 399.5546, val_MinusLogProbMetric: 399.5546

Epoch 217: val_loss did not improve from 398.11273
196/196 - 19s - loss: 394.9820 - MinusLogProbMetric: 394.9820 - val_loss: 399.5546 - val_MinusLogProbMetric: 399.5546 - lr: 3.3333e-04 - 19s/epoch - 94ms/step
Epoch 218/1000
2023-09-09 14:40:24.465 
Epoch 218/1000 
	 loss: 395.2948, MinusLogProbMetric: 395.2948, val_loss: 399.3215, val_MinusLogProbMetric: 399.3215

Epoch 218: val_loss did not improve from 398.11273
196/196 - 19s - loss: 395.2948 - MinusLogProbMetric: 395.2948 - val_loss: 399.3215 - val_MinusLogProbMetric: 399.3215 - lr: 3.3333e-04 - 19s/epoch - 95ms/step
Epoch 219/1000
2023-09-09 14:40:42.427 
Epoch 219/1000 
	 loss: 394.8568, MinusLogProbMetric: 394.8568, val_loss: 401.3048, val_MinusLogProbMetric: 401.3048

Epoch 219: val_loss did not improve from 398.11273
196/196 - 18s - loss: 394.8568 - MinusLogProbMetric: 394.8568 - val_loss: 401.3048 - val_MinusLogProbMetric: 401.3048 - lr: 3.3333e-04 - 18s/epoch - 92ms/step
Epoch 220/1000
2023-09-09 14:40:59.086 
Epoch 220/1000 
	 loss: 395.6091, MinusLogProbMetric: 395.6091, val_loss: 399.0848, val_MinusLogProbMetric: 399.0848

Epoch 220: val_loss did not improve from 398.11273
196/196 - 17s - loss: 395.6091 - MinusLogProbMetric: 395.6091 - val_loss: 399.0848 - val_MinusLogProbMetric: 399.0848 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 221/1000
2023-09-09 14:41:15.509 
Epoch 221/1000 
	 loss: 395.1918, MinusLogProbMetric: 395.1918, val_loss: 399.6597, val_MinusLogProbMetric: 399.6597

Epoch 221: val_loss did not improve from 398.11273
196/196 - 16s - loss: 395.1918 - MinusLogProbMetric: 395.1918 - val_loss: 399.6597 - val_MinusLogProbMetric: 399.6597 - lr: 3.3333e-04 - 16s/epoch - 84ms/step
Epoch 222/1000
2023-09-09 14:41:31.842 
Epoch 222/1000 
	 loss: 394.8152, MinusLogProbMetric: 394.8152, val_loss: 398.3051, val_MinusLogProbMetric: 398.3051

Epoch 222: val_loss did not improve from 398.11273
196/196 - 16s - loss: 394.8152 - MinusLogProbMetric: 394.8152 - val_loss: 398.3051 - val_MinusLogProbMetric: 398.3051 - lr: 3.3333e-04 - 16s/epoch - 83ms/step
Epoch 223/1000
2023-09-09 14:41:48.486 
Epoch 223/1000 
	 loss: 395.3059, MinusLogProbMetric: 395.3059, val_loss: 399.9209, val_MinusLogProbMetric: 399.9209

Epoch 223: val_loss did not improve from 398.11273
196/196 - 17s - loss: 395.3059 - MinusLogProbMetric: 395.3059 - val_loss: 399.9209 - val_MinusLogProbMetric: 399.9209 - lr: 3.3333e-04 - 17s/epoch - 85ms/step
Epoch 224/1000
2023-09-09 14:42:05.257 
Epoch 224/1000 
	 loss: 394.7656, MinusLogProbMetric: 394.7656, val_loss: 398.6949, val_MinusLogProbMetric: 398.6949

Epoch 224: val_loss did not improve from 398.11273
196/196 - 17s - loss: 394.7656 - MinusLogProbMetric: 394.7656 - val_loss: 398.6949 - val_MinusLogProbMetric: 398.6949 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 225/1000
2023-09-09 14:42:22.195 
Epoch 225/1000 
	 loss: 395.0393, MinusLogProbMetric: 395.0393, val_loss: 401.6835, val_MinusLogProbMetric: 401.6835

Epoch 225: val_loss did not improve from 398.11273
196/196 - 17s - loss: 395.0393 - MinusLogProbMetric: 395.0393 - val_loss: 401.6835 - val_MinusLogProbMetric: 401.6835 - lr: 3.3333e-04 - 17s/epoch - 86ms/step
Epoch 226/1000
2023-09-09 14:42:39.336 
Epoch 226/1000 
	 loss: 394.9088, MinusLogProbMetric: 394.9088, val_loss: 399.6243, val_MinusLogProbMetric: 399.6243

Epoch 226: val_loss did not improve from 398.11273
196/196 - 17s - loss: 394.9088 - MinusLogProbMetric: 394.9088 - val_loss: 399.6243 - val_MinusLogProbMetric: 399.6243 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 227/1000
2023-09-09 14:42:56.944 
Epoch 227/1000 
	 loss: 394.7055, MinusLogProbMetric: 394.7055, val_loss: 398.8075, val_MinusLogProbMetric: 398.8075

Epoch 227: val_loss did not improve from 398.11273
196/196 - 18s - loss: 394.7055 - MinusLogProbMetric: 394.7055 - val_loss: 398.8075 - val_MinusLogProbMetric: 398.8075 - lr: 3.3333e-04 - 18s/epoch - 90ms/step
Epoch 228/1000
2023-09-09 14:43:14.046 
Epoch 228/1000 
	 loss: 394.8027, MinusLogProbMetric: 394.8027, val_loss: 398.7628, val_MinusLogProbMetric: 398.7628

Epoch 228: val_loss did not improve from 398.11273
196/196 - 17s - loss: 394.8027 - MinusLogProbMetric: 394.8027 - val_loss: 398.7628 - val_MinusLogProbMetric: 398.7628 - lr: 3.3333e-04 - 17s/epoch - 87ms/step
Epoch 229/1000
2023-09-09 14:43:32.888 
Epoch 229/1000 
	 loss: 394.7530, MinusLogProbMetric: 394.7530, val_loss: 399.4151, val_MinusLogProbMetric: 399.4151

Epoch 229: val_loss did not improve from 398.11273
196/196 - 19s - loss: 394.7530 - MinusLogProbMetric: 394.7530 - val_loss: 399.4151 - val_MinusLogProbMetric: 399.4151 - lr: 3.3333e-04 - 19s/epoch - 96ms/step
Epoch 230/1000
2023-09-09 14:43:48.606 
Epoch 230/1000 
	 loss: 394.7365, MinusLogProbMetric: 394.7365, val_loss: 399.2989, val_MinusLogProbMetric: 399.2989

Epoch 230: val_loss did not improve from 398.11273
196/196 - 16s - loss: 394.7365 - MinusLogProbMetric: 394.7365 - val_loss: 399.2989 - val_MinusLogProbMetric: 399.2989 - lr: 3.3333e-04 - 16s/epoch - 80ms/step
Epoch 231/1000
2023-09-09 14:44:03.708 
Epoch 231/1000 
	 loss: 394.8441, MinusLogProbMetric: 394.8441, val_loss: 400.4818, val_MinusLogProbMetric: 400.4818

Epoch 231: val_loss did not improve from 398.11273
196/196 - 15s - loss: 394.8441 - MinusLogProbMetric: 394.8441 - val_loss: 400.4818 - val_MinusLogProbMetric: 400.4818 - lr: 3.3333e-04 - 15s/epoch - 77ms/step
Epoch 232/1000
2023-09-09 14:44:18.752 
Epoch 232/1000 
	 loss: 394.8877, MinusLogProbMetric: 394.8877, val_loss: 399.6476, val_MinusLogProbMetric: 399.6476

Epoch 232: val_loss did not improve from 398.11273
196/196 - 15s - loss: 394.8877 - MinusLogProbMetric: 394.8877 - val_loss: 399.6476 - val_MinusLogProbMetric: 399.6476 - lr: 3.3333e-04 - 15s/epoch - 77ms/step
Epoch 233/1000
2023-09-09 14:44:33.598 
Epoch 233/1000 
	 loss: 394.4644, MinusLogProbMetric: 394.4644, val_loss: 400.9085, val_MinusLogProbMetric: 400.9085

Epoch 233: val_loss did not improve from 398.11273
196/196 - 15s - loss: 394.4644 - MinusLogProbMetric: 394.4644 - val_loss: 400.9085 - val_MinusLogProbMetric: 400.9085 - lr: 3.3333e-04 - 15s/epoch - 76ms/step
Epoch 234/1000
2023-09-09 14:44:48.584 
Epoch 234/1000 
	 loss: 394.9433, MinusLogProbMetric: 394.9433, val_loss: 398.9956, val_MinusLogProbMetric: 398.9956

Epoch 234: val_loss did not improve from 398.11273
196/196 - 15s - loss: 394.9433 - MinusLogProbMetric: 394.9433 - val_loss: 398.9956 - val_MinusLogProbMetric: 398.9956 - lr: 3.3333e-04 - 15s/epoch - 76ms/step
Epoch 235/1000
2023-09-09 14:45:03.909 
Epoch 235/1000 
	 loss: 394.5100, MinusLogProbMetric: 394.5100, val_loss: 398.4526, val_MinusLogProbMetric: 398.4526

Epoch 235: val_loss did not improve from 398.11273
196/196 - 15s - loss: 394.5100 - MinusLogProbMetric: 394.5100 - val_loss: 398.4526 - val_MinusLogProbMetric: 398.4526 - lr: 3.3333e-04 - 15s/epoch - 78ms/step
Epoch 236/1000
2023-09-09 14:45:19.171 
Epoch 236/1000 
	 loss: 394.4635, MinusLogProbMetric: 394.4635, val_loss: 403.0612, val_MinusLogProbMetric: 403.0612

Epoch 236: val_loss did not improve from 398.11273
196/196 - 15s - loss: 394.4635 - MinusLogProbMetric: 394.4635 - val_loss: 403.0612 - val_MinusLogProbMetric: 403.0612 - lr: 3.3333e-04 - 15s/epoch - 78ms/step
Epoch 237/1000
2023-09-09 14:45:34.550 
Epoch 237/1000 
	 loss: 394.7244, MinusLogProbMetric: 394.7244, val_loss: 399.3733, val_MinusLogProbMetric: 399.3733

Epoch 237: val_loss did not improve from 398.11273
196/196 - 15s - loss: 394.7244 - MinusLogProbMetric: 394.7244 - val_loss: 399.3733 - val_MinusLogProbMetric: 399.3733 - lr: 3.3333e-04 - 15s/epoch - 78ms/step
Epoch 238/1000
2023-09-09 14:45:50.808 
Epoch 238/1000 
	 loss: 394.4034, MinusLogProbMetric: 394.4034, val_loss: 400.1481, val_MinusLogProbMetric: 400.1481

Epoch 238: val_loss did not improve from 398.11273
196/196 - 16s - loss: 394.4034 - MinusLogProbMetric: 394.4034 - val_loss: 400.1481 - val_MinusLogProbMetric: 400.1481 - lr: 3.3333e-04 - 16s/epoch - 83ms/step
Epoch 239/1000
2023-09-09 14:46:06.777 
Epoch 239/1000 
	 loss: 394.4379, MinusLogProbMetric: 394.4379, val_loss: 400.8344, val_MinusLogProbMetric: 400.8344

Epoch 239: val_loss did not improve from 398.11273
196/196 - 16s - loss: 394.4379 - MinusLogProbMetric: 394.4379 - val_loss: 400.8344 - val_MinusLogProbMetric: 400.8344 - lr: 3.3333e-04 - 16s/epoch - 81ms/step
Epoch 240/1000
2023-09-09 14:46:23.099 
Epoch 240/1000 
	 loss: 390.5062, MinusLogProbMetric: 390.5062, val_loss: 396.1611, val_MinusLogProbMetric: 396.1611

Epoch 240: val_loss improved from 398.11273 to 396.16110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 17s - loss: 390.5062 - MinusLogProbMetric: 390.5062 - val_loss: 396.1611 - val_MinusLogProbMetric: 396.1611 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 241/1000
2023-09-09 14:46:40.399 
Epoch 241/1000 
	 loss: 390.2428, MinusLogProbMetric: 390.2428, val_loss: 395.7650, val_MinusLogProbMetric: 395.7650

Epoch 241: val_loss improved from 396.16110 to 395.76505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 18s - loss: 390.2428 - MinusLogProbMetric: 390.2428 - val_loss: 395.7650 - val_MinusLogProbMetric: 395.7650 - lr: 1.6667e-04 - 18s/epoch - 89ms/step
Epoch 242/1000
2023-09-09 14:46:59.254 
Epoch 242/1000 
	 loss: 390.6999, MinusLogProbMetric: 390.6999, val_loss: 397.0941, val_MinusLogProbMetric: 397.0941

Epoch 242: val_loss did not improve from 395.76505
196/196 - 18s - loss: 390.6999 - MinusLogProbMetric: 390.6999 - val_loss: 397.0941 - val_MinusLogProbMetric: 397.0941 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 243/1000
2023-09-09 14:47:19.357 
Epoch 243/1000 
	 loss: 390.6467, MinusLogProbMetric: 390.6467, val_loss: 395.9735, val_MinusLogProbMetric: 395.9735

Epoch 243: val_loss did not improve from 395.76505
196/196 - 20s - loss: 390.6467 - MinusLogProbMetric: 390.6467 - val_loss: 395.9735 - val_MinusLogProbMetric: 395.9735 - lr: 1.6667e-04 - 20s/epoch - 103ms/step
Epoch 244/1000
2023-09-09 14:47:38.287 
Epoch 244/1000 
	 loss: 390.6416, MinusLogProbMetric: 390.6416, val_loss: 396.9785, val_MinusLogProbMetric: 396.9785

Epoch 244: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.6416 - MinusLogProbMetric: 390.6416 - val_loss: 396.9785 - val_MinusLogProbMetric: 396.9785 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 245/1000
2023-09-09 14:47:57.609 
Epoch 245/1000 
	 loss: 390.6651, MinusLogProbMetric: 390.6651, val_loss: 397.3410, val_MinusLogProbMetric: 397.3410

Epoch 245: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.6651 - MinusLogProbMetric: 390.6651 - val_loss: 397.3410 - val_MinusLogProbMetric: 397.3410 - lr: 1.6667e-04 - 19s/epoch - 99ms/step
Epoch 246/1000
2023-09-09 14:48:16.731 
Epoch 246/1000 
	 loss: 390.5309, MinusLogProbMetric: 390.5309, val_loss: 395.8405, val_MinusLogProbMetric: 395.8405

Epoch 246: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.5309 - MinusLogProbMetric: 390.5309 - val_loss: 395.8405 - val_MinusLogProbMetric: 395.8405 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 247/1000
2023-09-09 14:48:35.039 
Epoch 247/1000 
	 loss: 391.1317, MinusLogProbMetric: 391.1317, val_loss: 395.9912, val_MinusLogProbMetric: 395.9912

Epoch 247: val_loss did not improve from 395.76505
196/196 - 18s - loss: 391.1317 - MinusLogProbMetric: 391.1317 - val_loss: 395.9912 - val_MinusLogProbMetric: 395.9912 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 248/1000
2023-09-09 14:48:55.175 
Epoch 248/1000 
	 loss: 390.5479, MinusLogProbMetric: 390.5479, val_loss: 396.9478, val_MinusLogProbMetric: 396.9478

Epoch 248: val_loss did not improve from 395.76505
196/196 - 20s - loss: 390.5479 - MinusLogProbMetric: 390.5479 - val_loss: 396.9478 - val_MinusLogProbMetric: 396.9478 - lr: 1.6667e-04 - 20s/epoch - 103ms/step
Epoch 249/1000
2023-09-09 14:49:14.488 
Epoch 249/1000 
	 loss: 390.3286, MinusLogProbMetric: 390.3286, val_loss: 397.9574, val_MinusLogProbMetric: 397.9574

Epoch 249: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.3286 - MinusLogProbMetric: 390.3286 - val_loss: 397.9574 - val_MinusLogProbMetric: 397.9574 - lr: 1.6667e-04 - 19s/epoch - 98ms/step
Epoch 250/1000
2023-09-09 14:49:34.093 
Epoch 250/1000 
	 loss: 390.7280, MinusLogProbMetric: 390.7280, val_loss: 396.4468, val_MinusLogProbMetric: 396.4468

Epoch 250: val_loss did not improve from 395.76505
196/196 - 20s - loss: 390.7280 - MinusLogProbMetric: 390.7280 - val_loss: 396.4468 - val_MinusLogProbMetric: 396.4468 - lr: 1.6667e-04 - 20s/epoch - 100ms/step
Epoch 251/1000
2023-09-09 14:49:53.194 
Epoch 251/1000 
	 loss: 390.6764, MinusLogProbMetric: 390.6764, val_loss: 397.4777, val_MinusLogProbMetric: 397.4777

Epoch 251: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.6764 - MinusLogProbMetric: 390.6764 - val_loss: 397.4777 - val_MinusLogProbMetric: 397.4777 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 252/1000
2023-09-09 14:50:11.722 
Epoch 252/1000 
	 loss: 390.5703, MinusLogProbMetric: 390.5703, val_loss: 398.0677, val_MinusLogProbMetric: 398.0677

Epoch 252: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.5703 - MinusLogProbMetric: 390.5703 - val_loss: 398.0677 - val_MinusLogProbMetric: 398.0677 - lr: 1.6667e-04 - 19s/epoch - 94ms/step
Epoch 253/1000
2023-09-09 14:50:29.875 
Epoch 253/1000 
	 loss: 390.6858, MinusLogProbMetric: 390.6858, val_loss: 398.5459, val_MinusLogProbMetric: 398.5459

Epoch 253: val_loss did not improve from 395.76505
196/196 - 18s - loss: 390.6858 - MinusLogProbMetric: 390.6858 - val_loss: 398.5459 - val_MinusLogProbMetric: 398.5459 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 254/1000
2023-09-09 14:50:48.533 
Epoch 254/1000 
	 loss: 390.2168, MinusLogProbMetric: 390.2168, val_loss: 395.8911, val_MinusLogProbMetric: 395.8911

Epoch 254: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.2168 - MinusLogProbMetric: 390.2168 - val_loss: 395.8911 - val_MinusLogProbMetric: 395.8911 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 255/1000
2023-09-09 14:51:06.612 
Epoch 255/1000 
	 loss: 390.8188, MinusLogProbMetric: 390.8188, val_loss: 397.0739, val_MinusLogProbMetric: 397.0739

Epoch 255: val_loss did not improve from 395.76505
196/196 - 18s - loss: 390.8188 - MinusLogProbMetric: 390.8188 - val_loss: 397.0739 - val_MinusLogProbMetric: 397.0739 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 256/1000
2023-09-09 14:51:24.527 
Epoch 256/1000 
	 loss: 390.4632, MinusLogProbMetric: 390.4632, val_loss: 397.7130, val_MinusLogProbMetric: 397.7130

Epoch 256: val_loss did not improve from 395.76505
196/196 - 18s - loss: 390.4632 - MinusLogProbMetric: 390.4632 - val_loss: 397.7130 - val_MinusLogProbMetric: 397.7130 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 257/1000
2023-09-09 14:51:41.341 
Epoch 257/1000 
	 loss: 390.8530, MinusLogProbMetric: 390.8530, val_loss: 396.9034, val_MinusLogProbMetric: 396.9034

Epoch 257: val_loss did not improve from 395.76505
196/196 - 17s - loss: 390.8530 - MinusLogProbMetric: 390.8530 - val_loss: 396.9034 - val_MinusLogProbMetric: 396.9034 - lr: 1.6667e-04 - 17s/epoch - 86ms/step
Epoch 258/1000
2023-09-09 14:51:59.323 
Epoch 258/1000 
	 loss: 390.4263, MinusLogProbMetric: 390.4263, val_loss: 398.5283, val_MinusLogProbMetric: 398.5283

Epoch 258: val_loss did not improve from 395.76505
196/196 - 18s - loss: 390.4263 - MinusLogProbMetric: 390.4263 - val_loss: 398.5283 - val_MinusLogProbMetric: 398.5283 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 259/1000
2023-09-09 14:52:17.153 
Epoch 259/1000 
	 loss: 390.3066, MinusLogProbMetric: 390.3066, val_loss: 396.3681, val_MinusLogProbMetric: 396.3681

Epoch 259: val_loss did not improve from 395.76505
196/196 - 18s - loss: 390.3066 - MinusLogProbMetric: 390.3066 - val_loss: 396.3681 - val_MinusLogProbMetric: 396.3681 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 260/1000
2023-09-09 14:52:35.639 
Epoch 260/1000 
	 loss: 390.6615, MinusLogProbMetric: 390.6615, val_loss: 396.5091, val_MinusLogProbMetric: 396.5091

Epoch 260: val_loss did not improve from 395.76505
196/196 - 18s - loss: 390.6615 - MinusLogProbMetric: 390.6615 - val_loss: 396.5091 - val_MinusLogProbMetric: 396.5091 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 261/1000
2023-09-09 14:52:54.569 
Epoch 261/1000 
	 loss: 390.2915, MinusLogProbMetric: 390.2915, val_loss: 397.0936, val_MinusLogProbMetric: 397.0936

Epoch 261: val_loss did not improve from 395.76505
196/196 - 19s - loss: 390.2915 - MinusLogProbMetric: 390.2915 - val_loss: 397.0936 - val_MinusLogProbMetric: 397.0936 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 262/1000
2023-09-09 14:53:14.594 
Epoch 262/1000 
	 loss: 390.4698, MinusLogProbMetric: 390.4698, val_loss: 395.6956, val_MinusLogProbMetric: 395.6956

Epoch 262: val_loss improved from 395.76505 to 395.69565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 21s - loss: 390.4698 - MinusLogProbMetric: 390.4698 - val_loss: 395.6956 - val_MinusLogProbMetric: 395.6956 - lr: 1.6667e-04 - 21s/epoch - 106ms/step
Epoch 263/1000
2023-09-09 14:53:36.034 
Epoch 263/1000 
	 loss: 390.4894, MinusLogProbMetric: 390.4894, val_loss: 397.7311, val_MinusLogProbMetric: 397.7311

Epoch 263: val_loss did not improve from 395.69565
196/196 - 21s - loss: 390.4894 - MinusLogProbMetric: 390.4894 - val_loss: 397.7311 - val_MinusLogProbMetric: 397.7311 - lr: 1.6667e-04 - 21s/epoch - 105ms/step
Epoch 264/1000
2023-09-09 14:53:56.484 
Epoch 264/1000 
	 loss: 390.3973, MinusLogProbMetric: 390.3973, val_loss: 396.2588, val_MinusLogProbMetric: 396.2588

Epoch 264: val_loss did not improve from 395.69565
196/196 - 20s - loss: 390.3973 - MinusLogProbMetric: 390.3973 - val_loss: 396.2588 - val_MinusLogProbMetric: 396.2588 - lr: 1.6667e-04 - 20s/epoch - 104ms/step
Epoch 265/1000
2023-09-09 14:54:16.118 
Epoch 265/1000 
	 loss: 390.2642, MinusLogProbMetric: 390.2642, val_loss: 396.2198, val_MinusLogProbMetric: 396.2198

Epoch 265: val_loss did not improve from 395.69565
196/196 - 20s - loss: 390.2642 - MinusLogProbMetric: 390.2642 - val_loss: 396.2198 - val_MinusLogProbMetric: 396.2198 - lr: 1.6667e-04 - 20s/epoch - 100ms/step
Epoch 266/1000
2023-09-09 14:54:35.130 
Epoch 266/1000 
	 loss: 390.4466, MinusLogProbMetric: 390.4466, val_loss: 395.6196, val_MinusLogProbMetric: 395.6196

Epoch 266: val_loss improved from 395.69565 to 395.61963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 20s - loss: 390.4466 - MinusLogProbMetric: 390.4466 - val_loss: 395.6196 - val_MinusLogProbMetric: 395.6196 - lr: 1.6667e-04 - 20s/epoch - 101ms/step
Epoch 267/1000
2023-09-09 14:54:56.238 
Epoch 267/1000 
	 loss: 390.2095, MinusLogProbMetric: 390.2095, val_loss: 395.9984, val_MinusLogProbMetric: 395.9984

Epoch 267: val_loss did not improve from 395.61963
196/196 - 20s - loss: 390.2095 - MinusLogProbMetric: 390.2095 - val_loss: 395.9984 - val_MinusLogProbMetric: 395.9984 - lr: 1.6667e-04 - 20s/epoch - 103ms/step
Epoch 268/1000
2023-09-09 14:55:15.962 
Epoch 268/1000 
	 loss: 390.3839, MinusLogProbMetric: 390.3839, val_loss: 395.9921, val_MinusLogProbMetric: 395.9921

Epoch 268: val_loss did not improve from 395.61963
196/196 - 20s - loss: 390.3839 - MinusLogProbMetric: 390.3839 - val_loss: 395.9921 - val_MinusLogProbMetric: 395.9921 - lr: 1.6667e-04 - 20s/epoch - 101ms/step
Epoch 269/1000
2023-09-09 14:55:36.023 
Epoch 269/1000 
	 loss: 390.8315, MinusLogProbMetric: 390.8315, val_loss: 395.7192, val_MinusLogProbMetric: 395.7192

Epoch 269: val_loss did not improve from 395.61963
196/196 - 20s - loss: 390.8315 - MinusLogProbMetric: 390.8315 - val_loss: 395.7192 - val_MinusLogProbMetric: 395.7192 - lr: 1.6667e-04 - 20s/epoch - 102ms/step
Epoch 270/1000
2023-09-09 14:55:56.379 
Epoch 270/1000 
	 loss: 390.0993, MinusLogProbMetric: 390.0993, val_loss: 399.2851, val_MinusLogProbMetric: 399.2851

Epoch 270: val_loss did not improve from 395.61963
196/196 - 20s - loss: 390.0993 - MinusLogProbMetric: 390.0993 - val_loss: 399.2851 - val_MinusLogProbMetric: 399.2851 - lr: 1.6667e-04 - 20s/epoch - 104ms/step
Epoch 271/1000
2023-09-09 14:56:16.778 
Epoch 271/1000 
	 loss: 390.2366, MinusLogProbMetric: 390.2366, val_loss: 396.2083, val_MinusLogProbMetric: 396.2083

Epoch 271: val_loss did not improve from 395.61963
196/196 - 20s - loss: 390.2366 - MinusLogProbMetric: 390.2366 - val_loss: 396.2083 - val_MinusLogProbMetric: 396.2083 - lr: 1.6667e-04 - 20s/epoch - 104ms/step
Epoch 272/1000
2023-09-09 14:56:34.270 
Epoch 272/1000 
	 loss: 390.5160, MinusLogProbMetric: 390.5160, val_loss: 396.2314, val_MinusLogProbMetric: 396.2314

Epoch 272: val_loss did not improve from 395.61963
196/196 - 17s - loss: 390.5160 - MinusLogProbMetric: 390.5160 - val_loss: 396.2314 - val_MinusLogProbMetric: 396.2314 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 273/1000
2023-09-09 14:56:52.498 
Epoch 273/1000 
	 loss: 390.0435, MinusLogProbMetric: 390.0435, val_loss: 395.7387, val_MinusLogProbMetric: 395.7387

Epoch 273: val_loss did not improve from 395.61963
196/196 - 18s - loss: 390.0435 - MinusLogProbMetric: 390.0435 - val_loss: 395.7387 - val_MinusLogProbMetric: 395.7387 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 274/1000
2023-09-09 14:57:11.168 
Epoch 274/1000 
	 loss: 390.3842, MinusLogProbMetric: 390.3842, val_loss: 396.1234, val_MinusLogProbMetric: 396.1234

Epoch 274: val_loss did not improve from 395.61963
196/196 - 19s - loss: 390.3842 - MinusLogProbMetric: 390.3842 - val_loss: 396.1234 - val_MinusLogProbMetric: 396.1234 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 275/1000
2023-09-09 14:57:29.875 
Epoch 275/1000 
	 loss: 390.2518, MinusLogProbMetric: 390.2518, val_loss: 395.6174, val_MinusLogProbMetric: 395.6174

Epoch 275: val_loss improved from 395.61963 to 395.61737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 19s - loss: 390.2518 - MinusLogProbMetric: 390.2518 - val_loss: 395.6174 - val_MinusLogProbMetric: 395.6174 - lr: 1.6667e-04 - 19s/epoch - 99ms/step
Epoch 276/1000
2023-09-09 14:57:47.799 
Epoch 276/1000 
	 loss: 392.0091, MinusLogProbMetric: 392.0091, val_loss: 396.5857, val_MinusLogProbMetric: 396.5857

Epoch 276: val_loss did not improve from 395.61737
196/196 - 17s - loss: 392.0091 - MinusLogProbMetric: 392.0091 - val_loss: 396.5857 - val_MinusLogProbMetric: 396.5857 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 277/1000
2023-09-09 14:58:06.879 
Epoch 277/1000 
	 loss: 390.1229, MinusLogProbMetric: 390.1229, val_loss: 396.2981, val_MinusLogProbMetric: 396.2981

Epoch 277: val_loss did not improve from 395.61737
196/196 - 19s - loss: 390.1229 - MinusLogProbMetric: 390.1229 - val_loss: 396.2981 - val_MinusLogProbMetric: 396.2981 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 278/1000
2023-09-09 14:58:24.531 
Epoch 278/1000 
	 loss: 390.0518, MinusLogProbMetric: 390.0518, val_loss: 395.9545, val_MinusLogProbMetric: 395.9545

Epoch 278: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.0518 - MinusLogProbMetric: 390.0518 - val_loss: 395.9545 - val_MinusLogProbMetric: 395.9545 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 279/1000
2023-09-09 14:58:42.932 
Epoch 279/1000 
	 loss: 389.8623, MinusLogProbMetric: 389.8623, val_loss: 401.9468, val_MinusLogProbMetric: 401.9468

Epoch 279: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.8623 - MinusLogProbMetric: 389.8623 - val_loss: 401.9468 - val_MinusLogProbMetric: 401.9468 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 280/1000
2023-09-09 14:59:01.513 
Epoch 280/1000 
	 loss: 390.2614, MinusLogProbMetric: 390.2614, val_loss: 397.0429, val_MinusLogProbMetric: 397.0429

Epoch 280: val_loss did not improve from 395.61737
196/196 - 19s - loss: 390.2614 - MinusLogProbMetric: 390.2614 - val_loss: 397.0429 - val_MinusLogProbMetric: 397.0429 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 281/1000
2023-09-09 14:59:20.610 
Epoch 281/1000 
	 loss: 390.0364, MinusLogProbMetric: 390.0364, val_loss: 396.3368, val_MinusLogProbMetric: 396.3368

Epoch 281: val_loss did not improve from 395.61737
196/196 - 19s - loss: 390.0364 - MinusLogProbMetric: 390.0364 - val_loss: 396.3368 - val_MinusLogProbMetric: 396.3368 - lr: 1.6667e-04 - 19s/epoch - 97ms/step
Epoch 282/1000
2023-09-09 14:59:39.032 
Epoch 282/1000 
	 loss: 390.1214, MinusLogProbMetric: 390.1214, val_loss: 396.3129, val_MinusLogProbMetric: 396.3129

Epoch 282: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.1214 - MinusLogProbMetric: 390.1214 - val_loss: 396.3129 - val_MinusLogProbMetric: 396.3129 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 283/1000
2023-09-09 14:59:57.353 
Epoch 283/1000 
	 loss: 390.2442, MinusLogProbMetric: 390.2442, val_loss: 397.2442, val_MinusLogProbMetric: 397.2442

Epoch 283: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.2442 - MinusLogProbMetric: 390.2442 - val_loss: 397.2442 - val_MinusLogProbMetric: 397.2442 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 284/1000
2023-09-09 15:00:15.179 
Epoch 284/1000 
	 loss: 390.3026, MinusLogProbMetric: 390.3026, val_loss: 397.5415, val_MinusLogProbMetric: 397.5415

Epoch 284: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.3026 - MinusLogProbMetric: 390.3026 - val_loss: 397.5415 - val_MinusLogProbMetric: 397.5415 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 285/1000
2023-09-09 15:00:34.738 
Epoch 285/1000 
	 loss: 389.6891, MinusLogProbMetric: 389.6891, val_loss: 400.2152, val_MinusLogProbMetric: 400.2152

Epoch 285: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.6891 - MinusLogProbMetric: 389.6891 - val_loss: 400.2152 - val_MinusLogProbMetric: 400.2152 - lr: 1.6667e-04 - 20s/epoch - 100ms/step
Epoch 286/1000
2023-09-09 15:00:52.964 
Epoch 286/1000 
	 loss: 390.2141, MinusLogProbMetric: 390.2141, val_loss: 397.1136, val_MinusLogProbMetric: 397.1136

Epoch 286: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.2141 - MinusLogProbMetric: 390.2141 - val_loss: 397.1136 - val_MinusLogProbMetric: 397.1136 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 287/1000
2023-09-09 15:01:12.440 
Epoch 287/1000 
	 loss: 390.5434, MinusLogProbMetric: 390.5434, val_loss: 397.1610, val_MinusLogProbMetric: 397.1610

Epoch 287: val_loss did not improve from 395.61737
196/196 - 19s - loss: 390.5434 - MinusLogProbMetric: 390.5434 - val_loss: 397.1610 - val_MinusLogProbMetric: 397.1610 - lr: 1.6667e-04 - 19s/epoch - 99ms/step
Epoch 288/1000
2023-09-09 15:01:31.209 
Epoch 288/1000 
	 loss: 389.8484, MinusLogProbMetric: 389.8484, val_loss: 397.5224, val_MinusLogProbMetric: 397.5224

Epoch 288: val_loss did not improve from 395.61737
196/196 - 19s - loss: 389.8484 - MinusLogProbMetric: 389.8484 - val_loss: 397.5224 - val_MinusLogProbMetric: 397.5224 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 289/1000
2023-09-09 15:01:50.736 
Epoch 289/1000 
	 loss: 389.9720, MinusLogProbMetric: 389.9720, val_loss: 395.9215, val_MinusLogProbMetric: 395.9215

Epoch 289: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.9720 - MinusLogProbMetric: 389.9720 - val_loss: 395.9215 - val_MinusLogProbMetric: 395.9215 - lr: 1.6667e-04 - 20s/epoch - 100ms/step
Epoch 290/1000
2023-09-09 15:02:09.619 
Epoch 290/1000 
	 loss: 390.2334, MinusLogProbMetric: 390.2334, val_loss: 395.9651, val_MinusLogProbMetric: 395.9651

Epoch 290: val_loss did not improve from 395.61737
196/196 - 19s - loss: 390.2334 - MinusLogProbMetric: 390.2334 - val_loss: 395.9651 - val_MinusLogProbMetric: 395.9651 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 291/1000
2023-09-09 15:02:27.797 
Epoch 291/1000 
	 loss: 390.0513, MinusLogProbMetric: 390.0513, val_loss: 395.8215, val_MinusLogProbMetric: 395.8215

Epoch 291: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.0513 - MinusLogProbMetric: 390.0513 - val_loss: 395.8215 - val_MinusLogProbMetric: 395.8215 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 292/1000
2023-09-09 15:02:48.096 
Epoch 292/1000 
	 loss: 390.4214, MinusLogProbMetric: 390.4214, val_loss: 399.0219, val_MinusLogProbMetric: 399.0219

Epoch 292: val_loss did not improve from 395.61737
196/196 - 20s - loss: 390.4214 - MinusLogProbMetric: 390.4214 - val_loss: 399.0219 - val_MinusLogProbMetric: 399.0219 - lr: 1.6667e-04 - 20s/epoch - 104ms/step
Epoch 293/1000
2023-09-09 15:03:06.019 
Epoch 293/1000 
	 loss: 389.6773, MinusLogProbMetric: 389.6773, val_loss: 396.4667, val_MinusLogProbMetric: 396.4667

Epoch 293: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.6773 - MinusLogProbMetric: 389.6773 - val_loss: 396.4667 - val_MinusLogProbMetric: 396.4667 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 294/1000
2023-09-09 15:03:25.794 
Epoch 294/1000 
	 loss: 390.0028, MinusLogProbMetric: 390.0028, val_loss: 397.4231, val_MinusLogProbMetric: 397.4231

Epoch 294: val_loss did not improve from 395.61737
196/196 - 20s - loss: 390.0028 - MinusLogProbMetric: 390.0028 - val_loss: 397.4231 - val_MinusLogProbMetric: 397.4231 - lr: 1.6667e-04 - 20s/epoch - 101ms/step
Epoch 295/1000
2023-09-09 15:03:43.306 
Epoch 295/1000 
	 loss: 390.0760, MinusLogProbMetric: 390.0760, val_loss: 395.7564, val_MinusLogProbMetric: 395.7564

Epoch 295: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.0760 - MinusLogProbMetric: 390.0760 - val_loss: 395.7564 - val_MinusLogProbMetric: 395.7564 - lr: 1.6667e-04 - 18s/epoch - 89ms/step
Epoch 296/1000
2023-09-09 15:04:00.594 
Epoch 296/1000 
	 loss: 389.9828, MinusLogProbMetric: 389.9828, val_loss: 397.1939, val_MinusLogProbMetric: 397.1939

Epoch 296: val_loss did not improve from 395.61737
196/196 - 17s - loss: 389.9828 - MinusLogProbMetric: 389.9828 - val_loss: 397.1939 - val_MinusLogProbMetric: 397.1939 - lr: 1.6667e-04 - 17s/epoch - 88ms/step
Epoch 297/1000
2023-09-09 15:04:18.599 
Epoch 297/1000 
	 loss: 390.0926, MinusLogProbMetric: 390.0926, val_loss: 396.4272, val_MinusLogProbMetric: 396.4272

Epoch 297: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.0926 - MinusLogProbMetric: 390.0926 - val_loss: 396.4272 - val_MinusLogProbMetric: 396.4272 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 298/1000
2023-09-09 15:04:38.525 
Epoch 298/1000 
	 loss: 389.9717, MinusLogProbMetric: 389.9717, val_loss: 396.6703, val_MinusLogProbMetric: 396.6703

Epoch 298: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.9717 - MinusLogProbMetric: 389.9717 - val_loss: 396.6703 - val_MinusLogProbMetric: 396.6703 - lr: 1.6667e-04 - 20s/epoch - 102ms/step
Epoch 299/1000
2023-09-09 15:04:56.911 
Epoch 299/1000 
	 loss: 389.9574, MinusLogProbMetric: 389.9574, val_loss: 396.0870, val_MinusLogProbMetric: 396.0870

Epoch 299: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.9574 - MinusLogProbMetric: 389.9574 - val_loss: 396.0870 - val_MinusLogProbMetric: 396.0870 - lr: 1.6667e-04 - 18s/epoch - 94ms/step
Epoch 300/1000
2023-09-09 15:05:14.007 
Epoch 300/1000 
	 loss: 389.9111, MinusLogProbMetric: 389.9111, val_loss: 396.2586, val_MinusLogProbMetric: 396.2586

Epoch 300: val_loss did not improve from 395.61737
196/196 - 17s - loss: 389.9111 - MinusLogProbMetric: 389.9111 - val_loss: 396.2586 - val_MinusLogProbMetric: 396.2586 - lr: 1.6667e-04 - 17s/epoch - 87ms/step
Epoch 301/1000
2023-09-09 15:05:32.056 
Epoch 301/1000 
	 loss: 389.9794, MinusLogProbMetric: 389.9794, val_loss: 397.2747, val_MinusLogProbMetric: 397.2747

Epoch 301: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.9794 - MinusLogProbMetric: 389.9794 - val_loss: 397.2747 - val_MinusLogProbMetric: 397.2747 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 302/1000
2023-09-09 15:05:51.769 
Epoch 302/1000 
	 loss: 389.9070, MinusLogProbMetric: 389.9070, val_loss: 396.3411, val_MinusLogProbMetric: 396.3411

Epoch 302: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.9070 - MinusLogProbMetric: 389.9070 - val_loss: 396.3411 - val_MinusLogProbMetric: 396.3411 - lr: 1.6667e-04 - 20s/epoch - 101ms/step
Epoch 303/1000
2023-09-09 15:06:10.425 
Epoch 303/1000 
	 loss: 389.8513, MinusLogProbMetric: 389.8513, val_loss: 397.4462, val_MinusLogProbMetric: 397.4462

Epoch 303: val_loss did not improve from 395.61737
196/196 - 19s - loss: 389.8513 - MinusLogProbMetric: 389.8513 - val_loss: 397.4462 - val_MinusLogProbMetric: 397.4462 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 304/1000
2023-09-09 15:06:29.959 
Epoch 304/1000 
	 loss: 390.1901, MinusLogProbMetric: 390.1901, val_loss: 396.4042, val_MinusLogProbMetric: 396.4042

Epoch 304: val_loss did not improve from 395.61737
196/196 - 20s - loss: 390.1901 - MinusLogProbMetric: 390.1901 - val_loss: 396.4042 - val_MinusLogProbMetric: 396.4042 - lr: 1.6667e-04 - 20s/epoch - 100ms/step
Epoch 305/1000
2023-09-09 15:06:50.322 
Epoch 305/1000 
	 loss: 389.5951, MinusLogProbMetric: 389.5951, val_loss: 396.2090, val_MinusLogProbMetric: 396.2090

Epoch 305: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.5951 - MinusLogProbMetric: 389.5951 - val_loss: 396.2090 - val_MinusLogProbMetric: 396.2090 - lr: 1.6667e-04 - 20s/epoch - 104ms/step
Epoch 306/1000
2023-09-09 15:07:10.316 
Epoch 306/1000 
	 loss: 389.7655, MinusLogProbMetric: 389.7655, val_loss: 396.7498, val_MinusLogProbMetric: 396.7498

Epoch 306: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.7655 - MinusLogProbMetric: 389.7655 - val_loss: 396.7498 - val_MinusLogProbMetric: 396.7498 - lr: 1.6667e-04 - 20s/epoch - 102ms/step
Epoch 307/1000
2023-09-09 15:07:30.805 
Epoch 307/1000 
	 loss: 390.0822, MinusLogProbMetric: 390.0822, val_loss: 395.9284, val_MinusLogProbMetric: 395.9284

Epoch 307: val_loss did not improve from 395.61737
196/196 - 20s - loss: 390.0822 - MinusLogProbMetric: 390.0822 - val_loss: 395.9284 - val_MinusLogProbMetric: 395.9284 - lr: 1.6667e-04 - 20s/epoch - 105ms/step
Epoch 308/1000
2023-09-09 15:07:50.487 
Epoch 308/1000 
	 loss: 389.9916, MinusLogProbMetric: 389.9916, val_loss: 397.3711, val_MinusLogProbMetric: 397.3711

Epoch 308: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.9916 - MinusLogProbMetric: 389.9916 - val_loss: 397.3711 - val_MinusLogProbMetric: 397.3711 - lr: 1.6667e-04 - 20s/epoch - 100ms/step
Epoch 309/1000
2023-09-09 15:08:10.359 
Epoch 309/1000 
	 loss: 389.9659, MinusLogProbMetric: 389.9659, val_loss: 401.8160, val_MinusLogProbMetric: 401.8160

Epoch 309: val_loss did not improve from 395.61737
196/196 - 20s - loss: 389.9659 - MinusLogProbMetric: 389.9659 - val_loss: 401.8160 - val_MinusLogProbMetric: 401.8160 - lr: 1.6667e-04 - 20s/epoch - 101ms/step
Epoch 310/1000
2023-09-09 15:08:32.340 
Epoch 310/1000 
	 loss: 389.8969, MinusLogProbMetric: 389.8969, val_loss: 398.3707, val_MinusLogProbMetric: 398.3707

Epoch 310: val_loss did not improve from 395.61737
196/196 - 22s - loss: 389.8969 - MinusLogProbMetric: 389.8969 - val_loss: 398.3707 - val_MinusLogProbMetric: 398.3707 - lr: 1.6667e-04 - 22s/epoch - 112ms/step
Epoch 311/1000
2023-09-09 15:08:51.181 
Epoch 311/1000 
	 loss: 389.9733, MinusLogProbMetric: 389.9733, val_loss: 396.5993, val_MinusLogProbMetric: 396.5993

Epoch 311: val_loss did not improve from 395.61737
196/196 - 19s - loss: 389.9733 - MinusLogProbMetric: 389.9733 - val_loss: 396.5993 - val_MinusLogProbMetric: 396.5993 - lr: 1.6667e-04 - 19s/epoch - 96ms/step
Epoch 312/1000
2023-09-09 15:09:09.031 
Epoch 312/1000 
	 loss: 389.8907, MinusLogProbMetric: 389.8907, val_loss: 398.7317, val_MinusLogProbMetric: 398.7317

Epoch 312: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.8907 - MinusLogProbMetric: 389.8907 - val_loss: 398.7317 - val_MinusLogProbMetric: 398.7317 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 313/1000
2023-09-09 15:09:27.203 
Epoch 313/1000 
	 loss: 389.8547, MinusLogProbMetric: 389.8547, val_loss: 397.8282, val_MinusLogProbMetric: 397.8282

Epoch 313: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.8547 - MinusLogProbMetric: 389.8547 - val_loss: 397.8282 - val_MinusLogProbMetric: 397.8282 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 314/1000
2023-09-09 15:09:45.401 
Epoch 314/1000 
	 loss: 390.0280, MinusLogProbMetric: 390.0280, val_loss: 398.5570, val_MinusLogProbMetric: 398.5570

Epoch 314: val_loss did not improve from 395.61737
196/196 - 18s - loss: 390.0280 - MinusLogProbMetric: 390.0280 - val_loss: 398.5570 - val_MinusLogProbMetric: 398.5570 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 315/1000
2023-09-09 15:10:03.535 
Epoch 315/1000 
	 loss: 389.8582, MinusLogProbMetric: 389.8582, val_loss: 396.5117, val_MinusLogProbMetric: 396.5117

Epoch 315: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.8582 - MinusLogProbMetric: 389.8582 - val_loss: 396.5117 - val_MinusLogProbMetric: 396.5117 - lr: 1.6667e-04 - 18s/epoch - 93ms/step
Epoch 316/1000
2023-09-09 15:10:21.153 
Epoch 316/1000 
	 loss: 389.6656, MinusLogProbMetric: 389.6656, val_loss: 396.4600, val_MinusLogProbMetric: 396.4600

Epoch 316: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.6656 - MinusLogProbMetric: 389.6656 - val_loss: 396.4600 - val_MinusLogProbMetric: 396.4600 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 317/1000
2023-09-09 15:10:39.754 
Epoch 317/1000 
	 loss: 390.1345, MinusLogProbMetric: 390.1345, val_loss: 397.4425, val_MinusLogProbMetric: 397.4425

Epoch 317: val_loss did not improve from 395.61737
196/196 - 19s - loss: 390.1345 - MinusLogProbMetric: 390.1345 - val_loss: 397.4425 - val_MinusLogProbMetric: 397.4425 - lr: 1.6667e-04 - 19s/epoch - 95ms/step
Epoch 318/1000
2023-09-09 15:10:57.144 
Epoch 318/1000 
	 loss: 389.6551, MinusLogProbMetric: 389.6551, val_loss: 395.7719, val_MinusLogProbMetric: 395.7719

Epoch 318: val_loss did not improve from 395.61737
196/196 - 17s - loss: 389.6551 - MinusLogProbMetric: 389.6551 - val_loss: 395.7719 - val_MinusLogProbMetric: 395.7719 - lr: 1.6667e-04 - 17s/epoch - 89ms/step
Epoch 319/1000
2023-09-09 15:11:14.922 
Epoch 319/1000 
	 loss: 389.4940, MinusLogProbMetric: 389.4940, val_loss: 396.9928, val_MinusLogProbMetric: 396.9928

Epoch 319: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.4940 - MinusLogProbMetric: 389.4940 - val_loss: 396.9928 - val_MinusLogProbMetric: 396.9928 - lr: 1.6667e-04 - 18s/epoch - 91ms/step
Epoch 320/1000
2023-09-09 15:11:32.559 
Epoch 320/1000 
	 loss: 389.7042, MinusLogProbMetric: 389.7042, val_loss: 398.6538, val_MinusLogProbMetric: 398.6538

Epoch 320: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.7042 - MinusLogProbMetric: 389.7042 - val_loss: 398.6538 - val_MinusLogProbMetric: 398.6538 - lr: 1.6667e-04 - 18s/epoch - 90ms/step
Epoch 321/1000
2023-09-09 15:11:50.581 
Epoch 321/1000 
	 loss: 389.7740, MinusLogProbMetric: 389.7740, val_loss: 397.4990, val_MinusLogProbMetric: 397.4990

Epoch 321: val_loss did not improve from 395.61737
196/196 - 18s - loss: 389.7740 - MinusLogProbMetric: 389.7740 - val_loss: 397.4990 - val_MinusLogProbMetric: 397.4990 - lr: 1.6667e-04 - 18s/epoch - 92ms/step
Epoch 322/1000
2023-09-09 15:12:06.977 
Epoch 322/1000 
	 loss: 389.6977, MinusLogProbMetric: 389.6977, val_loss: 397.2122, val_MinusLogProbMetric: 397.2122

Epoch 322: val_loss did not improve from 395.61737
196/196 - 16s - loss: 389.6977 - MinusLogProbMetric: 389.6977 - val_loss: 397.2122 - val_MinusLogProbMetric: 397.2122 - lr: 1.6667e-04 - 16s/epoch - 84ms/step
Epoch 323/1000
2023-09-09 15:12:22.963 
Epoch 323/1000 
	 loss: 389.7249, MinusLogProbMetric: 389.7249, val_loss: 396.3651, val_MinusLogProbMetric: 396.3651

Epoch 323: val_loss did not improve from 395.61737
196/196 - 16s - loss: 389.7249 - MinusLogProbMetric: 389.7249 - val_loss: 396.3651 - val_MinusLogProbMetric: 396.3651 - lr: 1.6667e-04 - 16s/epoch - 82ms/step
Epoch 324/1000
2023-09-09 15:12:39.300 
Epoch 324/1000 
	 loss: 389.7414, MinusLogProbMetric: 389.7414, val_loss: 400.9001, val_MinusLogProbMetric: 400.9001

Epoch 324: val_loss did not improve from 395.61737
196/196 - 16s - loss: 389.7414 - MinusLogProbMetric: 389.7414 - val_loss: 400.9001 - val_MinusLogProbMetric: 400.9001 - lr: 1.6667e-04 - 16s/epoch - 83ms/step
Epoch 325/1000
2023-09-09 15:12:55.195 
Epoch 325/1000 
	 loss: 389.7011, MinusLogProbMetric: 389.7011, val_loss: 396.3427, val_MinusLogProbMetric: 396.3427

Epoch 325: val_loss did not improve from 395.61737
196/196 - 16s - loss: 389.7011 - MinusLogProbMetric: 389.7011 - val_loss: 396.3427 - val_MinusLogProbMetric: 396.3427 - lr: 1.6667e-04 - 16s/epoch - 81ms/step
Epoch 326/1000
2023-09-09 15:13:10.831 
Epoch 326/1000 
	 loss: 387.5263, MinusLogProbMetric: 387.5263, val_loss: 395.3443, val_MinusLogProbMetric: 395.3443

Epoch 326: val_loss improved from 395.61737 to 395.34430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 16s - loss: 387.5263 - MinusLogProbMetric: 387.5263 - val_loss: 395.3443 - val_MinusLogProbMetric: 395.3443 - lr: 8.3333e-05 - 16s/epoch - 84ms/step
Epoch 327/1000
2023-09-09 15:13:27.943 
Epoch 327/1000 
	 loss: 387.3375, MinusLogProbMetric: 387.3375, val_loss: 395.4316, val_MinusLogProbMetric: 395.4316

Epoch 327: val_loss did not improve from 395.34430
196/196 - 16s - loss: 387.3375 - MinusLogProbMetric: 387.3375 - val_loss: 395.4316 - val_MinusLogProbMetric: 395.4316 - lr: 8.3333e-05 - 16s/epoch - 83ms/step
Epoch 328/1000
2023-09-09 15:13:43.913 
Epoch 328/1000 
	 loss: 387.3610, MinusLogProbMetric: 387.3610, val_loss: 396.1466, val_MinusLogProbMetric: 396.1466

Epoch 328: val_loss did not improve from 395.34430
196/196 - 16s - loss: 387.3610 - MinusLogProbMetric: 387.3610 - val_loss: 396.1466 - val_MinusLogProbMetric: 396.1466 - lr: 8.3333e-05 - 16s/epoch - 82ms/step
Epoch 329/1000
2023-09-09 15:13:59.750 
Epoch 329/1000 
	 loss: 387.5086, MinusLogProbMetric: 387.5086, val_loss: 395.2754, val_MinusLogProbMetric: 395.2754

Epoch 329: val_loss improved from 395.34430 to 395.27536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_324/weights/best_weights.h5
196/196 - 17s - loss: 387.5086 - MinusLogProbMetric: 387.5086 - val_loss: 395.2754 - val_MinusLogProbMetric: 395.2754 - lr: 8.3333e-05 - 17s/epoch - 85ms/step
Epoch 330/1000
2023-09-09 15:14:15.693 
Epoch 330/1000 
	 loss: 387.3522, MinusLogProbMetric: 387.3522, val_loss: 395.9192, val_MinusLogProbMetric: 395.9192

Epoch 330: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3522 - MinusLogProbMetric: 387.3522 - val_loss: 395.9192 - val_MinusLogProbMetric: 395.9192 - lr: 8.3333e-05 - 15s/epoch - 77ms/step
Epoch 331/1000
2023-09-09 15:14:31.400 
Epoch 331/1000 
	 loss: 387.5078, MinusLogProbMetric: 387.5078, val_loss: 395.6691, val_MinusLogProbMetric: 395.6691

Epoch 331: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.5078 - MinusLogProbMetric: 387.5078 - val_loss: 395.6691 - val_MinusLogProbMetric: 395.6691 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 332/1000
2023-09-09 15:14:46.755 
Epoch 332/1000 
	 loss: 387.4811, MinusLogProbMetric: 387.4811, val_loss: 395.7131, val_MinusLogProbMetric: 395.7131

Epoch 332: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4811 - MinusLogProbMetric: 387.4811 - val_loss: 395.7131 - val_MinusLogProbMetric: 395.7131 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 333/1000
2023-09-09 15:15:03.209 
Epoch 333/1000 
	 loss: 387.6064, MinusLogProbMetric: 387.6064, val_loss: 395.7074, val_MinusLogProbMetric: 395.7074

Epoch 333: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.6064 - MinusLogProbMetric: 387.6064 - val_loss: 395.7074 - val_MinusLogProbMetric: 395.7074 - lr: 8.3333e-05 - 16s/epoch - 84ms/step
Epoch 334/1000
2023-09-09 15:15:18.978 
Epoch 334/1000 
	 loss: 387.6236, MinusLogProbMetric: 387.6236, val_loss: 396.6004, val_MinusLogProbMetric: 396.6004

Epoch 334: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.6236 - MinusLogProbMetric: 387.6236 - val_loss: 396.6004 - val_MinusLogProbMetric: 396.6004 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 335/1000
2023-09-09 15:15:34.632 
Epoch 335/1000 
	 loss: 387.4922, MinusLogProbMetric: 387.4922, val_loss: 395.5042, val_MinusLogProbMetric: 395.5042

Epoch 335: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.4922 - MinusLogProbMetric: 387.4922 - val_loss: 395.5042 - val_MinusLogProbMetric: 395.5042 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 336/1000
2023-09-09 15:15:50.816 
Epoch 336/1000 
	 loss: 387.4413, MinusLogProbMetric: 387.4413, val_loss: 395.4766, val_MinusLogProbMetric: 395.4766

Epoch 336: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.4413 - MinusLogProbMetric: 387.4413 - val_loss: 395.4766 - val_MinusLogProbMetric: 395.4766 - lr: 8.3333e-05 - 16s/epoch - 82ms/step
Epoch 337/1000
2023-09-09 15:16:06.195 
Epoch 337/1000 
	 loss: 387.4640, MinusLogProbMetric: 387.4640, val_loss: 395.3910, val_MinusLogProbMetric: 395.3910

Epoch 337: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4640 - MinusLogProbMetric: 387.4640 - val_loss: 395.3910 - val_MinusLogProbMetric: 395.3910 - lr: 8.3333e-05 - 15s/epoch - 79ms/step
Epoch 338/1000
2023-09-09 15:16:21.765 
Epoch 338/1000 
	 loss: 387.4655, MinusLogProbMetric: 387.4655, val_loss: 395.7719, val_MinusLogProbMetric: 395.7719

Epoch 338: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.4655 - MinusLogProbMetric: 387.4655 - val_loss: 395.7719 - val_MinusLogProbMetric: 395.7719 - lr: 8.3333e-05 - 16s/epoch - 79ms/step
Epoch 339/1000
2023-09-09 15:16:37.379 
Epoch 339/1000 
	 loss: 387.6024, MinusLogProbMetric: 387.6024, val_loss: 395.6342, val_MinusLogProbMetric: 395.6342

Epoch 339: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.6024 - MinusLogProbMetric: 387.6024 - val_loss: 395.6342 - val_MinusLogProbMetric: 395.6342 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 340/1000
2023-09-09 15:16:53.865 
Epoch 340/1000 
	 loss: 387.4147, MinusLogProbMetric: 387.4147, val_loss: 396.6788, val_MinusLogProbMetric: 396.6788

Epoch 340: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.4147 - MinusLogProbMetric: 387.4147 - val_loss: 396.6788 - val_MinusLogProbMetric: 396.6788 - lr: 8.3333e-05 - 16s/epoch - 84ms/step
Epoch 341/1000
2023-09-09 15:17:09.202 
Epoch 341/1000 
	 loss: 387.5625, MinusLogProbMetric: 387.5625, val_loss: 395.6754, val_MinusLogProbMetric: 395.6754

Epoch 341: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.5625 - MinusLogProbMetric: 387.5625 - val_loss: 395.6754 - val_MinusLogProbMetric: 395.6754 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 342/1000
2023-09-09 15:17:24.842 
Epoch 342/1000 
	 loss: 387.3643, MinusLogProbMetric: 387.3643, val_loss: 396.2987, val_MinusLogProbMetric: 396.2987

Epoch 342: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.3643 - MinusLogProbMetric: 387.3643 - val_loss: 396.2987 - val_MinusLogProbMetric: 396.2987 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 343/1000
2023-09-09 15:17:40.602 
Epoch 343/1000 
	 loss: 387.5654, MinusLogProbMetric: 387.5654, val_loss: 396.0659, val_MinusLogProbMetric: 396.0659

Epoch 343: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.5654 - MinusLogProbMetric: 387.5654 - val_loss: 396.0659 - val_MinusLogProbMetric: 396.0659 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 344/1000
2023-09-09 15:17:56.157 
Epoch 344/1000 
	 loss: 387.5274, MinusLogProbMetric: 387.5274, val_loss: 395.6593, val_MinusLogProbMetric: 395.6593

Epoch 344: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.5274 - MinusLogProbMetric: 387.5274 - val_loss: 395.6593 - val_MinusLogProbMetric: 395.6593 - lr: 8.3333e-05 - 16s/epoch - 79ms/step
Epoch 345/1000
2023-09-09 15:18:11.399 
Epoch 345/1000 
	 loss: 387.3753, MinusLogProbMetric: 387.3753, val_loss: 395.7515, val_MinusLogProbMetric: 395.7515

Epoch 345: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3753 - MinusLogProbMetric: 387.3753 - val_loss: 395.7515 - val_MinusLogProbMetric: 395.7515 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 346/1000
2023-09-09 15:18:26.420 
Epoch 346/1000 
	 loss: 387.7573, MinusLogProbMetric: 387.7573, val_loss: 396.3601, val_MinusLogProbMetric: 396.3601

Epoch 346: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.7573 - MinusLogProbMetric: 387.7573 - val_loss: 396.3601 - val_MinusLogProbMetric: 396.3601 - lr: 8.3333e-05 - 15s/epoch - 77ms/step
Epoch 347/1000
2023-09-09 15:18:41.460 
Epoch 347/1000 
	 loss: 387.4311, MinusLogProbMetric: 387.4311, val_loss: 396.0372, val_MinusLogProbMetric: 396.0372

Epoch 347: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4311 - MinusLogProbMetric: 387.4311 - val_loss: 396.0372 - val_MinusLogProbMetric: 396.0372 - lr: 8.3333e-05 - 15s/epoch - 77ms/step
Epoch 348/1000
2023-09-09 15:18:56.672 
Epoch 348/1000 
	 loss: 387.4673, MinusLogProbMetric: 387.4673, val_loss: 396.6871, val_MinusLogProbMetric: 396.6871

Epoch 348: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4673 - MinusLogProbMetric: 387.4673 - val_loss: 396.6871 - val_MinusLogProbMetric: 396.6871 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 349/1000
2023-09-09 15:19:11.954 
Epoch 349/1000 
	 loss: 387.2925, MinusLogProbMetric: 387.2925, val_loss: 395.6999, val_MinusLogProbMetric: 395.6999

Epoch 349: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.2925 - MinusLogProbMetric: 387.2925 - val_loss: 395.6999 - val_MinusLogProbMetric: 395.6999 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 350/1000
2023-09-09 15:19:26.782 
Epoch 350/1000 
	 loss: 387.9653, MinusLogProbMetric: 387.9653, val_loss: 395.4828, val_MinusLogProbMetric: 395.4828

Epoch 350: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.9653 - MinusLogProbMetric: 387.9653 - val_loss: 395.4828 - val_MinusLogProbMetric: 395.4828 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 351/1000
2023-09-09 15:19:42.043 
Epoch 351/1000 
	 loss: 387.1567, MinusLogProbMetric: 387.1567, val_loss: 396.3090, val_MinusLogProbMetric: 396.3090

Epoch 351: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.1567 - MinusLogProbMetric: 387.1567 - val_loss: 396.3090 - val_MinusLogProbMetric: 396.3090 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 352/1000
2023-09-09 15:19:57.362 
Epoch 352/1000 
	 loss: 387.2790, MinusLogProbMetric: 387.2790, val_loss: 396.7013, val_MinusLogProbMetric: 396.7013

Epoch 352: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.2790 - MinusLogProbMetric: 387.2790 - val_loss: 396.7013 - val_MinusLogProbMetric: 396.7013 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 353/1000
2023-09-09 15:20:12.294 
Epoch 353/1000 
	 loss: 387.5248, MinusLogProbMetric: 387.5248, val_loss: 396.2640, val_MinusLogProbMetric: 396.2640

Epoch 353: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.5248 - MinusLogProbMetric: 387.5248 - val_loss: 396.2640 - val_MinusLogProbMetric: 396.2640 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 354/1000
2023-09-09 15:20:27.442 
Epoch 354/1000 
	 loss: 387.1376, MinusLogProbMetric: 387.1376, val_loss: 396.1016, val_MinusLogProbMetric: 396.1016

Epoch 354: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.1376 - MinusLogProbMetric: 387.1376 - val_loss: 396.1016 - val_MinusLogProbMetric: 396.1016 - lr: 8.3333e-05 - 15s/epoch - 77ms/step
Epoch 355/1000
2023-09-09 15:20:42.573 
Epoch 355/1000 
	 loss: 387.4262, MinusLogProbMetric: 387.4262, val_loss: 396.1156, val_MinusLogProbMetric: 396.1156

Epoch 355: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4262 - MinusLogProbMetric: 387.4262 - val_loss: 396.1156 - val_MinusLogProbMetric: 396.1156 - lr: 8.3333e-05 - 15s/epoch - 77ms/step
Epoch 356/1000
2023-09-09 15:20:57.366 
Epoch 356/1000 
	 loss: 387.3513, MinusLogProbMetric: 387.3513, val_loss: 395.7227, val_MinusLogProbMetric: 395.7227

Epoch 356: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3513 - MinusLogProbMetric: 387.3513 - val_loss: 395.7227 - val_MinusLogProbMetric: 395.7227 - lr: 8.3333e-05 - 15s/epoch - 75ms/step
Epoch 357/1000
2023-09-09 15:21:12.723 
Epoch 357/1000 
	 loss: 387.3740, MinusLogProbMetric: 387.3740, val_loss: 395.5508, val_MinusLogProbMetric: 395.5508

Epoch 357: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3740 - MinusLogProbMetric: 387.3740 - val_loss: 395.5508 - val_MinusLogProbMetric: 395.5508 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 358/1000
2023-09-09 15:21:27.930 
Epoch 358/1000 
	 loss: 387.4384, MinusLogProbMetric: 387.4384, val_loss: 396.3332, val_MinusLogProbMetric: 396.3332

Epoch 358: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4384 - MinusLogProbMetric: 387.4384 - val_loss: 396.3332 - val_MinusLogProbMetric: 396.3332 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 359/1000
2023-09-09 15:21:43.140 
Epoch 359/1000 
	 loss: 387.4368, MinusLogProbMetric: 387.4368, val_loss: 395.6672, val_MinusLogProbMetric: 395.6672

Epoch 359: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4368 - MinusLogProbMetric: 387.4368 - val_loss: 395.6672 - val_MinusLogProbMetric: 395.6672 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 360/1000
2023-09-09 15:21:58.276 
Epoch 360/1000 
	 loss: 387.3301, MinusLogProbMetric: 387.3301, val_loss: 396.8957, val_MinusLogProbMetric: 396.8957

Epoch 360: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3301 - MinusLogProbMetric: 387.3301 - val_loss: 396.8957 - val_MinusLogProbMetric: 396.8957 - lr: 8.3333e-05 - 15s/epoch - 77ms/step
Epoch 361/1000
2023-09-09 15:22:13.529 
Epoch 361/1000 
	 loss: 387.4344, MinusLogProbMetric: 387.4344, val_loss: 396.3346, val_MinusLogProbMetric: 396.3346

Epoch 361: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4344 - MinusLogProbMetric: 387.4344 - val_loss: 396.3346 - val_MinusLogProbMetric: 396.3346 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 362/1000
2023-09-09 15:22:28.321 
Epoch 362/1000 
	 loss: 387.4752, MinusLogProbMetric: 387.4752, val_loss: 396.6009, val_MinusLogProbMetric: 396.6009

Epoch 362: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4752 - MinusLogProbMetric: 387.4752 - val_loss: 396.6009 - val_MinusLogProbMetric: 396.6009 - lr: 8.3333e-05 - 15s/epoch - 75ms/step
Epoch 363/1000
2023-09-09 15:22:43.180 
Epoch 363/1000 
	 loss: 387.3769, MinusLogProbMetric: 387.3769, val_loss: 396.0859, val_MinusLogProbMetric: 396.0859

Epoch 363: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3769 - MinusLogProbMetric: 387.3769 - val_loss: 396.0859 - val_MinusLogProbMetric: 396.0859 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 364/1000
2023-09-09 15:22:58.116 
Epoch 364/1000 
	 loss: 387.6500, MinusLogProbMetric: 387.6500, val_loss: 396.0973, val_MinusLogProbMetric: 396.0973

Epoch 364: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.6500 - MinusLogProbMetric: 387.6500 - val_loss: 396.0973 - val_MinusLogProbMetric: 396.0973 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 365/1000
2023-09-09 15:23:13.011 
Epoch 365/1000 
	 loss: 387.3243, MinusLogProbMetric: 387.3243, val_loss: 396.1923, val_MinusLogProbMetric: 396.1923

Epoch 365: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3243 - MinusLogProbMetric: 387.3243 - val_loss: 396.1923 - val_MinusLogProbMetric: 396.1923 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 366/1000
2023-09-09 15:23:27.967 
Epoch 366/1000 
	 loss: 387.2399, MinusLogProbMetric: 387.2399, val_loss: 396.1739, val_MinusLogProbMetric: 396.1739

Epoch 366: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.2399 - MinusLogProbMetric: 387.2399 - val_loss: 396.1739 - val_MinusLogProbMetric: 396.1739 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 367/1000
2023-09-09 15:23:43.287 
Epoch 367/1000 
	 loss: 387.2484, MinusLogProbMetric: 387.2484, val_loss: 396.8288, val_MinusLogProbMetric: 396.8288

Epoch 367: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.2484 - MinusLogProbMetric: 387.2484 - val_loss: 396.8288 - val_MinusLogProbMetric: 396.8288 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 368/1000
2023-09-09 15:23:58.215 
Epoch 368/1000 
	 loss: 387.4258, MinusLogProbMetric: 387.4258, val_loss: 397.1170, val_MinusLogProbMetric: 397.1170

Epoch 368: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.4258 - MinusLogProbMetric: 387.4258 - val_loss: 397.1170 - val_MinusLogProbMetric: 397.1170 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 369/1000
2023-09-09 15:24:13.435 
Epoch 369/1000 
	 loss: 387.3992, MinusLogProbMetric: 387.3992, val_loss: 397.8107, val_MinusLogProbMetric: 397.8107

Epoch 369: val_loss did not improve from 395.27536
196/196 - 15s - loss: 387.3992 - MinusLogProbMetric: 387.3992 - val_loss: 397.8107 - val_MinusLogProbMetric: 397.8107 - lr: 8.3333e-05 - 15s/epoch - 78ms/step
Epoch 370/1000
2023-09-09 15:24:29.130 
Epoch 370/1000 
	 loss: 386.9689, MinusLogProbMetric: 386.9689, val_loss: 396.1722, val_MinusLogProbMetric: 396.1722

Epoch 370: val_loss did not improve from 395.27536
196/196 - 16s - loss: 386.9689 - MinusLogProbMetric: 386.9689 - val_loss: 396.1722 - val_MinusLogProbMetric: 396.1722 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 371/1000
2023-09-09 15:24:44.748 
Epoch 371/1000 
	 loss: 387.2347, MinusLogProbMetric: 387.2347, val_loss: 396.4397, val_MinusLogProbMetric: 396.4397

Epoch 371: val_loss did not improve from 395.27536
196/196 - 16s - loss: 387.2347 - MinusLogProbMetric: 387.2347 - val_loss: 396.4397 - val_MinusLogProbMetric: 396.4397 - lr: 8.3333e-05 - 16s/epoch - 80ms/step
Epoch 372/1000
2023-09-09 15:24:59.639 
Epoch 372/1000 
	 loss: 386.9804, MinusLogProbMetric: 386.9804, val_loss: 396.1785, val_MinusLogProbMetric: 396.1785

Epoch 372: val_loss did not improve from 395.27536
196/196 - 15s - loss: 386.9804 - MinusLogProbMetric: 386.9804 - val_loss: 396.1785 - val_MinusLogProbMetric: 396.1785 - lr: 8.3333e-05 - 15s/epoch - 76ms/step
Epoch 373/1000
2023-09-09 15:25:19.418 
Epoch 373/1000 
	 loss: 387.3314, MinusLogProbMetric: 387.3314, val_loss: 399.0952, val_MinusLogProbMetric: 399.0952

Epoch 373: val_loss did not improve from 395.27536
196/196 - 20s - loss: 387.3314 - MinusLogProbMetric: 387.3314 - val_loss: 399.0952 - val_MinusLogProbMetric: 399.0952 - lr: 8.3333e-05 - 20s/epoch - 101ms/step
Epoch 374/1000
2023-09-09 15:25:37.724 
Epoch 374/1000 
	 loss: 387.3554, MinusLogProbMetric: 387.3554, val_loss: 397.3582, val_MinusLogProbMetric: 397.3582

Epoch 374: val_loss did not improve from 395.27536
196/196 - 18s - loss: 387.3554 - MinusLogProbMetric: 387.3554 - val_loss: 397.3582 - val_MinusLogProbMetric: 397.3582 - lr: 8.3333e-05 - 18s/epoch - 93ms/step
Epoch 375/1000
2023-09-09 15:25:56.734 
Epoch 375/1000 
	 loss: 387.1050, MinusLogProbMetric: 387.1050, val_loss: 396.0649, val_MinusLogProbMetric: 396.0649

Epoch 375: val_loss did not improve from 395.27536
196/196 - 19s - loss: 387.1050 - MinusLogProbMetric: 387.1050 - val_loss: 396.0649 - val_MinusLogProbMetric: 396.0649 - lr: 8.3333e-05 - 19s/epoch - 97ms/step
Epoch 376/1000
2023-09-09 15:26:17.110 
Epoch 376/1000 
	 loss: 387.2735, MinusLogProbMetric: 387.2735, val_loss: 396.3747, val_MinusLogProbMetric: 396.3747

Epoch 376: val_loss did not improve from 395.27536
196/196 - 20s - loss: 387.2735 - MinusLogProbMetric: 387.2735 - val_loss: 396.3747 - val_MinusLogProbMetric: 396.3747 - lr: 8.3333e-05 - 20s/epoch - 104ms/step
Epoch 377/1000
2023-09-09 15:26:36.919 
Epoch 377/1000 
	 loss: 387.1697, MinusLogProbMetric: 387.1697, val_loss: 397.0822, val_MinusLogProbMetric: 397.0822

Epoch 377: val_loss did not improve from 395.27536
196/196 - 20s - loss: 387.1697 - MinusLogProbMetric: 387.1697 - val_loss: 397.0822 - val_MinusLogProbMetric: 397.0822 - lr: 8.3333e-05 - 20s/epoch - 101ms/step
Epoch 378/1000
2023-09-09 15:26:57.546 
Epoch 378/1000 
	 loss: 387.1831, MinusLogProbMetric: 387.1831, val_loss: 396.3111, val_MinusLogProbMetric: 396.3111

Epoch 378: val_loss did not improve from 395.27536
196/196 - 21s - loss: 387.1831 - MinusLogProbMetric: 387.1831 - val_loss: 396.3111 - val_MinusLogProbMetric: 396.3111 - lr: 8.3333e-05 - 21s/epoch - 105ms/step
Epoch 379/1000
2023-09-09 15:27:17.311 
Epoch 379/1000 
	 loss: 387.1615, MinusLogProbMetric: 387.1615, val_loss: 396.1588, val_MinusLogProbMetric: 396.1588

Epoch 379: val_loss did not improve from 395.27536
196/196 - 20s - loss: 387.1615 - MinusLogProbMetric: 387.1615 - val_loss: 396.1588 - val_MinusLogProbMetric: 396.1588 - lr: 8.3333e-05 - 20s/epoch - 101ms/step
Epoch 380/1000
2023-09-09 15:27:33.180 
Epoch 380/1000 
	 loss: 386.1317, MinusLogProbMetric: 386.1317, val_loss: 395.7820, val_MinusLogProbMetric: 395.7820

Epoch 380: val_loss did not improve from 395.27536
196/196 - 16s - loss: 386.1317 - MinusLogProbMetric: 386.1317 - val_loss: 395.7820 - val_MinusLogProbMetric: 395.7820 - lr: 4.1667e-05 - 16s/epoch - 81ms/step
Epoch 381/1000
2023-09-09 15:27:52.438 
Epoch 381/1000 
	 loss: 386.1119, MinusLogProbMetric: 386.1119, val_loss: 395.7487, val_MinusLogProbMetric: 395.7487

Epoch 381: val_loss did not improve from 395.27536
196/196 - 19s - loss: 386.1119 - MinusLogProbMetric: 386.1119 - val_loss: 395.7487 - val_MinusLogProbMetric: 395.7487 - lr: 4.1667e-05 - 19s/epoch - 98ms/step
Epoch 382/1000
2023-09-09 15:28:10.676 
Epoch 382/1000 
	 loss: 386.1015, MinusLogProbMetric: 386.1015, val_loss: 395.8169, val_MinusLogProbMetric: 395.8169

Epoch 382: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.1015 - MinusLogProbMetric: 386.1015 - val_loss: 395.8169 - val_MinusLogProbMetric: 395.8169 - lr: 4.1667e-05 - 18s/epoch - 93ms/step
Epoch 383/1000
2023-09-09 15:28:27.906 
Epoch 383/1000 
	 loss: 386.1024, MinusLogProbMetric: 386.1024, val_loss: 395.7069, val_MinusLogProbMetric: 395.7069

Epoch 383: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.1024 - MinusLogProbMetric: 386.1024 - val_loss: 395.7069 - val_MinusLogProbMetric: 395.7069 - lr: 4.1667e-05 - 17s/epoch - 88ms/step
Epoch 384/1000
2023-09-09 15:28:45.298 
Epoch 384/1000 
	 loss: 386.1096, MinusLogProbMetric: 386.1096, val_loss: 395.7885, val_MinusLogProbMetric: 395.7885

Epoch 384: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.1096 - MinusLogProbMetric: 386.1096 - val_loss: 395.7885 - val_MinusLogProbMetric: 395.7885 - lr: 4.1667e-05 - 17s/epoch - 89ms/step
Epoch 385/1000
2023-09-09 15:29:02.994 
Epoch 385/1000 
	 loss: 386.1062, MinusLogProbMetric: 386.1062, val_loss: 395.6549, val_MinusLogProbMetric: 395.6549

Epoch 385: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.1062 - MinusLogProbMetric: 386.1062 - val_loss: 395.6549 - val_MinusLogProbMetric: 395.6549 - lr: 4.1667e-05 - 18s/epoch - 90ms/step
Epoch 386/1000
2023-09-09 15:29:21.375 
Epoch 386/1000 
	 loss: 386.0510, MinusLogProbMetric: 386.0510, val_loss: 395.7968, val_MinusLogProbMetric: 395.7968

Epoch 386: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.0510 - MinusLogProbMetric: 386.0510 - val_loss: 395.7968 - val_MinusLogProbMetric: 395.7968 - lr: 4.1667e-05 - 18s/epoch - 94ms/step
Epoch 387/1000
2023-09-09 15:29:38.385 
Epoch 387/1000 
	 loss: 386.0831, MinusLogProbMetric: 386.0831, val_loss: 395.9755, val_MinusLogProbMetric: 395.9755

Epoch 387: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.0831 - MinusLogProbMetric: 386.0831 - val_loss: 395.9755 - val_MinusLogProbMetric: 395.9755 - lr: 4.1667e-05 - 17s/epoch - 87ms/step
Epoch 388/1000
2023-09-09 15:29:57.164 
Epoch 388/1000 
	 loss: 386.2165, MinusLogProbMetric: 386.2165, val_loss: 395.9634, val_MinusLogProbMetric: 395.9634

Epoch 388: val_loss did not improve from 395.27536
196/196 - 19s - loss: 386.2165 - MinusLogProbMetric: 386.2165 - val_loss: 395.9634 - val_MinusLogProbMetric: 395.9634 - lr: 4.1667e-05 - 19s/epoch - 96ms/step
Epoch 389/1000
2023-09-09 15:30:14.880 
Epoch 389/1000 
	 loss: 386.1598, MinusLogProbMetric: 386.1598, val_loss: 395.6499, val_MinusLogProbMetric: 395.6499

Epoch 389: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.1598 - MinusLogProbMetric: 386.1598 - val_loss: 395.6499 - val_MinusLogProbMetric: 395.6499 - lr: 4.1667e-05 - 18s/epoch - 90ms/step
Epoch 390/1000
2023-09-09 15:30:32.918 
Epoch 390/1000 
	 loss: 386.0401, MinusLogProbMetric: 386.0401, val_loss: 395.7170, val_MinusLogProbMetric: 395.7170

Epoch 390: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.0401 - MinusLogProbMetric: 386.0401 - val_loss: 395.7170 - val_MinusLogProbMetric: 395.7170 - lr: 4.1667e-05 - 18s/epoch - 92ms/step
Epoch 391/1000
2023-09-09 15:30:50.692 
Epoch 391/1000 
	 loss: 386.0972, MinusLogProbMetric: 386.0972, val_loss: 396.1736, val_MinusLogProbMetric: 396.1736

Epoch 391: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.0972 - MinusLogProbMetric: 386.0972 - val_loss: 396.1736 - val_MinusLogProbMetric: 396.1736 - lr: 4.1667e-05 - 18s/epoch - 91ms/step
Epoch 392/1000
2023-09-09 15:31:09.739 
Epoch 392/1000 
	 loss: 386.0779, MinusLogProbMetric: 386.0779, val_loss: 396.1010, val_MinusLogProbMetric: 396.1010

Epoch 392: val_loss did not improve from 395.27536
196/196 - 19s - loss: 386.0779 - MinusLogProbMetric: 386.0779 - val_loss: 396.1010 - val_MinusLogProbMetric: 396.1010 - lr: 4.1667e-05 - 19s/epoch - 97ms/step
Epoch 393/1000
2023-09-09 15:31:27.512 
Epoch 393/1000 
	 loss: 386.1516, MinusLogProbMetric: 386.1516, val_loss: 395.9722, val_MinusLogProbMetric: 395.9722

Epoch 393: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.1516 - MinusLogProbMetric: 386.1516 - val_loss: 395.9722 - val_MinusLogProbMetric: 395.9722 - lr: 4.1667e-05 - 18s/epoch - 90ms/step
Epoch 394/1000
2023-09-09 15:31:44.978 
Epoch 394/1000 
	 loss: 386.1793, MinusLogProbMetric: 386.1793, val_loss: 396.0170, val_MinusLogProbMetric: 396.0170

Epoch 394: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.1793 - MinusLogProbMetric: 386.1793 - val_loss: 396.0170 - val_MinusLogProbMetric: 396.0170 - lr: 4.1667e-05 - 17s/epoch - 89ms/step
Epoch 395/1000
2023-09-09 15:32:01.640 
Epoch 395/1000 
	 loss: 386.2040, MinusLogProbMetric: 386.2040, val_loss: 396.3505, val_MinusLogProbMetric: 396.3505

Epoch 395: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.2040 - MinusLogProbMetric: 386.2040 - val_loss: 396.3505 - val_MinusLogProbMetric: 396.3505 - lr: 4.1667e-05 - 17s/epoch - 85ms/step
Epoch 396/1000
2023-09-09 15:32:20.275 
Epoch 396/1000 
	 loss: 386.2676, MinusLogProbMetric: 386.2676, val_loss: 396.4426, val_MinusLogProbMetric: 396.4426

Epoch 396: val_loss did not improve from 395.27536
196/196 - 19s - loss: 386.2676 - MinusLogProbMetric: 386.2676 - val_loss: 396.4426 - val_MinusLogProbMetric: 396.4426 - lr: 4.1667e-05 - 19s/epoch - 95ms/step
Epoch 397/1000
2023-09-09 15:32:38.615 
Epoch 397/1000 
	 loss: 386.2984, MinusLogProbMetric: 386.2984, val_loss: 396.1443, val_MinusLogProbMetric: 396.1443

Epoch 397: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.2984 - MinusLogProbMetric: 386.2984 - val_loss: 396.1443 - val_MinusLogProbMetric: 396.1443 - lr: 4.1667e-05 - 18s/epoch - 94ms/step
Epoch 398/1000
2023-09-09 15:32:55.874 
Epoch 398/1000 
	 loss: 386.0546, MinusLogProbMetric: 386.0546, val_loss: 396.0186, val_MinusLogProbMetric: 396.0186

Epoch 398: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.0546 - MinusLogProbMetric: 386.0546 - val_loss: 396.0186 - val_MinusLogProbMetric: 396.0186 - lr: 4.1667e-05 - 17s/epoch - 88ms/step
Epoch 399/1000
2023-09-09 15:33:13.071 
Epoch 399/1000 
	 loss: 386.1376, MinusLogProbMetric: 386.1376, val_loss: 395.9364, val_MinusLogProbMetric: 395.9364

Epoch 399: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.1376 - MinusLogProbMetric: 386.1376 - val_loss: 395.9364 - val_MinusLogProbMetric: 395.9364 - lr: 4.1667e-05 - 17s/epoch - 88ms/step
Epoch 400/1000
2023-09-09 15:33:30.753 
Epoch 400/1000 
	 loss: 386.1368, MinusLogProbMetric: 386.1368, val_loss: 396.2383, val_MinusLogProbMetric: 396.2383

Epoch 400: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.1368 - MinusLogProbMetric: 386.1368 - val_loss: 396.2383 - val_MinusLogProbMetric: 396.2383 - lr: 4.1667e-05 - 18s/epoch - 90ms/step
Epoch 401/1000
2023-09-09 15:33:49.285 
Epoch 401/1000 
	 loss: 386.1500, MinusLogProbMetric: 386.1500, val_loss: 395.8354, val_MinusLogProbMetric: 395.8354

Epoch 401: val_loss did not improve from 395.27536
196/196 - 19s - loss: 386.1500 - MinusLogProbMetric: 386.1500 - val_loss: 395.8354 - val_MinusLogProbMetric: 395.8354 - lr: 4.1667e-05 - 19s/epoch - 94ms/step
Epoch 402/1000
2023-09-09 15:34:06.781 
Epoch 402/1000 
	 loss: 386.0662, MinusLogProbMetric: 386.0662, val_loss: 396.2027, val_MinusLogProbMetric: 396.2027

Epoch 402: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.0662 - MinusLogProbMetric: 386.0662 - val_loss: 396.2027 - val_MinusLogProbMetric: 396.2027 - lr: 4.1667e-05 - 17s/epoch - 89ms/step
Epoch 403/1000
2023-09-09 15:34:25.243 
Epoch 403/1000 
	 loss: 386.0748, MinusLogProbMetric: 386.0748, val_loss: 396.0091, val_MinusLogProbMetric: 396.0091

Epoch 403: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.0748 - MinusLogProbMetric: 386.0748 - val_loss: 396.0091 - val_MinusLogProbMetric: 396.0091 - lr: 4.1667e-05 - 18s/epoch - 94ms/step
Epoch 404/1000
2023-09-09 15:34:43.271 
Epoch 404/1000 
	 loss: 386.0624, MinusLogProbMetric: 386.0624, val_loss: 396.0815, val_MinusLogProbMetric: 396.0815

Epoch 404: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.0624 - MinusLogProbMetric: 386.0624 - val_loss: 396.0815 - val_MinusLogProbMetric: 396.0815 - lr: 4.1667e-05 - 18s/epoch - 92ms/step
Epoch 405/1000
2023-09-09 15:35:02.221 
Epoch 405/1000 
	 loss: 385.9955, MinusLogProbMetric: 385.9955, val_loss: 396.2179, val_MinusLogProbMetric: 396.2179

Epoch 405: val_loss did not improve from 395.27536
196/196 - 19s - loss: 385.9955 - MinusLogProbMetric: 385.9955 - val_loss: 396.2179 - val_MinusLogProbMetric: 396.2179 - lr: 4.1667e-05 - 19s/epoch - 96ms/step
Epoch 406/1000
2023-09-09 15:35:20.236 
Epoch 406/1000 
	 loss: 385.9764, MinusLogProbMetric: 385.9764, val_loss: 395.9758, val_MinusLogProbMetric: 395.9758

Epoch 406: val_loss did not improve from 395.27536
196/196 - 18s - loss: 385.9764 - MinusLogProbMetric: 385.9764 - val_loss: 395.9758 - val_MinusLogProbMetric: 395.9758 - lr: 4.1667e-05 - 18s/epoch - 92ms/step
Epoch 407/1000
2023-09-09 15:35:37.349 
Epoch 407/1000 
	 loss: 385.9908, MinusLogProbMetric: 385.9908, val_loss: 396.0229, val_MinusLogProbMetric: 396.0229

Epoch 407: val_loss did not improve from 395.27536
196/196 - 17s - loss: 385.9908 - MinusLogProbMetric: 385.9908 - val_loss: 396.0229 - val_MinusLogProbMetric: 396.0229 - lr: 4.1667e-05 - 17s/epoch - 87ms/step
Epoch 408/1000
2023-09-09 15:35:55.294 
Epoch 408/1000 
	 loss: 386.0434, MinusLogProbMetric: 386.0434, val_loss: 395.9516, val_MinusLogProbMetric: 395.9516

Epoch 408: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.0434 - MinusLogProbMetric: 386.0434 - val_loss: 395.9516 - val_MinusLogProbMetric: 395.9516 - lr: 4.1667e-05 - 18s/epoch - 92ms/step
Epoch 409/1000
2023-09-09 15:36:12.573 
Epoch 409/1000 
	 loss: 385.9119, MinusLogProbMetric: 385.9119, val_loss: 396.0771, val_MinusLogProbMetric: 396.0771

Epoch 409: val_loss did not improve from 395.27536
196/196 - 17s - loss: 385.9119 - MinusLogProbMetric: 385.9119 - val_loss: 396.0771 - val_MinusLogProbMetric: 396.0771 - lr: 4.1667e-05 - 17s/epoch - 88ms/step
Epoch 410/1000
2023-09-09 15:36:29.997 
Epoch 410/1000 
	 loss: 385.9164, MinusLogProbMetric: 385.9164, val_loss: 396.1265, val_MinusLogProbMetric: 396.1265

Epoch 410: val_loss did not improve from 395.27536
196/196 - 17s - loss: 385.9164 - MinusLogProbMetric: 385.9164 - val_loss: 396.1265 - val_MinusLogProbMetric: 396.1265 - lr: 4.1667e-05 - 17s/epoch - 89ms/step
Epoch 411/1000
2023-09-09 15:36:48.093 
Epoch 411/1000 
	 loss: 385.9256, MinusLogProbMetric: 385.9256, val_loss: 396.1806, val_MinusLogProbMetric: 396.1806

Epoch 411: val_loss did not improve from 395.27536
196/196 - 18s - loss: 385.9256 - MinusLogProbMetric: 385.9256 - val_loss: 396.1806 - val_MinusLogProbMetric: 396.1806 - lr: 4.1667e-05 - 18s/epoch - 92ms/step
Epoch 412/1000
2023-09-09 15:37:07.950 
Epoch 412/1000 
	 loss: 385.9621, MinusLogProbMetric: 385.9621, val_loss: 396.0325, val_MinusLogProbMetric: 396.0325

Epoch 412: val_loss did not improve from 395.27536
196/196 - 20s - loss: 385.9621 - MinusLogProbMetric: 385.9621 - val_loss: 396.0325 - val_MinusLogProbMetric: 396.0325 - lr: 4.1667e-05 - 20s/epoch - 101ms/step
Epoch 413/1000
2023-09-09 15:37:27.490 
Epoch 413/1000 
	 loss: 386.0919, MinusLogProbMetric: 386.0919, val_loss: 396.8210, val_MinusLogProbMetric: 396.8210

Epoch 413: val_loss did not improve from 395.27536
196/196 - 20s - loss: 386.0919 - MinusLogProbMetric: 386.0919 - val_loss: 396.8210 - val_MinusLogProbMetric: 396.8210 - lr: 4.1667e-05 - 20s/epoch - 100ms/step
Epoch 414/1000
2023-09-09 15:37:46.317 
Epoch 414/1000 
	 loss: 386.1239, MinusLogProbMetric: 386.1239, val_loss: 396.9645, val_MinusLogProbMetric: 396.9645

Epoch 414: val_loss did not improve from 395.27536
196/196 - 19s - loss: 386.1239 - MinusLogProbMetric: 386.1239 - val_loss: 396.9645 - val_MinusLogProbMetric: 396.9645 - lr: 4.1667e-05 - 19s/epoch - 96ms/step
Epoch 415/1000
2023-09-09 15:38:03.260 
Epoch 415/1000 
	 loss: 386.0633, MinusLogProbMetric: 386.0633, val_loss: 396.2880, val_MinusLogProbMetric: 396.2880

Epoch 415: val_loss did not improve from 395.27536
196/196 - 17s - loss: 386.0633 - MinusLogProbMetric: 386.0633 - val_loss: 396.2880 - val_MinusLogProbMetric: 396.2880 - lr: 4.1667e-05 - 17s/epoch - 86ms/step
Epoch 416/1000
2023-09-09 15:38:21.011 
Epoch 416/1000 
	 loss: 385.9358, MinusLogProbMetric: 385.9358, val_loss: 396.2095, val_MinusLogProbMetric: 396.2095

Epoch 416: val_loss did not improve from 395.27536
196/196 - 18s - loss: 385.9358 - MinusLogProbMetric: 385.9358 - val_loss: 396.2095 - val_MinusLogProbMetric: 396.2095 - lr: 4.1667e-05 - 18s/epoch - 91ms/step
Epoch 417/1000
2023-09-09 15:38:38.002 
Epoch 417/1000 
	 loss: 385.9361, MinusLogProbMetric: 385.9361, val_loss: 396.5403, val_MinusLogProbMetric: 396.5403

Epoch 417: val_loss did not improve from 395.27536
196/196 - 17s - loss: 385.9361 - MinusLogProbMetric: 385.9361 - val_loss: 396.5403 - val_MinusLogProbMetric: 396.5403 - lr: 4.1667e-05 - 17s/epoch - 87ms/step
Epoch 418/1000
2023-09-09 15:38:55.409 
Epoch 418/1000 
	 loss: 385.9159, MinusLogProbMetric: 385.9159, val_loss: 396.4240, val_MinusLogProbMetric: 396.4240

Epoch 418: val_loss did not improve from 395.27536
196/196 - 17s - loss: 385.9159 - MinusLogProbMetric: 385.9159 - val_loss: 396.4240 - val_MinusLogProbMetric: 396.4240 - lr: 4.1667e-05 - 17s/epoch - 89ms/step
Epoch 419/1000
2023-09-09 15:39:13.454 
Epoch 419/1000 
	 loss: 386.1062, MinusLogProbMetric: 386.1062, val_loss: 396.2619, val_MinusLogProbMetric: 396.2619

Epoch 419: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.1062 - MinusLogProbMetric: 386.1062 - val_loss: 396.2619 - val_MinusLogProbMetric: 396.2619 - lr: 4.1667e-05 - 18s/epoch - 92ms/step
Epoch 420/1000
2023-09-09 15:39:34.313 
Epoch 420/1000 
	 loss: 386.1098, MinusLogProbMetric: 386.1098, val_loss: 396.4090, val_MinusLogProbMetric: 396.4090

Epoch 420: val_loss did not improve from 395.27536
196/196 - 21s - loss: 386.1098 - MinusLogProbMetric: 386.1098 - val_loss: 396.4090 - val_MinusLogProbMetric: 396.4090 - lr: 4.1667e-05 - 21s/epoch - 106ms/step
Epoch 421/1000
2023-09-09 15:39:55.587 
Epoch 421/1000 
	 loss: 386.1429, MinusLogProbMetric: 386.1429, val_loss: 396.2022, val_MinusLogProbMetric: 396.2022

Epoch 421: val_loss did not improve from 395.27536
196/196 - 21s - loss: 386.1429 - MinusLogProbMetric: 386.1429 - val_loss: 396.2022 - val_MinusLogProbMetric: 396.2022 - lr: 4.1667e-05 - 21s/epoch - 108ms/step
Epoch 422/1000
2023-09-09 15:40:13.765 
Epoch 422/1000 
	 loss: 386.0305, MinusLogProbMetric: 386.0305, val_loss: 396.2854, val_MinusLogProbMetric: 396.2854

Epoch 422: val_loss did not improve from 395.27536
196/196 - 18s - loss: 386.0305 - MinusLogProbMetric: 386.0305 - val_loss: 396.2854 - val_MinusLogProbMetric: 396.2854 - lr: 4.1667e-05 - 18s/epoch - 93ms/step
Epoch 423/1000
2023-09-09 15:40:30.457 
Epoch 423/1000 
	 loss: 385.9981, MinusLogProbMetric: 385.9981, val_loss: 396.2779, val_MinusLogProbMetric: 396.2779

Epoch 423: val_loss did not improve from 395.27536
196/196 - 17s - loss: 385.9981 - MinusLogProbMetric: 385.9981 - val_loss: 396.2779 - val_MinusLogProbMetric: 396.2779 - lr: 4.1667e-05 - 17s/epoch - 85ms/step
Epoch 424/1000
2023-09-09 15:40:46.365 
Epoch 424/1000 
	 loss: 386.0348, MinusLogProbMetric: 386.0348, val_loss: 396.4416, val_MinusLogProbMetric: 396.4416

Epoch 424: val_loss did not improve from 395.27536
196/196 - 16s - loss: 386.0348 - MinusLogProbMetric: 386.0348 - val_loss: 396.4416 - val_MinusLogProbMetric: 396.4416 - lr: 4.1667e-05 - 16s/epoch - 81ms/step
Epoch 425/1000
2023-09-09 15:41:02.180 
Epoch 425/1000 
	 loss: 385.9022, MinusLogProbMetric: 385.9022, val_loss: 396.2898, val_MinusLogProbMetric: 396.2898

Epoch 425: val_loss did not improve from 395.27536
196/196 - 16s - loss: 385.9022 - MinusLogProbMetric: 385.9022 - val_loss: 396.2898 - val_MinusLogProbMetric: 396.2898 - lr: 4.1667e-05 - 16s/epoch - 81ms/step
Epoch 426/1000
2023-09-09 15:41:17.477 
Epoch 426/1000 
	 loss: 386.1536, MinusLogProbMetric: 386.1536, val_loss: 396.7068, val_MinusLogProbMetric: 396.7068

Epoch 426: val_loss did not improve from 395.27536
196/196 - 15s - loss: 386.1536 - MinusLogProbMetric: 386.1536 - val_loss: 396.7068 - val_MinusLogProbMetric: 396.7068 - lr: 4.1667e-05 - 15s/epoch - 78ms/step
Epoch 427/1000
2023-09-09 15:41:33.345 
Epoch 427/1000 
	 loss: 386.0759, MinusLogProbMetric: 386.0759, val_loss: 396.7820, val_MinusLogProbMetric: 396.7820

Epoch 427: val_loss did not improve from 395.27536
196/196 - 16s - loss: 386.0759 - MinusLogProbMetric: 386.0759 - val_loss: 396.7820 - val_MinusLogProbMetric: 396.7820 - lr: 4.1667e-05 - 16s/epoch - 81ms/step
Epoch 428/1000
2023-09-09 15:41:48.923 
Epoch 428/1000 
	 loss: 385.8541, MinusLogProbMetric: 385.8541, val_loss: 396.6390, val_MinusLogProbMetric: 396.6390

Epoch 428: val_loss did not improve from 395.27536
196/196 - 16s - loss: 385.8541 - MinusLogProbMetric: 385.8541 - val_loss: 396.6390 - val_MinusLogProbMetric: 396.6390 - lr: 4.1667e-05 - 16s/epoch - 79ms/step
Epoch 429/1000
2023-09-09 15:42:04.125 
Epoch 429/1000 
	 loss: 385.9493, MinusLogProbMetric: 385.9493, val_loss: 396.5116, val_MinusLogProbMetric: 396.5116

Epoch 429: val_loss did not improve from 395.27536
Restoring model weights from the end of the best epoch: 329.
196/196 - 15s - loss: 385.9493 - MinusLogProbMetric: 385.9493 - val_loss: 396.5116 - val_MinusLogProbMetric: 396.5116 - lr: 4.1667e-05 - 15s/epoch - 79ms/step
Epoch 429: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 2043.943309757975 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 24438.170914986986 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 22419.500105719082 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
SWD metric calculation completed in 23832.186559649068 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
FN metric calculation completed in 27751.619412965956 seconds.
Training succeeded with seed 0.
Model trained in 7739.20 s.

===========
Computing predictions
===========

Computing metrics...
===========
Failed on GPU, re-trying on CPU
===========

Computing metrics...
Metrics computed in 98702.56 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 470, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 100998.38 s.
===========
Run 324/360 done in 109013.75 s.
===========

Directory ../../results/MAFN_new/run_325/ already exists.
Skipping it.
===========
Run 325/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_326/ already exists.
Skipping it.
===========
Run 326/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_327/ already exists.
Skipping it.
===========
Run 327/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_328/ already exists.
Skipping it.
===========
Run 328/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_329/ already exists.
Skipping it.
===========
Run 329/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_330/ already exists.
Skipping it.
===========
Run 330/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_331/ already exists.
Skipping it.
===========
Run 331/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_332/ already exists.
Skipping it.
===========
Run 332/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_333/ already exists.
Skipping it.
===========
Run 333/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_334/ already exists.
Skipping it.
===========
Run 334/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_335/ already exists.
Skipping it.
===========
Run 335/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_336/ already exists.
Skipping it.
===========
Run 336/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_337/ already exists.
Skipping it.
===========
Run 337/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_338/ already exists.
Skipping it.
===========
Run 338/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_339/ already exists.
Skipping it.
===========
Run 339/360 already exists. Skipping it.
===========

===========
Generating train data for run 340.
===========
===========
Run 340/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 341.
===========
===========
Run 341/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 342.
===========
===========
Run 342/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 343.
===========
===========
Run 343/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 344.
===========
===========
Run 344/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 345.
===========
===========
Run 345/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 346.
===========
===========
Run 346/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 347.
===========
===========
Run 347/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 348.
===========
===========
Run 348/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 349.
===========
===========
Run 349/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 350.
===========
===========
Run 350/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 351.
===========
===========
Run 351/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 352.
===========
===========
Run 352/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 353.
===========
===========
Run 353/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 354.
===========
===========
Run 354/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 355.
===========
===========
Run 355/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 356.
===========
===========
Run 356/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 357.
===========
===========
Run 357/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 358.
===========
===========
Run 358/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 359.
===========
===========
Run 359/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__StatelessRandomNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomNormalV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 320, Func.Name : _sample_n, Message : samples.append(self.components[c].sample(n, seed=seeds[c + 1]))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py , Line : 333, Func.Name : _call_sample_n, Message : x = self._maybe_broadcast_distribution_batch_shape().sample(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/sample.py , Line : 224, Func.Name : _sample_n, Message : x = self.distribution.sample(ps.concat([[n], sample_shape], axis=0),', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/normal.py , Line : 179, Func.Name : _sample_n, Message : sampled = samplers.normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/internal/samplers.py , Line : 271, Func.Name : normal, Message : return tf.random.stateless_normal(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

===========
Generating train data for run 360.
===========
===========
Run 360/360 failed.
Exception type: ResourceExhaustedError
Exception message: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[100000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 635, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py , Line : 225, Func.Name : train_function, Message : X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/../../../code/Utils.py , Line : 417, Func.Name : generate_train_data, Message : X_data_train: tf.Tensor = targ_dist.sample(nsamples_train, seed=seed_train).numpy()', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1205, Func.Name : sample, Message : return self._call_sample_n(sample_shape, seed, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py , Line : 1182, Func.Name : _call_sample_n, Message : samples = self._sample_n(', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow_probability/python/distributions/mixture.py , Line : 339, Func.Name : _sample_n, Message : return tf.gather(x, cat_samples, axis=stack_axis,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py , Line : 153, Func.Name : error_handler, Message : raise e.with_traceback(filtered_tb) from None', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py , Line : 7262, Func.Name : raise_from_not_ok_status, Message : raise core._status_to_exception(e) from None  # pylint: disable=protected-access']
===========

Traceback (most recent call last):
  File "/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py", line 741, in <module>
    results_frame: pd.DataFrame = pd.DataFrame(dict_copy)
  File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/pandas/core/frame.py", line 709, in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
  File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/pandas/core/internals/construction.py", line 481, in dict_to_mgr
    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)
  File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/pandas/core/internals/construction.py", line 115, in arrays_to_mgr
    index = _extract_index(arrays)
  File "/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/pandas/core/internals/construction.py", line 655, in _extract_index
    raise ValueError("All arrays must be of the same length")
ValueError: All arrays must be of the same length
