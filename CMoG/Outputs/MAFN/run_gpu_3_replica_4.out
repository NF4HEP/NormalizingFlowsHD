2023-09-09 09:35:56.455698: Importing os...
2023-09-09 09:35:56.455764: Importing sys...
2023-09-09 09:35:56.455778: Importing and initializing argparse...
Visible devices: [3]
2023-09-09 09:35:56.472049: Importing timer from timeit...
2023-09-09 09:35:56.472614: Setting env variables for tf import (only device [3] will be available)...
2023-09-09 09:35:56.472657: Importing numpy...
2023-09-09 09:35:56.620993: Importing pandas...
2023-09-09 09:35:56.808884: Importing shutil...
2023-09-09 09:35:56.808913: Importing subprocess...
2023-09-09 09:35:56.808921: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-09 09:35:58.929587: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-09 09:35:59.314394: Importing textwrap...
2023-09-09 09:35:59.314425: Importing timeit...
2023-09-09 09:35:59.314435: Importing traceback...
2023-09-09 09:35:59.314441: Importing typing...
2023-09-09 09:35:59.314452: Setting tf configs...
2023-09-09 09:35:59.483654: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-09 09:36:01.061966: All modues imported successfully.
Directory ../../results/MAFN_new/ already exists.
Directory ../../results/MAFN_new/run_1/ already exists.
Skipping it.
===========
Run 1/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_2/ already exists.
Skipping it.
===========
Run 2/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_3/ already exists.
Skipping it.
===========
Run 3/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_4/ already exists.
Skipping it.
===========
Run 4/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_5/ already exists.
Skipping it.
===========
Run 5/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_6/ already exists.
Skipping it.
===========
Run 6/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_7/ already exists.
Skipping it.
===========
Run 7/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_8/ already exists.
Skipping it.
===========
Run 8/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_9/ already exists.
Skipping it.
===========
Run 9/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_10/ already exists.
Skipping it.
===========
Run 10/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_11/ already exists.
Skipping it.
===========
Run 11/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_12/ already exists.
Skipping it.
===========
Run 12/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_13/ already exists.
Skipping it.
===========
Run 13/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_14/ already exists.
Skipping it.
===========
Run 14/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_15/ already exists.
Skipping it.
===========
Run 15/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_16/ already exists.
Skipping it.
===========
Run 16/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_17/ already exists.
Skipping it.
===========
Run 17/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_18/ already exists.
Skipping it.
===========
Run 18/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_19/ already exists.
Skipping it.
===========
Run 19/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_20/ already exists.
Skipping it.
===========
Run 20/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_21/ already exists.
Skipping it.
===========
Run 21/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_22/ already exists.
Skipping it.
===========
Run 22/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_23/ already exists.
Skipping it.
===========
Run 23/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_24/ already exists.
Skipping it.
===========
Run 24/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_25/ already exists.
Skipping it.
===========
Run 25/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_26/ already exists.
Skipping it.
===========
Run 26/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_27/ already exists.
Skipping it.
===========
Run 27/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_28/ already exists.
Skipping it.
===========
Run 28/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_29/ already exists.
Skipping it.
===========
Run 29/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_30/ already exists.
Skipping it.
===========
Run 30/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_31/ already exists.
Skipping it.
===========
Run 31/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_32/ already exists.
Skipping it.
===========
Run 32/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_33/ already exists.
Skipping it.
===========
Run 33/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_34/ already exists.
Skipping it.
===========
Run 34/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_35/ already exists.
Skipping it.
===========
Run 35/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_36/ already exists.
Skipping it.
===========
Run 36/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_37/ already exists.
Skipping it.
===========
Run 37/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_38/ already exists.
Skipping it.
===========
Run 38/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_39/ already exists.
Skipping it.
===========
Run 39/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_40/ already exists.
Skipping it.
===========
Run 40/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_41/ already exists.
Skipping it.
===========
Run 41/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_42/ already exists.
Skipping it.
===========
Run 42/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_43/ already exists.
Skipping it.
===========
Run 43/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_44/ already exists.
Skipping it.
===========
Run 44/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_45/ already exists.
Skipping it.
===========
Run 45/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_46/ already exists.
Skipping it.
===========
Run 46/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_47/ already exists.
Skipping it.
===========
Run 47/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_48/ already exists.
Skipping it.
===========
Run 48/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_49/ already exists.
Skipping it.
===========
Run 49/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_50/ already exists.
Skipping it.
===========
Run 50/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_51/ already exists.
Skipping it.
===========
Run 51/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_52/ already exists.
Skipping it.
===========
Run 52/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_53/ already exists.
Skipping it.
===========
Run 53/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_54/ already exists.
Skipping it.
===========
Run 54/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_55/ already exists.
Skipping it.
===========
Run 55/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_56/ already exists.
Skipping it.
===========
Run 56/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_57/ already exists.
Skipping it.
===========
Run 57/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_58/ already exists.
Skipping it.
===========
Run 58/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_59/ already exists.
Skipping it.
===========
Run 59/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_60/ already exists.
Skipping it.
===========
Run 60/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_61/ already exists.
Skipping it.
===========
Run 61/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_62/ already exists.
Skipping it.
===========
Run 62/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_63/ already exists.
Skipping it.
===========
Run 63/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_64/ already exists.
Skipping it.
===========
Run 64/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_65/ already exists.
Skipping it.
===========
Run 65/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_66/ already exists.
Skipping it.
===========
Run 66/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_67/ already exists.
Skipping it.
===========
Run 67/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_68/ already exists.
Skipping it.
===========
Run 68/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_69/ already exists.
Skipping it.
===========
Run 69/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_70/ already exists.
Skipping it.
===========
Run 70/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_71/ already exists.
Skipping it.
===========
Run 71/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_72/ already exists.
Skipping it.
===========
Run 72/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_73/ already exists.
Skipping it.
===========
Run 73/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_74/ already exists.
Skipping it.
===========
Run 74/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_75/ already exists.
Skipping it.
===========
Run 75/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_76/ already exists.
Skipping it.
===========
Run 76/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_77/ already exists.
Skipping it.
===========
Run 77/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_78/ already exists.
Skipping it.
===========
Run 78/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_79/ already exists.
Skipping it.
===========
Run 79/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_80/ already exists.
Skipping it.
===========
Run 80/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_81/ already exists.
Skipping it.
===========
Run 81/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_82/ already exists.
Skipping it.
===========
Run 82/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_83/ already exists.
Skipping it.
===========
Run 83/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_84/ already exists.
Skipping it.
===========
Run 84/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_85/ already exists.
Skipping it.
===========
Run 85/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_86/ already exists.
Skipping it.
===========
Run 86/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_87/ already exists.
Skipping it.
===========
Run 87/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_88/ already exists.
Skipping it.
===========
Run 88/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_89/ already exists.
Skipping it.
===========
Run 89/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_90/ already exists.
Skipping it.
===========
Run 90/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_91/ already exists.
Skipping it.
===========
Run 91/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_92/ already exists.
Skipping it.
===========
Run 92/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_93/ already exists.
Skipping it.
===========
Run 93/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_94/ already exists.
Skipping it.
===========
Run 94/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_95/ already exists.
Skipping it.
===========
Run 95/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_96/ already exists.
Skipping it.
===========
Run 96/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_97/ already exists.
Skipping it.
===========
Run 97/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_98/ already exists.
Skipping it.
===========
Run 98/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_99/ already exists.
Skipping it.
===========
Run 99/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_100/ already exists.
Skipping it.
===========
Run 100/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_101/ already exists.
Skipping it.
===========
Run 101/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_102/ already exists.
Skipping it.
===========
Run 102/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_103/ already exists.
Skipping it.
===========
Run 103/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_104/ already exists.
Skipping it.
===========
Run 104/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_105/ already exists.
Skipping it.
===========
Run 105/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_106/ already exists.
Skipping it.
===========
Run 106/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_107/ already exists.
Skipping it.
===========
Run 107/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_108/ already exists.
Skipping it.
===========
Run 108/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_109/ already exists.
Skipping it.
===========
Run 109/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_110/ already exists.
Skipping it.
===========
Run 110/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_111/ already exists.
Skipping it.
===========
Run 111/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_112/ already exists.
Skipping it.
===========
Run 112/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_113/ already exists.
Skipping it.
===========
Run 113/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_114/ already exists.
Skipping it.
===========
Run 114/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_115/ already exists.
Skipping it.
===========
Run 115/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_116/ already exists.
Skipping it.
===========
Run 116/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_117/ already exists.
Skipping it.
===========
Run 117/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_118/ already exists.
Skipping it.
===========
Run 118/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_119/ already exists.
Skipping it.
===========
Run 119/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_120/ already exists.
Skipping it.
===========
Run 120/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_121/ already exists.
Skipping it.
===========
Run 121/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_122/ already exists.
Skipping it.
===========
Run 122/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_123/ already exists.
Skipping it.
===========
Run 123/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_124/ already exists.
Skipping it.
===========
Run 124/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_125/ already exists.
Skipping it.
===========
Run 125/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_126/ already exists.
Skipping it.
===========
Run 126/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_127/ already exists.
Skipping it.
===========
Run 127/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_128/ already exists.
Skipping it.
===========
Run 128/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_129/ already exists.
Skipping it.
===========
Run 129/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_130/ already exists.
Skipping it.
===========
Run 130/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_131/ already exists.
Skipping it.
===========
Run 131/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_132/ already exists.
Skipping it.
===========
Run 132/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_133/ already exists.
Skipping it.
===========
Run 133/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_134/ already exists.
Skipping it.
===========
Run 134/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_135/ already exists.
Skipping it.
===========
Run 135/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_136/ already exists.
Skipping it.
===========
Run 136/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_137/ already exists.
Skipping it.
===========
Run 137/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_138/ already exists.
Skipping it.
===========
Run 138/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_139/ already exists.
Skipping it.
===========
Run 139/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_140/ already exists.
Skipping it.
===========
Run 140/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_141/ already exists.
Skipping it.
===========
Run 141/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_142/ already exists.
Skipping it.
===========
Run 142/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_143/ already exists.
Skipping it.
===========
Run 143/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_144/ already exists.
Skipping it.
===========
Run 144/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_145/ already exists.
Skipping it.
===========
Run 145/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_146/ already exists.
Skipping it.
===========
Run 146/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_147/ already exists.
Skipping it.
===========
Run 147/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_148/ already exists.
Skipping it.
===========
Run 148/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_149/ already exists.
Skipping it.
===========
Run 149/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_150/ already exists.
Skipping it.
===========
Run 150/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_151/ already exists.
Skipping it.
===========
Run 151/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_152/ already exists.
Skipping it.
===========
Run 152/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_153/ already exists.
Skipping it.
===========
Run 153/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_154/ already exists.
Skipping it.
===========
Run 154/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_155/ already exists.
Skipping it.
===========
Run 155/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_156/ already exists.
Skipping it.
===========
Run 156/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_157/ already exists.
Skipping it.
===========
Run 157/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_158/ already exists.
Skipping it.
===========
Run 158/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_159/ already exists.
Skipping it.
===========
Run 159/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_160/ already exists.
Skipping it.
===========
Run 160/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_161/ already exists.
Skipping it.
===========
Run 161/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_162/ already exists.
Skipping it.
===========
Run 162/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_163/ already exists.
Skipping it.
===========
Run 163/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_164/ already exists.
Skipping it.
===========
Run 164/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_165/ already exists.
Skipping it.
===========
Run 165/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_166/ already exists.
Skipping it.
===========
Run 166/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_167/ already exists.
Skipping it.
===========
Run 167/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_168/ already exists.
Skipping it.
===========
Run 168/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_169/ already exists.
Skipping it.
===========
Run 169/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_170/ already exists.
Skipping it.
===========
Run 170/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_171/ already exists.
Skipping it.
===========
Run 171/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_172/ already exists.
Skipping it.
===========
Run 172/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_173/ already exists.
Skipping it.
===========
Run 173/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_174/ already exists.
Skipping it.
===========
Run 174/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_175/ already exists.
Skipping it.
===========
Run 175/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_176/ already exists.
Skipping it.
===========
Run 176/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_177/ already exists.
Skipping it.
===========
Run 177/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_178/ already exists.
Skipping it.
===========
Run 178/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_179/ already exists.
Skipping it.
===========
Run 179/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_180/ already exists.
Skipping it.
===========
Run 180/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_181/ already exists.
Skipping it.
===========
Run 181/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_182/ already exists.
Skipping it.
===========
Run 182/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_183/ already exists.
Skipping it.
===========
Run 183/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_184/ already exists.
Skipping it.
===========
Run 184/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_185/ already exists.
Skipping it.
===========
Run 185/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_186/ already exists.
Skipping it.
===========
Run 186/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_187/ already exists.
Skipping it.
===========
Run 187/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_188/ already exists.
Skipping it.
===========
Run 188/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_189/ already exists.
Skipping it.
===========
Run 189/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_190/ already exists.
Skipping it.
===========
Run 190/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_191/ already exists.
Skipping it.
===========
Run 191/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_192/ already exists.
Skipping it.
===========
Run 192/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_193/ already exists.
Skipping it.
===========
Run 193/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_194/ already exists.
Skipping it.
===========
Run 194/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_195/ already exists.
Skipping it.
===========
Run 195/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_196/ already exists.
Skipping it.
===========
Run 196/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_197/ already exists.
Skipping it.
===========
Run 197/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_198/ already exists.
Skipping it.
===========
Run 198/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_199/ already exists.
Skipping it.
===========
Run 199/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_200/ already exists.
Skipping it.
===========
Run 200/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_201/ already exists.
Skipping it.
===========
Run 201/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_202/ already exists.
Skipping it.
===========
Run 202/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_203/ already exists.
Skipping it.
===========
Run 203/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_204/ already exists.
Skipping it.
===========
Run 204/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_205/ already exists.
Skipping it.
===========
Run 205/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_206/ already exists.
Skipping it.
===========
Run 206/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_207/ already exists.
Skipping it.
===========
Run 207/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_208/ already exists.
Skipping it.
===========
Run 208/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_209/ already exists.
Skipping it.
===========
Run 209/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_210/ already exists.
Skipping it.
===========
Run 210/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_211/ already exists.
Skipping it.
===========
Run 211/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_212/ already exists.
Skipping it.
===========
Run 212/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_213/ already exists.
Skipping it.
===========
Run 213/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_214/ already exists.
Skipping it.
===========
Run 214/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_215/ already exists.
Skipping it.
===========
Run 215/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_216/ already exists.
Skipping it.
===========
Run 216/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_217/ already exists.
Skipping it.
===========
Run 217/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_218/ already exists.
Skipping it.
===========
Run 218/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_219/ already exists.
Skipping it.
===========
Run 219/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_220/ already exists.
Skipping it.
===========
Run 220/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_221/ already exists.
Skipping it.
===========
Run 221/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_222/ already exists.
Skipping it.
===========
Run 222/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_223/ already exists.
Skipping it.
===========
Run 223/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_224/ already exists.
Skipping it.
===========
Run 224/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_225/ already exists.
Skipping it.
===========
Run 225/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_226/ already exists.
Skipping it.
===========
Run 226/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_227/ already exists.
Skipping it.
===========
Run 227/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_228/ already exists.
Skipping it.
===========
Run 228/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_229/ already exists.
Skipping it.
===========
Run 229/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_230/ already exists.
Skipping it.
===========
Run 230/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_231/ already exists.
Skipping it.
===========
Run 231/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_232/ already exists.
Skipping it.
===========
Run 232/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_233/ already exists.
Skipping it.
===========
Run 233/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_234/ already exists.
Skipping it.
===========
Run 234/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_235/ already exists.
Skipping it.
===========
Run 235/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_236/ already exists.
Skipping it.
===========
Run 236/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_237/ already exists.
Skipping it.
===========
Run 237/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_238/ already exists.
Skipping it.
===========
Run 238/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_239/ already exists.
Skipping it.
===========
Run 239/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_240/ already exists.
Skipping it.
===========
Run 240/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_241/ already exists.
Skipping it.
===========
Run 241/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_242/ already exists.
Skipping it.
===========
Run 242/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_243/ already exists.
Skipping it.
===========
Run 243/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_244/ already exists.
Skipping it.
===========
Run 244/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_245/ already exists.
Skipping it.
===========
Run 245/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_246/ already exists.
Skipping it.
===========
Run 246/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_247/ already exists.
Skipping it.
===========
Run 247/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_248/ already exists.
Skipping it.
===========
Run 248/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_249/ already exists.
Skipping it.
===========
Run 249/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_250/ already exists.
Skipping it.
===========
Run 250/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_251/ already exists.
Skipping it.
===========
Run 251/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_252/ already exists.
Skipping it.
===========
Run 252/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_253/ already exists.
Skipping it.
===========
Run 253/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_254/ already exists.
Skipping it.
===========
Run 254/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_255/ already exists.
Skipping it.
===========
Run 255/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_256/ already exists.
Skipping it.
===========
Run 256/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_257/ already exists.
Skipping it.
===========
Run 257/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_258/ already exists.
Skipping it.
===========
Run 258/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_259/ already exists.
Skipping it.
===========
Run 259/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_260/ already exists.
Skipping it.
===========
Run 260/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_261/ already exists.
Skipping it.
===========
Run 261/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_262/ already exists.
Skipping it.
===========
Run 262/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_263/ already exists.
Skipping it.
===========
Run 263/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_264/ already exists.
Skipping it.
===========
Run 264/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_265/ already exists.
Skipping it.
===========
Run 265/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_266/ already exists.
Skipping it.
===========
Run 266/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_267/ already exists.
Skipping it.
===========
Run 267/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_268/ already exists.
Skipping it.
===========
Run 268/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_269/ already exists.
Skipping it.
===========
Run 269/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_270/ already exists.
Skipping it.
===========
Run 270/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_271/ already exists.
Skipping it.
===========
Run 271/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_272/ already exists.
Skipping it.
===========
Run 272/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_273/ already exists.
Skipping it.
===========
Run 273/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_274/ already exists.
Skipping it.
===========
Run 274/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_275/ already exists.
Skipping it.
===========
Run 275/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_276/ already exists.
Skipping it.
===========
Run 276/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_277/ already exists.
Skipping it.
===========
Run 277/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_278/ already exists.
Skipping it.
===========
Run 278/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_279/ already exists.
Skipping it.
===========
Run 279/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_280/ already exists.
Skipping it.
===========
Run 280/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_281/ already exists.
Skipping it.
===========
Run 281/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_282/ already exists.
Skipping it.
===========
Run 282/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_283/ already exists.
Skipping it.
===========
Run 283/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_284/ already exists.
Skipping it.
===========
Run 284/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_285/ already exists.
Skipping it.
===========
Run 285/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_286/ already exists.
Skipping it.
===========
Run 286/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_287/ already exists.
Skipping it.
===========
Run 287/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_288/ already exists.
Skipping it.
===========
Run 288/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_289/ already exists.
Skipping it.
===========
Run 289/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_290/ already exists.
Skipping it.
===========
Run 290/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_291/ already exists.
Skipping it.
===========
Run 291/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_292/ already exists.
Skipping it.
===========
Run 292/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_293/ already exists.
Skipping it.
===========
Run 293/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_294/ already exists.
Skipping it.
===========
Run 294/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_295/ already exists.
Skipping it.
===========
Run 295/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_296/ already exists.
Skipping it.
===========
Run 296/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_297/ already exists.
Skipping it.
===========
Run 297/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_298/ already exists.
Skipping it.
===========
Run 298/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_299/ already exists.
Skipping it.
===========
Run 299/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_300/ already exists.
Skipping it.
===========
Run 300/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_301/ already exists.
Skipping it.
===========
Run 301/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_302/ already exists.
Skipping it.
===========
Run 302/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_303/ already exists.
Skipping it.
===========
Run 303/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_304/ already exists.
Skipping it.
===========
Run 304/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_305/ already exists.
Skipping it.
===========
Run 305/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_306/ already exists.
Skipping it.
===========
Run 306/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_307/ already exists.
Skipping it.
===========
Run 307/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_308/ already exists.
Skipping it.
===========
Run 308/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_309/ already exists.
Skipping it.
===========
Run 309/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_310/ already exists.
Skipping it.
===========
Run 310/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_311/ already exists.
Skipping it.
===========
Run 311/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_312/ already exists.
Skipping it.
===========
Run 312/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_313/ already exists.
Skipping it.
===========
Run 313/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_314/ already exists.
Skipping it.
===========
Run 314/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_315/ already exists.
Skipping it.
===========
Run 315/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_316/ already exists.
Skipping it.
===========
Run 316/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_317/ already exists.
Skipping it.
===========
Run 317/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_318/ already exists.
Skipping it.
===========
Run 318/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_319/ already exists.
Skipping it.
===========
Run 319/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_320/ already exists.
Skipping it.
===========
Run 320/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_321/ already exists.
Skipping it.
===========
Run 321/360 already exists. Skipping it.
===========

Directory ../../results/MAFN_new/run_322/ already exists.
Skipping it.
===========
Run 322/360 already exists. Skipping it.
===========

===========
Generating train data for run 323.
===========
Train data generated in 0.93 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_323
self.data_kwargs: {'seed': 0}
self.x_data: [[8.210789   4.3589015  5.1949253  ... 2.1186912  8.28666    6.44192   ]
 [5.776202   0.0413513  4.6274076  ... 4.7782993  6.688914   5.570342  ]
 [5.7263374  6.2561994  5.7377024  ... 8.820506   0.58727944 6.515802  ]
 ...
 [5.7884197  0.56533414 4.7867274  ... 4.878848   6.0022035  4.9900055 ]
 [5.9892163  0.222366   4.8566475  ... 4.5662537  6.433208   4.1020565 ]
 [7.8653784  4.6189337  5.157348   ... 2.6189103  7.542449   6.328924  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1000)]            0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  4191520   
 r)                                                              
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f0de7b34c10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0de7b36830>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0de7b36830>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0de7b115a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0de00b5240>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0de00b59c0>, <keras.callbacks.ModelCheckpoint object at 0x7f0de00b5b10>, <keras.callbacks.EarlyStopping object at 0x7f0de00b5d20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0de00b5d50>, <keras.callbacks.TerminateOnNaN object at 0x7f0de00b5a80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/MAFN_new/run_323/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 323/360 with hyperparameters:
timestamp = 2023-09-09 09:36:06.317959
ndims = 1000
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 8.21078873e+00  4.35890150e+00  5.19492531e+00  2.84795880e+00
  6.23694658e+00  2.48332572e+00  6.01452780e+00  1.49544990e+00
  1.56305087e+00  4.33542109e+00  3.63919592e+00  2.33469009e+00
  7.12970674e-01  4.46248204e-01  1.55621916e-01  3.05101776e+00
  4.20721769e+00  7.49161625e+00  3.96869850e+00  9.05481625e+00
  4.47121114e-01  2.05655861e+00  3.52404070e+00  5.90453529e+00
  8.73787975e+00  3.53096032e+00  2.76253247e+00  6.50554955e-01
  8.03491688e+00  3.65929008e+00  4.93096781e+00  1.00386086e+01
  5.29845119e-01  1.22730446e+00  7.25914860e+00  7.58729315e+00
  7.71210337e+00  5.14168453e+00  9.58227444e+00  4.79759741e+00
  6.05360174e+00 -3.22920084e-03  4.30121565e+00  7.28324509e+00
  2.30840397e+00  9.72805214e+00  7.01551247e+00  3.15825868e+00
  7.77309942e+00  1.27091324e+00  9.88113785e+00  5.76226175e-01
  9.93493366e+00  6.19022250e-01  7.72215176e+00  1.45727849e+00
  2.63241100e+00  7.56394577e+00  4.26764488e+00  1.30306756e+00
  4.47696209e+00  4.10884666e+00  6.25260830e+00  7.17153311e+00
  7.79660606e+00  4.86149597e+00  4.01030636e+00  1.48931491e+00
  8.25993347e+00  6.99169493e+00  4.15396929e+00  5.40362930e+00
  6.69921494e+00  1.80113685e+00 -2.28963286e-01  3.53704309e+00
  5.97519684e+00  5.03495884e+00  5.96093559e+00  8.27530861e+00
  5.11110401e+00  1.71916723e-01  1.00485516e+01  5.39971399e+00
  6.97965956e+00  6.67698717e+00  9.53224850e+00  2.60424829e+00
  6.92656088e+00  2.48836160e+00  1.50950837e+00  3.11806774e+00
  3.62086797e+00  9.48242664e+00  2.35715413e+00  1.47683930e+00
  6.57032585e+00  2.31288910e+00  5.07415724e+00  8.22896194e+00
  2.67307043e+00  4.70531940e+00  5.81045723e+00  3.97613907e+00
  1.00836468e+01  6.80917358e+00  7.91102648e+00  9.83973694e+00
  3.97975469e+00  2.81087971e+00  6.61194921e-01  2.09042239e+00
  1.80242181e+00  2.45311332e+00  7.75794411e+00  5.35263777e+00
  6.36038160e+00  8.03104973e+00  2.23086929e+00  3.26186609e+00
  7.10611820e+00  4.36860800e+00  5.38674593e-01  5.03258133e+00
  8.79059315e+00  4.12663126e+00  2.96597481e+00  9.92143154e+00
  6.63155603e+00  2.29087067e+00  9.67418194e-01  8.54019165e+00
  7.32748413e+00  9.67831326e+00  3.08930302e+00  1.43350685e+00
  7.23957300e+00  2.06572795e+00  1.31994998e+00  2.38906145e+00
  8.15470123e+00  4.92610788e+00  4.60461289e-01  4.69264269e+00
  7.11180496e+00  6.15540504e+00  5.58532810e+00  1.97622848e+00
  8.55810928e+00  5.60682297e+00  6.50732088e+00  9.74595356e+00
  3.70203424e+00  8.65994740e+00  3.66820455e-01 -1.19887441e-01
  5.62859249e+00  3.21332264e+00  2.41353798e+00  2.97626567e+00
  3.33519387e+00  7.60388327e+00  5.94638872e+00  7.90076303e+00
  8.14614868e+00  8.43794918e+00  5.11629820e+00  4.45509529e+00
  8.96594524e+00  6.83408117e+00  5.29020429e-01  5.47313118e+00
  3.56114149e+00  9.86122417e+00 -8.47060025e-01  5.97660685e+00
  7.56903505e+00  3.27315211e+00  9.39664459e+00  9.18616390e+00
  9.16251087e+00  3.12576860e-01  5.81769562e+00  2.36430216e+00
  8.91152668e+00  6.80723286e+00  5.49286032e+00  1.25512540e+00
  6.51518726e+00  6.32031965e+00  9.03301430e+00  5.99218893e+00
  9.68340588e+00  9.75206280e+00  6.48233700e+00  5.02287769e+00
  4.40520668e+00  8.71606636e+00  1.15936518e-01  1.15061617e+00
  1.13465822e+00  7.72926044e+00  3.59013176e+00  6.88610840e+00
  6.62146950e+00  3.82466817e+00  5.06336927e+00  6.39645910e+00
  9.77296638e+00  6.23589993e+00  6.13958120e+00  6.46353066e-01
  7.98168039e+00  5.24744987e+00  8.84934807e+00  7.04309762e-01
  4.59585285e+00  2.57471061e+00  2.76045990e+00  7.58923578e+00
  6.31161690e+00  3.93680930e+00  3.41281295e-03  9.66006947e+00
  9.57943439e+00  2.44953036e+00  7.60993719e-01  5.62422276e+00
  9.45851040e+00  9.12294960e+00  5.39434481e+00  2.33969688e+00
  3.87983751e+00  8.94526768e+00  4.94882011e+00  7.46908426e-01
  3.23506546e+00  2.77883267e+00  1.57590222e+00  5.10829020e+00
  9.52500534e+00  6.17966223e+00  1.10281193e+00  5.18856943e-01
  2.46926522e+00  3.78143835e+00  6.96128178e+00  1.00144138e+01
  3.66667557e+00  5.97449923e+00  9.95713520e+00  7.44061518e+00
  6.02692795e+00  7.26248920e-01  3.43354082e+00  1.11553822e+01
  1.09044552e+00  8.32242012e+00  7.46725512e+00  3.50707126e+00
  4.66837549e+00  6.27135944e+00  7.01602077e+00  4.99376917e+00
  1.22947979e+00  6.21938133e+00  4.57749987e+00  9.84504700e+00
  3.30335212e+00  2.10937715e+00  8.59310150e-01  5.95748758e+00
  7.51813459e+00  1.18187451e+00  3.30510187e+00  6.36148548e+00
  4.50776958e+00  4.50498581e+00  1.05220985e+01  9.05682564e+00
  6.02789402e+00  7.13881588e+00  7.25975084e+00  6.13174152e+00
  2.50386310e+00  6.21227741e+00  6.38945484e+00  8.56712151e+00
  3.63186836e-01  9.32480907e+00  5.54315186e+00 -3.98739636e-01
  3.37429929e+00  7.95210361e+00  5.01406813e+00  9.63213921e+00
  3.99637389e+00  2.97415352e+00  8.29203606e+00  6.00829792e+00
  6.21376610e+00  7.23727989e+00  6.71069527e+00  5.01448536e+00
  9.13507938e-01  6.87569332e+00  9.86827469e+00  4.45279694e+00
  8.80540848e+00  5.50182295e+00  6.56988764e+00  8.22664070e+00
  6.11226082e+00  9.90539372e-01  8.70905399e+00  2.35227823e+00
  6.95939922e+00  4.88893890e+00  4.05508137e+00  1.27094746e+00
  7.41610336e+00  9.57014084e-01  1.03208637e+01  8.77536011e+00
  5.40437126e+00  3.66490221e+00  1.92103791e+00  5.05432844e+00
  6.88084650e+00  2.36994123e+00  4.34519386e+00  1.71688604e+00
  3.39162683e+00  1.85334861e+00  9.56473351e+00  6.55594063e+00
  7.24569368e+00  9.57989931e-01  3.30405354e+00  8.91251564e+00
  9.43958402e-01  2.91835928e+00  6.76908159e+00  6.41434669e+00
  8.02711487e+00  8.37068748e+00  1.76286387e+00  2.83487058e+00
  4.26155806e+00 -1.64624956e-02  5.80787063e-02  2.90215087e+00
  4.82958937e+00  2.67308688e+00  8.74459743e+00  3.35418940e+00
  9.50176620e+00  9.24923992e+00  5.82722712e+00  3.35397267e+00
  3.11244607e+00  7.65114069e+00  2.58552289e+00  3.75996971e+00
  5.17049551e+00  5.17774963e+00  9.58946896e+00  2.46264791e+00
  4.03688717e+00  2.72447777e+00  2.75106120e+00  1.24247396e+00
  2.18880701e+00  8.80450439e+00  2.16815996e+00  4.50869846e+00
  9.47779369e+00  6.92755127e+00  5.21426487e+00  7.63384438e+00
  1.33627713e+00  5.05314064e+00  8.01588058e-01  2.33411765e+00
  2.04597259e+00  3.71735334e+00  3.99399781e+00  9.83961201e+00
  2.91163659e+00  3.73780441e+00  7.86999273e+00  7.62870979e+00
  7.85087109e+00  2.87814641e+00  5.46488953e+00  1.44017196e+00
  2.16978312e+00  4.34651899e+00  7.40160513e+00  5.01842690e+00
  8.79978561e+00  8.01273918e+00  6.23900414e-01  5.34396315e+00
  5.84303570e+00  9.49287987e+00 -2.03253716e-01  5.55819988e+00
  2.56452465e+00  5.52358687e-01  5.80285740e+00  8.33415508e+00
  8.18197632e+00 -8.59678984e-02  3.97874522e+00  4.08348382e-01
  2.77977419e+00  2.01314735e+00  2.23359156e+00  3.15110850e+00
  3.16415262e+00  9.69314194e+00  3.46537042e+00  8.90620518e+00
  1.80000591e+00  6.60689974e+00 -7.20865309e-01  4.78756142e+00
 -1.09752685e-01  9.03241920e+00  8.91885853e+00  6.74625039e-01
  9.64687729e+00  5.28304863e+00  3.44711399e+00  9.08611012e+00
  3.02275133e+00  4.11477327e+00  1.01315606e+00  7.55140066e+00
  7.66734123e-01  4.76552486e+00  6.70099640e+00  7.79399300e+00
  1.63697755e+00  6.94904470e+00  6.56399965e+00  4.96204281e+00
  5.89378309e+00  5.83356380e-01  6.12774992e+00  4.05969334e+00
  8.60876942e+00  3.77930760e+00  5.77260780e+00  4.28877592e+00
  9.17067528e+00  2.55027103e+00  1.33500242e+00  8.23266697e+00
  4.27657557e+00  5.92882109e+00  3.65143597e-01  1.05127277e+01
  9.12497520e+00  4.73808956e+00  1.15415120e+00  4.24439526e+00
  9.80254364e+00  7.13764381e+00  2.21075535e+00  4.46259594e+00
  9.60248947e+00  6.37608194e+00  3.89600515e-01  6.30370569e+00
  2.47006059e+00  9.89494145e-01  4.10520649e+00  5.44608831e+00
  5.12012959e-01  4.04597473e+00  5.92461109e+00  8.09785938e+00
  5.32709646e+00  5.15376234e+00  7.13395691e+00  8.32365990e+00
  4.20349646e+00  6.05286407e+00  6.26060724e+00  6.60055876e-03
  8.09364128e+00  3.49219418e+00  7.10890245e+00  4.91867685e+00
  7.30661583e+00  6.67151976e+00  2.00603342e+00  6.53424406e+00
  8.17309189e+00  2.37493563e+00  7.19742537e+00  6.21698093e+00
  1.02654848e+01  4.02459478e+00  2.00096679e+00  3.60146332e+00
  2.72641587e+00  7.55267096e+00  7.33097744e+00  6.48247480e+00
  1.84161115e+00  8.67273045e+00  1.03891554e+01  5.11711931e+00
  6.14349842e+00  4.63130045e+00  1.04338384e+00  3.16097307e+00
  2.48957181e+00  2.08219433e+00  9.02428818e+00  7.58692598e+00
  3.53382182e+00  2.87967825e+00  9.43597221e+00  1.85656917e+00
  5.57112312e+00  9.00374889e-01  1.43457806e+00  5.70213127e+00
  6.18463993e+00  5.21287870e+00  8.30662155e+00  9.78884792e+00
  8.07082462e+00  4.94385624e+00  9.57595062e+00  4.81561422e-01
  5.93677044e+00  6.03051233e+00  1.27146268e+00  9.59977341e+00
  3.53188229e+00  5.92599010e+00  8.90297794e+00  5.73115826e+00
  8.29036653e-01  6.28442287e+00  4.10012245e+00  8.16020679e+00
  9.57242489e+00  8.73237801e+00  3.82743692e+00  7.33982182e+00
  9.39437389e+00  5.32039928e+00  4.49351358e+00  7.41679072e-01
  7.33104944e+00  8.68616486e+00  2.54391122e+00  2.88136458e+00
  5.05854321e+00  4.12581384e-01  7.20838428e-01  4.39249325e+00
  2.12579203e+00  5.69973421e+00  3.94032979e+00  1.01283722e+01
  9.79717064e+00  2.52009940e+00  8.86107635e+00  8.79512429e-01
  4.84236622e+00  1.72291732e+00  3.82897258e+00  6.94665861e+00
  4.18651247e+00  8.54575062e+00  7.19269276e+00  9.72444630e+00
  5.51988173e+00  8.01771069e+00  3.28205919e+00  3.97813463e+00
  4.18136644e+00  5.16412115e+00  1.75697100e+00  5.15417719e+00
  1.50469279e+00  9.25226021e+00  4.35976595e-01  4.19854736e+00
  3.64054823e+00  1.06782198e-01  8.30288601e+00  7.06305122e+00
  1.87979758e+00  6.35672998e+00  8.16109753e+00  6.51096630e+00
  5.61127961e-01  6.76316595e+00  6.57550716e+00  7.76742983e+00
  5.40051842e+00  4.99695921e+00  1.05247574e+01  7.60159779e+00
  5.60209799e+00  3.34395909e+00  9.19912910e+00  4.68194294e+00
  1.00466835e+00  5.51672554e+00  8.06357193e+00  4.60604143e+00
  7.99039245e-01  6.11217165e+00  9.98776340e+00  7.64210176e+00
  6.11796856e+00  9.89960003e+00  7.96584511e+00  6.26075697e+00
  4.63107634e+00  7.38753414e+00  6.49842548e+00  7.23403335e-01
  7.84123564e+00  3.43258095e+00  5.87750053e+00  8.16937256e+00
  4.84820747e+00  1.59829724e+00  4.18775225e+00  1.30482543e+00
  6.89824677e+00  9.70094967e+00  7.83443594e+00  4.79529858e+00
  6.26568747e+00  3.08977580e+00  5.72115278e+00  2.92952847e+00
  7.45006084e+00  2.38916183e+00  9.71957111e+00  6.54768562e+00
  6.25139415e-01 -2.21325979e-02  1.01162004e+01  1.64097524e+00
  3.30466270e+00  2.32712507e+00  9.82605267e+00  5.42485619e+00
  4.57557726e+00  5.65397930e+00  2.95226574e+00  6.00716829e+00
  4.69566488e+00  1.86493647e+00  5.55787706e+00  4.29599905e+00
  8.97355652e+00  4.49551582e+00  5.21416712e+00  7.87060261e+00
  4.67125940e+00  9.76075363e+00  8.19973755e+00  8.41541100e+00
  9.90537739e+00  2.93996453e+00  5.85271358e+00  7.30432892e+00
  7.22648525e+00  9.17044544e+00  2.83920431e+00  3.20150447e+00
  8.31029034e+00  7.78128242e+00  2.25734663e+00  5.91900253e+00
  7.87056684e+00  3.05571938e+00  3.93086529e+00  5.76315022e+00
  5.30861235e+00  7.55313301e+00 -1.47460684e-01  4.09288597e+00
  1.74171937e+00  4.78674221e+00  7.88887119e+00  7.54741371e-01
  1.54648781e+00  7.28928089e+00  7.57703447e+00  5.61207056e+00
  9.85756111e+00  4.10112953e+00  1.57129598e+00  6.85865021e+00
  6.33887672e+00  6.33726215e+00  5.35512924e+00  8.78589249e+00
  8.93958569e-01 -1.22548640e-01  1.54779208e+00  2.83668208e+00
  5.92615891e+00  5.92532015e+00  2.29801345e+00  4.49938583e+00
  4.36498499e+00  5.37602377e+00  8.03752041e+00  5.66677046e+00
  8.83142376e+00  1.65858328e+00  5.42761660e+00  8.26859760e+00
  1.79626775e+00  4.40248251e+00  5.96399355e+00  2.07387352e+00
  7.58685493e+00  6.19924068e-01  9.63695049e+00  4.80245113e+00
  2.84362292e+00  1.11408031e+00  4.65407324e+00  6.60547924e+00
  1.22691751e+00  3.05578947e+00  1.09281607e+01  6.40130091e+00
  2.10371280e+00  1.27200794e+00  8.89848232e+00  2.80182910e+00
  2.19280529e+00  1.05606747e+01  2.94065785e+00  1.69049454e+00
  5.48146868e+00  4.31994724e+00  9.06744099e+00  1.15592277e+00
 -7.79310703e-01  1.04373407e+00  8.86706924e+00  4.28514481e+00
  1.81781006e+00  5.78969526e+00  5.54994345e+00  2.30405951e+00
  5.54053879e+00  2.74468279e+00  2.27062321e+00  4.07878542e+00
  2.84625888e+00  2.04214787e+00  8.94879723e+00  6.88817441e-01
  2.71495771e+00  3.84625363e+00  2.55016398e+00  2.09636283e+00
  7.99209976e+00  3.07564521e+00  1.11745620e+00  5.30355263e+00
  9.93744850e-01  2.74349833e+00  8.41612911e+00  4.34795761e+00
  6.55439472e+00  6.15104818e+00  5.19303381e-01  6.36637783e+00
  8.49066544e+00  8.24014544e-01  6.90668011e+00  7.08179665e+00
  9.01312637e+00  3.36554408e+00  2.87336349e+00  6.86589861e+00
 -2.01100767e-01  7.31349230e+00  8.66450596e+00  4.29815626e+00
  3.38823771e+00  3.01338983e+00  8.08154404e-01  1.78297210e+00
  9.61242580e+00  8.81327534e+00  2.97019339e+00  8.63016319e+00
  3.02107668e+00  2.72670150e+00  2.13139439e+00  8.94742870e+00
  9.83511925e+00  3.19685555e+00  6.81030130e+00  3.50014639e+00
  1.05297625e+00  4.07330465e+00  5.53509521e+00  1.18256664e+00
  8.23826122e+00  5.53470898e+00  2.24809480e+00  5.40734196e+00
  7.84483671e+00  2.75392199e+00  7.62425852e+00  4.27786589e+00
  9.72611332e+00  8.83753872e+00  3.43726850e+00  1.27960294e-01
  7.29834032e+00  2.04023504e+00  1.30689442e+00  9.04135704e+00
  1.00866823e+01  2.58009624e+00  3.01804805e+00  1.68803823e+00
  4.80738878e+00  8.68211079e+00  6.73749733e+00  8.63989162e+00
  2.21114421e+00  7.04848576e+00  9.34753704e+00  2.76184368e+00
  6.90854836e+00  4.05415773e+00  3.81649780e+00  9.40583229e+00
  4.05083704e+00  8.92635822e-01  3.82358503e+00  4.43289518e+00
  7.26910877e+00  8.82122993e+00  8.22476578e+00  9.33980370e+00
  4.71105146e+00  6.95373154e+00  3.06833911e+00 -4.22824502e-01
  3.65492988e+00  6.86570597e+00  2.05108023e+00  8.74809170e+00
  8.31052685e+00  1.87380075e+00  2.81262565e+00  8.66123104e+00
  3.31760716e+00  3.31471944e+00  3.48811197e+00  7.28613329e+00
 -9.66772735e-02 -7.69796014e-01  6.32836771e+00  3.86802888e+00
  5.99920893e+00  2.40654874e+00  9.12174988e+00  8.34914875e+00
  7.07096219e-01  7.57762194e+00  9.58280849e+00  8.65084171e+00
  6.67687321e+00  1.21811235e+00  5.86707783e+00  3.30112028e+00
  1.01402960e+01  7.17766523e+00  5.74374008e+00  6.99432039e+00
  4.40368319e+00  2.86542940e+00  9.42987251e+00  3.30265784e+00
  3.83230895e-01  4.20491457e+00  8.71614361e+00  7.57816601e+00
  9.77553654e+00  6.46106720e-01  9.44217777e+00  4.46305180e+00
  4.83198547e+00  3.06845570e+00  1.06641746e+00  4.49923372e+00
  7.57343435e+00  5.86748123e-03  1.58664107e+00  5.31323195e-01
  8.55088329e+00  6.48872995e+00  1.54820132e+00  7.71537733e+00
  8.20388412e+00  9.80402565e+00  9.79537868e+00 -8.38677704e-01
  5.94942427e+00  2.88661289e+00  5.26643085e+00  3.18921828e+00
  7.79481411e-01  4.42821693e+00  5.33531380e+00  9.32881355e+00
  2.65765786e-02  3.08747840e+00  8.52437019e+00  6.85948515e+00
  8.21984768e+00  6.24451685e+00  7.30141687e+00  4.90936804e+00
  2.20824790e+00  8.48395348e+00  5.67844057e+00  1.24840343e+00
  8.05260468e+00  9.64081573e+00  3.07099938e+00  3.18240905e+00
  9.12044704e-01  5.96499348e+00  9.19813156e+00  3.36040854e-01
  4.73263264e+00  1.30067658e+00  5.19609153e-01  6.70743704e+00
  8.23411751e+00  3.52812862e+00  8.22581482e+00  1.19572031e+00
  2.63071656e+00  9.79853821e+00  4.98861170e+00  6.00117397e+00
  3.19664389e-01  1.32244027e+00  6.05848253e-01  7.35753870e+00
  1.13696325e+00  8.69066429e+00  3.58892393e+00  7.88068628e+00
  3.04448676e+00  2.65313506e+00  9.56735551e-01  2.75976801e+00
  4.98727465e+00  7.08365965e+00  5.05365181e+00  9.17652988e+00
  5.92350817e+00  1.26345801e+00  3.95679140e+00  7.46081781e+00
  2.64605379e+00  6.97033882e+00  5.93741179e+00  1.01776525e-01
  7.47330189e+00  5.93837404e+00  8.12254488e-01  3.60206747e+00
  3.23938537e+00  8.24193764e+00  3.56294060e+00  9.41426849e+00
  5.64243269e+00  2.11869121e+00  8.28666019e+00  6.44191980e+00]
Epoch 1/1000
2023-09-09 09:36:46.150 
Epoch 1/1000 
	 loss: 1914.4790, MinusLogProbMetric: 1914.4790, val_loss: 669.0754, val_MinusLogProbMetric: 669.0754

Epoch 1: val_loss improved from inf to 669.07544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 40s - loss: 1914.4790 - MinusLogProbMetric: 1914.4790 - val_loss: 669.0754 - val_MinusLogProbMetric: 669.0754 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 2/1000
2023-09-09 09:36:57.134 
Epoch 2/1000 
	 loss: 584.3699, MinusLogProbMetric: 584.3699, val_loss: 557.1028, val_MinusLogProbMetric: 557.1028

Epoch 2: val_loss improved from 669.07544 to 557.10284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 584.3699 - MinusLogProbMetric: 584.3699 - val_loss: 557.1028 - val_MinusLogProbMetric: 557.1028 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 3/1000
2023-09-09 09:37:07.841 
Epoch 3/1000 
	 loss: 518.2261, MinusLogProbMetric: 518.2261, val_loss: 507.8224, val_MinusLogProbMetric: 507.8224

Epoch 3: val_loss improved from 557.10284 to 507.82239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 518.2261 - MinusLogProbMetric: 518.2261 - val_loss: 507.8224 - val_MinusLogProbMetric: 507.8224 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 4/1000
2023-09-09 09:37:19.223 
Epoch 4/1000 
	 loss: 494.7589, MinusLogProbMetric: 494.7589, val_loss: 499.6040, val_MinusLogProbMetric: 499.6040

Epoch 4: val_loss improved from 507.82239 to 499.60400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 494.7589 - MinusLogProbMetric: 494.7589 - val_loss: 499.6040 - val_MinusLogProbMetric: 499.6040 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 5/1000
2023-09-09 09:37:30.602 
Epoch 5/1000 
	 loss: 483.0595, MinusLogProbMetric: 483.0595, val_loss: 479.5150, val_MinusLogProbMetric: 479.5150

Epoch 5: val_loss improved from 499.60400 to 479.51495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 483.0595 - MinusLogProbMetric: 483.0595 - val_loss: 479.5150 - val_MinusLogProbMetric: 479.5150 - lr: 0.0010 - 12s/epoch - 59ms/step
Epoch 6/1000
2023-09-09 09:37:42.574 
Epoch 6/1000 
	 loss: 472.2729, MinusLogProbMetric: 472.2729, val_loss: 476.4692, val_MinusLogProbMetric: 476.4692

Epoch 6: val_loss improved from 479.51495 to 476.46918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 472.2729 - MinusLogProbMetric: 472.2729 - val_loss: 476.4692 - val_MinusLogProbMetric: 476.4692 - lr: 0.0010 - 12s/epoch - 60ms/step
Epoch 7/1000
2023-09-09 09:37:53.717 
Epoch 7/1000 
	 loss: 464.9895, MinusLogProbMetric: 464.9895, val_loss: 459.5305, val_MinusLogProbMetric: 459.5305

Epoch 7: val_loss improved from 476.46918 to 459.53049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 464.9895 - MinusLogProbMetric: 464.9895 - val_loss: 459.5305 - val_MinusLogProbMetric: 459.5305 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 8/1000
2023-09-09 09:38:05.038 
Epoch 8/1000 
	 loss: 458.4386, MinusLogProbMetric: 458.4386, val_loss: 451.1392, val_MinusLogProbMetric: 451.1392

Epoch 8: val_loss improved from 459.53049 to 451.13919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 458.4386 - MinusLogProbMetric: 458.4386 - val_loss: 451.1392 - val_MinusLogProbMetric: 451.1392 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 9/1000
2023-09-09 09:38:16.386 
Epoch 9/1000 
	 loss: 454.8873, MinusLogProbMetric: 454.8873, val_loss: 463.2817, val_MinusLogProbMetric: 463.2817

Epoch 9: val_loss did not improve from 451.13919
196/196 - 11s - loss: 454.8873 - MinusLogProbMetric: 454.8873 - val_loss: 463.2817 - val_MinusLogProbMetric: 463.2817 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 10/1000
2023-09-09 09:38:28.391 
Epoch 10/1000 
	 loss: 453.2953, MinusLogProbMetric: 453.2953, val_loss: 450.4117, val_MinusLogProbMetric: 450.4117

Epoch 10: val_loss improved from 451.13919 to 450.41165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 453.2953 - MinusLogProbMetric: 453.2953 - val_loss: 450.4117 - val_MinusLogProbMetric: 450.4117 - lr: 0.0010 - 12s/epoch - 63ms/step
Epoch 11/1000
2023-09-09 09:38:40.196 
Epoch 11/1000 
	 loss: 448.9819, MinusLogProbMetric: 448.9819, val_loss: 445.9140, val_MinusLogProbMetric: 445.9140

Epoch 11: val_loss improved from 450.41165 to 445.91397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 448.9819 - MinusLogProbMetric: 448.9819 - val_loss: 445.9140 - val_MinusLogProbMetric: 445.9140 - lr: 0.0010 - 12s/epoch - 60ms/step
Epoch 12/1000
2023-09-09 09:38:52.386 
Epoch 12/1000 
	 loss: 445.6850, MinusLogProbMetric: 445.6850, val_loss: 440.4565, val_MinusLogProbMetric: 440.4565

Epoch 12: val_loss improved from 445.91397 to 440.45654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 445.6850 - MinusLogProbMetric: 445.6850 - val_loss: 440.4565 - val_MinusLogProbMetric: 440.4565 - lr: 0.0010 - 12s/epoch - 62ms/step
Epoch 13/1000
2023-09-09 09:39:04.425 
Epoch 13/1000 
	 loss: 447.1595, MinusLogProbMetric: 447.1595, val_loss: 437.7064, val_MinusLogProbMetric: 437.7064

Epoch 13: val_loss improved from 440.45654 to 437.70636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 447.1595 - MinusLogProbMetric: 447.1595 - val_loss: 437.7064 - val_MinusLogProbMetric: 437.7064 - lr: 0.0010 - 12s/epoch - 62ms/step
Epoch 14/1000
2023-09-09 09:39:16.963 
Epoch 14/1000 
	 loss: 441.6773, MinusLogProbMetric: 441.6773, val_loss: 441.8948, val_MinusLogProbMetric: 441.8948

Epoch 14: val_loss did not improve from 437.70636
196/196 - 12s - loss: 441.6773 - MinusLogProbMetric: 441.6773 - val_loss: 441.8948 - val_MinusLogProbMetric: 441.8948 - lr: 0.0010 - 12s/epoch - 62ms/step
Epoch 15/1000
2023-09-09 09:39:28.609 
Epoch 15/1000 
	 loss: 438.2649, MinusLogProbMetric: 438.2649, val_loss: 435.7966, val_MinusLogProbMetric: 435.7966

Epoch 15: val_loss improved from 437.70636 to 435.79660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 438.2649 - MinusLogProbMetric: 438.2649 - val_loss: 435.7966 - val_MinusLogProbMetric: 435.7966 - lr: 0.0010 - 12s/epoch - 61ms/step
Epoch 16/1000
2023-09-09 09:39:40.789 
Epoch 16/1000 
	 loss: 436.5732, MinusLogProbMetric: 436.5732, val_loss: 434.4721, val_MinusLogProbMetric: 434.4721

Epoch 16: val_loss improved from 435.79660 to 434.47208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 436.5732 - MinusLogProbMetric: 436.5732 - val_loss: 434.4721 - val_MinusLogProbMetric: 434.4721 - lr: 0.0010 - 12s/epoch - 63ms/step
Epoch 17/1000
2023-09-09 09:39:53.049 
Epoch 17/1000 
	 loss: 435.8408, MinusLogProbMetric: 435.8408, val_loss: 440.1144, val_MinusLogProbMetric: 440.1144

Epoch 17: val_loss did not improve from 434.47208
196/196 - 12s - loss: 435.8408 - MinusLogProbMetric: 435.8408 - val_loss: 440.1144 - val_MinusLogProbMetric: 440.1144 - lr: 0.0010 - 12s/epoch - 60ms/step
Epoch 18/1000
2023-09-09 09:40:04.702 
Epoch 18/1000 
	 loss: 436.6804, MinusLogProbMetric: 436.6804, val_loss: 462.7961, val_MinusLogProbMetric: 462.7961

Epoch 18: val_loss did not improve from 434.47208
196/196 - 12s - loss: 436.6804 - MinusLogProbMetric: 436.6804 - val_loss: 462.7961 - val_MinusLogProbMetric: 462.7961 - lr: 0.0010 - 12s/epoch - 59ms/step
Epoch 19/1000
2023-09-09 09:40:16.061 
Epoch 19/1000 
	 loss: 433.0197, MinusLogProbMetric: 433.0197, val_loss: 446.3488, val_MinusLogProbMetric: 446.3488

Epoch 19: val_loss did not improve from 434.47208
196/196 - 11s - loss: 433.0197 - MinusLogProbMetric: 433.0197 - val_loss: 446.3488 - val_MinusLogProbMetric: 446.3488 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 20/1000
2023-09-09 09:40:26.631 
Epoch 20/1000 
	 loss: 432.8202, MinusLogProbMetric: 432.8202, val_loss: 428.7238, val_MinusLogProbMetric: 428.7238

Epoch 20: val_loss improved from 434.47208 to 428.72385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 432.8202 - MinusLogProbMetric: 432.8202 - val_loss: 428.7238 - val_MinusLogProbMetric: 428.7238 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 21/1000
2023-09-09 09:40:38.231 
Epoch 21/1000 
	 loss: 429.4030, MinusLogProbMetric: 429.4030, val_loss: 429.8016, val_MinusLogProbMetric: 429.8016

Epoch 21: val_loss did not improve from 428.72385
196/196 - 11s - loss: 429.4030 - MinusLogProbMetric: 429.4030 - val_loss: 429.8016 - val_MinusLogProbMetric: 429.8016 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 22/1000
2023-09-09 09:40:49.369 
Epoch 22/1000 
	 loss: 430.9326, MinusLogProbMetric: 430.9326, val_loss: 437.5801, val_MinusLogProbMetric: 437.5801

Epoch 22: val_loss did not improve from 428.72385
196/196 - 11s - loss: 430.9326 - MinusLogProbMetric: 430.9326 - val_loss: 437.5801 - val_MinusLogProbMetric: 437.5801 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 23/1000
2023-09-09 09:41:00.627 
Epoch 23/1000 
	 loss: 428.8452, MinusLogProbMetric: 428.8452, val_loss: 436.5343, val_MinusLogProbMetric: 436.5343

Epoch 23: val_loss did not improve from 428.72385
196/196 - 11s - loss: 428.8452 - MinusLogProbMetric: 428.8452 - val_loss: 436.5343 - val_MinusLogProbMetric: 436.5343 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 24/1000
2023-09-09 09:41:11.444 
Epoch 24/1000 
	 loss: 427.1345, MinusLogProbMetric: 427.1345, val_loss: 429.6937, val_MinusLogProbMetric: 429.6937

Epoch 24: val_loss did not improve from 428.72385
196/196 - 11s - loss: 427.1345 - MinusLogProbMetric: 427.1345 - val_loss: 429.6937 - val_MinusLogProbMetric: 429.6937 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 25/1000
2023-09-09 09:41:22.523 
Epoch 25/1000 
	 loss: 426.0740, MinusLogProbMetric: 426.0740, val_loss: 426.8856, val_MinusLogProbMetric: 426.8856

Epoch 25: val_loss improved from 428.72385 to 426.88556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 426.0740 - MinusLogProbMetric: 426.0740 - val_loss: 426.8856 - val_MinusLogProbMetric: 426.8856 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 26/1000
2023-09-09 09:41:33.545 
Epoch 26/1000 
	 loss: 427.4389, MinusLogProbMetric: 427.4389, val_loss: 426.2760, val_MinusLogProbMetric: 426.2760

Epoch 26: val_loss improved from 426.88556 to 426.27600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 427.4389 - MinusLogProbMetric: 427.4389 - val_loss: 426.2760 - val_MinusLogProbMetric: 426.2760 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 27/1000
2023-09-09 09:41:44.612 
Epoch 27/1000 
	 loss: 424.9739, MinusLogProbMetric: 424.9739, val_loss: 457.7883, val_MinusLogProbMetric: 457.7883

Epoch 27: val_loss did not improve from 426.27600
196/196 - 11s - loss: 424.9739 - MinusLogProbMetric: 424.9739 - val_loss: 457.7883 - val_MinusLogProbMetric: 457.7883 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 28/1000
2023-09-09 09:41:55.902 
Epoch 28/1000 
	 loss: 424.3452, MinusLogProbMetric: 424.3452, val_loss: 428.2333, val_MinusLogProbMetric: 428.2333

Epoch 28: val_loss did not improve from 426.27600
196/196 - 11s - loss: 424.3452 - MinusLogProbMetric: 424.3452 - val_loss: 428.2333 - val_MinusLogProbMetric: 428.2333 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 29/1000
2023-09-09 09:42:07.236 
Epoch 29/1000 
	 loss: 423.1013, MinusLogProbMetric: 423.1013, val_loss: 425.4628, val_MinusLogProbMetric: 425.4628

Epoch 29: val_loss improved from 426.27600 to 425.46280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 423.1013 - MinusLogProbMetric: 423.1013 - val_loss: 425.4628 - val_MinusLogProbMetric: 425.4628 - lr: 0.0010 - 12s/epoch - 60ms/step
Epoch 30/1000
2023-09-09 09:42:18.856 
Epoch 30/1000 
	 loss: 428.3246, MinusLogProbMetric: 428.3246, val_loss: 422.1471, val_MinusLogProbMetric: 422.1471

Epoch 30: val_loss improved from 425.46280 to 422.14713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 428.3246 - MinusLogProbMetric: 428.3246 - val_loss: 422.1471 - val_MinusLogProbMetric: 422.1471 - lr: 0.0010 - 12s/epoch - 59ms/step
Epoch 31/1000
2023-09-09 09:42:30.565 
Epoch 31/1000 
	 loss: 421.0523, MinusLogProbMetric: 421.0523, val_loss: 436.5904, val_MinusLogProbMetric: 436.5904

Epoch 31: val_loss did not improve from 422.14713
196/196 - 11s - loss: 421.0523 - MinusLogProbMetric: 421.0523 - val_loss: 436.5904 - val_MinusLogProbMetric: 436.5904 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 32/1000
2023-09-09 09:42:41.481 
Epoch 32/1000 
	 loss: 420.7127, MinusLogProbMetric: 420.7127, val_loss: 425.9878, val_MinusLogProbMetric: 425.9878

Epoch 32: val_loss did not improve from 422.14713
196/196 - 11s - loss: 420.7127 - MinusLogProbMetric: 420.7127 - val_loss: 425.9878 - val_MinusLogProbMetric: 425.9878 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 33/1000
2023-09-09 09:42:52.818 
Epoch 33/1000 
	 loss: 421.4499, MinusLogProbMetric: 421.4499, val_loss: 425.2849, val_MinusLogProbMetric: 425.2849

Epoch 33: val_loss did not improve from 422.14713
196/196 - 11s - loss: 421.4499 - MinusLogProbMetric: 421.4499 - val_loss: 425.2849 - val_MinusLogProbMetric: 425.2849 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 34/1000
2023-09-09 09:43:03.933 
Epoch 34/1000 
	 loss: 421.9605, MinusLogProbMetric: 421.9605, val_loss: 422.2830, val_MinusLogProbMetric: 422.2830

Epoch 34: val_loss did not improve from 422.14713
196/196 - 11s - loss: 421.9605 - MinusLogProbMetric: 421.9605 - val_loss: 422.2830 - val_MinusLogProbMetric: 422.2830 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 35/1000
2023-09-09 09:43:14.512 
Epoch 35/1000 
	 loss: 422.3655, MinusLogProbMetric: 422.3655, val_loss: 427.3627, val_MinusLogProbMetric: 427.3627

Epoch 35: val_loss did not improve from 422.14713
196/196 - 11s - loss: 422.3655 - MinusLogProbMetric: 422.3655 - val_loss: 427.3627 - val_MinusLogProbMetric: 427.3627 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 36/1000
2023-09-09 09:43:24.937 
Epoch 36/1000 
	 loss: 418.3824, MinusLogProbMetric: 418.3824, val_loss: 417.0841, val_MinusLogProbMetric: 417.0841

Epoch 36: val_loss improved from 422.14713 to 417.08408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 418.3824 - MinusLogProbMetric: 418.3824 - val_loss: 417.0841 - val_MinusLogProbMetric: 417.0841 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 37/1000
2023-09-09 09:43:35.647 
Epoch 37/1000 
	 loss: 418.8195, MinusLogProbMetric: 418.8195, val_loss: 441.3387, val_MinusLogProbMetric: 441.3387

Epoch 37: val_loss did not improve from 417.08408
196/196 - 10s - loss: 418.8195 - MinusLogProbMetric: 418.8195 - val_loss: 441.3387 - val_MinusLogProbMetric: 441.3387 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 38/1000
2023-09-09 09:43:46.234 
Epoch 38/1000 
	 loss: 420.6648, MinusLogProbMetric: 420.6648, val_loss: 421.4097, val_MinusLogProbMetric: 421.4097

Epoch 38: val_loss did not improve from 417.08408
196/196 - 11s - loss: 420.6648 - MinusLogProbMetric: 420.6648 - val_loss: 421.4097 - val_MinusLogProbMetric: 421.4097 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 39/1000
2023-09-09 09:43:56.072 
Epoch 39/1000 
	 loss: 589.0229, MinusLogProbMetric: 589.0229, val_loss: 632.8034, val_MinusLogProbMetric: 632.8034

Epoch 39: val_loss did not improve from 417.08408
196/196 - 10s - loss: 589.0229 - MinusLogProbMetric: 589.0229 - val_loss: 632.8034 - val_MinusLogProbMetric: 632.8034 - lr: 0.0010 - 10s/epoch - 50ms/step
Epoch 40/1000
2023-09-09 09:44:07.065 
Epoch 40/1000 
	 loss: 478.4037, MinusLogProbMetric: 478.4037, val_loss: 440.5965, val_MinusLogProbMetric: 440.5965

Epoch 40: val_loss did not improve from 417.08408
196/196 - 11s - loss: 478.4037 - MinusLogProbMetric: 478.4037 - val_loss: 440.5965 - val_MinusLogProbMetric: 440.5965 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 41/1000
2023-09-09 09:44:18.039 
Epoch 41/1000 
	 loss: 435.1949, MinusLogProbMetric: 435.1949, val_loss: 431.5560, val_MinusLogProbMetric: 431.5560

Epoch 41: val_loss did not improve from 417.08408
196/196 - 11s - loss: 435.1949 - MinusLogProbMetric: 435.1949 - val_loss: 431.5560 - val_MinusLogProbMetric: 431.5560 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 42/1000
2023-09-09 09:44:29.286 
Epoch 42/1000 
	 loss: 428.7585, MinusLogProbMetric: 428.7585, val_loss: 426.1182, val_MinusLogProbMetric: 426.1182

Epoch 42: val_loss did not improve from 417.08408
196/196 - 11s - loss: 428.7585 - MinusLogProbMetric: 428.7585 - val_loss: 426.1182 - val_MinusLogProbMetric: 426.1182 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 43/1000
2023-09-09 09:44:40.468 
Epoch 43/1000 
	 loss: 425.8499, MinusLogProbMetric: 425.8499, val_loss: 426.9401, val_MinusLogProbMetric: 426.9401

Epoch 43: val_loss did not improve from 417.08408
196/196 - 11s - loss: 425.8499 - MinusLogProbMetric: 425.8499 - val_loss: 426.9401 - val_MinusLogProbMetric: 426.9401 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 44/1000
2023-09-09 09:44:51.482 
Epoch 44/1000 
	 loss: 422.9364, MinusLogProbMetric: 422.9364, val_loss: 425.0814, val_MinusLogProbMetric: 425.0814

Epoch 44: val_loss did not improve from 417.08408
196/196 - 11s - loss: 422.9364 - MinusLogProbMetric: 422.9364 - val_loss: 425.0814 - val_MinusLogProbMetric: 425.0814 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 45/1000
2023-09-09 09:45:01.454 
Epoch 45/1000 
	 loss: 421.6085, MinusLogProbMetric: 421.6085, val_loss: 424.1700, val_MinusLogProbMetric: 424.1700

Epoch 45: val_loss did not improve from 417.08408
196/196 - 10s - loss: 421.6085 - MinusLogProbMetric: 421.6085 - val_loss: 424.1700 - val_MinusLogProbMetric: 424.1700 - lr: 0.0010 - 10s/epoch - 51ms/step
Epoch 46/1000
2023-09-09 09:45:12.740 
Epoch 46/1000 
	 loss: 421.5198, MinusLogProbMetric: 421.5198, val_loss: 417.9471, val_MinusLogProbMetric: 417.9471

Epoch 46: val_loss did not improve from 417.08408
196/196 - 11s - loss: 421.5198 - MinusLogProbMetric: 421.5198 - val_loss: 417.9471 - val_MinusLogProbMetric: 417.9471 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 47/1000
2023-09-09 09:45:23.726 
Epoch 47/1000 
	 loss: 418.9001, MinusLogProbMetric: 418.9001, val_loss: 419.4584, val_MinusLogProbMetric: 419.4584

Epoch 47: val_loss did not improve from 417.08408
196/196 - 11s - loss: 418.9001 - MinusLogProbMetric: 418.9001 - val_loss: 419.4584 - val_MinusLogProbMetric: 419.4584 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 48/1000
2023-09-09 09:45:34.689 
Epoch 48/1000 
	 loss: 421.9375, MinusLogProbMetric: 421.9375, val_loss: 420.4055, val_MinusLogProbMetric: 420.4055

Epoch 48: val_loss did not improve from 417.08408
196/196 - 11s - loss: 421.9375 - MinusLogProbMetric: 421.9375 - val_loss: 420.4055 - val_MinusLogProbMetric: 420.4055 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 49/1000
2023-09-09 09:45:45.790 
Epoch 49/1000 
	 loss: 416.5948, MinusLogProbMetric: 416.5948, val_loss: 441.9998, val_MinusLogProbMetric: 441.9998

Epoch 49: val_loss did not improve from 417.08408
196/196 - 11s - loss: 416.5948 - MinusLogProbMetric: 416.5948 - val_loss: 441.9998 - val_MinusLogProbMetric: 441.9998 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 50/1000
2023-09-09 09:45:56.679 
Epoch 50/1000 
	 loss: 417.1925, MinusLogProbMetric: 417.1925, val_loss: 427.3537, val_MinusLogProbMetric: 427.3537

Epoch 50: val_loss did not improve from 417.08408
196/196 - 11s - loss: 417.1925 - MinusLogProbMetric: 417.1925 - val_loss: 427.3537 - val_MinusLogProbMetric: 427.3537 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 51/1000
2023-09-09 09:46:07.819 
Epoch 51/1000 
	 loss: 416.5401, MinusLogProbMetric: 416.5401, val_loss: 426.9307, val_MinusLogProbMetric: 426.9307

Epoch 51: val_loss did not improve from 417.08408
196/196 - 11s - loss: 416.5401 - MinusLogProbMetric: 416.5401 - val_loss: 426.9307 - val_MinusLogProbMetric: 426.9307 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 52/1000
2023-09-09 09:46:19.085 
Epoch 52/1000 
	 loss: 416.8842, MinusLogProbMetric: 416.8842, val_loss: 415.6202, val_MinusLogProbMetric: 415.6202

Epoch 52: val_loss improved from 417.08408 to 415.62024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 416.8842 - MinusLogProbMetric: 416.8842 - val_loss: 415.6202 - val_MinusLogProbMetric: 415.6202 - lr: 0.0010 - 12s/epoch - 59ms/step
Epoch 53/1000
2023-09-09 09:46:30.783 
Epoch 53/1000 
	 loss: 415.3779, MinusLogProbMetric: 415.3779, val_loss: 435.8063, val_MinusLogProbMetric: 435.8063

Epoch 53: val_loss did not improve from 415.62024
196/196 - 11s - loss: 415.3779 - MinusLogProbMetric: 415.3779 - val_loss: 435.8063 - val_MinusLogProbMetric: 435.8063 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 54/1000
2023-09-09 09:46:41.964 
Epoch 54/1000 
	 loss: 414.3387, MinusLogProbMetric: 414.3387, val_loss: 416.8943, val_MinusLogProbMetric: 416.8943

Epoch 54: val_loss did not improve from 415.62024
196/196 - 11s - loss: 414.3387 - MinusLogProbMetric: 414.3387 - val_loss: 416.8943 - val_MinusLogProbMetric: 416.8943 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 55/1000
2023-09-09 09:46:53.309 
Epoch 55/1000 
	 loss: 414.9743, MinusLogProbMetric: 414.9743, val_loss: 419.5882, val_MinusLogProbMetric: 419.5882

Epoch 55: val_loss did not improve from 415.62024
196/196 - 11s - loss: 414.9743 - MinusLogProbMetric: 414.9743 - val_loss: 419.5882 - val_MinusLogProbMetric: 419.5882 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 56/1000
2023-09-09 09:47:04.452 
Epoch 56/1000 
	 loss: 414.2592, MinusLogProbMetric: 414.2592, val_loss: 413.5608, val_MinusLogProbMetric: 413.5608

Epoch 56: val_loss improved from 415.62024 to 413.56076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 414.2592 - MinusLogProbMetric: 414.2592 - val_loss: 413.5608 - val_MinusLogProbMetric: 413.5608 - lr: 0.0010 - 11s/epoch - 59ms/step
Epoch 57/1000
2023-09-09 09:47:15.802 
Epoch 57/1000 
	 loss: 413.0980, MinusLogProbMetric: 413.0980, val_loss: 414.8199, val_MinusLogProbMetric: 414.8199

Epoch 57: val_loss did not improve from 413.56076
196/196 - 11s - loss: 413.0980 - MinusLogProbMetric: 413.0980 - val_loss: 414.8199 - val_MinusLogProbMetric: 414.8199 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 58/1000
2023-09-09 09:47:26.834 
Epoch 58/1000 
	 loss: 415.5418, MinusLogProbMetric: 415.5418, val_loss: 414.6508, val_MinusLogProbMetric: 414.6508

Epoch 58: val_loss did not improve from 413.56076
196/196 - 11s - loss: 415.5418 - MinusLogProbMetric: 415.5418 - val_loss: 414.6508 - val_MinusLogProbMetric: 414.6508 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 59/1000
2023-09-09 09:47:37.820 
Epoch 59/1000 
	 loss: 412.6143, MinusLogProbMetric: 412.6143, val_loss: 429.1250, val_MinusLogProbMetric: 429.1250

Epoch 59: val_loss did not improve from 413.56076
196/196 - 11s - loss: 412.6143 - MinusLogProbMetric: 412.6143 - val_loss: 429.1250 - val_MinusLogProbMetric: 429.1250 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 60/1000
2023-09-09 09:47:48.815 
Epoch 60/1000 
	 loss: 414.1339, MinusLogProbMetric: 414.1339, val_loss: 414.8187, val_MinusLogProbMetric: 414.8187

Epoch 60: val_loss did not improve from 413.56076
196/196 - 11s - loss: 414.1339 - MinusLogProbMetric: 414.1339 - val_loss: 414.8187 - val_MinusLogProbMetric: 414.8187 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 61/1000
2023-09-09 09:47:59.996 
Epoch 61/1000 
	 loss: 412.1208, MinusLogProbMetric: 412.1208, val_loss: 413.1412, val_MinusLogProbMetric: 413.1412

Epoch 61: val_loss improved from 413.56076 to 413.14117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 412.1208 - MinusLogProbMetric: 412.1208 - val_loss: 413.1412 - val_MinusLogProbMetric: 413.1412 - lr: 0.0010 - 12s/epoch - 59ms/step
Epoch 62/1000
2023-09-09 09:48:11.355 
Epoch 62/1000 
	 loss: 413.5893, MinusLogProbMetric: 413.5893, val_loss: 410.4868, val_MinusLogProbMetric: 410.4868

Epoch 62: val_loss improved from 413.14117 to 410.48685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 413.5893 - MinusLogProbMetric: 413.5893 - val_loss: 410.4868 - val_MinusLogProbMetric: 410.4868 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 63/1000
2023-09-09 09:48:22.326 
Epoch 63/1000 
	 loss: 414.4598, MinusLogProbMetric: 414.4598, val_loss: 413.9256, val_MinusLogProbMetric: 413.9256

Epoch 63: val_loss did not improve from 410.48685
196/196 - 11s - loss: 414.4598 - MinusLogProbMetric: 414.4598 - val_loss: 413.9256 - val_MinusLogProbMetric: 413.9256 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 64/1000
2023-09-09 09:48:33.229 
Epoch 64/1000 
	 loss: 411.0750, MinusLogProbMetric: 411.0750, val_loss: 409.0343, val_MinusLogProbMetric: 409.0343

Epoch 64: val_loss improved from 410.48685 to 409.03433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 411.0750 - MinusLogProbMetric: 411.0750 - val_loss: 409.0343 - val_MinusLogProbMetric: 409.0343 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 65/1000
2023-09-09 09:48:44.368 
Epoch 65/1000 
	 loss: 410.3732, MinusLogProbMetric: 410.3732, val_loss: 410.9765, val_MinusLogProbMetric: 410.9765

Epoch 65: val_loss did not improve from 409.03433
196/196 - 11s - loss: 410.3732 - MinusLogProbMetric: 410.3732 - val_loss: 410.9765 - val_MinusLogProbMetric: 410.9765 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 66/1000
2023-09-09 09:48:54.908 
Epoch 66/1000 
	 loss: 410.5780, MinusLogProbMetric: 410.5780, val_loss: 409.9361, val_MinusLogProbMetric: 409.9361

Epoch 66: val_loss did not improve from 409.03433
196/196 - 11s - loss: 410.5780 - MinusLogProbMetric: 410.5780 - val_loss: 409.9361 - val_MinusLogProbMetric: 409.9361 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 67/1000
2023-09-09 09:49:06.166 
Epoch 67/1000 
	 loss: 410.0188, MinusLogProbMetric: 410.0188, val_loss: 410.3788, val_MinusLogProbMetric: 410.3788

Epoch 67: val_loss did not improve from 409.03433
196/196 - 11s - loss: 410.0188 - MinusLogProbMetric: 410.0188 - val_loss: 410.3788 - val_MinusLogProbMetric: 410.3788 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 68/1000
2023-09-09 09:49:17.161 
Epoch 68/1000 
	 loss: 409.7921, MinusLogProbMetric: 409.7921, val_loss: 413.4964, val_MinusLogProbMetric: 413.4964

Epoch 68: val_loss did not improve from 409.03433
196/196 - 11s - loss: 409.7921 - MinusLogProbMetric: 409.7921 - val_loss: 413.4964 - val_MinusLogProbMetric: 413.4964 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 69/1000
2023-09-09 09:49:28.161 
Epoch 69/1000 
	 loss: 414.1512, MinusLogProbMetric: 414.1512, val_loss: 410.0850, val_MinusLogProbMetric: 410.0850

Epoch 69: val_loss did not improve from 409.03433
196/196 - 11s - loss: 414.1512 - MinusLogProbMetric: 414.1512 - val_loss: 410.0850 - val_MinusLogProbMetric: 410.0850 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 70/1000
2023-09-09 09:49:39.261 
Epoch 70/1000 
	 loss: 409.3396, MinusLogProbMetric: 409.3396, val_loss: 411.1058, val_MinusLogProbMetric: 411.1058

Epoch 70: val_loss did not improve from 409.03433
196/196 - 11s - loss: 409.3396 - MinusLogProbMetric: 409.3396 - val_loss: 411.1058 - val_MinusLogProbMetric: 411.1058 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 71/1000
2023-09-09 09:49:50.346 
Epoch 71/1000 
	 loss: 411.4382, MinusLogProbMetric: 411.4382, val_loss: 407.6781, val_MinusLogProbMetric: 407.6781

Epoch 71: val_loss improved from 409.03433 to 407.67807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 411.4382 - MinusLogProbMetric: 411.4382 - val_loss: 407.6781 - val_MinusLogProbMetric: 407.6781 - lr: 0.0010 - 11s/epoch - 58ms/step
Epoch 72/1000
2023-09-09 09:50:01.758 
Epoch 72/1000 
	 loss: 409.7568, MinusLogProbMetric: 409.7568, val_loss: 411.2594, val_MinusLogProbMetric: 411.2594

Epoch 72: val_loss did not improve from 407.67807
196/196 - 11s - loss: 409.7568 - MinusLogProbMetric: 409.7568 - val_loss: 411.2594 - val_MinusLogProbMetric: 411.2594 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 73/1000
2023-09-09 09:50:12.829 
Epoch 73/1000 
	 loss: 409.1672, MinusLogProbMetric: 409.1672, val_loss: 407.7689, val_MinusLogProbMetric: 407.7689

Epoch 73: val_loss did not improve from 407.67807
196/196 - 11s - loss: 409.1672 - MinusLogProbMetric: 409.1672 - val_loss: 407.7689 - val_MinusLogProbMetric: 407.7689 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 74/1000
2023-09-09 09:50:23.529 
Epoch 74/1000 
	 loss: 409.1039, MinusLogProbMetric: 409.1039, val_loss: 416.2536, val_MinusLogProbMetric: 416.2536

Epoch 74: val_loss did not improve from 407.67807
196/196 - 11s - loss: 409.1039 - MinusLogProbMetric: 409.1039 - val_loss: 416.2536 - val_MinusLogProbMetric: 416.2536 - lr: 0.0010 - 11s/epoch - 55ms/step
Epoch 75/1000
2023-09-09 09:50:33.981 
Epoch 75/1000 
	 loss: 409.4360, MinusLogProbMetric: 409.4360, val_loss: 409.6830, val_MinusLogProbMetric: 409.6830

Epoch 75: val_loss did not improve from 407.67807
196/196 - 10s - loss: 409.4360 - MinusLogProbMetric: 409.4360 - val_loss: 409.6830 - val_MinusLogProbMetric: 409.6830 - lr: 0.0010 - 10s/epoch - 53ms/step
Epoch 76/1000
2023-09-09 09:50:45.005 
Epoch 76/1000 
	 loss: 408.6952, MinusLogProbMetric: 408.6952, val_loss: 418.2040, val_MinusLogProbMetric: 418.2040

Epoch 76: val_loss did not improve from 407.67807
196/196 - 11s - loss: 408.6952 - MinusLogProbMetric: 408.6952 - val_loss: 418.2040 - val_MinusLogProbMetric: 418.2040 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 77/1000
2023-09-09 09:50:56.081 
Epoch 77/1000 
	 loss: 408.1730, MinusLogProbMetric: 408.1730, val_loss: 412.1012, val_MinusLogProbMetric: 412.1012

Epoch 77: val_loss did not improve from 407.67807
196/196 - 11s - loss: 408.1730 - MinusLogProbMetric: 408.1730 - val_loss: 412.1012 - val_MinusLogProbMetric: 412.1012 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 78/1000
2023-09-09 09:51:06.756 
Epoch 78/1000 
	 loss: 407.9351, MinusLogProbMetric: 407.9351, val_loss: 406.4324, val_MinusLogProbMetric: 406.4324

Epoch 78: val_loss improved from 407.67807 to 406.43240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 407.9351 - MinusLogProbMetric: 407.9351 - val_loss: 406.4324 - val_MinusLogProbMetric: 406.4324 - lr: 0.0010 - 11s/epoch - 56ms/step
Epoch 79/1000
2023-09-09 09:51:18.188 
Epoch 79/1000 
	 loss: 407.9943, MinusLogProbMetric: 407.9943, val_loss: 411.6969, val_MinusLogProbMetric: 411.6969

Epoch 79: val_loss did not improve from 406.43240
196/196 - 11s - loss: 407.9943 - MinusLogProbMetric: 407.9943 - val_loss: 411.6969 - val_MinusLogProbMetric: 411.6969 - lr: 0.0010 - 11s/epoch - 57ms/step
Epoch 80/1000
2023-09-09 09:51:28.856 
Epoch 80/1000 
	 loss: 408.0053, MinusLogProbMetric: 408.0053, val_loss: 409.6143, val_MinusLogProbMetric: 409.6143

Epoch 80: val_loss did not improve from 406.43240
196/196 - 11s - loss: 408.0053 - MinusLogProbMetric: 408.0053 - val_loss: 409.6143 - val_MinusLogProbMetric: 409.6143 - lr: 0.0010 - 11s/epoch - 54ms/step
Epoch 81/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 168: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-09 09:51:38.711 
Epoch 81/1000 
	 loss: nan, MinusLogProbMetric: 461.9008, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 81: val_loss did not improve from 406.43240
196/196 - 10s - loss: nan - MinusLogProbMetric: 461.9008 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 10s/epoch - 50ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 323.
===========
Train data generated in 1.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_323
self.data_kwargs: {'seed': 0}
self.x_data: [[8.210789   4.3589015  5.1949253  ... 2.1186912  8.28666    6.44192   ]
 [5.776202   0.0413513  4.6274076  ... 4.7782993  6.688914   5.570342  ]
 [5.7263374  6.2561994  5.7377024  ... 8.820506   0.58727944 6.515802  ]
 ...
 [5.7884197  0.56533414 4.7867274  ... 4.878848   6.0022035  4.9900055 ]
 [5.9892163  0.222366   4.8566475  ... 4.5662537  6.433208   4.1020565 ]
 [7.8653784  4.6189337  5.157348   ... 2.6189103  7.542449   6.328924  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  4191520   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f11823cd2a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1180282dd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1180282dd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f11823a31c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1180258d90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f11823b04c0>, <keras.callbacks.ModelCheckpoint object at 0x7f11802fa410>, <keras.callbacks.EarlyStopping object at 0x7f117fbcd960>, <keras.callbacks.ReduceLROnPlateau object at 0x7f11823a2b00>, <keras.callbacks.TerminateOnNaN object at 0x7f11801a3a00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 323/360 with hyperparameters:
timestamp = 2023-09-09 09:51:42.633271
ndims = 1000
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 8.21078873e+00  4.35890150e+00  5.19492531e+00  2.84795880e+00
  6.23694658e+00  2.48332572e+00  6.01452780e+00  1.49544990e+00
  1.56305087e+00  4.33542109e+00  3.63919592e+00  2.33469009e+00
  7.12970674e-01  4.46248204e-01  1.55621916e-01  3.05101776e+00
  4.20721769e+00  7.49161625e+00  3.96869850e+00  9.05481625e+00
  4.47121114e-01  2.05655861e+00  3.52404070e+00  5.90453529e+00
  8.73787975e+00  3.53096032e+00  2.76253247e+00  6.50554955e-01
  8.03491688e+00  3.65929008e+00  4.93096781e+00  1.00386086e+01
  5.29845119e-01  1.22730446e+00  7.25914860e+00  7.58729315e+00
  7.71210337e+00  5.14168453e+00  9.58227444e+00  4.79759741e+00
  6.05360174e+00 -3.22920084e-03  4.30121565e+00  7.28324509e+00
  2.30840397e+00  9.72805214e+00  7.01551247e+00  3.15825868e+00
  7.77309942e+00  1.27091324e+00  9.88113785e+00  5.76226175e-01
  9.93493366e+00  6.19022250e-01  7.72215176e+00  1.45727849e+00
  2.63241100e+00  7.56394577e+00  4.26764488e+00  1.30306756e+00
  4.47696209e+00  4.10884666e+00  6.25260830e+00  7.17153311e+00
  7.79660606e+00  4.86149597e+00  4.01030636e+00  1.48931491e+00
  8.25993347e+00  6.99169493e+00  4.15396929e+00  5.40362930e+00
  6.69921494e+00  1.80113685e+00 -2.28963286e-01  3.53704309e+00
  5.97519684e+00  5.03495884e+00  5.96093559e+00  8.27530861e+00
  5.11110401e+00  1.71916723e-01  1.00485516e+01  5.39971399e+00
  6.97965956e+00  6.67698717e+00  9.53224850e+00  2.60424829e+00
  6.92656088e+00  2.48836160e+00  1.50950837e+00  3.11806774e+00
  3.62086797e+00  9.48242664e+00  2.35715413e+00  1.47683930e+00
  6.57032585e+00  2.31288910e+00  5.07415724e+00  8.22896194e+00
  2.67307043e+00  4.70531940e+00  5.81045723e+00  3.97613907e+00
  1.00836468e+01  6.80917358e+00  7.91102648e+00  9.83973694e+00
  3.97975469e+00  2.81087971e+00  6.61194921e-01  2.09042239e+00
  1.80242181e+00  2.45311332e+00  7.75794411e+00  5.35263777e+00
  6.36038160e+00  8.03104973e+00  2.23086929e+00  3.26186609e+00
  7.10611820e+00  4.36860800e+00  5.38674593e-01  5.03258133e+00
  8.79059315e+00  4.12663126e+00  2.96597481e+00  9.92143154e+00
  6.63155603e+00  2.29087067e+00  9.67418194e-01  8.54019165e+00
  7.32748413e+00  9.67831326e+00  3.08930302e+00  1.43350685e+00
  7.23957300e+00  2.06572795e+00  1.31994998e+00  2.38906145e+00
  8.15470123e+00  4.92610788e+00  4.60461289e-01  4.69264269e+00
  7.11180496e+00  6.15540504e+00  5.58532810e+00  1.97622848e+00
  8.55810928e+00  5.60682297e+00  6.50732088e+00  9.74595356e+00
  3.70203424e+00  8.65994740e+00  3.66820455e-01 -1.19887441e-01
  5.62859249e+00  3.21332264e+00  2.41353798e+00  2.97626567e+00
  3.33519387e+00  7.60388327e+00  5.94638872e+00  7.90076303e+00
  8.14614868e+00  8.43794918e+00  5.11629820e+00  4.45509529e+00
  8.96594524e+00  6.83408117e+00  5.29020429e-01  5.47313118e+00
  3.56114149e+00  9.86122417e+00 -8.47060025e-01  5.97660685e+00
  7.56903505e+00  3.27315211e+00  9.39664459e+00  9.18616390e+00
  9.16251087e+00  3.12576860e-01  5.81769562e+00  2.36430216e+00
  8.91152668e+00  6.80723286e+00  5.49286032e+00  1.25512540e+00
  6.51518726e+00  6.32031965e+00  9.03301430e+00  5.99218893e+00
  9.68340588e+00  9.75206280e+00  6.48233700e+00  5.02287769e+00
  4.40520668e+00  8.71606636e+00  1.15936518e-01  1.15061617e+00
  1.13465822e+00  7.72926044e+00  3.59013176e+00  6.88610840e+00
  6.62146950e+00  3.82466817e+00  5.06336927e+00  6.39645910e+00
  9.77296638e+00  6.23589993e+00  6.13958120e+00  6.46353066e-01
  7.98168039e+00  5.24744987e+00  8.84934807e+00  7.04309762e-01
  4.59585285e+00  2.57471061e+00  2.76045990e+00  7.58923578e+00
  6.31161690e+00  3.93680930e+00  3.41281295e-03  9.66006947e+00
  9.57943439e+00  2.44953036e+00  7.60993719e-01  5.62422276e+00
  9.45851040e+00  9.12294960e+00  5.39434481e+00  2.33969688e+00
  3.87983751e+00  8.94526768e+00  4.94882011e+00  7.46908426e-01
  3.23506546e+00  2.77883267e+00  1.57590222e+00  5.10829020e+00
  9.52500534e+00  6.17966223e+00  1.10281193e+00  5.18856943e-01
  2.46926522e+00  3.78143835e+00  6.96128178e+00  1.00144138e+01
  3.66667557e+00  5.97449923e+00  9.95713520e+00  7.44061518e+00
  6.02692795e+00  7.26248920e-01  3.43354082e+00  1.11553822e+01
  1.09044552e+00  8.32242012e+00  7.46725512e+00  3.50707126e+00
  4.66837549e+00  6.27135944e+00  7.01602077e+00  4.99376917e+00
  1.22947979e+00  6.21938133e+00  4.57749987e+00  9.84504700e+00
  3.30335212e+00  2.10937715e+00  8.59310150e-01  5.95748758e+00
  7.51813459e+00  1.18187451e+00  3.30510187e+00  6.36148548e+00
  4.50776958e+00  4.50498581e+00  1.05220985e+01  9.05682564e+00
  6.02789402e+00  7.13881588e+00  7.25975084e+00  6.13174152e+00
  2.50386310e+00  6.21227741e+00  6.38945484e+00  8.56712151e+00
  3.63186836e-01  9.32480907e+00  5.54315186e+00 -3.98739636e-01
  3.37429929e+00  7.95210361e+00  5.01406813e+00  9.63213921e+00
  3.99637389e+00  2.97415352e+00  8.29203606e+00  6.00829792e+00
  6.21376610e+00  7.23727989e+00  6.71069527e+00  5.01448536e+00
  9.13507938e-01  6.87569332e+00  9.86827469e+00  4.45279694e+00
  8.80540848e+00  5.50182295e+00  6.56988764e+00  8.22664070e+00
  6.11226082e+00  9.90539372e-01  8.70905399e+00  2.35227823e+00
  6.95939922e+00  4.88893890e+00  4.05508137e+00  1.27094746e+00
  7.41610336e+00  9.57014084e-01  1.03208637e+01  8.77536011e+00
  5.40437126e+00  3.66490221e+00  1.92103791e+00  5.05432844e+00
  6.88084650e+00  2.36994123e+00  4.34519386e+00  1.71688604e+00
  3.39162683e+00  1.85334861e+00  9.56473351e+00  6.55594063e+00
  7.24569368e+00  9.57989931e-01  3.30405354e+00  8.91251564e+00
  9.43958402e-01  2.91835928e+00  6.76908159e+00  6.41434669e+00
  8.02711487e+00  8.37068748e+00  1.76286387e+00  2.83487058e+00
  4.26155806e+00 -1.64624956e-02  5.80787063e-02  2.90215087e+00
  4.82958937e+00  2.67308688e+00  8.74459743e+00  3.35418940e+00
  9.50176620e+00  9.24923992e+00  5.82722712e+00  3.35397267e+00
  3.11244607e+00  7.65114069e+00  2.58552289e+00  3.75996971e+00
  5.17049551e+00  5.17774963e+00  9.58946896e+00  2.46264791e+00
  4.03688717e+00  2.72447777e+00  2.75106120e+00  1.24247396e+00
  2.18880701e+00  8.80450439e+00  2.16815996e+00  4.50869846e+00
  9.47779369e+00  6.92755127e+00  5.21426487e+00  7.63384438e+00
  1.33627713e+00  5.05314064e+00  8.01588058e-01  2.33411765e+00
  2.04597259e+00  3.71735334e+00  3.99399781e+00  9.83961201e+00
  2.91163659e+00  3.73780441e+00  7.86999273e+00  7.62870979e+00
  7.85087109e+00  2.87814641e+00  5.46488953e+00  1.44017196e+00
  2.16978312e+00  4.34651899e+00  7.40160513e+00  5.01842690e+00
  8.79978561e+00  8.01273918e+00  6.23900414e-01  5.34396315e+00
  5.84303570e+00  9.49287987e+00 -2.03253716e-01  5.55819988e+00
  2.56452465e+00  5.52358687e-01  5.80285740e+00  8.33415508e+00
  8.18197632e+00 -8.59678984e-02  3.97874522e+00  4.08348382e-01
  2.77977419e+00  2.01314735e+00  2.23359156e+00  3.15110850e+00
  3.16415262e+00  9.69314194e+00  3.46537042e+00  8.90620518e+00
  1.80000591e+00  6.60689974e+00 -7.20865309e-01  4.78756142e+00
 -1.09752685e-01  9.03241920e+00  8.91885853e+00  6.74625039e-01
  9.64687729e+00  5.28304863e+00  3.44711399e+00  9.08611012e+00
  3.02275133e+00  4.11477327e+00  1.01315606e+00  7.55140066e+00
  7.66734123e-01  4.76552486e+00  6.70099640e+00  7.79399300e+00
  1.63697755e+00  6.94904470e+00  6.56399965e+00  4.96204281e+00
  5.89378309e+00  5.83356380e-01  6.12774992e+00  4.05969334e+00
  8.60876942e+00  3.77930760e+00  5.77260780e+00  4.28877592e+00
  9.17067528e+00  2.55027103e+00  1.33500242e+00  8.23266697e+00
  4.27657557e+00  5.92882109e+00  3.65143597e-01  1.05127277e+01
  9.12497520e+00  4.73808956e+00  1.15415120e+00  4.24439526e+00
  9.80254364e+00  7.13764381e+00  2.21075535e+00  4.46259594e+00
  9.60248947e+00  6.37608194e+00  3.89600515e-01  6.30370569e+00
  2.47006059e+00  9.89494145e-01  4.10520649e+00  5.44608831e+00
  5.12012959e-01  4.04597473e+00  5.92461109e+00  8.09785938e+00
  5.32709646e+00  5.15376234e+00  7.13395691e+00  8.32365990e+00
  4.20349646e+00  6.05286407e+00  6.26060724e+00  6.60055876e-03
  8.09364128e+00  3.49219418e+00  7.10890245e+00  4.91867685e+00
  7.30661583e+00  6.67151976e+00  2.00603342e+00  6.53424406e+00
  8.17309189e+00  2.37493563e+00  7.19742537e+00  6.21698093e+00
  1.02654848e+01  4.02459478e+00  2.00096679e+00  3.60146332e+00
  2.72641587e+00  7.55267096e+00  7.33097744e+00  6.48247480e+00
  1.84161115e+00  8.67273045e+00  1.03891554e+01  5.11711931e+00
  6.14349842e+00  4.63130045e+00  1.04338384e+00  3.16097307e+00
  2.48957181e+00  2.08219433e+00  9.02428818e+00  7.58692598e+00
  3.53382182e+00  2.87967825e+00  9.43597221e+00  1.85656917e+00
  5.57112312e+00  9.00374889e-01  1.43457806e+00  5.70213127e+00
  6.18463993e+00  5.21287870e+00  8.30662155e+00  9.78884792e+00
  8.07082462e+00  4.94385624e+00  9.57595062e+00  4.81561422e-01
  5.93677044e+00  6.03051233e+00  1.27146268e+00  9.59977341e+00
  3.53188229e+00  5.92599010e+00  8.90297794e+00  5.73115826e+00
  8.29036653e-01  6.28442287e+00  4.10012245e+00  8.16020679e+00
  9.57242489e+00  8.73237801e+00  3.82743692e+00  7.33982182e+00
  9.39437389e+00  5.32039928e+00  4.49351358e+00  7.41679072e-01
  7.33104944e+00  8.68616486e+00  2.54391122e+00  2.88136458e+00
  5.05854321e+00  4.12581384e-01  7.20838428e-01  4.39249325e+00
  2.12579203e+00  5.69973421e+00  3.94032979e+00  1.01283722e+01
  9.79717064e+00  2.52009940e+00  8.86107635e+00  8.79512429e-01
  4.84236622e+00  1.72291732e+00  3.82897258e+00  6.94665861e+00
  4.18651247e+00  8.54575062e+00  7.19269276e+00  9.72444630e+00
  5.51988173e+00  8.01771069e+00  3.28205919e+00  3.97813463e+00
  4.18136644e+00  5.16412115e+00  1.75697100e+00  5.15417719e+00
  1.50469279e+00  9.25226021e+00  4.35976595e-01  4.19854736e+00
  3.64054823e+00  1.06782198e-01  8.30288601e+00  7.06305122e+00
  1.87979758e+00  6.35672998e+00  8.16109753e+00  6.51096630e+00
  5.61127961e-01  6.76316595e+00  6.57550716e+00  7.76742983e+00
  5.40051842e+00  4.99695921e+00  1.05247574e+01  7.60159779e+00
  5.60209799e+00  3.34395909e+00  9.19912910e+00  4.68194294e+00
  1.00466835e+00  5.51672554e+00  8.06357193e+00  4.60604143e+00
  7.99039245e-01  6.11217165e+00  9.98776340e+00  7.64210176e+00
  6.11796856e+00  9.89960003e+00  7.96584511e+00  6.26075697e+00
  4.63107634e+00  7.38753414e+00  6.49842548e+00  7.23403335e-01
  7.84123564e+00  3.43258095e+00  5.87750053e+00  8.16937256e+00
  4.84820747e+00  1.59829724e+00  4.18775225e+00  1.30482543e+00
  6.89824677e+00  9.70094967e+00  7.83443594e+00  4.79529858e+00
  6.26568747e+00  3.08977580e+00  5.72115278e+00  2.92952847e+00
  7.45006084e+00  2.38916183e+00  9.71957111e+00  6.54768562e+00
  6.25139415e-01 -2.21325979e-02  1.01162004e+01  1.64097524e+00
  3.30466270e+00  2.32712507e+00  9.82605267e+00  5.42485619e+00
  4.57557726e+00  5.65397930e+00  2.95226574e+00  6.00716829e+00
  4.69566488e+00  1.86493647e+00  5.55787706e+00  4.29599905e+00
  8.97355652e+00  4.49551582e+00  5.21416712e+00  7.87060261e+00
  4.67125940e+00  9.76075363e+00  8.19973755e+00  8.41541100e+00
  9.90537739e+00  2.93996453e+00  5.85271358e+00  7.30432892e+00
  7.22648525e+00  9.17044544e+00  2.83920431e+00  3.20150447e+00
  8.31029034e+00  7.78128242e+00  2.25734663e+00  5.91900253e+00
  7.87056684e+00  3.05571938e+00  3.93086529e+00  5.76315022e+00
  5.30861235e+00  7.55313301e+00 -1.47460684e-01  4.09288597e+00
  1.74171937e+00  4.78674221e+00  7.88887119e+00  7.54741371e-01
  1.54648781e+00  7.28928089e+00  7.57703447e+00  5.61207056e+00
  9.85756111e+00  4.10112953e+00  1.57129598e+00  6.85865021e+00
  6.33887672e+00  6.33726215e+00  5.35512924e+00  8.78589249e+00
  8.93958569e-01 -1.22548640e-01  1.54779208e+00  2.83668208e+00
  5.92615891e+00  5.92532015e+00  2.29801345e+00  4.49938583e+00
  4.36498499e+00  5.37602377e+00  8.03752041e+00  5.66677046e+00
  8.83142376e+00  1.65858328e+00  5.42761660e+00  8.26859760e+00
  1.79626775e+00  4.40248251e+00  5.96399355e+00  2.07387352e+00
  7.58685493e+00  6.19924068e-01  9.63695049e+00  4.80245113e+00
  2.84362292e+00  1.11408031e+00  4.65407324e+00  6.60547924e+00
  1.22691751e+00  3.05578947e+00  1.09281607e+01  6.40130091e+00
  2.10371280e+00  1.27200794e+00  8.89848232e+00  2.80182910e+00
  2.19280529e+00  1.05606747e+01  2.94065785e+00  1.69049454e+00
  5.48146868e+00  4.31994724e+00  9.06744099e+00  1.15592277e+00
 -7.79310703e-01  1.04373407e+00  8.86706924e+00  4.28514481e+00
  1.81781006e+00  5.78969526e+00  5.54994345e+00  2.30405951e+00
  5.54053879e+00  2.74468279e+00  2.27062321e+00  4.07878542e+00
  2.84625888e+00  2.04214787e+00  8.94879723e+00  6.88817441e-01
  2.71495771e+00  3.84625363e+00  2.55016398e+00  2.09636283e+00
  7.99209976e+00  3.07564521e+00  1.11745620e+00  5.30355263e+00
  9.93744850e-01  2.74349833e+00  8.41612911e+00  4.34795761e+00
  6.55439472e+00  6.15104818e+00  5.19303381e-01  6.36637783e+00
  8.49066544e+00  8.24014544e-01  6.90668011e+00  7.08179665e+00
  9.01312637e+00  3.36554408e+00  2.87336349e+00  6.86589861e+00
 -2.01100767e-01  7.31349230e+00  8.66450596e+00  4.29815626e+00
  3.38823771e+00  3.01338983e+00  8.08154404e-01  1.78297210e+00
  9.61242580e+00  8.81327534e+00  2.97019339e+00  8.63016319e+00
  3.02107668e+00  2.72670150e+00  2.13139439e+00  8.94742870e+00
  9.83511925e+00  3.19685555e+00  6.81030130e+00  3.50014639e+00
  1.05297625e+00  4.07330465e+00  5.53509521e+00  1.18256664e+00
  8.23826122e+00  5.53470898e+00  2.24809480e+00  5.40734196e+00
  7.84483671e+00  2.75392199e+00  7.62425852e+00  4.27786589e+00
  9.72611332e+00  8.83753872e+00  3.43726850e+00  1.27960294e-01
  7.29834032e+00  2.04023504e+00  1.30689442e+00  9.04135704e+00
  1.00866823e+01  2.58009624e+00  3.01804805e+00  1.68803823e+00
  4.80738878e+00  8.68211079e+00  6.73749733e+00  8.63989162e+00
  2.21114421e+00  7.04848576e+00  9.34753704e+00  2.76184368e+00
  6.90854836e+00  4.05415773e+00  3.81649780e+00  9.40583229e+00
  4.05083704e+00  8.92635822e-01  3.82358503e+00  4.43289518e+00
  7.26910877e+00  8.82122993e+00  8.22476578e+00  9.33980370e+00
  4.71105146e+00  6.95373154e+00  3.06833911e+00 -4.22824502e-01
  3.65492988e+00  6.86570597e+00  2.05108023e+00  8.74809170e+00
  8.31052685e+00  1.87380075e+00  2.81262565e+00  8.66123104e+00
  3.31760716e+00  3.31471944e+00  3.48811197e+00  7.28613329e+00
 -9.66772735e-02 -7.69796014e-01  6.32836771e+00  3.86802888e+00
  5.99920893e+00  2.40654874e+00  9.12174988e+00  8.34914875e+00
  7.07096219e-01  7.57762194e+00  9.58280849e+00  8.65084171e+00
  6.67687321e+00  1.21811235e+00  5.86707783e+00  3.30112028e+00
  1.01402960e+01  7.17766523e+00  5.74374008e+00  6.99432039e+00
  4.40368319e+00  2.86542940e+00  9.42987251e+00  3.30265784e+00
  3.83230895e-01  4.20491457e+00  8.71614361e+00  7.57816601e+00
  9.77553654e+00  6.46106720e-01  9.44217777e+00  4.46305180e+00
  4.83198547e+00  3.06845570e+00  1.06641746e+00  4.49923372e+00
  7.57343435e+00  5.86748123e-03  1.58664107e+00  5.31323195e-01
  8.55088329e+00  6.48872995e+00  1.54820132e+00  7.71537733e+00
  8.20388412e+00  9.80402565e+00  9.79537868e+00 -8.38677704e-01
  5.94942427e+00  2.88661289e+00  5.26643085e+00  3.18921828e+00
  7.79481411e-01  4.42821693e+00  5.33531380e+00  9.32881355e+00
  2.65765786e-02  3.08747840e+00  8.52437019e+00  6.85948515e+00
  8.21984768e+00  6.24451685e+00  7.30141687e+00  4.90936804e+00
  2.20824790e+00  8.48395348e+00  5.67844057e+00  1.24840343e+00
  8.05260468e+00  9.64081573e+00  3.07099938e+00  3.18240905e+00
  9.12044704e-01  5.96499348e+00  9.19813156e+00  3.36040854e-01
  4.73263264e+00  1.30067658e+00  5.19609153e-01  6.70743704e+00
  8.23411751e+00  3.52812862e+00  8.22581482e+00  1.19572031e+00
  2.63071656e+00  9.79853821e+00  4.98861170e+00  6.00117397e+00
  3.19664389e-01  1.32244027e+00  6.05848253e-01  7.35753870e+00
  1.13696325e+00  8.69066429e+00  3.58892393e+00  7.88068628e+00
  3.04448676e+00  2.65313506e+00  9.56735551e-01  2.75976801e+00
  4.98727465e+00  7.08365965e+00  5.05365181e+00  9.17652988e+00
  5.92350817e+00  1.26345801e+00  3.95679140e+00  7.46081781e+00
  2.64605379e+00  6.97033882e+00  5.93741179e+00  1.01776525e-01
  7.47330189e+00  5.93837404e+00  8.12254488e-01  3.60206747e+00
  3.23938537e+00  8.24193764e+00  3.56294060e+00  9.41426849e+00
  5.64243269e+00  2.11869121e+00  8.28666019e+00  6.44191980e+00]
Epoch 1/1000
2023-09-09 09:52:21.585 
Epoch 1/1000 
	 loss: 459.5994, MinusLogProbMetric: 459.5994, val_loss: 404.3336, val_MinusLogProbMetric: 404.3336

Epoch 1: val_loss improved from inf to 404.33356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 39s - loss: 459.5994 - MinusLogProbMetric: 459.5994 - val_loss: 404.3336 - val_MinusLogProbMetric: 404.3336 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 2/1000
2023-09-09 09:52:33.085 
Epoch 2/1000 
	 loss: 401.9029, MinusLogProbMetric: 401.9029, val_loss: 403.1487, val_MinusLogProbMetric: 403.1487

Epoch 2: val_loss improved from 404.33356 to 403.14871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 401.9029 - MinusLogProbMetric: 401.9029 - val_loss: 403.1487 - val_MinusLogProbMetric: 403.1487 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 3/1000
2023-09-09 09:52:44.626 
Epoch 3/1000 
	 loss: 401.7671, MinusLogProbMetric: 401.7671, val_loss: 403.9845, val_MinusLogProbMetric: 403.9845

Epoch 3: val_loss did not improve from 403.14871
196/196 - 11s - loss: 401.7671 - MinusLogProbMetric: 401.7671 - val_loss: 403.9845 - val_MinusLogProbMetric: 403.9845 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 4/1000
2023-09-09 09:52:55.974 
Epoch 4/1000 
	 loss: 402.5855, MinusLogProbMetric: 402.5855, val_loss: 402.7451, val_MinusLogProbMetric: 402.7451

Epoch 4: val_loss improved from 403.14871 to 402.74506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 402.5855 - MinusLogProbMetric: 402.5855 - val_loss: 402.7451 - val_MinusLogProbMetric: 402.7451 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 5/1000
2023-09-09 09:53:07.790 
Epoch 5/1000 
	 loss: 401.3118, MinusLogProbMetric: 401.3118, val_loss: 402.5731, val_MinusLogProbMetric: 402.5731

Epoch 5: val_loss improved from 402.74506 to 402.57306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 401.3118 - MinusLogProbMetric: 401.3118 - val_loss: 402.5731 - val_MinusLogProbMetric: 402.5731 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 6/1000
2023-09-09 09:53:19.487 
Epoch 6/1000 
	 loss: 401.5664, MinusLogProbMetric: 401.5664, val_loss: 402.2536, val_MinusLogProbMetric: 402.2536

Epoch 6: val_loss improved from 402.57306 to 402.25357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 401.5664 - MinusLogProbMetric: 401.5664 - val_loss: 402.2536 - val_MinusLogProbMetric: 402.2536 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 7/1000
2023-09-09 09:53:31.482 
Epoch 7/1000 
	 loss: 401.5686, MinusLogProbMetric: 401.5686, val_loss: 404.0451, val_MinusLogProbMetric: 404.0451

Epoch 7: val_loss did not improve from 402.25357
196/196 - 12s - loss: 401.5686 - MinusLogProbMetric: 401.5686 - val_loss: 404.0451 - val_MinusLogProbMetric: 404.0451 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 8/1000
2023-09-09 09:53:41.836 
Epoch 8/1000 
	 loss: 401.6069, MinusLogProbMetric: 401.6069, val_loss: 407.6233, val_MinusLogProbMetric: 407.6233

Epoch 8: val_loss did not improve from 402.25357
196/196 - 10s - loss: 401.6069 - MinusLogProbMetric: 401.6069 - val_loss: 407.6233 - val_MinusLogProbMetric: 407.6233 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 9/1000
2023-09-09 09:53:53.175 
Epoch 9/1000 
	 loss: 401.5251, MinusLogProbMetric: 401.5251, val_loss: 402.4760, val_MinusLogProbMetric: 402.4760

Epoch 9: val_loss did not improve from 402.25357
196/196 - 11s - loss: 401.5251 - MinusLogProbMetric: 401.5251 - val_loss: 402.4760 - val_MinusLogProbMetric: 402.4760 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 10/1000
2023-09-09 09:54:04.505 
Epoch 10/1000 
	 loss: 401.5763, MinusLogProbMetric: 401.5763, val_loss: 401.7560, val_MinusLogProbMetric: 401.7560

Epoch 10: val_loss improved from 402.25357 to 401.75604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 401.5763 - MinusLogProbMetric: 401.5763 - val_loss: 401.7560 - val_MinusLogProbMetric: 401.7560 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 11/1000
2023-09-09 09:54:16.197 
Epoch 11/1000 
	 loss: 401.0717, MinusLogProbMetric: 401.0717, val_loss: 403.5631, val_MinusLogProbMetric: 403.5631

Epoch 11: val_loss did not improve from 401.75604
196/196 - 11s - loss: 401.0717 - MinusLogProbMetric: 401.0717 - val_loss: 403.5631 - val_MinusLogProbMetric: 403.5631 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 12/1000
2023-09-09 09:54:27.214 
Epoch 12/1000 
	 loss: 400.9074, MinusLogProbMetric: 400.9074, val_loss: 401.4777, val_MinusLogProbMetric: 401.4777

Epoch 12: val_loss improved from 401.75604 to 401.47766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 400.9074 - MinusLogProbMetric: 400.9074 - val_loss: 401.4777 - val_MinusLogProbMetric: 401.4777 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 13/1000
2023-09-09 09:54:38.709 
Epoch 13/1000 
	 loss: 400.8143, MinusLogProbMetric: 400.8143, val_loss: 404.4340, val_MinusLogProbMetric: 404.4340

Epoch 13: val_loss did not improve from 401.47766
196/196 - 11s - loss: 400.8143 - MinusLogProbMetric: 400.8143 - val_loss: 404.4340 - val_MinusLogProbMetric: 404.4340 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 14/1000
2023-09-09 09:54:49.518 
Epoch 14/1000 
	 loss: 400.9111, MinusLogProbMetric: 400.9111, val_loss: 401.2899, val_MinusLogProbMetric: 401.2899

Epoch 14: val_loss improved from 401.47766 to 401.28986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 400.9111 - MinusLogProbMetric: 400.9111 - val_loss: 401.2899 - val_MinusLogProbMetric: 401.2899 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 15/1000
2023-09-09 09:55:00.893 
Epoch 15/1000 
	 loss: 400.9583, MinusLogProbMetric: 400.9583, val_loss: 401.5800, val_MinusLogProbMetric: 401.5800

Epoch 15: val_loss did not improve from 401.28986
196/196 - 11s - loss: 400.9583 - MinusLogProbMetric: 400.9583 - val_loss: 401.5800 - val_MinusLogProbMetric: 401.5800 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 16/1000
2023-09-09 09:55:12.384 
Epoch 16/1000 
	 loss: 400.5652, MinusLogProbMetric: 400.5652, val_loss: 400.6251, val_MinusLogProbMetric: 400.6251

Epoch 16: val_loss improved from 401.28986 to 400.62512, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 400.5652 - MinusLogProbMetric: 400.5652 - val_loss: 400.6251 - val_MinusLogProbMetric: 400.6251 - lr: 3.3333e-04 - 12s/epoch - 61ms/step
Epoch 17/1000
2023-09-09 09:55:24.275 
Epoch 17/1000 
	 loss: 400.3621, MinusLogProbMetric: 400.3621, val_loss: 401.1418, val_MinusLogProbMetric: 401.1418

Epoch 17: val_loss did not improve from 400.62512
196/196 - 11s - loss: 400.3621 - MinusLogProbMetric: 400.3621 - val_loss: 401.1418 - val_MinusLogProbMetric: 401.1418 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 18/1000
2023-09-09 09:55:35.560 
Epoch 18/1000 
	 loss: 400.2635, MinusLogProbMetric: 400.2635, val_loss: 401.6040, val_MinusLogProbMetric: 401.6040

Epoch 18: val_loss did not improve from 400.62512
196/196 - 11s - loss: 400.2635 - MinusLogProbMetric: 400.2635 - val_loss: 401.6040 - val_MinusLogProbMetric: 401.6040 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 19/1000
2023-09-09 09:55:46.637 
Epoch 19/1000 
	 loss: 400.0090, MinusLogProbMetric: 400.0090, val_loss: 400.5381, val_MinusLogProbMetric: 400.5381

Epoch 19: val_loss improved from 400.62512 to 400.53809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 400.0090 - MinusLogProbMetric: 400.0090 - val_loss: 400.5381 - val_MinusLogProbMetric: 400.5381 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 20/1000
2023-09-09 09:55:57.191 
Epoch 20/1000 
	 loss: 400.2336, MinusLogProbMetric: 400.2336, val_loss: 402.3467, val_MinusLogProbMetric: 402.3467

Epoch 20: val_loss did not improve from 400.53809
196/196 - 10s - loss: 400.2336 - MinusLogProbMetric: 400.2336 - val_loss: 402.3467 - val_MinusLogProbMetric: 402.3467 - lr: 3.3333e-04 - 10s/epoch - 52ms/step
Epoch 21/1000
2023-09-09 09:56:07.626 
Epoch 21/1000 
	 loss: 400.7478, MinusLogProbMetric: 400.7478, val_loss: 400.6452, val_MinusLogProbMetric: 400.6452

Epoch 21: val_loss did not improve from 400.53809
196/196 - 10s - loss: 400.7478 - MinusLogProbMetric: 400.7478 - val_loss: 400.6452 - val_MinusLogProbMetric: 400.6452 - lr: 3.3333e-04 - 10s/epoch - 53ms/step
Epoch 22/1000
2023-09-09 09:56:18.596 
Epoch 22/1000 
	 loss: 399.8090, MinusLogProbMetric: 399.8090, val_loss: 400.0944, val_MinusLogProbMetric: 400.0944

Epoch 22: val_loss improved from 400.53809 to 400.09436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 399.8090 - MinusLogProbMetric: 399.8090 - val_loss: 400.0944 - val_MinusLogProbMetric: 400.0944 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 23/1000
2023-09-09 09:56:29.550 
Epoch 23/1000 
	 loss: 399.7708, MinusLogProbMetric: 399.7708, val_loss: 401.5252, val_MinusLogProbMetric: 401.5252

Epoch 23: val_loss did not improve from 400.09436
196/196 - 11s - loss: 399.7708 - MinusLogProbMetric: 399.7708 - val_loss: 401.5252 - val_MinusLogProbMetric: 401.5252 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 24/1000
2023-09-09 09:56:40.217 
Epoch 24/1000 
	 loss: 399.9919, MinusLogProbMetric: 399.9919, val_loss: 400.4818, val_MinusLogProbMetric: 400.4818

Epoch 24: val_loss did not improve from 400.09436
196/196 - 11s - loss: 399.9919 - MinusLogProbMetric: 399.9919 - val_loss: 400.4818 - val_MinusLogProbMetric: 400.4818 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 25/1000
2023-09-09 09:56:51.036 
Epoch 25/1000 
	 loss: 398.7458, MinusLogProbMetric: 398.7458, val_loss: 400.7801, val_MinusLogProbMetric: 400.7801

Epoch 25: val_loss did not improve from 400.09436
196/196 - 11s - loss: 398.7458 - MinusLogProbMetric: 398.7458 - val_loss: 400.7801 - val_MinusLogProbMetric: 400.7801 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 26/1000
2023-09-09 09:57:02.156 
Epoch 26/1000 
	 loss: 420.2525, MinusLogProbMetric: 420.2525, val_loss: 436.9236, val_MinusLogProbMetric: 436.9236

Epoch 26: val_loss did not improve from 400.09436
196/196 - 11s - loss: 420.2525 - MinusLogProbMetric: 420.2525 - val_loss: 436.9236 - val_MinusLogProbMetric: 436.9236 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 27/1000
2023-09-09 09:57:13.467 
Epoch 27/1000 
	 loss: 404.9533, MinusLogProbMetric: 404.9533, val_loss: 401.8300, val_MinusLogProbMetric: 401.8300

Epoch 27: val_loss did not improve from 400.09436
196/196 - 11s - loss: 404.9533 - MinusLogProbMetric: 404.9533 - val_loss: 401.8300 - val_MinusLogProbMetric: 401.8300 - lr: 3.3333e-04 - 11s/epoch - 58ms/step
Epoch 28/1000
2023-09-09 09:57:24.563 
Epoch 28/1000 
	 loss: 399.5512, MinusLogProbMetric: 399.5512, val_loss: 403.3491, val_MinusLogProbMetric: 403.3491

Epoch 28: val_loss did not improve from 400.09436
196/196 - 11s - loss: 399.5512 - MinusLogProbMetric: 399.5512 - val_loss: 403.3491 - val_MinusLogProbMetric: 403.3491 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 29/1000
2023-09-09 09:57:35.988 
Epoch 29/1000 
	 loss: 399.2880, MinusLogProbMetric: 399.2880, val_loss: 399.8625, val_MinusLogProbMetric: 399.8625

Epoch 29: val_loss improved from 400.09436 to 399.86246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 399.2880 - MinusLogProbMetric: 399.2880 - val_loss: 399.8625 - val_MinusLogProbMetric: 399.8625 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 30/1000
2023-09-09 09:57:47.191 
Epoch 30/1000 
	 loss: 398.8678, MinusLogProbMetric: 398.8678, val_loss: 400.3320, val_MinusLogProbMetric: 400.3320

Epoch 30: val_loss did not improve from 399.86246
196/196 - 11s - loss: 398.8678 - MinusLogProbMetric: 398.8678 - val_loss: 400.3320 - val_MinusLogProbMetric: 400.3320 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 31/1000
2023-09-09 09:57:58.483 
Epoch 31/1000 
	 loss: 398.4806, MinusLogProbMetric: 398.4806, val_loss: 399.4964, val_MinusLogProbMetric: 399.4964

Epoch 31: val_loss improved from 399.86246 to 399.49637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 398.4806 - MinusLogProbMetric: 398.4806 - val_loss: 399.4964 - val_MinusLogProbMetric: 399.4964 - lr: 3.3333e-04 - 12s/epoch - 59ms/step
Epoch 32/1000
2023-09-09 09:58:09.955 
Epoch 32/1000 
	 loss: 398.6026, MinusLogProbMetric: 398.6026, val_loss: 407.6744, val_MinusLogProbMetric: 407.6744

Epoch 32: val_loss did not improve from 399.49637
196/196 - 11s - loss: 398.6026 - MinusLogProbMetric: 398.6026 - val_loss: 407.6744 - val_MinusLogProbMetric: 407.6744 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 33/1000
2023-09-09 09:58:21.112 
Epoch 33/1000 
	 loss: 398.4446, MinusLogProbMetric: 398.4446, val_loss: 399.6220, val_MinusLogProbMetric: 399.6220

Epoch 33: val_loss did not improve from 399.49637
196/196 - 11s - loss: 398.4446 - MinusLogProbMetric: 398.4446 - val_loss: 399.6220 - val_MinusLogProbMetric: 399.6220 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 34/1000
2023-09-09 09:58:31.997 
Epoch 34/1000 
	 loss: 398.0677, MinusLogProbMetric: 398.0677, val_loss: 399.6190, val_MinusLogProbMetric: 399.6190

Epoch 34: val_loss did not improve from 399.49637
196/196 - 11s - loss: 398.0677 - MinusLogProbMetric: 398.0677 - val_loss: 399.6190 - val_MinusLogProbMetric: 399.6190 - lr: 3.3333e-04 - 11s/epoch - 56ms/step
Epoch 35/1000
2023-09-09 09:58:43.151 
Epoch 35/1000 
	 loss: 397.9676, MinusLogProbMetric: 397.9676, val_loss: 402.8961, val_MinusLogProbMetric: 402.8961

Epoch 35: val_loss did not improve from 399.49637
196/196 - 11s - loss: 397.9676 - MinusLogProbMetric: 397.9676 - val_loss: 402.8961 - val_MinusLogProbMetric: 402.8961 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 36/1000
2023-09-09 09:58:53.572 
Epoch 36/1000 
	 loss: 398.4509, MinusLogProbMetric: 398.4509, val_loss: 399.4404, val_MinusLogProbMetric: 399.4404

Epoch 36: val_loss improved from 399.49637 to 399.44037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 398.4509 - MinusLogProbMetric: 398.4509 - val_loss: 399.4404 - val_MinusLogProbMetric: 399.4404 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 37/1000
2023-09-09 09:59:05.192 
Epoch 37/1000 
	 loss: 397.9847, MinusLogProbMetric: 397.9847, val_loss: 399.7645, val_MinusLogProbMetric: 399.7645

Epoch 37: val_loss did not improve from 399.44037
196/196 - 11s - loss: 397.9847 - MinusLogProbMetric: 397.9847 - val_loss: 399.7645 - val_MinusLogProbMetric: 399.7645 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 38/1000
2023-09-09 09:59:16.299 
Epoch 38/1000 
	 loss: 397.7425, MinusLogProbMetric: 397.7425, val_loss: 402.5478, val_MinusLogProbMetric: 402.5478

Epoch 38: val_loss did not improve from 399.44037
196/196 - 11s - loss: 397.7425 - MinusLogProbMetric: 397.7425 - val_loss: 402.5478 - val_MinusLogProbMetric: 402.5478 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 39/1000
2023-09-09 09:59:27.705 
Epoch 39/1000 
	 loss: 398.2047, MinusLogProbMetric: 398.2047, val_loss: 398.5723, val_MinusLogProbMetric: 398.5723

Epoch 39: val_loss improved from 399.44037 to 398.57227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 398.2047 - MinusLogProbMetric: 398.2047 - val_loss: 398.5723 - val_MinusLogProbMetric: 398.5723 - lr: 3.3333e-04 - 12s/epoch - 60ms/step
Epoch 40/1000
2023-09-09 09:59:39.286 
Epoch 40/1000 
	 loss: 399.4658, MinusLogProbMetric: 399.4658, val_loss: 398.8926, val_MinusLogProbMetric: 398.8926

Epoch 40: val_loss did not improve from 398.57227
196/196 - 11s - loss: 399.4658 - MinusLogProbMetric: 399.4658 - val_loss: 398.8926 - val_MinusLogProbMetric: 398.8926 - lr: 3.3333e-04 - 11s/epoch - 57ms/step
Epoch 41/1000
2023-09-09 09:59:50.153 
Epoch 41/1000 
	 loss: 396.9782, MinusLogProbMetric: 396.9782, val_loss: 399.8334, val_MinusLogProbMetric: 399.8334

Epoch 41: val_loss did not improve from 398.57227
196/196 - 11s - loss: 396.9782 - MinusLogProbMetric: 396.9782 - val_loss: 399.8334 - val_MinusLogProbMetric: 399.8334 - lr: 3.3333e-04 - 11s/epoch - 55ms/step
Epoch 42/1000
2023-09-09 10:00:00.784 
Epoch 42/1000 
	 loss: 397.5077, MinusLogProbMetric: 397.5077, val_loss: 399.0821, val_MinusLogProbMetric: 399.0821

Epoch 42: val_loss did not improve from 398.57227
196/196 - 11s - loss: 397.5077 - MinusLogProbMetric: 397.5077 - val_loss: 399.0821 - val_MinusLogProbMetric: 399.0821 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 43/1000
2023-09-09 10:00:11.025 
Epoch 43/1000 
	 loss: 397.3320, MinusLogProbMetric: 397.3320, val_loss: 398.3937, val_MinusLogProbMetric: 398.3937

Epoch 43: val_loss improved from 398.57227 to 398.39371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 397.3320 - MinusLogProbMetric: 397.3320 - val_loss: 398.3937 - val_MinusLogProbMetric: 398.3937 - lr: 3.3333e-04 - 11s/epoch - 54ms/step
Epoch 44/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 36: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-09 10:00:14.016 
Epoch 44/1000 
	 loss: nan, MinusLogProbMetric: 11806596227947646263150772224.0000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 44: val_loss did not improve from 398.39371
196/196 - 3s - loss: nan - MinusLogProbMetric: 11806596227947646263150772224.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 3s/epoch - 14ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 323.
===========
Train data generated in 0.92 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
x_data_train shape: (100000, 1000)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal", batch_shape=[], event_shape=[1000], dtype=float32)
self.io_kwargs: {'results_path': '../../results/MAFN_new/run_323/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/MAFN_new/run_323
self.data_kwargs: {'seed': 0}
self.x_data: [[8.210789   4.3589015  5.1949253  ... 2.1186912  8.28666    6.44192   ]
 [5.776202   0.0413513  4.6274076  ... 4.7782993  6.688914   5.570342  ]
 [5.7263374  6.2561994  5.7377024  ... 8.820506   0.58727944 6.515802  ]
 ...
 [5.7884197  0.56533414 4.7867274  ... 4.878848   6.0022035  4.9900055 ]
 [5.9892163  0.222366   4.8566475  ... 4.5662537  6.433208   4.1020565 ]
 [7.8653784  4.6189337  5.157348   ... 2.6189103  7.542449   6.328924  ]]
self.y_data: []
self.ndims: 1000
Model defined.
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_23 (InputLayer)       [(None, 1000)]            0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  4191520   
 yer)                                                            
                                                                 
=================================================================
Total params: 4,191,520
Trainable params: 4,191,520
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f0d186d1810>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d9019f670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d9019f670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0dc816e6e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d286a5120>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d286a5600>, <keras.callbacks.ModelCheckpoint object at 0x7f0d286a56c0>, <keras.callbacks.EarlyStopping object at 0x7f0d286a5930>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d286a5960>, <keras.callbacks.TerminateOnNaN object at 0x7f0d286a55a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.277253 , 6.974316 , 5.431759 , ..., 8.818214 , 1.8372726,
        6.879843 ],
       [8.248872 , 4.6043487, 5.1827307, ..., 2.8883853, 8.569977 ,
        6.566897 ],
       [7.511346 , 4.894093 , 5.228927 , ..., 2.959833 , 8.1193695,
        7.606636 ],
       ...,
       [5.4142895, 7.32243  , 7.81261  , ..., 9.441056 , 0.9493766,
        6.7277093],
       [5.4575763, 6.765944 , 6.234851 , ..., 9.695062 , 1.5681067,
        6.672586 ],
       [5.466369 , 6.341171 , 6.2548084, ..., 9.017296 , 2.8553374,
        6.70359  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 323/360 with hyperparameters:
timestamp = 2023-09-09 10:00:17.217791
ndims = 1000
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = MAFN
nbijectors = 10
spline_knots = --
range_min = -5
hidden_layers = 128-128-128
trainable_parameters = 4191520
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 8.21078873e+00  4.35890150e+00  5.19492531e+00  2.84795880e+00
  6.23694658e+00  2.48332572e+00  6.01452780e+00  1.49544990e+00
  1.56305087e+00  4.33542109e+00  3.63919592e+00  2.33469009e+00
  7.12970674e-01  4.46248204e-01  1.55621916e-01  3.05101776e+00
  4.20721769e+00  7.49161625e+00  3.96869850e+00  9.05481625e+00
  4.47121114e-01  2.05655861e+00  3.52404070e+00  5.90453529e+00
  8.73787975e+00  3.53096032e+00  2.76253247e+00  6.50554955e-01
  8.03491688e+00  3.65929008e+00  4.93096781e+00  1.00386086e+01
  5.29845119e-01  1.22730446e+00  7.25914860e+00  7.58729315e+00
  7.71210337e+00  5.14168453e+00  9.58227444e+00  4.79759741e+00
  6.05360174e+00 -3.22920084e-03  4.30121565e+00  7.28324509e+00
  2.30840397e+00  9.72805214e+00  7.01551247e+00  3.15825868e+00
  7.77309942e+00  1.27091324e+00  9.88113785e+00  5.76226175e-01
  9.93493366e+00  6.19022250e-01  7.72215176e+00  1.45727849e+00
  2.63241100e+00  7.56394577e+00  4.26764488e+00  1.30306756e+00
  4.47696209e+00  4.10884666e+00  6.25260830e+00  7.17153311e+00
  7.79660606e+00  4.86149597e+00  4.01030636e+00  1.48931491e+00
  8.25993347e+00  6.99169493e+00  4.15396929e+00  5.40362930e+00
  6.69921494e+00  1.80113685e+00 -2.28963286e-01  3.53704309e+00
  5.97519684e+00  5.03495884e+00  5.96093559e+00  8.27530861e+00
  5.11110401e+00  1.71916723e-01  1.00485516e+01  5.39971399e+00
  6.97965956e+00  6.67698717e+00  9.53224850e+00  2.60424829e+00
  6.92656088e+00  2.48836160e+00  1.50950837e+00  3.11806774e+00
  3.62086797e+00  9.48242664e+00  2.35715413e+00  1.47683930e+00
  6.57032585e+00  2.31288910e+00  5.07415724e+00  8.22896194e+00
  2.67307043e+00  4.70531940e+00  5.81045723e+00  3.97613907e+00
  1.00836468e+01  6.80917358e+00  7.91102648e+00  9.83973694e+00
  3.97975469e+00  2.81087971e+00  6.61194921e-01  2.09042239e+00
  1.80242181e+00  2.45311332e+00  7.75794411e+00  5.35263777e+00
  6.36038160e+00  8.03104973e+00  2.23086929e+00  3.26186609e+00
  7.10611820e+00  4.36860800e+00  5.38674593e-01  5.03258133e+00
  8.79059315e+00  4.12663126e+00  2.96597481e+00  9.92143154e+00
  6.63155603e+00  2.29087067e+00  9.67418194e-01  8.54019165e+00
  7.32748413e+00  9.67831326e+00  3.08930302e+00  1.43350685e+00
  7.23957300e+00  2.06572795e+00  1.31994998e+00  2.38906145e+00
  8.15470123e+00  4.92610788e+00  4.60461289e-01  4.69264269e+00
  7.11180496e+00  6.15540504e+00  5.58532810e+00  1.97622848e+00
  8.55810928e+00  5.60682297e+00  6.50732088e+00  9.74595356e+00
  3.70203424e+00  8.65994740e+00  3.66820455e-01 -1.19887441e-01
  5.62859249e+00  3.21332264e+00  2.41353798e+00  2.97626567e+00
  3.33519387e+00  7.60388327e+00  5.94638872e+00  7.90076303e+00
  8.14614868e+00  8.43794918e+00  5.11629820e+00  4.45509529e+00
  8.96594524e+00  6.83408117e+00  5.29020429e-01  5.47313118e+00
  3.56114149e+00  9.86122417e+00 -8.47060025e-01  5.97660685e+00
  7.56903505e+00  3.27315211e+00  9.39664459e+00  9.18616390e+00
  9.16251087e+00  3.12576860e-01  5.81769562e+00  2.36430216e+00
  8.91152668e+00  6.80723286e+00  5.49286032e+00  1.25512540e+00
  6.51518726e+00  6.32031965e+00  9.03301430e+00  5.99218893e+00
  9.68340588e+00  9.75206280e+00  6.48233700e+00  5.02287769e+00
  4.40520668e+00  8.71606636e+00  1.15936518e-01  1.15061617e+00
  1.13465822e+00  7.72926044e+00  3.59013176e+00  6.88610840e+00
  6.62146950e+00  3.82466817e+00  5.06336927e+00  6.39645910e+00
  9.77296638e+00  6.23589993e+00  6.13958120e+00  6.46353066e-01
  7.98168039e+00  5.24744987e+00  8.84934807e+00  7.04309762e-01
  4.59585285e+00  2.57471061e+00  2.76045990e+00  7.58923578e+00
  6.31161690e+00  3.93680930e+00  3.41281295e-03  9.66006947e+00
  9.57943439e+00  2.44953036e+00  7.60993719e-01  5.62422276e+00
  9.45851040e+00  9.12294960e+00  5.39434481e+00  2.33969688e+00
  3.87983751e+00  8.94526768e+00  4.94882011e+00  7.46908426e-01
  3.23506546e+00  2.77883267e+00  1.57590222e+00  5.10829020e+00
  9.52500534e+00  6.17966223e+00  1.10281193e+00  5.18856943e-01
  2.46926522e+00  3.78143835e+00  6.96128178e+00  1.00144138e+01
  3.66667557e+00  5.97449923e+00  9.95713520e+00  7.44061518e+00
  6.02692795e+00  7.26248920e-01  3.43354082e+00  1.11553822e+01
  1.09044552e+00  8.32242012e+00  7.46725512e+00  3.50707126e+00
  4.66837549e+00  6.27135944e+00  7.01602077e+00  4.99376917e+00
  1.22947979e+00  6.21938133e+00  4.57749987e+00  9.84504700e+00
  3.30335212e+00  2.10937715e+00  8.59310150e-01  5.95748758e+00
  7.51813459e+00  1.18187451e+00  3.30510187e+00  6.36148548e+00
  4.50776958e+00  4.50498581e+00  1.05220985e+01  9.05682564e+00
  6.02789402e+00  7.13881588e+00  7.25975084e+00  6.13174152e+00
  2.50386310e+00  6.21227741e+00  6.38945484e+00  8.56712151e+00
  3.63186836e-01  9.32480907e+00  5.54315186e+00 -3.98739636e-01
  3.37429929e+00  7.95210361e+00  5.01406813e+00  9.63213921e+00
  3.99637389e+00  2.97415352e+00  8.29203606e+00  6.00829792e+00
  6.21376610e+00  7.23727989e+00  6.71069527e+00  5.01448536e+00
  9.13507938e-01  6.87569332e+00  9.86827469e+00  4.45279694e+00
  8.80540848e+00  5.50182295e+00  6.56988764e+00  8.22664070e+00
  6.11226082e+00  9.90539372e-01  8.70905399e+00  2.35227823e+00
  6.95939922e+00  4.88893890e+00  4.05508137e+00  1.27094746e+00
  7.41610336e+00  9.57014084e-01  1.03208637e+01  8.77536011e+00
  5.40437126e+00  3.66490221e+00  1.92103791e+00  5.05432844e+00
  6.88084650e+00  2.36994123e+00  4.34519386e+00  1.71688604e+00
  3.39162683e+00  1.85334861e+00  9.56473351e+00  6.55594063e+00
  7.24569368e+00  9.57989931e-01  3.30405354e+00  8.91251564e+00
  9.43958402e-01  2.91835928e+00  6.76908159e+00  6.41434669e+00
  8.02711487e+00  8.37068748e+00  1.76286387e+00  2.83487058e+00
  4.26155806e+00 -1.64624956e-02  5.80787063e-02  2.90215087e+00
  4.82958937e+00  2.67308688e+00  8.74459743e+00  3.35418940e+00
  9.50176620e+00  9.24923992e+00  5.82722712e+00  3.35397267e+00
  3.11244607e+00  7.65114069e+00  2.58552289e+00  3.75996971e+00
  5.17049551e+00  5.17774963e+00  9.58946896e+00  2.46264791e+00
  4.03688717e+00  2.72447777e+00  2.75106120e+00  1.24247396e+00
  2.18880701e+00  8.80450439e+00  2.16815996e+00  4.50869846e+00
  9.47779369e+00  6.92755127e+00  5.21426487e+00  7.63384438e+00
  1.33627713e+00  5.05314064e+00  8.01588058e-01  2.33411765e+00
  2.04597259e+00  3.71735334e+00  3.99399781e+00  9.83961201e+00
  2.91163659e+00  3.73780441e+00  7.86999273e+00  7.62870979e+00
  7.85087109e+00  2.87814641e+00  5.46488953e+00  1.44017196e+00
  2.16978312e+00  4.34651899e+00  7.40160513e+00  5.01842690e+00
  8.79978561e+00  8.01273918e+00  6.23900414e-01  5.34396315e+00
  5.84303570e+00  9.49287987e+00 -2.03253716e-01  5.55819988e+00
  2.56452465e+00  5.52358687e-01  5.80285740e+00  8.33415508e+00
  8.18197632e+00 -8.59678984e-02  3.97874522e+00  4.08348382e-01
  2.77977419e+00  2.01314735e+00  2.23359156e+00  3.15110850e+00
  3.16415262e+00  9.69314194e+00  3.46537042e+00  8.90620518e+00
  1.80000591e+00  6.60689974e+00 -7.20865309e-01  4.78756142e+00
 -1.09752685e-01  9.03241920e+00  8.91885853e+00  6.74625039e-01
  9.64687729e+00  5.28304863e+00  3.44711399e+00  9.08611012e+00
  3.02275133e+00  4.11477327e+00  1.01315606e+00  7.55140066e+00
  7.66734123e-01  4.76552486e+00  6.70099640e+00  7.79399300e+00
  1.63697755e+00  6.94904470e+00  6.56399965e+00  4.96204281e+00
  5.89378309e+00  5.83356380e-01  6.12774992e+00  4.05969334e+00
  8.60876942e+00  3.77930760e+00  5.77260780e+00  4.28877592e+00
  9.17067528e+00  2.55027103e+00  1.33500242e+00  8.23266697e+00
  4.27657557e+00  5.92882109e+00  3.65143597e-01  1.05127277e+01
  9.12497520e+00  4.73808956e+00  1.15415120e+00  4.24439526e+00
  9.80254364e+00  7.13764381e+00  2.21075535e+00  4.46259594e+00
  9.60248947e+00  6.37608194e+00  3.89600515e-01  6.30370569e+00
  2.47006059e+00  9.89494145e-01  4.10520649e+00  5.44608831e+00
  5.12012959e-01  4.04597473e+00  5.92461109e+00  8.09785938e+00
  5.32709646e+00  5.15376234e+00  7.13395691e+00  8.32365990e+00
  4.20349646e+00  6.05286407e+00  6.26060724e+00  6.60055876e-03
  8.09364128e+00  3.49219418e+00  7.10890245e+00  4.91867685e+00
  7.30661583e+00  6.67151976e+00  2.00603342e+00  6.53424406e+00
  8.17309189e+00  2.37493563e+00  7.19742537e+00  6.21698093e+00
  1.02654848e+01  4.02459478e+00  2.00096679e+00  3.60146332e+00
  2.72641587e+00  7.55267096e+00  7.33097744e+00  6.48247480e+00
  1.84161115e+00  8.67273045e+00  1.03891554e+01  5.11711931e+00
  6.14349842e+00  4.63130045e+00  1.04338384e+00  3.16097307e+00
  2.48957181e+00  2.08219433e+00  9.02428818e+00  7.58692598e+00
  3.53382182e+00  2.87967825e+00  9.43597221e+00  1.85656917e+00
  5.57112312e+00  9.00374889e-01  1.43457806e+00  5.70213127e+00
  6.18463993e+00  5.21287870e+00  8.30662155e+00  9.78884792e+00
  8.07082462e+00  4.94385624e+00  9.57595062e+00  4.81561422e-01
  5.93677044e+00  6.03051233e+00  1.27146268e+00  9.59977341e+00
  3.53188229e+00  5.92599010e+00  8.90297794e+00  5.73115826e+00
  8.29036653e-01  6.28442287e+00  4.10012245e+00  8.16020679e+00
  9.57242489e+00  8.73237801e+00  3.82743692e+00  7.33982182e+00
  9.39437389e+00  5.32039928e+00  4.49351358e+00  7.41679072e-01
  7.33104944e+00  8.68616486e+00  2.54391122e+00  2.88136458e+00
  5.05854321e+00  4.12581384e-01  7.20838428e-01  4.39249325e+00
  2.12579203e+00  5.69973421e+00  3.94032979e+00  1.01283722e+01
  9.79717064e+00  2.52009940e+00  8.86107635e+00  8.79512429e-01
  4.84236622e+00  1.72291732e+00  3.82897258e+00  6.94665861e+00
  4.18651247e+00  8.54575062e+00  7.19269276e+00  9.72444630e+00
  5.51988173e+00  8.01771069e+00  3.28205919e+00  3.97813463e+00
  4.18136644e+00  5.16412115e+00  1.75697100e+00  5.15417719e+00
  1.50469279e+00  9.25226021e+00  4.35976595e-01  4.19854736e+00
  3.64054823e+00  1.06782198e-01  8.30288601e+00  7.06305122e+00
  1.87979758e+00  6.35672998e+00  8.16109753e+00  6.51096630e+00
  5.61127961e-01  6.76316595e+00  6.57550716e+00  7.76742983e+00
  5.40051842e+00  4.99695921e+00  1.05247574e+01  7.60159779e+00
  5.60209799e+00  3.34395909e+00  9.19912910e+00  4.68194294e+00
  1.00466835e+00  5.51672554e+00  8.06357193e+00  4.60604143e+00
  7.99039245e-01  6.11217165e+00  9.98776340e+00  7.64210176e+00
  6.11796856e+00  9.89960003e+00  7.96584511e+00  6.26075697e+00
  4.63107634e+00  7.38753414e+00  6.49842548e+00  7.23403335e-01
  7.84123564e+00  3.43258095e+00  5.87750053e+00  8.16937256e+00
  4.84820747e+00  1.59829724e+00  4.18775225e+00  1.30482543e+00
  6.89824677e+00  9.70094967e+00  7.83443594e+00  4.79529858e+00
  6.26568747e+00  3.08977580e+00  5.72115278e+00  2.92952847e+00
  7.45006084e+00  2.38916183e+00  9.71957111e+00  6.54768562e+00
  6.25139415e-01 -2.21325979e-02  1.01162004e+01  1.64097524e+00
  3.30466270e+00  2.32712507e+00  9.82605267e+00  5.42485619e+00
  4.57557726e+00  5.65397930e+00  2.95226574e+00  6.00716829e+00
  4.69566488e+00  1.86493647e+00  5.55787706e+00  4.29599905e+00
  8.97355652e+00  4.49551582e+00  5.21416712e+00  7.87060261e+00
  4.67125940e+00  9.76075363e+00  8.19973755e+00  8.41541100e+00
  9.90537739e+00  2.93996453e+00  5.85271358e+00  7.30432892e+00
  7.22648525e+00  9.17044544e+00  2.83920431e+00  3.20150447e+00
  8.31029034e+00  7.78128242e+00  2.25734663e+00  5.91900253e+00
  7.87056684e+00  3.05571938e+00  3.93086529e+00  5.76315022e+00
  5.30861235e+00  7.55313301e+00 -1.47460684e-01  4.09288597e+00
  1.74171937e+00  4.78674221e+00  7.88887119e+00  7.54741371e-01
  1.54648781e+00  7.28928089e+00  7.57703447e+00  5.61207056e+00
  9.85756111e+00  4.10112953e+00  1.57129598e+00  6.85865021e+00
  6.33887672e+00  6.33726215e+00  5.35512924e+00  8.78589249e+00
  8.93958569e-01 -1.22548640e-01  1.54779208e+00  2.83668208e+00
  5.92615891e+00  5.92532015e+00  2.29801345e+00  4.49938583e+00
  4.36498499e+00  5.37602377e+00  8.03752041e+00  5.66677046e+00
  8.83142376e+00  1.65858328e+00  5.42761660e+00  8.26859760e+00
  1.79626775e+00  4.40248251e+00  5.96399355e+00  2.07387352e+00
  7.58685493e+00  6.19924068e-01  9.63695049e+00  4.80245113e+00
  2.84362292e+00  1.11408031e+00  4.65407324e+00  6.60547924e+00
  1.22691751e+00  3.05578947e+00  1.09281607e+01  6.40130091e+00
  2.10371280e+00  1.27200794e+00  8.89848232e+00  2.80182910e+00
  2.19280529e+00  1.05606747e+01  2.94065785e+00  1.69049454e+00
  5.48146868e+00  4.31994724e+00  9.06744099e+00  1.15592277e+00
 -7.79310703e-01  1.04373407e+00  8.86706924e+00  4.28514481e+00
  1.81781006e+00  5.78969526e+00  5.54994345e+00  2.30405951e+00
  5.54053879e+00  2.74468279e+00  2.27062321e+00  4.07878542e+00
  2.84625888e+00  2.04214787e+00  8.94879723e+00  6.88817441e-01
  2.71495771e+00  3.84625363e+00  2.55016398e+00  2.09636283e+00
  7.99209976e+00  3.07564521e+00  1.11745620e+00  5.30355263e+00
  9.93744850e-01  2.74349833e+00  8.41612911e+00  4.34795761e+00
  6.55439472e+00  6.15104818e+00  5.19303381e-01  6.36637783e+00
  8.49066544e+00  8.24014544e-01  6.90668011e+00  7.08179665e+00
  9.01312637e+00  3.36554408e+00  2.87336349e+00  6.86589861e+00
 -2.01100767e-01  7.31349230e+00  8.66450596e+00  4.29815626e+00
  3.38823771e+00  3.01338983e+00  8.08154404e-01  1.78297210e+00
  9.61242580e+00  8.81327534e+00  2.97019339e+00  8.63016319e+00
  3.02107668e+00  2.72670150e+00  2.13139439e+00  8.94742870e+00
  9.83511925e+00  3.19685555e+00  6.81030130e+00  3.50014639e+00
  1.05297625e+00  4.07330465e+00  5.53509521e+00  1.18256664e+00
  8.23826122e+00  5.53470898e+00  2.24809480e+00  5.40734196e+00
  7.84483671e+00  2.75392199e+00  7.62425852e+00  4.27786589e+00
  9.72611332e+00  8.83753872e+00  3.43726850e+00  1.27960294e-01
  7.29834032e+00  2.04023504e+00  1.30689442e+00  9.04135704e+00
  1.00866823e+01  2.58009624e+00  3.01804805e+00  1.68803823e+00
  4.80738878e+00  8.68211079e+00  6.73749733e+00  8.63989162e+00
  2.21114421e+00  7.04848576e+00  9.34753704e+00  2.76184368e+00
  6.90854836e+00  4.05415773e+00  3.81649780e+00  9.40583229e+00
  4.05083704e+00  8.92635822e-01  3.82358503e+00  4.43289518e+00
  7.26910877e+00  8.82122993e+00  8.22476578e+00  9.33980370e+00
  4.71105146e+00  6.95373154e+00  3.06833911e+00 -4.22824502e-01
  3.65492988e+00  6.86570597e+00  2.05108023e+00  8.74809170e+00
  8.31052685e+00  1.87380075e+00  2.81262565e+00  8.66123104e+00
  3.31760716e+00  3.31471944e+00  3.48811197e+00  7.28613329e+00
 -9.66772735e-02 -7.69796014e-01  6.32836771e+00  3.86802888e+00
  5.99920893e+00  2.40654874e+00  9.12174988e+00  8.34914875e+00
  7.07096219e-01  7.57762194e+00  9.58280849e+00  8.65084171e+00
  6.67687321e+00  1.21811235e+00  5.86707783e+00  3.30112028e+00
  1.01402960e+01  7.17766523e+00  5.74374008e+00  6.99432039e+00
  4.40368319e+00  2.86542940e+00  9.42987251e+00  3.30265784e+00
  3.83230895e-01  4.20491457e+00  8.71614361e+00  7.57816601e+00
  9.77553654e+00  6.46106720e-01  9.44217777e+00  4.46305180e+00
  4.83198547e+00  3.06845570e+00  1.06641746e+00  4.49923372e+00
  7.57343435e+00  5.86748123e-03  1.58664107e+00  5.31323195e-01
  8.55088329e+00  6.48872995e+00  1.54820132e+00  7.71537733e+00
  8.20388412e+00  9.80402565e+00  9.79537868e+00 -8.38677704e-01
  5.94942427e+00  2.88661289e+00  5.26643085e+00  3.18921828e+00
  7.79481411e-01  4.42821693e+00  5.33531380e+00  9.32881355e+00
  2.65765786e-02  3.08747840e+00  8.52437019e+00  6.85948515e+00
  8.21984768e+00  6.24451685e+00  7.30141687e+00  4.90936804e+00
  2.20824790e+00  8.48395348e+00  5.67844057e+00  1.24840343e+00
  8.05260468e+00  9.64081573e+00  3.07099938e+00  3.18240905e+00
  9.12044704e-01  5.96499348e+00  9.19813156e+00  3.36040854e-01
  4.73263264e+00  1.30067658e+00  5.19609153e-01  6.70743704e+00
  8.23411751e+00  3.52812862e+00  8.22581482e+00  1.19572031e+00
  2.63071656e+00  9.79853821e+00  4.98861170e+00  6.00117397e+00
  3.19664389e-01  1.32244027e+00  6.05848253e-01  7.35753870e+00
  1.13696325e+00  8.69066429e+00  3.58892393e+00  7.88068628e+00
  3.04448676e+00  2.65313506e+00  9.56735551e-01  2.75976801e+00
  4.98727465e+00  7.08365965e+00  5.05365181e+00  9.17652988e+00
  5.92350817e+00  1.26345801e+00  3.95679140e+00  7.46081781e+00
  2.64605379e+00  6.97033882e+00  5.93741179e+00  1.01776525e-01
  7.47330189e+00  5.93837404e+00  8.12254488e-01  3.60206747e+00
  3.23938537e+00  8.24193764e+00  3.56294060e+00  9.41426849e+00
  5.64243269e+00  2.11869121e+00  8.28666019e+00  6.44191980e+00]
Epoch 1/1000
2023-09-09 10:00:51.587 
Epoch 1/1000 
	 loss: 398.2834, MinusLogProbMetric: 398.2834, val_loss: 395.6219, val_MinusLogProbMetric: 395.6219

Epoch 1: val_loss improved from inf to 395.62195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 35s - loss: 398.2834 - MinusLogProbMetric: 398.2834 - val_loss: 395.6219 - val_MinusLogProbMetric: 395.6219 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 2/1000
2023-09-09 10:01:03.029 
Epoch 2/1000 
	 loss: 392.6408, MinusLogProbMetric: 392.6408, val_loss: 395.0266, val_MinusLogProbMetric: 395.0266

Epoch 2: val_loss improved from 395.62195 to 395.02658, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 392.6408 - MinusLogProbMetric: 392.6408 - val_loss: 395.0266 - val_MinusLogProbMetric: 395.0266 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 3/1000
2023-09-09 10:01:13.881 
Epoch 3/1000 
	 loss: 392.7520, MinusLogProbMetric: 392.7520, val_loss: 394.7889, val_MinusLogProbMetric: 394.7889

Epoch 3: val_loss improved from 395.02658 to 394.78891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 392.7520 - MinusLogProbMetric: 392.7520 - val_loss: 394.7889 - val_MinusLogProbMetric: 394.7889 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 4/1000
2023-09-09 10:01:25.334 
Epoch 4/1000 
	 loss: 392.7700, MinusLogProbMetric: 392.7700, val_loss: 395.0260, val_MinusLogProbMetric: 395.0260

Epoch 4: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.7700 - MinusLogProbMetric: 392.7700 - val_loss: 395.0260 - val_MinusLogProbMetric: 395.0260 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 5/1000
2023-09-09 10:01:36.328 
Epoch 5/1000 
	 loss: 392.8661, MinusLogProbMetric: 392.8661, val_loss: 395.5952, val_MinusLogProbMetric: 395.5952

Epoch 5: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.8661 - MinusLogProbMetric: 392.8661 - val_loss: 395.5952 - val_MinusLogProbMetric: 395.5952 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 6/1000
2023-09-09 10:01:47.152 
Epoch 6/1000 
	 loss: 392.4016, MinusLogProbMetric: 392.4016, val_loss: 395.9775, val_MinusLogProbMetric: 395.9775

Epoch 6: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.4016 - MinusLogProbMetric: 392.4016 - val_loss: 395.9775 - val_MinusLogProbMetric: 395.9775 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 7/1000
2023-09-09 10:01:58.052 
Epoch 7/1000 
	 loss: 392.3844, MinusLogProbMetric: 392.3844, val_loss: 395.6321, val_MinusLogProbMetric: 395.6321

Epoch 7: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.3844 - MinusLogProbMetric: 392.3844 - val_loss: 395.6321 - val_MinusLogProbMetric: 395.6321 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 8/1000
2023-09-09 10:02:09.344 
Epoch 8/1000 
	 loss: 392.5095, MinusLogProbMetric: 392.5095, val_loss: 396.0970, val_MinusLogProbMetric: 396.0970

Epoch 8: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.5095 - MinusLogProbMetric: 392.5095 - val_loss: 396.0970 - val_MinusLogProbMetric: 396.0970 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 9/1000
2023-09-09 10:02:20.445 
Epoch 9/1000 
	 loss: 392.2520, MinusLogProbMetric: 392.2520, val_loss: 395.2388, val_MinusLogProbMetric: 395.2388

Epoch 9: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.2520 - MinusLogProbMetric: 392.2520 - val_loss: 395.2388 - val_MinusLogProbMetric: 395.2388 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 10/1000
2023-09-09 10:02:31.645 
Epoch 10/1000 
	 loss: 392.2587, MinusLogProbMetric: 392.2587, val_loss: 395.9459, val_MinusLogProbMetric: 395.9459

Epoch 10: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.2587 - MinusLogProbMetric: 392.2587 - val_loss: 395.9459 - val_MinusLogProbMetric: 395.9459 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 11/1000
2023-09-09 10:02:42.915 
Epoch 11/1000 
	 loss: 392.0696, MinusLogProbMetric: 392.0696, val_loss: 394.7931, val_MinusLogProbMetric: 394.7931

Epoch 11: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.0696 - MinusLogProbMetric: 392.0696 - val_loss: 394.7931 - val_MinusLogProbMetric: 394.7931 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 12/1000
2023-09-09 10:02:53.958 
Epoch 12/1000 
	 loss: 392.0702, MinusLogProbMetric: 392.0702, val_loss: 395.6288, val_MinusLogProbMetric: 395.6288

Epoch 12: val_loss did not improve from 394.78891
196/196 - 11s - loss: 392.0702 - MinusLogProbMetric: 392.0702 - val_loss: 395.6288 - val_MinusLogProbMetric: 395.6288 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 13/1000
2023-09-09 10:03:04.600 
Epoch 13/1000 
	 loss: 392.2368, MinusLogProbMetric: 392.2368, val_loss: 394.7627, val_MinusLogProbMetric: 394.7627

Epoch 13: val_loss improved from 394.78891 to 394.76266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 392.2368 - MinusLogProbMetric: 392.2368 - val_loss: 394.7627 - val_MinusLogProbMetric: 394.7627 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 14/1000
2023-09-09 10:03:16.070 
Epoch 14/1000 
	 loss: 391.8965, MinusLogProbMetric: 391.8965, val_loss: 395.6087, val_MinusLogProbMetric: 395.6087

Epoch 14: val_loss did not improve from 394.76266
196/196 - 11s - loss: 391.8965 - MinusLogProbMetric: 391.8965 - val_loss: 395.6087 - val_MinusLogProbMetric: 395.6087 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 15/1000
2023-09-09 10:03:26.619 
Epoch 15/1000 
	 loss: 392.3450, MinusLogProbMetric: 392.3450, val_loss: 395.9931, val_MinusLogProbMetric: 395.9931

Epoch 15: val_loss did not improve from 394.76266
196/196 - 11s - loss: 392.3450 - MinusLogProbMetric: 392.3450 - val_loss: 395.9931 - val_MinusLogProbMetric: 395.9931 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 16/1000
2023-09-09 10:03:37.638 
Epoch 16/1000 
	 loss: 391.7744, MinusLogProbMetric: 391.7744, val_loss: 395.1337, val_MinusLogProbMetric: 395.1337

Epoch 16: val_loss did not improve from 394.76266
196/196 - 11s - loss: 391.7744 - MinusLogProbMetric: 391.7744 - val_loss: 395.1337 - val_MinusLogProbMetric: 395.1337 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 17/1000
2023-09-09 10:03:48.359 
Epoch 17/1000 
	 loss: 392.0099, MinusLogProbMetric: 392.0099, val_loss: 394.3768, val_MinusLogProbMetric: 394.3768

Epoch 17: val_loss improved from 394.76266 to 394.37683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 392.0099 - MinusLogProbMetric: 392.0099 - val_loss: 394.3768 - val_MinusLogProbMetric: 394.3768 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 18/1000
2023-09-09 10:03:59.475 
Epoch 18/1000 
	 loss: 391.6040, MinusLogProbMetric: 391.6040, val_loss: 394.4552, val_MinusLogProbMetric: 394.4552

Epoch 18: val_loss did not improve from 394.37683
196/196 - 11s - loss: 391.6040 - MinusLogProbMetric: 391.6040 - val_loss: 394.4552 - val_MinusLogProbMetric: 394.4552 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 19/1000
2023-09-09 10:04:10.468 
Epoch 19/1000 
	 loss: 391.7196, MinusLogProbMetric: 391.7196, val_loss: 394.3248, val_MinusLogProbMetric: 394.3248

Epoch 19: val_loss improved from 394.37683 to 394.32477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 391.7196 - MinusLogProbMetric: 391.7196 - val_loss: 394.3248 - val_MinusLogProbMetric: 394.3248 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 20/1000
2023-09-09 10:04:22.054 
Epoch 20/1000 
	 loss: 391.6221, MinusLogProbMetric: 391.6221, val_loss: 395.5784, val_MinusLogProbMetric: 395.5784

Epoch 20: val_loss did not improve from 394.32477
196/196 - 11s - loss: 391.6221 - MinusLogProbMetric: 391.6221 - val_loss: 395.5784 - val_MinusLogProbMetric: 395.5784 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 21/1000
2023-09-09 10:04:33.041 
Epoch 21/1000 
	 loss: 391.5946, MinusLogProbMetric: 391.5946, val_loss: 394.7383, val_MinusLogProbMetric: 394.7383

Epoch 21: val_loss did not improve from 394.32477
196/196 - 11s - loss: 391.5946 - MinusLogProbMetric: 391.5946 - val_loss: 394.7383 - val_MinusLogProbMetric: 394.7383 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 22/1000
2023-09-09 10:04:44.259 
Epoch 22/1000 
	 loss: 391.5753, MinusLogProbMetric: 391.5753, val_loss: 394.3923, val_MinusLogProbMetric: 394.3923

Epoch 22: val_loss did not improve from 394.32477
196/196 - 11s - loss: 391.5753 - MinusLogProbMetric: 391.5753 - val_loss: 394.3923 - val_MinusLogProbMetric: 394.3923 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 23/1000
2023-09-09 10:04:55.274 
Epoch 23/1000 
	 loss: 391.3945, MinusLogProbMetric: 391.3945, val_loss: 394.5354, val_MinusLogProbMetric: 394.5354

Epoch 23: val_loss did not improve from 394.32477
196/196 - 11s - loss: 391.3945 - MinusLogProbMetric: 391.3945 - val_loss: 394.5354 - val_MinusLogProbMetric: 394.5354 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 24/1000
2023-09-09 10:05:06.616 
Epoch 24/1000 
	 loss: 391.7379, MinusLogProbMetric: 391.7379, val_loss: 394.1453, val_MinusLogProbMetric: 394.1453

Epoch 24: val_loss improved from 394.32477 to 394.14532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 391.7379 - MinusLogProbMetric: 391.7379 - val_loss: 394.1453 - val_MinusLogProbMetric: 394.1453 - lr: 1.1111e-04 - 12s/epoch - 60ms/step
Epoch 25/1000
2023-09-09 10:05:17.600 
Epoch 25/1000 
	 loss: 391.4625, MinusLogProbMetric: 391.4625, val_loss: 394.8175, val_MinusLogProbMetric: 394.8175

Epoch 25: val_loss did not improve from 394.14532
196/196 - 11s - loss: 391.4625 - MinusLogProbMetric: 391.4625 - val_loss: 394.8175 - val_MinusLogProbMetric: 394.8175 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 26/1000
2023-09-09 10:05:28.437 
Epoch 26/1000 
	 loss: 391.5728, MinusLogProbMetric: 391.5728, val_loss: 394.7430, val_MinusLogProbMetric: 394.7430

Epoch 26: val_loss did not improve from 394.14532
196/196 - 11s - loss: 391.5728 - MinusLogProbMetric: 391.5728 - val_loss: 394.7430 - val_MinusLogProbMetric: 394.7430 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 27/1000
2023-09-09 10:05:39.376 
Epoch 27/1000 
	 loss: 391.2658, MinusLogProbMetric: 391.2658, val_loss: 394.1317, val_MinusLogProbMetric: 394.1317

Epoch 27: val_loss improved from 394.14532 to 394.13171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 391.2658 - MinusLogProbMetric: 391.2658 - val_loss: 394.1317 - val_MinusLogProbMetric: 394.1317 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 28/1000
2023-09-09 10:05:50.894 
Epoch 28/1000 
	 loss: 391.4405, MinusLogProbMetric: 391.4405, val_loss: 393.9291, val_MinusLogProbMetric: 393.9291

Epoch 28: val_loss improved from 394.13171 to 393.92911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 391.4405 - MinusLogProbMetric: 391.4405 - val_loss: 393.9291 - val_MinusLogProbMetric: 393.9291 - lr: 1.1111e-04 - 12s/epoch - 59ms/step
Epoch 29/1000
2023-09-09 10:06:02.419 
Epoch 29/1000 
	 loss: 391.4082, MinusLogProbMetric: 391.4082, val_loss: 394.7178, val_MinusLogProbMetric: 394.7178

Epoch 29: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.4082 - MinusLogProbMetric: 391.4082 - val_loss: 394.7178 - val_MinusLogProbMetric: 394.7178 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 30/1000
2023-09-09 10:06:12.934 
Epoch 30/1000 
	 loss: 391.4732, MinusLogProbMetric: 391.4732, val_loss: 394.1255, val_MinusLogProbMetric: 394.1255

Epoch 30: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.4732 - MinusLogProbMetric: 391.4732 - val_loss: 394.1255 - val_MinusLogProbMetric: 394.1255 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 31/1000
2023-09-09 10:06:23.588 
Epoch 31/1000 
	 loss: 391.1922, MinusLogProbMetric: 391.1922, val_loss: 394.8347, val_MinusLogProbMetric: 394.8347

Epoch 31: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.1922 - MinusLogProbMetric: 391.1922 - val_loss: 394.8347 - val_MinusLogProbMetric: 394.8347 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 32/1000
2023-09-09 10:06:34.819 
Epoch 32/1000 
	 loss: 391.1787, MinusLogProbMetric: 391.1787, val_loss: 394.8256, val_MinusLogProbMetric: 394.8256

Epoch 32: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.1787 - MinusLogProbMetric: 391.1787 - val_loss: 394.8256 - val_MinusLogProbMetric: 394.8256 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 33/1000
2023-09-09 10:06:46.048 
Epoch 33/1000 
	 loss: 391.1122, MinusLogProbMetric: 391.1122, val_loss: 394.2291, val_MinusLogProbMetric: 394.2291

Epoch 33: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.1122 - MinusLogProbMetric: 391.1122 - val_loss: 394.2291 - val_MinusLogProbMetric: 394.2291 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 34/1000
2023-09-09 10:06:57.099 
Epoch 34/1000 
	 loss: 391.3414, MinusLogProbMetric: 391.3414, val_loss: 394.1431, val_MinusLogProbMetric: 394.1431

Epoch 34: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.3414 - MinusLogProbMetric: 391.3414 - val_loss: 394.1431 - val_MinusLogProbMetric: 394.1431 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 35/1000
2023-09-09 10:07:07.930 
Epoch 35/1000 
	 loss: 391.2185, MinusLogProbMetric: 391.2185, val_loss: 395.6094, val_MinusLogProbMetric: 395.6094

Epoch 35: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.2185 - MinusLogProbMetric: 391.2185 - val_loss: 395.6094 - val_MinusLogProbMetric: 395.6094 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 36/1000
2023-09-09 10:07:19.164 
Epoch 36/1000 
	 loss: 391.0573, MinusLogProbMetric: 391.0573, val_loss: 393.9961, val_MinusLogProbMetric: 393.9961

Epoch 36: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.0573 - MinusLogProbMetric: 391.0573 - val_loss: 393.9961 - val_MinusLogProbMetric: 393.9961 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 37/1000
2023-09-09 10:07:29.901 
Epoch 37/1000 
	 loss: 391.3184, MinusLogProbMetric: 391.3184, val_loss: 394.2869, val_MinusLogProbMetric: 394.2869

Epoch 37: val_loss did not improve from 393.92911
196/196 - 11s - loss: 391.3184 - MinusLogProbMetric: 391.3184 - val_loss: 394.2869 - val_MinusLogProbMetric: 394.2869 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 38/1000
2023-09-09 10:07:41.052 
Epoch 38/1000 
	 loss: 390.8642, MinusLogProbMetric: 390.8642, val_loss: 395.8665, val_MinusLogProbMetric: 395.8665

Epoch 38: val_loss did not improve from 393.92911
196/196 - 11s - loss: 390.8642 - MinusLogProbMetric: 390.8642 - val_loss: 395.8665 - val_MinusLogProbMetric: 395.8665 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 39/1000
2023-09-09 10:07:52.448 
Epoch 39/1000 
	 loss: 390.9181, MinusLogProbMetric: 390.9181, val_loss: 394.6557, val_MinusLogProbMetric: 394.6557

Epoch 39: val_loss did not improve from 393.92911
196/196 - 11s - loss: 390.9181 - MinusLogProbMetric: 390.9181 - val_loss: 394.6557 - val_MinusLogProbMetric: 394.6557 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 40/1000
2023-09-09 10:08:03.052 
Epoch 40/1000 
	 loss: 391.3173, MinusLogProbMetric: 391.3173, val_loss: 393.8304, val_MinusLogProbMetric: 393.8304

Epoch 40: val_loss improved from 393.92911 to 393.83041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 391.3173 - MinusLogProbMetric: 391.3173 - val_loss: 393.8304 - val_MinusLogProbMetric: 393.8304 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 41/1000
2023-09-09 10:08:14.494 
Epoch 41/1000 
	 loss: 390.7701, MinusLogProbMetric: 390.7701, val_loss: 393.7348, val_MinusLogProbMetric: 393.7348

Epoch 41: val_loss improved from 393.83041 to 393.73480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 390.7701 - MinusLogProbMetric: 390.7701 - val_loss: 393.7348 - val_MinusLogProbMetric: 393.7348 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 42/1000
2023-09-09 10:08:25.987 
Epoch 42/1000 
	 loss: 391.1347, MinusLogProbMetric: 391.1347, val_loss: 394.2610, val_MinusLogProbMetric: 394.2610

Epoch 42: val_loss did not improve from 393.73480
196/196 - 11s - loss: 391.1347 - MinusLogProbMetric: 391.1347 - val_loss: 394.2610 - val_MinusLogProbMetric: 394.2610 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 43/1000
2023-09-09 10:08:37.374 
Epoch 43/1000 
	 loss: 390.7426, MinusLogProbMetric: 390.7426, val_loss: 394.7853, val_MinusLogProbMetric: 394.7853

Epoch 43: val_loss did not improve from 393.73480
196/196 - 11s - loss: 390.7426 - MinusLogProbMetric: 390.7426 - val_loss: 394.7853 - val_MinusLogProbMetric: 394.7853 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 44/1000
2023-09-09 10:08:48.568 
Epoch 44/1000 
	 loss: 391.1846, MinusLogProbMetric: 391.1846, val_loss: 397.7112, val_MinusLogProbMetric: 397.7112

Epoch 44: val_loss did not improve from 393.73480
196/196 - 11s - loss: 391.1846 - MinusLogProbMetric: 391.1846 - val_loss: 397.7112 - val_MinusLogProbMetric: 397.7112 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 45/1000
2023-09-09 10:08:59.549 
Epoch 45/1000 
	 loss: 390.9310, MinusLogProbMetric: 390.9310, val_loss: 395.2056, val_MinusLogProbMetric: 395.2056

Epoch 45: val_loss did not improve from 393.73480
196/196 - 11s - loss: 390.9310 - MinusLogProbMetric: 390.9310 - val_loss: 395.2056 - val_MinusLogProbMetric: 395.2056 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 46/1000
2023-09-09 10:09:10.595 
Epoch 46/1000 
	 loss: 390.7603, MinusLogProbMetric: 390.7603, val_loss: 396.3200, val_MinusLogProbMetric: 396.3200

Epoch 46: val_loss did not improve from 393.73480
196/196 - 11s - loss: 390.7603 - MinusLogProbMetric: 390.7603 - val_loss: 396.3200 - val_MinusLogProbMetric: 396.3200 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 47/1000
2023-09-09 10:09:21.251 
Epoch 47/1000 
	 loss: 390.7429, MinusLogProbMetric: 390.7429, val_loss: 395.0454, val_MinusLogProbMetric: 395.0454

Epoch 47: val_loss did not improve from 393.73480
196/196 - 11s - loss: 390.7429 - MinusLogProbMetric: 390.7429 - val_loss: 395.0454 - val_MinusLogProbMetric: 395.0454 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 48/1000
2023-09-09 10:09:31.670 
Epoch 48/1000 
	 loss: 390.7703, MinusLogProbMetric: 390.7703, val_loss: 393.4203, val_MinusLogProbMetric: 393.4203

Epoch 48: val_loss improved from 393.73480 to 393.42032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 11s - loss: 390.7703 - MinusLogProbMetric: 390.7703 - val_loss: 393.4203 - val_MinusLogProbMetric: 393.4203 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 49/1000
2023-09-09 10:09:43.080 
Epoch 49/1000 
	 loss: 390.6170, MinusLogProbMetric: 390.6170, val_loss: 398.8364, val_MinusLogProbMetric: 398.8364

Epoch 49: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.6170 - MinusLogProbMetric: 390.6170 - val_loss: 398.8364 - val_MinusLogProbMetric: 398.8364 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 50/1000
2023-09-09 10:09:53.188 
Epoch 50/1000 
	 loss: 474.1122, MinusLogProbMetric: 474.1122, val_loss: 515.7929, val_MinusLogProbMetric: 515.7929

Epoch 50: val_loss did not improve from 393.42032
196/196 - 10s - loss: 474.1122 - MinusLogProbMetric: 474.1122 - val_loss: 515.7929 - val_MinusLogProbMetric: 515.7929 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 51/1000
2023-09-09 10:10:04.309 
Epoch 51/1000 
	 loss: 439.7321, MinusLogProbMetric: 439.7321, val_loss: 418.1792, val_MinusLogProbMetric: 418.1792

Epoch 51: val_loss did not improve from 393.42032
196/196 - 11s - loss: 439.7321 - MinusLogProbMetric: 439.7321 - val_loss: 418.1792 - val_MinusLogProbMetric: 418.1792 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 52/1000
2023-09-09 10:10:14.515 
Epoch 52/1000 
	 loss: 409.5749, MinusLogProbMetric: 409.5749, val_loss: 407.8296, val_MinusLogProbMetric: 407.8296

Epoch 52: val_loss did not improve from 393.42032
196/196 - 10s - loss: 409.5749 - MinusLogProbMetric: 409.5749 - val_loss: 407.8296 - val_MinusLogProbMetric: 407.8296 - lr: 1.1111e-04 - 10s/epoch - 52ms/step
Epoch 53/1000
2023-09-09 10:10:24.876 
Epoch 53/1000 
	 loss: 402.8594, MinusLogProbMetric: 402.8594, val_loss: 403.8945, val_MinusLogProbMetric: 403.8945

Epoch 53: val_loss did not improve from 393.42032
196/196 - 10s - loss: 402.8594 - MinusLogProbMetric: 402.8594 - val_loss: 403.8945 - val_MinusLogProbMetric: 403.8945 - lr: 1.1111e-04 - 10s/epoch - 53ms/step
Epoch 54/1000
2023-09-09 10:10:35.880 
Epoch 54/1000 
	 loss: 399.7485, MinusLogProbMetric: 399.7485, val_loss: 401.7357, val_MinusLogProbMetric: 401.7357

Epoch 54: val_loss did not improve from 393.42032
196/196 - 11s - loss: 399.7485 - MinusLogProbMetric: 399.7485 - val_loss: 401.7357 - val_MinusLogProbMetric: 401.7357 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 55/1000
2023-09-09 10:10:46.727 
Epoch 55/1000 
	 loss: 397.9385, MinusLogProbMetric: 397.9385, val_loss: 400.3653, val_MinusLogProbMetric: 400.3653

Epoch 55: val_loss did not improve from 393.42032
196/196 - 11s - loss: 397.9385 - MinusLogProbMetric: 397.9385 - val_loss: 400.3653 - val_MinusLogProbMetric: 400.3653 - lr: 1.1111e-04 - 11s/epoch - 55ms/step
Epoch 56/1000
2023-09-09 10:10:57.719 
Epoch 56/1000 
	 loss: 396.7761, MinusLogProbMetric: 396.7761, val_loss: 399.4459, val_MinusLogProbMetric: 399.4459

Epoch 56: val_loss did not improve from 393.42032
196/196 - 11s - loss: 396.7761 - MinusLogProbMetric: 396.7761 - val_loss: 399.4459 - val_MinusLogProbMetric: 399.4459 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 57/1000
2023-09-09 10:11:08.731 
Epoch 57/1000 
	 loss: 395.9533, MinusLogProbMetric: 395.9533, val_loss: 398.9645, val_MinusLogProbMetric: 398.9645

Epoch 57: val_loss did not improve from 393.42032
196/196 - 11s - loss: 395.9533 - MinusLogProbMetric: 395.9533 - val_loss: 398.9645 - val_MinusLogProbMetric: 398.9645 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 58/1000
2023-09-09 10:11:20.001 
Epoch 58/1000 
	 loss: 395.3190, MinusLogProbMetric: 395.3190, val_loss: 398.0367, val_MinusLogProbMetric: 398.0367

Epoch 58: val_loss did not improve from 393.42032
196/196 - 11s - loss: 395.3190 - MinusLogProbMetric: 395.3190 - val_loss: 398.0367 - val_MinusLogProbMetric: 398.0367 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 59/1000
2023-09-09 10:11:31.182 
Epoch 59/1000 
	 loss: 394.7962, MinusLogProbMetric: 394.7962, val_loss: 398.1532, val_MinusLogProbMetric: 398.1532

Epoch 59: val_loss did not improve from 393.42032
196/196 - 11s - loss: 394.7962 - MinusLogProbMetric: 394.7962 - val_loss: 398.1532 - val_MinusLogProbMetric: 398.1532 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 60/1000
2023-09-09 10:11:41.854 
Epoch 60/1000 
	 loss: 394.3163, MinusLogProbMetric: 394.3163, val_loss: 398.1600, val_MinusLogProbMetric: 398.1600

Epoch 60: val_loss did not improve from 393.42032
196/196 - 11s - loss: 394.3163 - MinusLogProbMetric: 394.3163 - val_loss: 398.1600 - val_MinusLogProbMetric: 398.1600 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 61/1000
2023-09-09 10:11:53.071 
Epoch 61/1000 
	 loss: 394.2578, MinusLogProbMetric: 394.2578, val_loss: 397.5594, val_MinusLogProbMetric: 397.5594

Epoch 61: val_loss did not improve from 393.42032
196/196 - 11s - loss: 394.2578 - MinusLogProbMetric: 394.2578 - val_loss: 397.5594 - val_MinusLogProbMetric: 397.5594 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 62/1000
2023-09-09 10:12:04.166 
Epoch 62/1000 
	 loss: 393.8270, MinusLogProbMetric: 393.8270, val_loss: 396.8596, val_MinusLogProbMetric: 396.8596

Epoch 62: val_loss did not improve from 393.42032
196/196 - 11s - loss: 393.8270 - MinusLogProbMetric: 393.8270 - val_loss: 396.8596 - val_MinusLogProbMetric: 396.8596 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 63/1000
2023-09-09 10:12:15.303 
Epoch 63/1000 
	 loss: 393.5492, MinusLogProbMetric: 393.5492, val_loss: 397.0612, val_MinusLogProbMetric: 397.0612

Epoch 63: val_loss did not improve from 393.42032
196/196 - 11s - loss: 393.5492 - MinusLogProbMetric: 393.5492 - val_loss: 397.0612 - val_MinusLogProbMetric: 397.0612 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 64/1000
2023-09-09 10:12:26.552 
Epoch 64/1000 
	 loss: 393.3980, MinusLogProbMetric: 393.3980, val_loss: 396.4508, val_MinusLogProbMetric: 396.4508

Epoch 64: val_loss did not improve from 393.42032
196/196 - 11s - loss: 393.3980 - MinusLogProbMetric: 393.3980 - val_loss: 396.4508 - val_MinusLogProbMetric: 396.4508 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 65/1000
2023-09-09 10:12:37.712 
Epoch 65/1000 
	 loss: 393.3435, MinusLogProbMetric: 393.3435, val_loss: 396.1893, val_MinusLogProbMetric: 396.1893

Epoch 65: val_loss did not improve from 393.42032
196/196 - 11s - loss: 393.3435 - MinusLogProbMetric: 393.3435 - val_loss: 396.1893 - val_MinusLogProbMetric: 396.1893 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 66/1000
2023-09-09 10:12:48.780 
Epoch 66/1000 
	 loss: 392.9571, MinusLogProbMetric: 392.9571, val_loss: 396.4341, val_MinusLogProbMetric: 396.4341

Epoch 66: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.9571 - MinusLogProbMetric: 392.9571 - val_loss: 396.4341 - val_MinusLogProbMetric: 396.4341 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 67/1000
2023-09-09 10:12:59.996 
Epoch 67/1000 
	 loss: 392.9632, MinusLogProbMetric: 392.9632, val_loss: 396.0096, val_MinusLogProbMetric: 396.0096

Epoch 67: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.9632 - MinusLogProbMetric: 392.9632 - val_loss: 396.0096 - val_MinusLogProbMetric: 396.0096 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 68/1000
2023-09-09 10:13:11.023 
Epoch 68/1000 
	 loss: 392.7809, MinusLogProbMetric: 392.7809, val_loss: 396.1921, val_MinusLogProbMetric: 396.1921

Epoch 68: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.7809 - MinusLogProbMetric: 392.7809 - val_loss: 396.1921 - val_MinusLogProbMetric: 396.1921 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 69/1000
2023-09-09 10:13:22.306 
Epoch 69/1000 
	 loss: 392.7197, MinusLogProbMetric: 392.7197, val_loss: 395.9938, val_MinusLogProbMetric: 395.9938

Epoch 69: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.7197 - MinusLogProbMetric: 392.7197 - val_loss: 395.9938 - val_MinusLogProbMetric: 395.9938 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 70/1000
2023-09-09 10:13:33.457 
Epoch 70/1000 
	 loss: 392.6175, MinusLogProbMetric: 392.6175, val_loss: 395.9325, val_MinusLogProbMetric: 395.9325

Epoch 70: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.6175 - MinusLogProbMetric: 392.6175 - val_loss: 395.9325 - val_MinusLogProbMetric: 395.9325 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 71/1000
2023-09-09 10:13:44.540 
Epoch 71/1000 
	 loss: 392.6166, MinusLogProbMetric: 392.6166, val_loss: 395.5083, val_MinusLogProbMetric: 395.5083

Epoch 71: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.6166 - MinusLogProbMetric: 392.6166 - val_loss: 395.5083 - val_MinusLogProbMetric: 395.5083 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 72/1000
2023-09-09 10:13:55.629 
Epoch 72/1000 
	 loss: 392.3698, MinusLogProbMetric: 392.3698, val_loss: 395.5200, val_MinusLogProbMetric: 395.5200

Epoch 72: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.3698 - MinusLogProbMetric: 392.3698 - val_loss: 395.5200 - val_MinusLogProbMetric: 395.5200 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 73/1000
2023-09-09 10:14:06.802 
Epoch 73/1000 
	 loss: 392.2522, MinusLogProbMetric: 392.2522, val_loss: 395.9941, val_MinusLogProbMetric: 395.9941

Epoch 73: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.2522 - MinusLogProbMetric: 392.2522 - val_loss: 395.9941 - val_MinusLogProbMetric: 395.9941 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 74/1000
2023-09-09 10:14:18.129 
Epoch 74/1000 
	 loss: 392.1996, MinusLogProbMetric: 392.1996, val_loss: 395.6430, val_MinusLogProbMetric: 395.6430

Epoch 74: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.1996 - MinusLogProbMetric: 392.1996 - val_loss: 395.6430 - val_MinusLogProbMetric: 395.6430 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 75/1000
2023-09-09 10:14:29.464 
Epoch 75/1000 
	 loss: 391.9794, MinusLogProbMetric: 391.9794, val_loss: 395.0425, val_MinusLogProbMetric: 395.0425

Epoch 75: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.9794 - MinusLogProbMetric: 391.9794 - val_loss: 395.0425 - val_MinusLogProbMetric: 395.0425 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 76/1000
2023-09-09 10:14:40.954 
Epoch 76/1000 
	 loss: 391.9106, MinusLogProbMetric: 391.9106, val_loss: 395.3055, val_MinusLogProbMetric: 395.3055

Epoch 76: val_loss did not improve from 393.42032
196/196 - 12s - loss: 391.9106 - MinusLogProbMetric: 391.9106 - val_loss: 395.3055 - val_MinusLogProbMetric: 395.3055 - lr: 1.1111e-04 - 12s/epoch - 59ms/step
Epoch 77/1000
2023-09-09 10:14:52.368 
Epoch 77/1000 
	 loss: 392.2465, MinusLogProbMetric: 392.2465, val_loss: 395.1817, val_MinusLogProbMetric: 395.1817

Epoch 77: val_loss did not improve from 393.42032
196/196 - 11s - loss: 392.2465 - MinusLogProbMetric: 392.2465 - val_loss: 395.1817 - val_MinusLogProbMetric: 395.1817 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 78/1000
2023-09-09 10:15:03.590 
Epoch 78/1000 
	 loss: 391.8152, MinusLogProbMetric: 391.8152, val_loss: 395.6285, val_MinusLogProbMetric: 395.6285

Epoch 78: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.8152 - MinusLogProbMetric: 391.8152 - val_loss: 395.6285 - val_MinusLogProbMetric: 395.6285 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 79/1000
2023-09-09 10:15:15.188 
Epoch 79/1000 
	 loss: 391.8130, MinusLogProbMetric: 391.8130, val_loss: 395.2401, val_MinusLogProbMetric: 395.2401

Epoch 79: val_loss did not improve from 393.42032
196/196 - 12s - loss: 391.8130 - MinusLogProbMetric: 391.8130 - val_loss: 395.2401 - val_MinusLogProbMetric: 395.2401 - lr: 1.1111e-04 - 12s/epoch - 59ms/step
Epoch 80/1000
2023-09-09 10:15:26.178 
Epoch 80/1000 
	 loss: 391.8973, MinusLogProbMetric: 391.8973, val_loss: 395.9184, val_MinusLogProbMetric: 395.9184

Epoch 80: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.8973 - MinusLogProbMetric: 391.8973 - val_loss: 395.9184 - val_MinusLogProbMetric: 395.9184 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 81/1000
2023-09-09 10:15:37.483 
Epoch 81/1000 
	 loss: 391.7804, MinusLogProbMetric: 391.7804, val_loss: 396.2425, val_MinusLogProbMetric: 396.2425

Epoch 81: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.7804 - MinusLogProbMetric: 391.7804 - val_loss: 396.2425 - val_MinusLogProbMetric: 396.2425 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 82/1000
2023-09-09 10:15:48.096 
Epoch 82/1000 
	 loss: 391.6870, MinusLogProbMetric: 391.6870, val_loss: 395.0239, val_MinusLogProbMetric: 395.0239

Epoch 82: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.6870 - MinusLogProbMetric: 391.6870 - val_loss: 395.0239 - val_MinusLogProbMetric: 395.0239 - lr: 1.1111e-04 - 11s/epoch - 54ms/step
Epoch 83/1000
2023-09-09 10:15:59.403 
Epoch 83/1000 
	 loss: 391.5476, MinusLogProbMetric: 391.5476, val_loss: 395.5516, val_MinusLogProbMetric: 395.5516

Epoch 83: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.5476 - MinusLogProbMetric: 391.5476 - val_loss: 395.5516 - val_MinusLogProbMetric: 395.5516 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 84/1000
2023-09-09 10:16:10.753 
Epoch 84/1000 
	 loss: 391.5546, MinusLogProbMetric: 391.5546, val_loss: 394.7726, val_MinusLogProbMetric: 394.7726

Epoch 84: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.5546 - MinusLogProbMetric: 391.5546 - val_loss: 394.7726 - val_MinusLogProbMetric: 394.7726 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 85/1000
2023-09-09 10:16:22.043 
Epoch 85/1000 
	 loss: 391.4012, MinusLogProbMetric: 391.4012, val_loss: 396.1749, val_MinusLogProbMetric: 396.1749

Epoch 85: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.4012 - MinusLogProbMetric: 391.4012 - val_loss: 396.1749 - val_MinusLogProbMetric: 396.1749 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 86/1000
2023-09-09 10:16:33.472 
Epoch 86/1000 
	 loss: 391.3365, MinusLogProbMetric: 391.3365, val_loss: 395.0235, val_MinusLogProbMetric: 395.0235

Epoch 86: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.3365 - MinusLogProbMetric: 391.3365 - val_loss: 395.0235 - val_MinusLogProbMetric: 395.0235 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 87/1000
2023-09-09 10:16:44.381 
Epoch 87/1000 
	 loss: 391.1900, MinusLogProbMetric: 391.1900, val_loss: 394.8886, val_MinusLogProbMetric: 394.8886

Epoch 87: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.1900 - MinusLogProbMetric: 391.1900 - val_loss: 394.8886 - val_MinusLogProbMetric: 394.8886 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 88/1000
2023-09-09 10:16:55.530 
Epoch 88/1000 
	 loss: 391.1442, MinusLogProbMetric: 391.1442, val_loss: 394.7593, val_MinusLogProbMetric: 394.7593

Epoch 88: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.1442 - MinusLogProbMetric: 391.1442 - val_loss: 394.7593 - val_MinusLogProbMetric: 394.7593 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 89/1000
2023-09-09 10:17:06.962 
Epoch 89/1000 
	 loss: 391.2010, MinusLogProbMetric: 391.2010, val_loss: 394.8344, val_MinusLogProbMetric: 394.8344

Epoch 89: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.2010 - MinusLogProbMetric: 391.2010 - val_loss: 394.8344 - val_MinusLogProbMetric: 394.8344 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 90/1000
2023-09-09 10:17:18.287 
Epoch 90/1000 
	 loss: 391.1227, MinusLogProbMetric: 391.1227, val_loss: 396.7805, val_MinusLogProbMetric: 396.7805

Epoch 90: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.1227 - MinusLogProbMetric: 391.1227 - val_loss: 396.7805 - val_MinusLogProbMetric: 396.7805 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 91/1000
2023-09-09 10:17:29.595 
Epoch 91/1000 
	 loss: 391.2602, MinusLogProbMetric: 391.2602, val_loss: 394.7428, val_MinusLogProbMetric: 394.7428

Epoch 91: val_loss did not improve from 393.42032
196/196 - 11s - loss: 391.2602 - MinusLogProbMetric: 391.2602 - val_loss: 394.7428 - val_MinusLogProbMetric: 394.7428 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 92/1000
2023-09-09 10:17:40.684 
Epoch 92/1000 
	 loss: 390.7991, MinusLogProbMetric: 390.7991, val_loss: 394.5910, val_MinusLogProbMetric: 394.5910

Epoch 92: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.7991 - MinusLogProbMetric: 390.7991 - val_loss: 394.5910 - val_MinusLogProbMetric: 394.5910 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 93/1000
2023-09-09 10:17:51.665 
Epoch 93/1000 
	 loss: 390.9096, MinusLogProbMetric: 390.9096, val_loss: 395.2659, val_MinusLogProbMetric: 395.2659

Epoch 93: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.9096 - MinusLogProbMetric: 390.9096 - val_loss: 395.2659 - val_MinusLogProbMetric: 395.2659 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 94/1000
2023-09-09 10:18:03.104 
Epoch 94/1000 
	 loss: 390.8862, MinusLogProbMetric: 390.8862, val_loss: 395.6263, val_MinusLogProbMetric: 395.6263

Epoch 94: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.8862 - MinusLogProbMetric: 390.8862 - val_loss: 395.6263 - val_MinusLogProbMetric: 395.6263 - lr: 1.1111e-04 - 11s/epoch - 58ms/step
Epoch 95/1000
2023-09-09 10:18:14.166 
Epoch 95/1000 
	 loss: 390.9749, MinusLogProbMetric: 390.9749, val_loss: 395.3426, val_MinusLogProbMetric: 395.3426

Epoch 95: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.9749 - MinusLogProbMetric: 390.9749 - val_loss: 395.3426 - val_MinusLogProbMetric: 395.3426 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 96/1000
2023-09-09 10:18:25.288 
Epoch 96/1000 
	 loss: 390.8727, MinusLogProbMetric: 390.8727, val_loss: 395.1704, val_MinusLogProbMetric: 395.1704

Epoch 96: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.8727 - MinusLogProbMetric: 390.8727 - val_loss: 395.1704 - val_MinusLogProbMetric: 395.1704 - lr: 1.1111e-04 - 11s/epoch - 57ms/step
Epoch 97/1000
2023-09-09 10:18:36.285 
Epoch 97/1000 
	 loss: 390.8993, MinusLogProbMetric: 390.8993, val_loss: 394.4637, val_MinusLogProbMetric: 394.4637

Epoch 97: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.8993 - MinusLogProbMetric: 390.8993 - val_loss: 394.4637 - val_MinusLogProbMetric: 394.4637 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 98/1000
2023-09-09 10:18:47.228 
Epoch 98/1000 
	 loss: 390.8138, MinusLogProbMetric: 390.8138, val_loss: 394.5921, val_MinusLogProbMetric: 394.5921

Epoch 98: val_loss did not improve from 393.42032
196/196 - 11s - loss: 390.8138 - MinusLogProbMetric: 390.8138 - val_loss: 394.5921 - val_MinusLogProbMetric: 394.5921 - lr: 1.1111e-04 - 11s/epoch - 56ms/step
Epoch 99/1000
2023-09-09 10:18:58.294 
Epoch 99/1000 
	 loss: 389.5615, MinusLogProbMetric: 389.5615, val_loss: 393.6384, val_MinusLogProbMetric: 393.6384

Epoch 99: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.5615 - MinusLogProbMetric: 389.5615 - val_loss: 393.6384 - val_MinusLogProbMetric: 393.6384 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 100/1000
2023-09-09 10:19:09.395 
Epoch 100/1000 
	 loss: 389.4982, MinusLogProbMetric: 389.4982, val_loss: 393.7661, val_MinusLogProbMetric: 393.7661

Epoch 100: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.4982 - MinusLogProbMetric: 389.4982 - val_loss: 393.7661 - val_MinusLogProbMetric: 393.7661 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 101/1000
2023-09-09 10:19:20.845 
Epoch 101/1000 
	 loss: 389.4264, MinusLogProbMetric: 389.4264, val_loss: 393.8309, val_MinusLogProbMetric: 393.8309

Epoch 101: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.4264 - MinusLogProbMetric: 389.4264 - val_loss: 393.8309 - val_MinusLogProbMetric: 393.8309 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 102/1000
2023-09-09 10:19:31.827 
Epoch 102/1000 
	 loss: 389.4514, MinusLogProbMetric: 389.4514, val_loss: 393.7206, val_MinusLogProbMetric: 393.7206

Epoch 102: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.4514 - MinusLogProbMetric: 389.4514 - val_loss: 393.7206 - val_MinusLogProbMetric: 393.7206 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 103/1000
2023-09-09 10:19:42.999 
Epoch 103/1000 
	 loss: 389.4452, MinusLogProbMetric: 389.4452, val_loss: 393.6284, val_MinusLogProbMetric: 393.6284

Epoch 103: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.4452 - MinusLogProbMetric: 389.4452 - val_loss: 393.6284 - val_MinusLogProbMetric: 393.6284 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 104/1000
2023-09-09 10:19:54.252 
Epoch 104/1000 
	 loss: 389.3706, MinusLogProbMetric: 389.3706, val_loss: 393.8770, val_MinusLogProbMetric: 393.8770

Epoch 104: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.3706 - MinusLogProbMetric: 389.3706 - val_loss: 393.8770 - val_MinusLogProbMetric: 393.8770 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 105/1000
2023-09-09 10:20:05.504 
Epoch 105/1000 
	 loss: 389.3416, MinusLogProbMetric: 389.3416, val_loss: 393.6075, val_MinusLogProbMetric: 393.6075

Epoch 105: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.3416 - MinusLogProbMetric: 389.3416 - val_loss: 393.6075 - val_MinusLogProbMetric: 393.6075 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 106/1000
2023-09-09 10:20:16.918 
Epoch 106/1000 
	 loss: 389.3519, MinusLogProbMetric: 389.3519, val_loss: 393.7552, val_MinusLogProbMetric: 393.7552

Epoch 106: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.3519 - MinusLogProbMetric: 389.3519 - val_loss: 393.7552 - val_MinusLogProbMetric: 393.7552 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 107/1000
2023-09-09 10:20:28.257 
Epoch 107/1000 
	 loss: 389.3557, MinusLogProbMetric: 389.3557, val_loss: 394.3545, val_MinusLogProbMetric: 394.3545

Epoch 107: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.3557 - MinusLogProbMetric: 389.3557 - val_loss: 394.3545 - val_MinusLogProbMetric: 394.3545 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 108/1000
2023-09-09 10:20:39.666 
Epoch 108/1000 
	 loss: 389.2946, MinusLogProbMetric: 389.2946, val_loss: 393.7750, val_MinusLogProbMetric: 393.7750

Epoch 108: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2946 - MinusLogProbMetric: 389.2946 - val_loss: 393.7750 - val_MinusLogProbMetric: 393.7750 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 109/1000
2023-09-09 10:20:50.556 
Epoch 109/1000 
	 loss: 389.2219, MinusLogProbMetric: 389.2219, val_loss: 393.8348, val_MinusLogProbMetric: 393.8348

Epoch 109: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2219 - MinusLogProbMetric: 389.2219 - val_loss: 393.8348 - val_MinusLogProbMetric: 393.8348 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 110/1000
2023-09-09 10:21:01.812 
Epoch 110/1000 
	 loss: 389.2505, MinusLogProbMetric: 389.2505, val_loss: 393.4765, val_MinusLogProbMetric: 393.4765

Epoch 110: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2505 - MinusLogProbMetric: 389.2505 - val_loss: 393.4765 - val_MinusLogProbMetric: 393.4765 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 111/1000
2023-09-09 10:21:13.083 
Epoch 111/1000 
	 loss: 389.2414, MinusLogProbMetric: 389.2414, val_loss: 393.7561, val_MinusLogProbMetric: 393.7561

Epoch 111: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2414 - MinusLogProbMetric: 389.2414 - val_loss: 393.7561 - val_MinusLogProbMetric: 393.7561 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 112/1000
2023-09-09 10:21:24.278 
Epoch 112/1000 
	 loss: 389.2084, MinusLogProbMetric: 389.2084, val_loss: 393.7251, val_MinusLogProbMetric: 393.7251

Epoch 112: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2084 - MinusLogProbMetric: 389.2084 - val_loss: 393.7251 - val_MinusLogProbMetric: 393.7251 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 113/1000
2023-09-09 10:21:35.476 
Epoch 113/1000 
	 loss: 389.2551, MinusLogProbMetric: 389.2551, val_loss: 394.1504, val_MinusLogProbMetric: 394.1504

Epoch 113: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2551 - MinusLogProbMetric: 389.2551 - val_loss: 394.1504 - val_MinusLogProbMetric: 394.1504 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 114/1000
2023-09-09 10:21:46.302 
Epoch 114/1000 
	 loss: 389.3191, MinusLogProbMetric: 389.3191, val_loss: 393.9862, val_MinusLogProbMetric: 393.9862

Epoch 114: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.3191 - MinusLogProbMetric: 389.3191 - val_loss: 393.9862 - val_MinusLogProbMetric: 393.9862 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 115/1000
2023-09-09 10:21:57.173 
Epoch 115/1000 
	 loss: 389.2334, MinusLogProbMetric: 389.2334, val_loss: 393.8918, val_MinusLogProbMetric: 393.8918

Epoch 115: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2334 - MinusLogProbMetric: 389.2334 - val_loss: 393.8918 - val_MinusLogProbMetric: 393.8918 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 116/1000
2023-09-09 10:22:08.566 
Epoch 116/1000 
	 loss: 389.1150, MinusLogProbMetric: 389.1150, val_loss: 393.5921, val_MinusLogProbMetric: 393.5921

Epoch 116: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.1150 - MinusLogProbMetric: 389.1150 - val_loss: 393.5921 - val_MinusLogProbMetric: 393.5921 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 117/1000
2023-09-09 10:22:19.742 
Epoch 117/1000 
	 loss: 389.0964, MinusLogProbMetric: 389.0964, val_loss: 393.8749, val_MinusLogProbMetric: 393.8749

Epoch 117: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0964 - MinusLogProbMetric: 389.0964 - val_loss: 393.8749 - val_MinusLogProbMetric: 393.8749 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 118/1000
2023-09-09 10:22:31.036 
Epoch 118/1000 
	 loss: 389.2677, MinusLogProbMetric: 389.2677, val_loss: 393.5808, val_MinusLogProbMetric: 393.5808

Epoch 118: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.2677 - MinusLogProbMetric: 389.2677 - val_loss: 393.5808 - val_MinusLogProbMetric: 393.5808 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 119/1000
2023-09-09 10:22:41.503 
Epoch 119/1000 
	 loss: 389.2266, MinusLogProbMetric: 389.2266, val_loss: 393.4398, val_MinusLogProbMetric: 393.4398

Epoch 119: val_loss did not improve from 393.42032
196/196 - 10s - loss: 389.2266 - MinusLogProbMetric: 389.2266 - val_loss: 393.4398 - val_MinusLogProbMetric: 393.4398 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 120/1000
2023-09-09 10:22:51.995 
Epoch 120/1000 
	 loss: 389.1030, MinusLogProbMetric: 389.1030, val_loss: 393.9075, val_MinusLogProbMetric: 393.9075

Epoch 120: val_loss did not improve from 393.42032
196/196 - 10s - loss: 389.1030 - MinusLogProbMetric: 389.1030 - val_loss: 393.9075 - val_MinusLogProbMetric: 393.9075 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 121/1000
2023-09-09 10:23:03.201 
Epoch 121/1000 
	 loss: 389.0649, MinusLogProbMetric: 389.0649, val_loss: 393.6300, val_MinusLogProbMetric: 393.6300

Epoch 121: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0649 - MinusLogProbMetric: 389.0649 - val_loss: 393.6300 - val_MinusLogProbMetric: 393.6300 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 122/1000
2023-09-09 10:23:14.236 
Epoch 122/1000 
	 loss: 389.0381, MinusLogProbMetric: 389.0381, val_loss: 393.9195, val_MinusLogProbMetric: 393.9195

Epoch 122: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0381 - MinusLogProbMetric: 389.0381 - val_loss: 393.9195 - val_MinusLogProbMetric: 393.9195 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 123/1000
2023-09-09 10:23:25.495 
Epoch 123/1000 
	 loss: 389.0775, MinusLogProbMetric: 389.0775, val_loss: 393.9071, val_MinusLogProbMetric: 393.9071

Epoch 123: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0775 - MinusLogProbMetric: 389.0775 - val_loss: 393.9071 - val_MinusLogProbMetric: 393.9071 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 124/1000
2023-09-09 10:23:36.686 
Epoch 124/1000 
	 loss: 389.1549, MinusLogProbMetric: 389.1549, val_loss: 393.6200, val_MinusLogProbMetric: 393.6200

Epoch 124: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.1549 - MinusLogProbMetric: 389.1549 - val_loss: 393.6200 - val_MinusLogProbMetric: 393.6200 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 125/1000
2023-09-09 10:23:47.521 
Epoch 125/1000 
	 loss: 389.0131, MinusLogProbMetric: 389.0131, val_loss: 393.7225, val_MinusLogProbMetric: 393.7225

Epoch 125: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0131 - MinusLogProbMetric: 389.0131 - val_loss: 393.7225 - val_MinusLogProbMetric: 393.7225 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 126/1000
2023-09-09 10:23:58.534 
Epoch 126/1000 
	 loss: 389.0776, MinusLogProbMetric: 389.0776, val_loss: 393.8538, val_MinusLogProbMetric: 393.8538

Epoch 126: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0776 - MinusLogProbMetric: 389.0776 - val_loss: 393.8538 - val_MinusLogProbMetric: 393.8538 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 127/1000
2023-09-09 10:24:08.896 
Epoch 127/1000 
	 loss: 389.1779, MinusLogProbMetric: 389.1779, val_loss: 393.7462, val_MinusLogProbMetric: 393.7462

Epoch 127: val_loss did not improve from 393.42032
196/196 - 10s - loss: 389.1779 - MinusLogProbMetric: 389.1779 - val_loss: 393.7462 - val_MinusLogProbMetric: 393.7462 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 128/1000
2023-09-09 10:24:19.703 
Epoch 128/1000 
	 loss: 388.9169, MinusLogProbMetric: 388.9169, val_loss: 393.9544, val_MinusLogProbMetric: 393.9544

Epoch 128: val_loss did not improve from 393.42032
196/196 - 11s - loss: 388.9169 - MinusLogProbMetric: 388.9169 - val_loss: 393.9544 - val_MinusLogProbMetric: 393.9544 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 129/1000
2023-09-09 10:24:31.168 
Epoch 129/1000 
	 loss: 389.1293, MinusLogProbMetric: 389.1293, val_loss: 393.8573, val_MinusLogProbMetric: 393.8573

Epoch 129: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.1293 - MinusLogProbMetric: 389.1293 - val_loss: 393.8573 - val_MinusLogProbMetric: 393.8573 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 130/1000
2023-09-09 10:24:42.375 
Epoch 130/1000 
	 loss: 389.0637, MinusLogProbMetric: 389.0637, val_loss: 393.5110, val_MinusLogProbMetric: 393.5110

Epoch 130: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0637 - MinusLogProbMetric: 389.0637 - val_loss: 393.5110 - val_MinusLogProbMetric: 393.5110 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 131/1000
2023-09-09 10:24:53.653 
Epoch 131/1000 
	 loss: 388.9931, MinusLogProbMetric: 388.9931, val_loss: 393.5892, val_MinusLogProbMetric: 393.5892

Epoch 131: val_loss did not improve from 393.42032
196/196 - 11s - loss: 388.9931 - MinusLogProbMetric: 388.9931 - val_loss: 393.5892 - val_MinusLogProbMetric: 393.5892 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 132/1000
2023-09-09 10:25:04.867 
Epoch 132/1000 
	 loss: 389.0079, MinusLogProbMetric: 389.0079, val_loss: 393.5721, val_MinusLogProbMetric: 393.5721

Epoch 132: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0079 - MinusLogProbMetric: 389.0079 - val_loss: 393.5721 - val_MinusLogProbMetric: 393.5721 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 133/1000
2023-09-09 10:25:16.087 
Epoch 133/1000 
	 loss: 388.8330, MinusLogProbMetric: 388.8330, val_loss: 393.5613, val_MinusLogProbMetric: 393.5613

Epoch 133: val_loss did not improve from 393.42032
196/196 - 11s - loss: 388.8330 - MinusLogProbMetric: 388.8330 - val_loss: 393.5613 - val_MinusLogProbMetric: 393.5613 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 134/1000
2023-09-09 10:25:27.439 
Epoch 134/1000 
	 loss: 388.8631, MinusLogProbMetric: 388.8631, val_loss: 393.6505, val_MinusLogProbMetric: 393.6505

Epoch 134: val_loss did not improve from 393.42032
196/196 - 11s - loss: 388.8631 - MinusLogProbMetric: 388.8631 - val_loss: 393.6505 - val_MinusLogProbMetric: 393.6505 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 135/1000
2023-09-09 10:25:37.834 
Epoch 135/1000 
	 loss: 388.8900, MinusLogProbMetric: 388.8900, val_loss: 393.8601, val_MinusLogProbMetric: 393.8601

Epoch 135: val_loss did not improve from 393.42032
196/196 - 10s - loss: 388.8900 - MinusLogProbMetric: 388.8900 - val_loss: 393.8601 - val_MinusLogProbMetric: 393.8601 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 136/1000
2023-09-09 10:25:49.057 
Epoch 136/1000 
	 loss: 389.0006, MinusLogProbMetric: 389.0006, val_loss: 393.6193, val_MinusLogProbMetric: 393.6193

Epoch 136: val_loss did not improve from 393.42032
196/196 - 11s - loss: 389.0006 - MinusLogProbMetric: 389.0006 - val_loss: 393.6193 - val_MinusLogProbMetric: 393.6193 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 137/1000
2023-09-09 10:25:59.421 
Epoch 137/1000 
	 loss: 388.9447, MinusLogProbMetric: 388.9447, val_loss: 393.5125, val_MinusLogProbMetric: 393.5125

Epoch 137: val_loss did not improve from 393.42032
196/196 - 10s - loss: 388.9447 - MinusLogProbMetric: 388.9447 - val_loss: 393.5125 - val_MinusLogProbMetric: 393.5125 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 138/1000
2023-09-09 10:26:10.030 
Epoch 138/1000 
	 loss: 388.8333, MinusLogProbMetric: 388.8333, val_loss: 393.7608, val_MinusLogProbMetric: 393.7608

Epoch 138: val_loss did not improve from 393.42032
196/196 - 11s - loss: 388.8333 - MinusLogProbMetric: 388.8333 - val_loss: 393.7608 - val_MinusLogProbMetric: 393.7608 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 139/1000
2023-09-09 10:26:21.060 
Epoch 139/1000 
	 loss: 388.8473, MinusLogProbMetric: 388.8473, val_loss: 394.0967, val_MinusLogProbMetric: 394.0967

Epoch 139: val_loss did not improve from 393.42032
196/196 - 11s - loss: 388.8473 - MinusLogProbMetric: 388.8473 - val_loss: 394.0967 - val_MinusLogProbMetric: 394.0967 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 140/1000
2023-09-09 10:26:31.563 
Epoch 140/1000 
	 loss: 388.7911, MinusLogProbMetric: 388.7911, val_loss: 393.4537, val_MinusLogProbMetric: 393.4537

Epoch 140: val_loss did not improve from 393.42032
196/196 - 10s - loss: 388.7911 - MinusLogProbMetric: 388.7911 - val_loss: 393.4537 - val_MinusLogProbMetric: 393.4537 - lr: 5.5556e-05 - 10s/epoch - 54ms/step
Epoch 141/1000
2023-09-09 10:26:42.941 
Epoch 141/1000 
	 loss: 388.8697, MinusLogProbMetric: 388.8697, val_loss: 393.3347, val_MinusLogProbMetric: 393.3347

Epoch 141: val_loss improved from 393.42032 to 393.33466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 388.8697 - MinusLogProbMetric: 388.8697 - val_loss: 393.3347 - val_MinusLogProbMetric: 393.3347 - lr: 5.5556e-05 - 12s/epoch - 60ms/step
Epoch 142/1000
2023-09-09 10:26:54.371 
Epoch 142/1000 
	 loss: 388.8523, MinusLogProbMetric: 388.8523, val_loss: 393.5520, val_MinusLogProbMetric: 393.5520

Epoch 142: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.8523 - MinusLogProbMetric: 388.8523 - val_loss: 393.5520 - val_MinusLogProbMetric: 393.5520 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 143/1000
2023-09-09 10:27:04.279 
Epoch 143/1000 
	 loss: 388.9107, MinusLogProbMetric: 388.9107, val_loss: 393.9931, val_MinusLogProbMetric: 393.9931

Epoch 143: val_loss did not improve from 393.33466
196/196 - 10s - loss: 388.9107 - MinusLogProbMetric: 388.9107 - val_loss: 393.9931 - val_MinusLogProbMetric: 393.9931 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 144/1000
2023-09-09 10:27:14.561 
Epoch 144/1000 
	 loss: 388.8078, MinusLogProbMetric: 388.8078, val_loss: 393.8843, val_MinusLogProbMetric: 393.8843

Epoch 144: val_loss did not improve from 393.33466
196/196 - 10s - loss: 388.8078 - MinusLogProbMetric: 388.8078 - val_loss: 393.8843 - val_MinusLogProbMetric: 393.8843 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 145/1000
2023-09-09 10:27:26.005 
Epoch 145/1000 
	 loss: 388.8267, MinusLogProbMetric: 388.8267, val_loss: 395.8035, val_MinusLogProbMetric: 395.8035

Epoch 145: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.8267 - MinusLogProbMetric: 388.8267 - val_loss: 395.8035 - val_MinusLogProbMetric: 395.8035 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 146/1000
2023-09-09 10:27:36.574 
Epoch 146/1000 
	 loss: 388.9922, MinusLogProbMetric: 388.9922, val_loss: 393.3469, val_MinusLogProbMetric: 393.3469

Epoch 146: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.9922 - MinusLogProbMetric: 388.9922 - val_loss: 393.3469 - val_MinusLogProbMetric: 393.3469 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 147/1000
2023-09-09 10:27:46.186 
Epoch 147/1000 
	 loss: 388.9462, MinusLogProbMetric: 388.9462, val_loss: 393.9732, val_MinusLogProbMetric: 393.9732

Epoch 147: val_loss did not improve from 393.33466
196/196 - 10s - loss: 388.9462 - MinusLogProbMetric: 388.9462 - val_loss: 393.9732 - val_MinusLogProbMetric: 393.9732 - lr: 5.5556e-05 - 10s/epoch - 49ms/step
Epoch 148/1000
2023-09-09 10:27:56.400 
Epoch 148/1000 
	 loss: 388.7680, MinusLogProbMetric: 388.7680, val_loss: 393.7797, val_MinusLogProbMetric: 393.7797

Epoch 148: val_loss did not improve from 393.33466
196/196 - 10s - loss: 388.7680 - MinusLogProbMetric: 388.7680 - val_loss: 393.7797 - val_MinusLogProbMetric: 393.7797 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 149/1000
2023-09-09 10:28:06.504 
Epoch 149/1000 
	 loss: 388.7681, MinusLogProbMetric: 388.7681, val_loss: 393.5985, val_MinusLogProbMetric: 393.5985

Epoch 149: val_loss did not improve from 393.33466
196/196 - 10s - loss: 388.7681 - MinusLogProbMetric: 388.7681 - val_loss: 393.5985 - val_MinusLogProbMetric: 393.5985 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 150/1000
2023-09-09 10:28:17.823 
Epoch 150/1000 
	 loss: 388.7149, MinusLogProbMetric: 388.7149, val_loss: 393.5950, val_MinusLogProbMetric: 393.5950

Epoch 150: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.7149 - MinusLogProbMetric: 388.7149 - val_loss: 393.5950 - val_MinusLogProbMetric: 393.5950 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 151/1000
2023-09-09 10:28:29.149 
Epoch 151/1000 
	 loss: 388.7871, MinusLogProbMetric: 388.7871, val_loss: 393.7522, val_MinusLogProbMetric: 393.7522

Epoch 151: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.7871 - MinusLogProbMetric: 388.7871 - val_loss: 393.7522 - val_MinusLogProbMetric: 393.7522 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 152/1000
2023-09-09 10:28:40.180 
Epoch 152/1000 
	 loss: 388.6376, MinusLogProbMetric: 388.6376, val_loss: 393.8012, val_MinusLogProbMetric: 393.8012

Epoch 152: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.6376 - MinusLogProbMetric: 388.6376 - val_loss: 393.8012 - val_MinusLogProbMetric: 393.8012 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 153/1000
2023-09-09 10:28:51.133 
Epoch 153/1000 
	 loss: 388.7397, MinusLogProbMetric: 388.7397, val_loss: 393.4652, val_MinusLogProbMetric: 393.4652

Epoch 153: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.7397 - MinusLogProbMetric: 388.7397 - val_loss: 393.4652 - val_MinusLogProbMetric: 393.4652 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 154/1000
2023-09-09 10:29:02.300 
Epoch 154/1000 
	 loss: 388.6409, MinusLogProbMetric: 388.6409, val_loss: 393.6547, val_MinusLogProbMetric: 393.6547

Epoch 154: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.6409 - MinusLogProbMetric: 388.6409 - val_loss: 393.6547 - val_MinusLogProbMetric: 393.6547 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 155/1000
2023-09-09 10:29:13.034 
Epoch 155/1000 
	 loss: 388.6042, MinusLogProbMetric: 388.6042, val_loss: 394.0566, val_MinusLogProbMetric: 394.0566

Epoch 155: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.6042 - MinusLogProbMetric: 388.6042 - val_loss: 394.0566 - val_MinusLogProbMetric: 394.0566 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 156/1000
2023-09-09 10:29:24.023 
Epoch 156/1000 
	 loss: 388.7458, MinusLogProbMetric: 388.7458, val_loss: 394.8840, val_MinusLogProbMetric: 394.8840

Epoch 156: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.7458 - MinusLogProbMetric: 388.7458 - val_loss: 394.8840 - val_MinusLogProbMetric: 394.8840 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 157/1000
2023-09-09 10:29:35.222 
Epoch 157/1000 
	 loss: 388.5246, MinusLogProbMetric: 388.5246, val_loss: 393.6129, val_MinusLogProbMetric: 393.6129

Epoch 157: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.5246 - MinusLogProbMetric: 388.5246 - val_loss: 393.6129 - val_MinusLogProbMetric: 393.6129 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 158/1000
2023-09-09 10:29:46.687 
Epoch 158/1000 
	 loss: 388.4896, MinusLogProbMetric: 388.4896, val_loss: 393.4663, val_MinusLogProbMetric: 393.4663

Epoch 158: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.4896 - MinusLogProbMetric: 388.4896 - val_loss: 393.4663 - val_MinusLogProbMetric: 393.4663 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 159/1000
2023-09-09 10:29:58.109 
Epoch 159/1000 
	 loss: 388.4433, MinusLogProbMetric: 388.4433, val_loss: 393.9161, val_MinusLogProbMetric: 393.9161

Epoch 159: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.4433 - MinusLogProbMetric: 388.4433 - val_loss: 393.9161 - val_MinusLogProbMetric: 393.9161 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 160/1000
2023-09-09 10:30:09.295 
Epoch 160/1000 
	 loss: 388.5095, MinusLogProbMetric: 388.5095, val_loss: 393.6100, val_MinusLogProbMetric: 393.6100

Epoch 160: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.5095 - MinusLogProbMetric: 388.5095 - val_loss: 393.6100 - val_MinusLogProbMetric: 393.6100 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 161/1000
2023-09-09 10:30:20.391 
Epoch 161/1000 
	 loss: 388.5151, MinusLogProbMetric: 388.5151, val_loss: 393.6358, val_MinusLogProbMetric: 393.6358

Epoch 161: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.5151 - MinusLogProbMetric: 388.5151 - val_loss: 393.6358 - val_MinusLogProbMetric: 393.6358 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 162/1000
2023-09-09 10:30:30.798 
Epoch 162/1000 
	 loss: 388.6135, MinusLogProbMetric: 388.6135, val_loss: 393.6923, val_MinusLogProbMetric: 393.6923

Epoch 162: val_loss did not improve from 393.33466
196/196 - 10s - loss: 388.6135 - MinusLogProbMetric: 388.6135 - val_loss: 393.6923 - val_MinusLogProbMetric: 393.6923 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 163/1000
2023-09-09 10:30:41.775 
Epoch 163/1000 
	 loss: 388.5902, MinusLogProbMetric: 388.5902, val_loss: 394.1533, val_MinusLogProbMetric: 394.1533

Epoch 163: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.5902 - MinusLogProbMetric: 388.5902 - val_loss: 394.1533 - val_MinusLogProbMetric: 394.1533 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 164/1000
2023-09-09 10:30:52.921 
Epoch 164/1000 
	 loss: 388.4711, MinusLogProbMetric: 388.4711, val_loss: 393.6317, val_MinusLogProbMetric: 393.6317

Epoch 164: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.4711 - MinusLogProbMetric: 388.4711 - val_loss: 393.6317 - val_MinusLogProbMetric: 393.6317 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 165/1000
2023-09-09 10:31:03.931 
Epoch 165/1000 
	 loss: 388.4804, MinusLogProbMetric: 388.4804, val_loss: 393.3877, val_MinusLogProbMetric: 393.3877

Epoch 165: val_loss did not improve from 393.33466
196/196 - 11s - loss: 388.4804 - MinusLogProbMetric: 388.4804 - val_loss: 393.3877 - val_MinusLogProbMetric: 393.3877 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 166/1000
2023-09-09 10:31:15.316 
Epoch 166/1000 
	 loss: 388.4336, MinusLogProbMetric: 388.4336, val_loss: 393.2091, val_MinusLogProbMetric: 393.2091

Epoch 166: val_loss improved from 393.33466 to 393.20908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_323/weights/best_weights.h5
196/196 - 12s - loss: 388.4336 - MinusLogProbMetric: 388.4336 - val_loss: 393.2091 - val_MinusLogProbMetric: 393.2091 - lr: 5.5556e-05 - 12s/epoch - 60ms/step
Epoch 167/1000
2023-09-09 10:31:27.075 
Epoch 167/1000 
	 loss: 388.4033, MinusLogProbMetric: 388.4033, val_loss: 393.4880, val_MinusLogProbMetric: 393.4880

Epoch 167: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.4033 - MinusLogProbMetric: 388.4033 - val_loss: 393.4880 - val_MinusLogProbMetric: 393.4880 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 168/1000
2023-09-09 10:31:38.580 
Epoch 168/1000 
	 loss: 388.4544, MinusLogProbMetric: 388.4544, val_loss: 393.4255, val_MinusLogProbMetric: 393.4255

Epoch 168: val_loss did not improve from 393.20908
196/196 - 12s - loss: 388.4544 - MinusLogProbMetric: 388.4544 - val_loss: 393.4255 - val_MinusLogProbMetric: 393.4255 - lr: 5.5556e-05 - 12s/epoch - 59ms/step
Epoch 169/1000
2023-09-09 10:31:49.411 
Epoch 169/1000 
	 loss: 388.4768, MinusLogProbMetric: 388.4768, val_loss: 394.2719, val_MinusLogProbMetric: 394.2719

Epoch 169: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.4768 - MinusLogProbMetric: 388.4768 - val_loss: 394.2719 - val_MinusLogProbMetric: 394.2719 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 170/1000
2023-09-09 10:32:00.638 
Epoch 170/1000 
	 loss: 388.3325, MinusLogProbMetric: 388.3325, val_loss: 393.5922, val_MinusLogProbMetric: 393.5922

Epoch 170: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.3325 - MinusLogProbMetric: 388.3325 - val_loss: 393.5922 - val_MinusLogProbMetric: 393.5922 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 171/1000
2023-09-09 10:32:11.350 
Epoch 171/1000 
	 loss: 388.4556, MinusLogProbMetric: 388.4556, val_loss: 393.7362, val_MinusLogProbMetric: 393.7362

Epoch 171: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.4556 - MinusLogProbMetric: 388.4556 - val_loss: 393.7362 - val_MinusLogProbMetric: 393.7362 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 172/1000
2023-09-09 10:32:22.719 
Epoch 172/1000 
	 loss: 388.6292, MinusLogProbMetric: 388.6292, val_loss: 393.5214, val_MinusLogProbMetric: 393.5214

Epoch 172: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.6292 - MinusLogProbMetric: 388.6292 - val_loss: 393.5214 - val_MinusLogProbMetric: 393.5214 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 173/1000
2023-09-09 10:32:33.294 
Epoch 173/1000 
	 loss: 388.3293, MinusLogProbMetric: 388.3293, val_loss: 394.1250, val_MinusLogProbMetric: 394.1250

Epoch 173: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.3293 - MinusLogProbMetric: 388.3293 - val_loss: 394.1250 - val_MinusLogProbMetric: 394.1250 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 174/1000
2023-09-09 10:32:44.562 
Epoch 174/1000 
	 loss: 388.2651, MinusLogProbMetric: 388.2651, val_loss: 393.7897, val_MinusLogProbMetric: 393.7897

Epoch 174: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2651 - MinusLogProbMetric: 388.2651 - val_loss: 393.7897 - val_MinusLogProbMetric: 393.7897 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 175/1000
2023-09-09 10:32:55.747 
Epoch 175/1000 
	 loss: 388.2367, MinusLogProbMetric: 388.2367, val_loss: 393.5744, val_MinusLogProbMetric: 393.5744

Epoch 175: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2367 - MinusLogProbMetric: 388.2367 - val_loss: 393.5744 - val_MinusLogProbMetric: 393.5744 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 176/1000
2023-09-09 10:33:06.655 
Epoch 176/1000 
	 loss: 388.2828, MinusLogProbMetric: 388.2828, val_loss: 393.5031, val_MinusLogProbMetric: 393.5031

Epoch 176: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2828 - MinusLogProbMetric: 388.2828 - val_loss: 393.5031 - val_MinusLogProbMetric: 393.5031 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 177/1000
2023-09-09 10:33:17.421 
Epoch 177/1000 
	 loss: 388.2490, MinusLogProbMetric: 388.2490, val_loss: 394.0002, val_MinusLogProbMetric: 394.0002

Epoch 177: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2490 - MinusLogProbMetric: 388.2490 - val_loss: 394.0002 - val_MinusLogProbMetric: 394.0002 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 178/1000
2023-09-09 10:33:28.635 
Epoch 178/1000 
	 loss: 388.3782, MinusLogProbMetric: 388.3782, val_loss: 393.3986, val_MinusLogProbMetric: 393.3986

Epoch 178: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.3782 - MinusLogProbMetric: 388.3782 - val_loss: 393.3986 - val_MinusLogProbMetric: 393.3986 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 179/1000
2023-09-09 10:33:39.800 
Epoch 179/1000 
	 loss: 388.4413, MinusLogProbMetric: 388.4413, val_loss: 393.7846, val_MinusLogProbMetric: 393.7846

Epoch 179: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.4413 - MinusLogProbMetric: 388.4413 - val_loss: 393.7846 - val_MinusLogProbMetric: 393.7846 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 180/1000
2023-09-09 10:33:51.360 
Epoch 180/1000 
	 loss: 388.5332, MinusLogProbMetric: 388.5332, val_loss: 393.8748, val_MinusLogProbMetric: 393.8748

Epoch 180: val_loss did not improve from 393.20908
196/196 - 12s - loss: 388.5332 - MinusLogProbMetric: 388.5332 - val_loss: 393.8748 - val_MinusLogProbMetric: 393.8748 - lr: 5.5556e-05 - 12s/epoch - 59ms/step
Epoch 181/1000
2023-09-09 10:34:02.702 
Epoch 181/1000 
	 loss: 388.3350, MinusLogProbMetric: 388.3350, val_loss: 393.8001, val_MinusLogProbMetric: 393.8001

Epoch 181: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.3350 - MinusLogProbMetric: 388.3350 - val_loss: 393.8001 - val_MinusLogProbMetric: 393.8001 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 182/1000
2023-09-09 10:34:13.981 
Epoch 182/1000 
	 loss: 388.2337, MinusLogProbMetric: 388.2337, val_loss: 393.6301, val_MinusLogProbMetric: 393.6301

Epoch 182: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2337 - MinusLogProbMetric: 388.2337 - val_loss: 393.6301 - val_MinusLogProbMetric: 393.6301 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 183/1000
2023-09-09 10:34:25.477 
Epoch 183/1000 
	 loss: 388.0987, MinusLogProbMetric: 388.0987, val_loss: 393.5629, val_MinusLogProbMetric: 393.5629

Epoch 183: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.0987 - MinusLogProbMetric: 388.0987 - val_loss: 393.5629 - val_MinusLogProbMetric: 393.5629 - lr: 5.5556e-05 - 11s/epoch - 59ms/step
Epoch 184/1000
2023-09-09 10:34:36.623 
Epoch 184/1000 
	 loss: 388.2056, MinusLogProbMetric: 388.2056, val_loss: 393.4695, val_MinusLogProbMetric: 393.4695

Epoch 184: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2056 - MinusLogProbMetric: 388.2056 - val_loss: 393.4695 - val_MinusLogProbMetric: 393.4695 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 185/1000
2023-09-09 10:34:47.721 
Epoch 185/1000 
	 loss: 388.1342, MinusLogProbMetric: 388.1342, val_loss: 393.4144, val_MinusLogProbMetric: 393.4144

Epoch 185: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.1342 - MinusLogProbMetric: 388.1342 - val_loss: 393.4144 - val_MinusLogProbMetric: 393.4144 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 186/1000
2023-09-09 10:34:59.114 
Epoch 186/1000 
	 loss: 388.0852, MinusLogProbMetric: 388.0852, val_loss: 393.7752, val_MinusLogProbMetric: 393.7752

Epoch 186: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.0852 - MinusLogProbMetric: 388.0852 - val_loss: 393.7752 - val_MinusLogProbMetric: 393.7752 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 187/1000
2023-09-09 10:35:09.430 
Epoch 187/1000 
	 loss: 388.1284, MinusLogProbMetric: 388.1284, val_loss: 393.9980, val_MinusLogProbMetric: 393.9980

Epoch 187: val_loss did not improve from 393.20908
196/196 - 10s - loss: 388.1284 - MinusLogProbMetric: 388.1284 - val_loss: 393.9980 - val_MinusLogProbMetric: 393.9980 - lr: 5.5556e-05 - 10s/epoch - 53ms/step
Epoch 188/1000
2023-09-09 10:35:20.713 
Epoch 188/1000 
	 loss: 388.2180, MinusLogProbMetric: 388.2180, val_loss: 393.8315, val_MinusLogProbMetric: 393.8315

Epoch 188: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2180 - MinusLogProbMetric: 388.2180 - val_loss: 393.8315 - val_MinusLogProbMetric: 393.8315 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 189/1000
2023-09-09 10:35:31.987 
Epoch 189/1000 
	 loss: 388.1144, MinusLogProbMetric: 388.1144, val_loss: 394.1506, val_MinusLogProbMetric: 394.1506

Epoch 189: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.1144 - MinusLogProbMetric: 388.1144 - val_loss: 394.1506 - val_MinusLogProbMetric: 394.1506 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 190/1000
2023-09-09 10:35:43.227 
Epoch 190/1000 
	 loss: 388.0813, MinusLogProbMetric: 388.0813, val_loss: 394.5395, val_MinusLogProbMetric: 394.5395

Epoch 190: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.0813 - MinusLogProbMetric: 388.0813 - val_loss: 394.5395 - val_MinusLogProbMetric: 394.5395 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 191/1000
2023-09-09 10:35:53.839 
Epoch 191/1000 
	 loss: 388.3938, MinusLogProbMetric: 388.3938, val_loss: 393.6454, val_MinusLogProbMetric: 393.6454

Epoch 191: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.3938 - MinusLogProbMetric: 388.3938 - val_loss: 393.6454 - val_MinusLogProbMetric: 393.6454 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 192/1000
2023-09-09 10:36:04.893 
Epoch 192/1000 
	 loss: 388.0109, MinusLogProbMetric: 388.0109, val_loss: 393.7693, val_MinusLogProbMetric: 393.7693

Epoch 192: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.0109 - MinusLogProbMetric: 388.0109 - val_loss: 393.7693 - val_MinusLogProbMetric: 393.7693 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 193/1000
2023-09-09 10:36:16.445 
Epoch 193/1000 
	 loss: 388.2348, MinusLogProbMetric: 388.2348, val_loss: 393.2153, val_MinusLogProbMetric: 393.2153

Epoch 193: val_loss did not improve from 393.20908
196/196 - 12s - loss: 388.2348 - MinusLogProbMetric: 388.2348 - val_loss: 393.2153 - val_MinusLogProbMetric: 393.2153 - lr: 5.5556e-05 - 12s/epoch - 59ms/step
Epoch 194/1000
2023-09-09 10:36:27.625 
Epoch 194/1000 
	 loss: 388.2361, MinusLogProbMetric: 388.2361, val_loss: 393.4613, val_MinusLogProbMetric: 393.4613

Epoch 194: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2361 - MinusLogProbMetric: 388.2361 - val_loss: 393.4613 - val_MinusLogProbMetric: 393.4613 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 195/1000
2023-09-09 10:36:38.931 
Epoch 195/1000 
	 loss: 388.0838, MinusLogProbMetric: 388.0838, val_loss: 393.6099, val_MinusLogProbMetric: 393.6099

Epoch 195: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.0838 - MinusLogProbMetric: 388.0838 - val_loss: 393.6099 - val_MinusLogProbMetric: 393.6099 - lr: 5.5556e-05 - 11s/epoch - 58ms/step
Epoch 196/1000
2023-09-09 10:36:49.602 
Epoch 196/1000 
	 loss: 388.3479, MinusLogProbMetric: 388.3479, val_loss: 394.0752, val_MinusLogProbMetric: 394.0752

Epoch 196: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.3479 - MinusLogProbMetric: 388.3479 - val_loss: 394.0752 - val_MinusLogProbMetric: 394.0752 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 197/1000
2023-09-09 10:36:59.740 
Epoch 197/1000 
	 loss: 388.1622, MinusLogProbMetric: 388.1622, val_loss: 394.0320, val_MinusLogProbMetric: 394.0320

Epoch 197: val_loss did not improve from 393.20908
196/196 - 10s - loss: 388.1622 - MinusLogProbMetric: 388.1622 - val_loss: 394.0320 - val_MinusLogProbMetric: 394.0320 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 198/1000
2023-09-09 10:37:10.325 
Epoch 198/1000 
	 loss: 388.2847, MinusLogProbMetric: 388.2847, val_loss: 393.7340, val_MinusLogProbMetric: 393.7340

Epoch 198: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.2847 - MinusLogProbMetric: 388.2847 - val_loss: 393.7340 - val_MinusLogProbMetric: 393.7340 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 199/1000
2023-09-09 10:37:21.596 
Epoch 199/1000 
	 loss: 388.1919, MinusLogProbMetric: 388.1919, val_loss: 393.5927, val_MinusLogProbMetric: 393.5927

Epoch 199: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.1919 - MinusLogProbMetric: 388.1919 - val_loss: 393.5927 - val_MinusLogProbMetric: 393.5927 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 200/1000
2023-09-09 10:37:33.101 
Epoch 200/1000 
	 loss: 388.0814, MinusLogProbMetric: 388.0814, val_loss: 394.3311, val_MinusLogProbMetric: 394.3311

Epoch 200: val_loss did not improve from 393.20908
196/196 - 12s - loss: 388.0814 - MinusLogProbMetric: 388.0814 - val_loss: 394.3311 - val_MinusLogProbMetric: 394.3311 - lr: 5.5556e-05 - 12s/epoch - 59ms/step
Epoch 201/1000
2023-09-09 10:37:44.634 
Epoch 201/1000 
	 loss: 388.1216, MinusLogProbMetric: 388.1216, val_loss: 393.4136, val_MinusLogProbMetric: 393.4136

Epoch 201: val_loss did not improve from 393.20908
196/196 - 12s - loss: 388.1216 - MinusLogProbMetric: 388.1216 - val_loss: 393.4136 - val_MinusLogProbMetric: 393.4136 - lr: 5.5556e-05 - 12s/epoch - 59ms/step
Epoch 202/1000
2023-09-09 10:37:56.129 
Epoch 202/1000 
	 loss: 387.9871, MinusLogProbMetric: 387.9871, val_loss: 393.7815, val_MinusLogProbMetric: 393.7815

Epoch 202: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.9871 - MinusLogProbMetric: 387.9871 - val_loss: 393.7815 - val_MinusLogProbMetric: 393.7815 - lr: 5.5556e-05 - 11s/epoch - 59ms/step
Epoch 203/1000
2023-09-09 10:38:06.696 
Epoch 203/1000 
	 loss: 388.0458, MinusLogProbMetric: 388.0458, val_loss: 393.3502, val_MinusLogProbMetric: 393.3502

Epoch 203: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.0458 - MinusLogProbMetric: 388.0458 - val_loss: 393.3502 - val_MinusLogProbMetric: 393.3502 - lr: 5.5556e-05 - 11s/epoch - 54ms/step
Epoch 204/1000
2023-09-09 10:38:17.809 
Epoch 204/1000 
	 loss: 388.0177, MinusLogProbMetric: 388.0177, val_loss: 394.1052, val_MinusLogProbMetric: 394.1052

Epoch 204: val_loss did not improve from 393.20908
196/196 - 11s - loss: 388.0177 - MinusLogProbMetric: 388.0177 - val_loss: 394.1052 - val_MinusLogProbMetric: 394.1052 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 205/1000
2023-09-09 10:38:28.952 
Epoch 205/1000 
	 loss: 387.8675, MinusLogProbMetric: 387.8675, val_loss: 394.2513, val_MinusLogProbMetric: 394.2513

Epoch 205: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.8675 - MinusLogProbMetric: 387.8675 - val_loss: 394.2513 - val_MinusLogProbMetric: 394.2513 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 206/1000
2023-09-09 10:38:39.775 
Epoch 206/1000 
	 loss: 387.9205, MinusLogProbMetric: 387.9205, val_loss: 394.1476, val_MinusLogProbMetric: 394.1476

Epoch 206: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.9205 - MinusLogProbMetric: 387.9205 - val_loss: 394.1476 - val_MinusLogProbMetric: 394.1476 - lr: 5.5556e-05 - 11s/epoch - 55ms/step
Epoch 207/1000
2023-09-09 10:38:49.809 
Epoch 207/1000 
	 loss: 387.9156, MinusLogProbMetric: 387.9156, val_loss: 393.8081, val_MinusLogProbMetric: 393.8081

Epoch 207: val_loss did not improve from 393.20908
196/196 - 10s - loss: 387.9156 - MinusLogProbMetric: 387.9156 - val_loss: 393.8081 - val_MinusLogProbMetric: 393.8081 - lr: 5.5556e-05 - 10s/epoch - 51ms/step
Epoch 208/1000
2023-09-09 10:39:00.767 
Epoch 208/1000 
	 loss: 387.9142, MinusLogProbMetric: 387.9142, val_loss: 393.4449, val_MinusLogProbMetric: 393.4449

Epoch 208: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.9142 - MinusLogProbMetric: 387.9142 - val_loss: 393.4449 - val_MinusLogProbMetric: 393.4449 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 209/1000
2023-09-09 10:39:11.804 
Epoch 209/1000 
	 loss: 387.8866, MinusLogProbMetric: 387.8866, val_loss: 393.9941, val_MinusLogProbMetric: 393.9941

Epoch 209: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.8866 - MinusLogProbMetric: 387.8866 - val_loss: 393.9941 - val_MinusLogProbMetric: 393.9941 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 210/1000
2023-09-09 10:39:22.810 
Epoch 210/1000 
	 loss: 387.9492, MinusLogProbMetric: 387.9492, val_loss: 393.6502, val_MinusLogProbMetric: 393.6502

Epoch 210: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.9492 - MinusLogProbMetric: 387.9492 - val_loss: 393.6502 - val_MinusLogProbMetric: 393.6502 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 211/1000
2023-09-09 10:39:33.985 
Epoch 211/1000 
	 loss: 387.9646, MinusLogProbMetric: 387.9646, val_loss: 394.0483, val_MinusLogProbMetric: 394.0483

Epoch 211: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.9646 - MinusLogProbMetric: 387.9646 - val_loss: 394.0483 - val_MinusLogProbMetric: 394.0483 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 212/1000
2023-09-09 10:39:44.197 
Epoch 212/1000 
	 loss: 387.8979, MinusLogProbMetric: 387.8979, val_loss: 393.5940, val_MinusLogProbMetric: 393.5940

Epoch 212: val_loss did not improve from 393.20908
196/196 - 10s - loss: 387.8979 - MinusLogProbMetric: 387.8979 - val_loss: 393.5940 - val_MinusLogProbMetric: 393.5940 - lr: 5.5556e-05 - 10s/epoch - 52ms/step
Epoch 213/1000
2023-09-09 10:39:55.208 
Epoch 213/1000 
	 loss: 387.8962, MinusLogProbMetric: 387.8962, val_loss: 393.4362, val_MinusLogProbMetric: 393.4362

Epoch 213: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.8962 - MinusLogProbMetric: 387.8962 - val_loss: 393.4362 - val_MinusLogProbMetric: 393.4362 - lr: 5.5556e-05 - 11s/epoch - 56ms/step
Epoch 214/1000
2023-09-09 10:40:06.463 
Epoch 214/1000 
	 loss: 387.8581, MinusLogProbMetric: 387.8581, val_loss: 393.5462, val_MinusLogProbMetric: 393.5462

Epoch 214: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.8581 - MinusLogProbMetric: 387.8581 - val_loss: 393.5462 - val_MinusLogProbMetric: 393.5462 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 215/1000
2023-09-09 10:40:17.563 
Epoch 215/1000 
	 loss: 387.9443, MinusLogProbMetric: 387.9443, val_loss: 393.5321, val_MinusLogProbMetric: 393.5321

Epoch 215: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.9443 - MinusLogProbMetric: 387.9443 - val_loss: 393.5321 - val_MinusLogProbMetric: 393.5321 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 216/1000
2023-09-09 10:40:28.660 
Epoch 216/1000 
	 loss: 387.8605, MinusLogProbMetric: 387.8605, val_loss: 393.6788, val_MinusLogProbMetric: 393.6788

Epoch 216: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.8605 - MinusLogProbMetric: 387.8605 - val_loss: 393.6788 - val_MinusLogProbMetric: 393.6788 - lr: 5.5556e-05 - 11s/epoch - 57ms/step
Epoch 217/1000
2023-09-09 10:40:39.221 
Epoch 217/1000 
	 loss: 387.2428, MinusLogProbMetric: 387.2428, val_loss: 393.4439, val_MinusLogProbMetric: 393.4439

Epoch 217: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2428 - MinusLogProbMetric: 387.2428 - val_loss: 393.4439 - val_MinusLogProbMetric: 393.4439 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 218/1000
2023-09-09 10:40:50.002 
Epoch 218/1000 
	 loss: 387.2197, MinusLogProbMetric: 387.2197, val_loss: 393.4475, val_MinusLogProbMetric: 393.4475

Epoch 218: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2197 - MinusLogProbMetric: 387.2197 - val_loss: 393.4475 - val_MinusLogProbMetric: 393.4475 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 219/1000
2023-09-09 10:41:01.071 
Epoch 219/1000 
	 loss: 387.2043, MinusLogProbMetric: 387.2043, val_loss: 393.4912, val_MinusLogProbMetric: 393.4912

Epoch 219: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2043 - MinusLogProbMetric: 387.2043 - val_loss: 393.4912 - val_MinusLogProbMetric: 393.4912 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 220/1000
2023-09-09 10:41:12.317 
Epoch 220/1000 
	 loss: 387.1924, MinusLogProbMetric: 387.1924, val_loss: 393.5966, val_MinusLogProbMetric: 393.5966

Epoch 220: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1924 - MinusLogProbMetric: 387.1924 - val_loss: 393.5966 - val_MinusLogProbMetric: 393.5966 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 221/1000
2023-09-09 10:41:23.411 
Epoch 221/1000 
	 loss: 387.2120, MinusLogProbMetric: 387.2120, val_loss: 393.5885, val_MinusLogProbMetric: 393.5885

Epoch 221: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2120 - MinusLogProbMetric: 387.2120 - val_loss: 393.5885 - val_MinusLogProbMetric: 393.5885 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 222/1000
2023-09-09 10:41:34.378 
Epoch 222/1000 
	 loss: 387.1833, MinusLogProbMetric: 387.1833, val_loss: 393.4173, val_MinusLogProbMetric: 393.4173

Epoch 222: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1833 - MinusLogProbMetric: 387.1833 - val_loss: 393.4173 - val_MinusLogProbMetric: 393.4173 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 223/1000
2023-09-09 10:41:45.472 
Epoch 223/1000 
	 loss: 387.2055, MinusLogProbMetric: 387.2055, val_loss: 393.3514, val_MinusLogProbMetric: 393.3514

Epoch 223: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2055 - MinusLogProbMetric: 387.2055 - val_loss: 393.3514 - val_MinusLogProbMetric: 393.3514 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 224/1000
2023-09-09 10:41:56.399 
Epoch 224/1000 
	 loss: 387.1977, MinusLogProbMetric: 387.1977, val_loss: 393.7792, val_MinusLogProbMetric: 393.7792

Epoch 224: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1977 - MinusLogProbMetric: 387.1977 - val_loss: 393.7792 - val_MinusLogProbMetric: 393.7792 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 225/1000
2023-09-09 10:42:07.581 
Epoch 225/1000 
	 loss: 387.2000, MinusLogProbMetric: 387.2000, val_loss: 393.6077, val_MinusLogProbMetric: 393.6077

Epoch 225: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2000 - MinusLogProbMetric: 387.2000 - val_loss: 393.6077 - val_MinusLogProbMetric: 393.6077 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 226/1000
2023-09-09 10:42:18.883 
Epoch 226/1000 
	 loss: 387.1761, MinusLogProbMetric: 387.1761, val_loss: 393.6695, val_MinusLogProbMetric: 393.6695

Epoch 226: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1761 - MinusLogProbMetric: 387.1761 - val_loss: 393.6695 - val_MinusLogProbMetric: 393.6695 - lr: 2.7778e-05 - 11s/epoch - 58ms/step
Epoch 227/1000
2023-09-09 10:42:29.506 
Epoch 227/1000 
	 loss: 387.1910, MinusLogProbMetric: 387.1910, val_loss: 393.8765, val_MinusLogProbMetric: 393.8765

Epoch 227: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1910 - MinusLogProbMetric: 387.1910 - val_loss: 393.8765 - val_MinusLogProbMetric: 393.8765 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 228/1000
2023-09-09 10:42:40.976 
Epoch 228/1000 
	 loss: 387.2016, MinusLogProbMetric: 387.2016, val_loss: 393.3725, val_MinusLogProbMetric: 393.3725

Epoch 228: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2016 - MinusLogProbMetric: 387.2016 - val_loss: 393.3725 - val_MinusLogProbMetric: 393.3725 - lr: 2.7778e-05 - 11s/epoch - 59ms/step
Epoch 229/1000
2023-09-09 10:42:51.370 
Epoch 229/1000 
	 loss: 387.1457, MinusLogProbMetric: 387.1457, val_loss: 393.5178, val_MinusLogProbMetric: 393.5178

Epoch 229: val_loss did not improve from 393.20908
196/196 - 10s - loss: 387.1457 - MinusLogProbMetric: 387.1457 - val_loss: 393.5178 - val_MinusLogProbMetric: 393.5178 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 230/1000
2023-09-09 10:43:02.158 
Epoch 230/1000 
	 loss: 387.1537, MinusLogProbMetric: 387.1537, val_loss: 393.8075, val_MinusLogProbMetric: 393.8075

Epoch 230: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1537 - MinusLogProbMetric: 387.1537 - val_loss: 393.8075 - val_MinusLogProbMetric: 393.8075 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 231/1000
2023-09-09 10:43:13.111 
Epoch 231/1000 
	 loss: 387.2047, MinusLogProbMetric: 387.2047, val_loss: 393.5350, val_MinusLogProbMetric: 393.5350

Epoch 231: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2047 - MinusLogProbMetric: 387.2047 - val_loss: 393.5350 - val_MinusLogProbMetric: 393.5350 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 232/1000
2023-09-09 10:43:23.377 
Epoch 232/1000 
	 loss: 387.1651, MinusLogProbMetric: 387.1651, val_loss: 393.3495, val_MinusLogProbMetric: 393.3495

Epoch 232: val_loss did not improve from 393.20908
196/196 - 10s - loss: 387.1651 - MinusLogProbMetric: 387.1651 - val_loss: 393.3495 - val_MinusLogProbMetric: 393.3495 - lr: 2.7778e-05 - 10s/epoch - 52ms/step
Epoch 233/1000
2023-09-09 10:43:34.227 
Epoch 233/1000 
	 loss: 387.1730, MinusLogProbMetric: 387.1730, val_loss: 393.3680, val_MinusLogProbMetric: 393.3680

Epoch 233: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1730 - MinusLogProbMetric: 387.1730 - val_loss: 393.3680 - val_MinusLogProbMetric: 393.3680 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 234/1000
2023-09-09 10:43:44.565 
Epoch 234/1000 
	 loss: 387.1517, MinusLogProbMetric: 387.1517, val_loss: 393.4019, val_MinusLogProbMetric: 393.4019

Epoch 234: val_loss did not improve from 393.20908
196/196 - 10s - loss: 387.1517 - MinusLogProbMetric: 387.1517 - val_loss: 393.4019 - val_MinusLogProbMetric: 393.4019 - lr: 2.7778e-05 - 10s/epoch - 53ms/step
Epoch 235/1000
2023-09-09 10:43:55.647 
Epoch 235/1000 
	 loss: 387.1929, MinusLogProbMetric: 387.1929, val_loss: 393.4304, val_MinusLogProbMetric: 393.4304

Epoch 235: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1929 - MinusLogProbMetric: 387.1929 - val_loss: 393.4304 - val_MinusLogProbMetric: 393.4304 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 236/1000
2023-09-09 10:44:06.367 
Epoch 236/1000 
	 loss: 387.2310, MinusLogProbMetric: 387.2310, val_loss: 394.0703, val_MinusLogProbMetric: 394.0703

Epoch 236: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2310 - MinusLogProbMetric: 387.2310 - val_loss: 394.0703 - val_MinusLogProbMetric: 394.0703 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 237/1000
2023-09-09 10:44:17.365 
Epoch 237/1000 
	 loss: 387.1929, MinusLogProbMetric: 387.1929, val_loss: 393.7863, val_MinusLogProbMetric: 393.7863

Epoch 237: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1929 - MinusLogProbMetric: 387.1929 - val_loss: 393.7863 - val_MinusLogProbMetric: 393.7863 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 238/1000
2023-09-09 10:44:28.607 
Epoch 238/1000 
	 loss: 387.2277, MinusLogProbMetric: 387.2277, val_loss: 393.4163, val_MinusLogProbMetric: 393.4163

Epoch 238: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2277 - MinusLogProbMetric: 387.2277 - val_loss: 393.4163 - val_MinusLogProbMetric: 393.4163 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 239/1000
2023-09-09 10:44:39.806 
Epoch 239/1000 
	 loss: 387.1900, MinusLogProbMetric: 387.1900, val_loss: 393.6864, val_MinusLogProbMetric: 393.6864

Epoch 239: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1900 - MinusLogProbMetric: 387.1900 - val_loss: 393.6864 - val_MinusLogProbMetric: 393.6864 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 240/1000
2023-09-09 10:44:50.777 
Epoch 240/1000 
	 loss: 387.1447, MinusLogProbMetric: 387.1447, val_loss: 393.6742, val_MinusLogProbMetric: 393.6742

Epoch 240: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1447 - MinusLogProbMetric: 387.1447 - val_loss: 393.6742 - val_MinusLogProbMetric: 393.6742 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 241/1000
2023-09-09 10:45:01.813 
Epoch 241/1000 
	 loss: 387.2550, MinusLogProbMetric: 387.2550, val_loss: 393.3660, val_MinusLogProbMetric: 393.3660

Epoch 241: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2550 - MinusLogProbMetric: 387.2550 - val_loss: 393.3660 - val_MinusLogProbMetric: 393.3660 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 242/1000
2023-09-09 10:45:12.652 
Epoch 242/1000 
	 loss: 387.2884, MinusLogProbMetric: 387.2884, val_loss: 393.4544, val_MinusLogProbMetric: 393.4544

Epoch 242: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.2884 - MinusLogProbMetric: 387.2884 - val_loss: 393.4544 - val_MinusLogProbMetric: 393.4544 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 243/1000
2023-09-09 10:45:23.779 
Epoch 243/1000 
	 loss: 387.1793, MinusLogProbMetric: 387.1793, val_loss: 393.8661, val_MinusLogProbMetric: 393.8661

Epoch 243: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.1793 - MinusLogProbMetric: 387.1793 - val_loss: 393.8661 - val_MinusLogProbMetric: 393.8661 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 244/1000
2023-09-09 10:45:34.947 
Epoch 244/1000 
	 loss: 387.0888, MinusLogProbMetric: 387.0888, val_loss: 393.7687, val_MinusLogProbMetric: 393.7687

Epoch 244: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0888 - MinusLogProbMetric: 387.0888 - val_loss: 393.7687 - val_MinusLogProbMetric: 393.7687 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 245/1000
2023-09-09 10:45:46.129 
Epoch 245/1000 
	 loss: 387.0706, MinusLogProbMetric: 387.0706, val_loss: 393.4858, val_MinusLogProbMetric: 393.4858

Epoch 245: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0706 - MinusLogProbMetric: 387.0706 - val_loss: 393.4858 - val_MinusLogProbMetric: 393.4858 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 246/1000
2023-09-09 10:45:57.478 
Epoch 246/1000 
	 loss: 387.0916, MinusLogProbMetric: 387.0916, val_loss: 393.6150, val_MinusLogProbMetric: 393.6150

Epoch 246: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0916 - MinusLogProbMetric: 387.0916 - val_loss: 393.6150 - val_MinusLogProbMetric: 393.6150 - lr: 2.7778e-05 - 11s/epoch - 58ms/step
Epoch 247/1000
2023-09-09 10:46:08.662 
Epoch 247/1000 
	 loss: 387.0580, MinusLogProbMetric: 387.0580, val_loss: 393.4876, val_MinusLogProbMetric: 393.4876

Epoch 247: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0580 - MinusLogProbMetric: 387.0580 - val_loss: 393.4876 - val_MinusLogProbMetric: 393.4876 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 248/1000
2023-09-09 10:46:19.836 
Epoch 248/1000 
	 loss: 387.0745, MinusLogProbMetric: 387.0745, val_loss: 393.6922, val_MinusLogProbMetric: 393.6922

Epoch 248: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0745 - MinusLogProbMetric: 387.0745 - val_loss: 393.6922 - val_MinusLogProbMetric: 393.6922 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 249/1000
2023-09-09 10:46:30.540 
Epoch 249/1000 
	 loss: 387.0872, MinusLogProbMetric: 387.0872, val_loss: 393.8295, val_MinusLogProbMetric: 393.8295

Epoch 249: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0872 - MinusLogProbMetric: 387.0872 - val_loss: 393.8295 - val_MinusLogProbMetric: 393.8295 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 250/1000
2023-09-09 10:46:41.815 
Epoch 250/1000 
	 loss: 387.0443, MinusLogProbMetric: 387.0443, val_loss: 393.3620, val_MinusLogProbMetric: 393.3620

Epoch 250: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0443 - MinusLogProbMetric: 387.0443 - val_loss: 393.3620 - val_MinusLogProbMetric: 393.3620 - lr: 2.7778e-05 - 11s/epoch - 58ms/step
Epoch 251/1000
2023-09-09 10:46:52.925 
Epoch 251/1000 
	 loss: 387.0490, MinusLogProbMetric: 387.0490, val_loss: 393.5881, val_MinusLogProbMetric: 393.5881

Epoch 251: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0490 - MinusLogProbMetric: 387.0490 - val_loss: 393.5881 - val_MinusLogProbMetric: 393.5881 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 252/1000
2023-09-09 10:47:04.127 
Epoch 252/1000 
	 loss: 387.0308, MinusLogProbMetric: 387.0308, val_loss: 393.5710, val_MinusLogProbMetric: 393.5710

Epoch 252: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0308 - MinusLogProbMetric: 387.0308 - val_loss: 393.5710 - val_MinusLogProbMetric: 393.5710 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 253/1000
2023-09-09 10:47:15.061 
Epoch 253/1000 
	 loss: 387.0220, MinusLogProbMetric: 387.0220, val_loss: 393.6076, val_MinusLogProbMetric: 393.6076

Epoch 253: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0220 - MinusLogProbMetric: 387.0220 - val_loss: 393.6076 - val_MinusLogProbMetric: 393.6076 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 254/1000
2023-09-09 10:47:26.052 
Epoch 254/1000 
	 loss: 387.0082, MinusLogProbMetric: 387.0082, val_loss: 393.7636, val_MinusLogProbMetric: 393.7636

Epoch 254: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0082 - MinusLogProbMetric: 387.0082 - val_loss: 393.7636 - val_MinusLogProbMetric: 393.7636 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 255/1000
2023-09-09 10:47:37.227 
Epoch 255/1000 
	 loss: 386.9988, MinusLogProbMetric: 386.9988, val_loss: 393.4984, val_MinusLogProbMetric: 393.4984

Epoch 255: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9988 - MinusLogProbMetric: 386.9988 - val_loss: 393.4984 - val_MinusLogProbMetric: 393.4984 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 256/1000
2023-09-09 10:47:47.825 
Epoch 256/1000 
	 loss: 386.9796, MinusLogProbMetric: 386.9796, val_loss: 393.3501, val_MinusLogProbMetric: 393.3501

Epoch 256: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9796 - MinusLogProbMetric: 386.9796 - val_loss: 393.3501 - val_MinusLogProbMetric: 393.3501 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 257/1000
2023-09-09 10:47:59.036 
Epoch 257/1000 
	 loss: 387.0052, MinusLogProbMetric: 387.0052, val_loss: 393.5622, val_MinusLogProbMetric: 393.5622

Epoch 257: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0052 - MinusLogProbMetric: 387.0052 - val_loss: 393.5622 - val_MinusLogProbMetric: 393.5622 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 258/1000
2023-09-09 10:48:09.786 
Epoch 258/1000 
	 loss: 386.9891, MinusLogProbMetric: 386.9891, val_loss: 393.4329, val_MinusLogProbMetric: 393.4329

Epoch 258: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9891 - MinusLogProbMetric: 386.9891 - val_loss: 393.4329 - val_MinusLogProbMetric: 393.4329 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 259/1000
2023-09-09 10:48:20.956 
Epoch 259/1000 
	 loss: 386.9663, MinusLogProbMetric: 386.9663, val_loss: 393.4565, val_MinusLogProbMetric: 393.4565

Epoch 259: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9663 - MinusLogProbMetric: 386.9663 - val_loss: 393.4565 - val_MinusLogProbMetric: 393.4565 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 260/1000
2023-09-09 10:48:31.644 
Epoch 260/1000 
	 loss: 386.9746, MinusLogProbMetric: 386.9746, val_loss: 393.6110, val_MinusLogProbMetric: 393.6110

Epoch 260: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9746 - MinusLogProbMetric: 386.9746 - val_loss: 393.6110 - val_MinusLogProbMetric: 393.6110 - lr: 2.7778e-05 - 11s/epoch - 55ms/step
Epoch 261/1000
2023-09-09 10:48:42.854 
Epoch 261/1000 
	 loss: 387.0060, MinusLogProbMetric: 387.0060, val_loss: 393.5966, val_MinusLogProbMetric: 393.5966

Epoch 261: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0060 - MinusLogProbMetric: 387.0060 - val_loss: 393.5966 - val_MinusLogProbMetric: 393.5966 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 262/1000
2023-09-09 10:48:53.529 
Epoch 262/1000 
	 loss: 386.9568, MinusLogProbMetric: 386.9568, val_loss: 393.7821, val_MinusLogProbMetric: 393.7821

Epoch 262: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9568 - MinusLogProbMetric: 386.9568 - val_loss: 393.7821 - val_MinusLogProbMetric: 393.7821 - lr: 2.7778e-05 - 11s/epoch - 54ms/step
Epoch 263/1000
2023-09-09 10:49:04.545 
Epoch 263/1000 
	 loss: 386.9927, MinusLogProbMetric: 386.9927, val_loss: 393.5942, val_MinusLogProbMetric: 393.5942

Epoch 263: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9927 - MinusLogProbMetric: 386.9927 - val_loss: 393.5942 - val_MinusLogProbMetric: 393.5942 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 264/1000
2023-09-09 10:49:15.576 
Epoch 264/1000 
	 loss: 386.9383, MinusLogProbMetric: 386.9383, val_loss: 393.6577, val_MinusLogProbMetric: 393.6577

Epoch 264: val_loss did not improve from 393.20908
196/196 - 11s - loss: 386.9383 - MinusLogProbMetric: 386.9383 - val_loss: 393.6577 - val_MinusLogProbMetric: 393.6577 - lr: 2.7778e-05 - 11s/epoch - 56ms/step
Epoch 265/1000
2023-09-09 10:49:26.771 
Epoch 265/1000 
	 loss: 387.0578, MinusLogProbMetric: 387.0578, val_loss: 393.6217, val_MinusLogProbMetric: 393.6217

Epoch 265: val_loss did not improve from 393.20908
196/196 - 11s - loss: 387.0578 - MinusLogProbMetric: 387.0578 - val_loss: 393.6217 - val_MinusLogProbMetric: 393.6217 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 266/1000
2023-09-09 10:49:37.688 
Epoch 266/1000 
	 loss: 387.0135, MinusLogProbMetric: 387.0135, val_loss: 393.3755, val_MinusLogProbMetric: 393.3755

Epoch 266: val_loss did not improve from 393.20908
Restoring model weights from the end of the best epoch: 166.
196/196 - 11s - loss: 387.0135 - MinusLogProbMetric: 387.0135 - val_loss: 393.3755 - val_MinusLogProbMetric: 393.3755 - lr: 2.7778e-05 - 11s/epoch - 57ms/step
Epoch 266: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 2147.9299805699848 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
LR metric calculation completed in 19933.170346694067 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
