{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to tf2_12 (Python 3.10.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 09:29:00.399922: Importing os...\n",
      "2023-08-25 09:29:00.400062: Importing sys...\n",
      "2023-08-25 09:29:00.400126: Importing and initializing argparse...\n",
      "Visible devices: [0]\n",
      "2023-08-25 09:29:00.400308: Importing timer from timeit...\n",
      "2023-08-25 09:29:00.400390: Setting env variables for tf import (only device [0] will be available)...\n",
      "2023-08-25 09:29:00.400520: Importing numpy...\n",
      "2023-08-25 09:29:00.490641: Importing pandas...\n",
      "2023-08-25 09:29:00.677796: Importing shutil...\n",
      "2023-08-25 09:29:00.677958: Importing subprocess...\n",
      "2023-08-25 09:29:00.678027: Importing tensorflow...\n",
      "Tensorflow version: 2.12.0\n",
      "2023-08-25 09:29:03.148614: Importing tensorflow_probability...\n",
      "Tensorflow probability version: 0.20.1\n",
      "2023-08-25 09:29:03.619347: Importing textwrap...\n",
      "2023-08-25 09:29:03.619455: Importing timeit...\n",
      "2023-08-25 09:29:03.619535: Importing traceback...\n",
      "2023-08-25 09:29:03.619600: Importing typing...\n",
      "2023-08-25 09:29:03.619680: Setting tf configs...\n",
      "2023-08-25 09:29:03.817258: Importing custom module...\n",
      "Successfully loaded GPU model: NVIDIA A40\n",
      "2023-08-25 09:29:04.967198: All modues imported successfully.\n",
      "Directory ../../results/MAFN_new/ already exists.\n",
      "Directory ../../results/MAFN_new/run_1/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 1/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_2/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 2/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_3/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 3/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_4/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 4/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_5/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 5/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_6/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 6/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_7/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 7/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_8/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 8/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_9/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 9/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "Directory ../../results/MAFN_new/run_10/ already exists.\n",
      "Skipping it.\n",
      "===========\n",
      "Run 10/360 already exists. Skipping it.\n",
      "===========\n",
      "\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.25 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer (LogProbLaye  (None,)                  346960    \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4fa8483e80>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f4f8842b190>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4f8842b190>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f4f884c3700>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4f884499c0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4f88302320>, <keras.callbacks.ModelCheckpoint object at 0x7f4f88302470>, <keras.callbacks.EarlyStopping object at 0x7f4f88302680>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4f883026b0>, <keras.callbacks.TerminateOnNaN object at 0x7f4f883023e0>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:29:09.554470\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:29:33.496 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167848192780994674146934784.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 24s - loss: nan - MinusLogProbMetric: 18167848192780994674146934784.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 24s/epoch - 121ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.05.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.13 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_1 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_1'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f535d34b7f0>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f535d22e770>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f535d22e770>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f535cbba2c0>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f535d25e8f0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f535d25ee90>, <keras.callbacks.ModelCheckpoint object at 0x7f535d25ef50>, <keras.callbacks.EarlyStopping object at 0x7f535d25f1c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f535d25f1f0>, <keras.callbacks.TerminateOnNaN object at 0x7f535d25ee30>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:29:34.918791\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:29:53.170 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167325190693016860939517952.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 18s - loss: nan - MinusLogProbMetric: 18167325190693016860939517952.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 18s/epoch - 93ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.025.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.14 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_2 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_2'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4f103021a0>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f4f3c537f40>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4f3c537f40>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f4f3c440b50>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4f3c4c07f0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4f3c4c1cc0>, <keras.callbacks.ModelCheckpoint object at 0x7f4f3c4c1d80>, <keras.callbacks.EarlyStopping object at 0x7f4f3c4c1ff0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4f3c4c2020>, <keras.callbacks.TerminateOnNaN object at 0x7f4f3c4c1c60>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:29:54.596587\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:30:13.856 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167328732467879013173428224.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 19s - loss: nan - MinusLogProbMetric: 18167328732467879013173428224.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 19s/epoch - 98ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.0125.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.17 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_34 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_3 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_3'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4f3c42c100>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f4ef45f5210>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4ef45f5210>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f4ef437f370>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f535cb13280>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f535cb12b30>, <keras.callbacks.ModelCheckpoint object at 0x7f535cb115d0>, <keras.callbacks.EarlyStopping object at 0x7f535cb12740>, <keras.callbacks.ReduceLROnPlateau object at 0x7f535cb11750>, <keras.callbacks.TerminateOnNaN object at 0x7f535cb11f00>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:30:16.076211\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:30:35.912 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167848192780994674146934784.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 20s - loss: nan - MinusLogProbMetric: 18167848192780994674146934784.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 20s/epoch - 101ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.00625.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.21 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_45 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_4 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_4'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4ef4351720>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f4f88605030>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4f88605030>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f4f88502ce0>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4f106a78e0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4f106a6cb0>, <keras.callbacks.ModelCheckpoint object at 0x7f4f106a60e0>, <keras.callbacks.EarlyStopping object at 0x7f4f106a78b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4f106a5ae0>, <keras.callbacks.TerminateOnNaN object at 0x7f4f106a7520>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:30:38.477647\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:31:00.210 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167321648918154708705607680.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 22s - loss: nan - MinusLogProbMetric: 18167321648918154708705607680.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 22s/epoch - 110ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.003125.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.16 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_56 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_5 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_5'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4ecc51b250>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f4f3c76cca0>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4f3c76cca0>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f5353e3ea10>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5353d81450>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4ec45d1390>, <keras.callbacks.ModelCheckpoint object at 0x7f5353ef2d10>, <keras.callbacks.EarlyStopping object at 0x7f4ec47996f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5353e3e7a0>, <keras.callbacks.TerminateOnNaN object at 0x7f4ec45c70a0>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:31:02.428760\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:31:23.432 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167321648918154708705607680.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 21s - loss: nan - MinusLogProbMetric: 18167321648918154708705607680.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 21s/epoch - 106ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.0015625.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.14 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_67 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_6 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_6'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4f3c197520>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f535d1e1510>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f535d1e1510>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f4ef43bbdc0>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4ef457a2c0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f4ef457a980>, <keras.callbacks.ModelCheckpoint object at 0x7f4ef457aa40>, <keras.callbacks.EarlyStopping object at 0x7f4ef457acb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4ef457ace0>, <keras.callbacks.TerminateOnNaN object at 0x7f4ef457a920>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:31:25.614404\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:31:44.872 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167324010101396143528214528.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 19s - loss: nan - MinusLogProbMetric: 18167324010101396143528214528.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 19s/epoch - 98ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.00078125.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.16 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_78 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_7 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_7'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f535d109000>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f4ef432cdf0>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4ef432cdf0>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f4f3c3f1270>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f50cd89b070>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f50cd89a440>, <keras.callbacks.ModelCheckpoint object at 0x7f50cd89b250>, <keras.callbacks.EarlyStopping object at 0x7f50cd899450>, <keras.callbacks.ReduceLROnPlateau object at 0x7f50cd89af20>, <keras.callbacks.TerminateOnNaN object at 0x7f50cd89a380>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:31:47.137827\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:32:08.155 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167841109231270369679114240.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 21s - loss: nan - MinusLogProbMetric: 18167841109231270369679114240.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 21s/epoch - 107ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.000390625.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.12 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_89 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_8 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_8'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4ec4468ca0>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f5353cf08e0>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f5353cf08e0>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f4ec445b7f0>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f5353a05de0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f5353a06290>, <keras.callbacks.ModelCheckpoint object at 0x7f5353a06350>, <keras.callbacks.EarlyStopping object at 0x7f5353a065c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f5353a065f0>, <keras.callbacks.TerminateOnNaN object at 0x7f5353a06230>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:32:09.608623\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:32:27.812 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167324010101396143528214528.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 18s - loss: nan - MinusLogProbMetric: 18167324010101396143528214528.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 18s/epoch - 93ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 0.0001953125.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.17 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_100 (InputLayer)      [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_9 (LogProbLa  (None,)                  346960    \n",
      " yer)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_9'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f535cc29e10>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f4ecc7c7d90>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f4ecc7c7d90>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f535cc29300>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f4ecc7718d0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f535cc28f70>, <keras.callbacks.ModelCheckpoint object at 0x7f4ecc2ff550>, <keras.callbacks.EarlyStopping object at 0x7f4ecc3b9690>, <keras.callbacks.ReduceLROnPlateau object at 0x7f4ef45f6170>, <keras.callbacks.TerminateOnNaN object at 0x7f4ef47b5c00>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:32:30.190050\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:32:49.880 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167324010101396143528214528.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 20s - loss: nan - MinusLogProbMetric: 18167324010101396143528214528.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 20s/epoch - 100ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 9.765625e-05.\n",
      "===========\n",
      "Generating train data for run 11.\n",
      "===========\n",
      "Train data generated in 0.20 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 377}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_new/run_11/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_new/run_11\n",
      "self.data_kwargs: {'seed': 377}\n",
      "self.x_data: [[ 5.418637   7.123012   6.073744   5.4346952]\n",
      " [ 4.2583976  7.4150143  3.5163307  9.460685 ]\n",
      " [ 4.928777   7.5789065  5.9634843  5.460042 ]\n",
      " ...\n",
      " [ 4.2756643  6.918659   5.811992   9.122478 ]\n",
      " [10.603988   4.4559946  7.6048694  5.576913 ]\n",
      " [ 6.3165264  7.204932   5.9651775  5.473836 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_111 (InputLayer)      [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer_10 (LogProbL  (None,)                  346960    \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,960\n",
      "Trainable params: 346,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer_10'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f4ef428dc60>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f535cc29c30>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f535cc29c30>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.1, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f535d0ed420>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f535c9fceb0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_new/run_11/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f535c9ffd00>, <keras.callbacks.ModelCheckpoint object at 0x7f535c9fcb80>, <keras.callbacks.EarlyStopping object at 0x7f535c9fed40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f535c9fef80>, <keras.callbacks.TerminateOnNaN object at 0x7f535c9ff280>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 9.941606 ,  3.1228497,  7.9799867,  4.1969004],\n",
      "       [ 5.01458  ,  6.331631 ,  6.064331 ,  5.413579 ],\n",
      "       [ 4.225043 ,  6.59531  ,  4.463144 , 10.807351 ],\n",
      "       ...,\n",
      "       [ 4.2250175,  4.8564253,  4.7948027,  8.0237875],\n",
      "       [ 4.219043 ,  6.430962 ,  5.3610916,  7.7749324],\n",
      "       [ 4.2046795,  6.9549227,  3.282429 ,  7.3072014]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_new/run_11/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 11/360 with hyperparameters:\n",
      "timestamp = 2023-08-25 09:32:52.096775\n",
      "ndims = 4\n",
      "seed_train = 377\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 10\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 346960\n",
      "epochs_input = 1000\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [5.418637  7.123012  6.073744  5.4346952]\n",
      "Epoch 1/1000\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "Warning: Encountered NaNs in loss. Fraction of non-NaN elements: Tensor(\"MLP/truediv:0\", shape=(), dtype=float32)\n",
      "2023-08-25 09:33:10.866 \n",
      "Epoch 1/1000 \n",
      "\t loss: nan, MinusLogProbMetric: 18167848192780994674146934784.0000, val_loss: nan, val_MinusLogProbMetric: nan\n",
      "\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "196/196 - 19s - loss: nan - MinusLogProbMetric: 18167848192780994674146934784.0000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.1000 - 19s/epoch - 95ms/step\n",
      "The loss history contains NaN values.\n",
      "Training failed: trying again with seed 326159 and lr 4.8828125e-05.\n",
      "===========\n",
      "Run 11/360 failed.\n",
      "Exception type: Exception\n",
      "Exception message: Training failed for the maximum number of retry.\n",
      "Stack trace: ['File : <ipython-input-1-6f02571c59ea> , Line : 656, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : <ipython-input-1-6f02571c59ea> , Line : 313, Func.Name : train_function, Message : raise Exception(\"Training failed for the maximum number of retry.\")']\n",
      "===========\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Passed run 11.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py\u001b[0m in \u001b[0;36mline 638\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py?line=635'>636</a>\u001b[0m \u001b[39mfor\u001b[39;00m hidden_layers \u001b[39min\u001b[39;00m hidden_layers_list:\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py?line=636'>637</a>\u001b[0m     \u001b[39mif\u001b[39;00m run_number \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py?line=637'>638</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPassed run 11.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py?line=638'>639</a>\u001b[0m     start_run: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m timer()\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/MAFN/c_Main_MAFN.py?line=639'>640</a>\u001b[0m     hllabel: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(e) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m hidden_layers)\n",
      "\u001b[0;31mException\u001b[0m: Passed run 11."
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "######################################### Initialize #########################################\n",
    "##############################################################################################\n",
    "\n",
    "visible_devices = [0]\n",
    "import datetime\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing os...\")\n",
    "import os\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing sys...\")\n",
    "import sys\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing and initializing argparse...\")\n",
    "if not any(\"ipykernel\" in arg for arg in sys.argv):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-v\", \"--visible_devices\", help=\"Set visible devices\", nargs='*', type=int, default=visible_devices)\n",
    "    args = parser.parse_args()\n",
    "    visible_devices = args.visible_devices if args.visible_devices else visible_devices\n",
    "print(\"Visible devices:\", visible_devices)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing timer from timeit...\")\n",
    "from timeit import default_timer as timer\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Setting env variables for tf import (only device\", visible_devices, \"will be available)...\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([str(i) for i in visible_devices])\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing numpy...\")\n",
    "import numpy as np\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing pandas...\")\n",
    "import pandas as pd\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing shutil...\")\n",
    "import shutil\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing subprocess...\")\n",
    "import subprocess\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing tensorflow...\")\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing tensorflow_probability...\")\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "print(\"Tensorflow probability version:\", tfp.__version__)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing textwrap...\")\n",
    "import textwrap\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing timeit...\")\n",
    "from timeit import default_timer as timer\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing traceback...\")\n",
    "import traceback\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing typing...\")\n",
    "from typing import List, Tuple, Dict, Union, Optional, Any\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Setting tf configs...\")\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu_device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu_device, True)\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing custom module...\")\n",
    "\n",
    "sys.path.append('../../../code')\n",
    "import Bijectors, Distributions, MixtureDistributions, Plotters, Trainer, Utils # type: ignore\n",
    "\n",
    "sys.path.insert(0,'../../../../')\n",
    "import GenerativeModelsMetrics as GMetrics # type: ignore\n",
    "\n",
    "def get_gpu_info() -> Optional[List[str]]:\n",
    "    try:\n",
    "        gpu_info: str = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv,noheader\"]).decode('utf-8')\n",
    "        return gpu_info.strip().split('\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "gpu_models: Optional[List[str]] = get_gpu_info()\n",
    "if gpu_models:\n",
    "    training_device: str = gpu_models[0]\n",
    "    print(\"Successfully loaded GPU model: {}\".format(training_device))\n",
    "else:\n",
    "    training_device = 'undetermined'\n",
    "    print(\"Failed to load GPU model. Defaulting to 'undetermined'.\")\n",
    "    \n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"All modues imported successfully.\")\n",
    "\n",
    "##############################################################################################\n",
    "####################################### Helper functions #####################################\n",
    "##############################################################################################\n",
    "\n",
    "def MixtureGaussian(ncomp: int,\n",
    "                    ndims: int,\n",
    "                    seed: int = 0) -> tfp.distributions.Mixture:\n",
    "    targ_dist: tfp.distributions.Mixture = MixtureDistributions.MixMultiNormal1(ncomp,ndims,seed=seed)\n",
    "    return targ_dist\n",
    "\n",
    "def get_io_kwargs(path_to_results: str) -> Dict[str,Any]:\n",
    "    return {'results_path': path_to_results,\n",
    "            'load_weights': True,\n",
    "            'load_results': True}\n",
    "    \n",
    "def get_data_kwargs(seed: int = 0) -> Dict[str,Any]:\n",
    "    return {'seed': seed}\n",
    "\n",
    "def get_compiler_kwargs(lr: float,\n",
    "                        ignore_nans: bool,\n",
    "                        nan_threshold: float\n",
    "                       ) -> Dict[str,Any]:\n",
    "    compiler_kwargs = {'optimizer': {'class_name': 'Custom>Adam', # this gives the new Adam optimizer\n",
    "                                     'config': {'learning_rate': lr,\n",
    "                                                'beta_1': 0.9,\n",
    "                                                'beta_2': 0.999,\n",
    "                                                'epsilon': 1e-07,\n",
    "                                                'amsgrad': True}},\n",
    "                       'metrics': [{'class_name': 'MinusLogProbMetric',\n",
    "                                    'config': {'ignore_nans': ignore_nans, \n",
    "                                               'debug_print_mode': False}}],\n",
    "                       'loss': {'class_name': 'MinusLogProbLoss', \n",
    "                                'config': {'name': \"MLP\", \n",
    "                                           'ignore_nans': ignore_nans, \n",
    "                                           'nan_threshold': nan_threshold, \n",
    "                                           'debug_print_mode': False}}}\n",
    "    return compiler_kwargs\n",
    "    \n",
    "def get_callbacks_kwargs(checkpoint_path: str,\n",
    "                         es_min_delta: float,\n",
    "                         es_patience: int,\n",
    "                         lr_reduce_factor: float,\n",
    "                         lr_min_delta: float,\n",
    "                         lr_patience: int,\n",
    "                         min_lr: float\n",
    "                         ) -> List[Dict[str,Any]]:\n",
    "    callbacks_kwargs = [{'class_name': 'PrintEpochInfo',\n",
    "                         'config': {}},\n",
    "                        #{'class_name': 'HandleNaNCallback',\n",
    "                        # 'config': {'checkpoint_path': checkpoint_path,\n",
    "                        #            'lr_reduction_factor': lr_reduce_factor_on_nan,\n",
    "                        #            'random_seed_var': np.random.randint(1000000)}},\n",
    "                        #{'class_name': 'TerminateOnNaNFractionCallback',\n",
    "                        # 'config': {'threshold': 0.1,\n",
    "                        #            'validation_data': X_data_val}},\n",
    "                        {'class_name': 'ModelCheckpoint',\n",
    "                         'config': {'filepath': checkpoint_path,\n",
    "                                    'monitor': 'val_loss',\n",
    "                                    'save_best_only': True,\n",
    "                                    'save_weights_only': True,\n",
    "                                    'verbose': 1,\n",
    "                                    'mode': 'auto',\n",
    "                                    'save_freq': 'epoch'}},\n",
    "                        {'class_name': 'EarlyStopping',\n",
    "                         'config': {'monitor': 'val_loss', \n",
    "                                    'min_delta': es_min_delta, \n",
    "                                    'patience': es_patience, \n",
    "                                    'verbose': 1,\n",
    "                                    'mode': 'auto', \n",
    "                                    'baseline': None, \n",
    "                                    'restore_best_weights': True}},\n",
    "                        {'class_name': 'ReduceLROnPlateau', \n",
    "                         'config': {'monitor': 'val_loss', \n",
    "                                    'factor': lr_reduce_factor, \n",
    "                                    'min_delta': lr_min_delta, \n",
    "                                    'patience': lr_patience, \n",
    "                                    'min_lr': min_lr}},\n",
    "                        {'class_name': 'TerminateOnNaN', 'config': {}}\n",
    "                        ]\n",
    "    return callbacks_kwargs\n",
    "\n",
    "def get_fit_kwargs(batch_size: int,\n",
    "                   epochs_input: int,\n",
    "                   validation_data: Tuple[Union[np.ndarray,tf.Tensor],Union[np.ndarray,tf.Tensor]],\n",
    "                   shuffle: bool,\n",
    "                   verbose: int\n",
    "                  ) -> Dict[str,Any]:\n",
    "    fit_kwargs = {'batch_size': batch_size, \n",
    "                  'epochs': epochs_input, \n",
    "                  'validation_data': validation_data,\n",
    "                  'shuffle': shuffle, \n",
    "                  'verbose': verbose}\n",
    "    return fit_kwargs\n",
    "\n",
    "def train_function(seeds: List[int],\n",
    "                   nsamples: List[int],\n",
    "                   run_number: int,\n",
    "                   base_dist: tfp.distributions.Distribution,\n",
    "                   targ_dist: tfp.distributions.Distribution,\n",
    "                   hyperparams_dict: Dict[str, Any],\n",
    "                   n_runs: int,\n",
    "                   ndims: int,\n",
    "                   bijector_name: str,\n",
    "                   nbijectors: int,\n",
    "                   spline_knots: Union[int,str],\n",
    "                   range_min: int,\n",
    "                   hidden_layers: List[int],\n",
    "                   batch_size: int,\n",
    "                   epochs_input: int,\n",
    "                   lr_orig: float,\n",
    "                   es_min_delta: float,\n",
    "                   es_patience: int,\n",
    "                   lr_reduce_factor: float,\n",
    "                   lr_min_delta: float,\n",
    "                   lr_patience: int,\n",
    "                   min_lr: float,\n",
    "                   activation: str,\n",
    "                   regulariser: Optional[str],\n",
    "                   eps_regulariser: float,\n",
    "                   training_device: str,\n",
    "                   path_to_results: str,\n",
    "                   checkpoint_path: str,\n",
    "                   max_retry: int,\n",
    "                   debug_print_mode: bool\n",
    "                  ) -> Tuple[Dict[str, Any], Trainer.Trainer, int, float]:\n",
    "    succeeded = False\n",
    "    retry: int = 0\n",
    "    lr: float = lr_orig\n",
    "    while not succeeded:\n",
    "        seed_train: int\n",
    "        seed_test: int\n",
    "        seed_dist: int\n",
    "        seed_metrics: int\n",
    "        seed_train, seed_test, seed_dist, seed_metrics = seeds\n",
    "        seed_test = seed_train + 1                     \n",
    "        Utils.reset_random_seeds(seed = seed_train)\n",
    "        nsamples_train: int\n",
    "        nsamples_val: int\n",
    "        nsamples_test: int\n",
    "        nsamples_train, nsamples_val, nsamples_test = nsamples\n",
    "        X_data_train: tf.Tensor\n",
    "        X_data_val: tf.Tensor\n",
    "        Y_data_train: tf.Tensor\n",
    "        Y_data_val: tf.Tensor\n",
    "        X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,\n",
    "                                                                                       targ_dist = targ_dist,\n",
    "                                                                                       nsamples_train = nsamples_train,\n",
    "                                                                                       nsamples_val = nsamples_val,\n",
    "                                                                                       seed_train = seed_train)\n",
    "        bijector: tfp.bijectors.Bijector = Bijectors.ChooseBijector(bijector_name = bijector_name,\n",
    "                                                                    ndims = ndims,\n",
    "                                                                    spline_knots = spline_knots,\n",
    "                                                                    nbijectors = nbijectors,\n",
    "                                                                    range_min = range_min,\n",
    "                                                                    hidden_layers = hidden_layers,\n",
    "                                                                    activation = activation,\n",
    "                                                                    regulariser = regulariser,\n",
    "                                                                    eps_regulariser = eps_regulariser)\n",
    "        Utils.save_bijector_info(path_to_results, bijector)\n",
    "        print(\"Building Trainer NFObject.\\n\")\n",
    "        NFObject: Trainer.Trainer = Trainer.Trainer(base_distribution = base_dist,\n",
    "                                                    flow = bijector, \n",
    "                                                    x_data_train = X_data_train,\n",
    "                                                    y_data_train = Y_data_train,\n",
    "                                                    io_kwargs = get_io_kwargs(path_to_results = path_to_results),\n",
    "                                                    data_kwargs = get_data_kwargs(seed = seed_train),\n",
    "                                                    compiler_kwargs = get_compiler_kwargs(lr = lr_orig,\n",
    "                                                                                          ignore_nans = True,\n",
    "                                                                                          nan_threshold = 0.1),\n",
    "                                                    callbacks_kwargs = get_callbacks_kwargs(checkpoint_path = checkpoint_path,\n",
    "                                                                                            es_min_delta = es_min_delta,\n",
    "                                                                                            es_patience = es_patience,\n",
    "                                                                                            lr_reduce_factor = lr_reduce_factor,\n",
    "                                                                                            lr_min_delta = lr_min_delta,\n",
    "                                                                                            lr_patience = lr_patience,\n",
    "                                                                                            min_lr = min_lr),\n",
    "                                                    fit_kwargs = get_fit_kwargs(batch_size = batch_size,\n",
    "                                                                                epochs_input = epochs_input,\n",
    "                                                                                validation_data = (X_data_val, Y_data_val),\n",
    "                                                                                shuffle = True,\n",
    "                                                                                verbose = 2),\n",
    "                                                    debug_print_mode = debug_print_mode)\n",
    "        trainable_params: int = NFObject.trainable_params\n",
    "        non_trainable_params: int = NFObject.non_trainable_params\n",
    "        hyperparams_dict = Utils.update_hyperparams_dict(hyperparams_dict = hyperparams_dict,\n",
    "                                                         run_number = run_number,\n",
    "                                                         n_runs = n_runs,\n",
    "                                                         seeds = [seed_train, seed_test, seed_dist, seed_metrics],\n",
    "                                                         nsamples = [nsamples_train, nsamples_val, nsamples_test],\n",
    "                                                         ndims = ndims,\n",
    "                                                         corr = None,\n",
    "                                                         bijector_name = bijector_name,\n",
    "                                                         nbijectors = nbijectors,\n",
    "                                                         spline_knots = spline_knots,\n",
    "                                                         range_min = range_min,\n",
    "                                                         hllabel = '-'.join(str(e) for e in hidden_layers),\n",
    "                                                         trainable_parameters = trainable_params,\n",
    "                                                         non_trainable_parameters = non_trainable_params,\n",
    "                                                         batch_size = batch_size,\n",
    "                                                         epochs_input = epochs_input,\n",
    "                                                         activation = activation,\n",
    "                                                         regulariser = regulariser,\n",
    "                                                         eps_regulariser = eps_regulariser,\n",
    "                                                         training_device = training_device)\n",
    "        Utils.save_hyperparams_dict(path_to_results, hyperparams_dict)\n",
    "        print(\"Training model.\\n\")\n",
    "        print(\"Train first sample:\", X_data_train[0]) # type: ignore\n",
    "        NFObject.train()\n",
    "        training_time: float = NFObject.training_time # type: ignore\n",
    "        loss_history = list(NFObject.history['loss'])\n",
    "        if np.isnan(loss_history).any():\n",
    "            print(\"The loss history contains NaN values.\")\n",
    "\n",
    "        if np.isinf(loss_history).any():\n",
    "            print(\"The loss history contains Inf values.\")\n",
    "\n",
    "        if len(loss_history) > 0:\n",
    "            if np.isnan(loss_history).any() or np.isinf(loss_history).any():\n",
    "                succeeded = False\n",
    "                seed_train = np.random.randint(1000000)\n",
    "                lr = lr * lr_reduce_factor_on_nan\n",
    "                retry = retry + 1\n",
    "                print(f\"Training failed: trying again with seed {seed_train} and lr {lr}.\")\n",
    "            else:\n",
    "                succeeded = True\n",
    "                print(f\"Training succeeded with seed {seed_train}.\")\n",
    "        else:\n",
    "            succeeded = False\n",
    "            seed_train = np.random.randint(1000000)\n",
    "            lr = lr * lr_reduce_factor_on_nan\n",
    "            retry = retry + 1\n",
    "            print(f\"Training failed: trying again with seed {seed_train} and lr {lr}.\")\n",
    "            \n",
    "        if retry > max_retry:\n",
    "            raise Exception(\"Training failed for the maximum number of retry.\")\n",
    "        \n",
    "    return hyperparams_dict, NFObject, seed_train, training_time # type: ignore\n",
    "    \n",
    "\n",
    "def prediction_function(results_dict: Dict[str, Any],\n",
    "                        gpu_models: Optional[List[str]],\n",
    "                        NFObject: Trainer.Trainer,\n",
    "                        targ_dist: tfp.distributions.Distribution,\n",
    "                        seed_test: int,\n",
    "                        seed_metrics: int,\n",
    "                        n_iter: int,\n",
    "                        nsamples_test: int,\n",
    "                        batch_size_gen: int,\n",
    "                        n_slices_factor: int,\n",
    "                        dtype: type,\n",
    "                        mirror_strategy: bool,\n",
    "                       ) -> Tuple[Dict[str, Any], float, float]:\n",
    "    start_pred: float = timer()\n",
    "    t_losses_all: list = list(NFObject.history['loss']) # type: ignore\n",
    "    v_losses_all: list = list(NFObject.history['val_loss']) # type: ignore\n",
    "    lr_all: list = list(NFObject.history['lr']) # type: ignore\n",
    "    epochs_output: int = len(t_losses_all)\n",
    "    training_time: float = NFObject.training_time # type: ignore\n",
    "    try:\n",
    "        print(\"===========\\nComputing predictions\\n===========\\n\")\n",
    "        if gpu_models:\n",
    "            print(f\"Generating samples on GPU(s)...\")\n",
    "        start = timer()\n",
    "        n_samples: int = nsamples_test*n_iter\n",
    "        X_data_test: tf.Tensor = Utils.generate_and_clean_data(dist = targ_dist, \n",
    "                                                               n_samples = n_samples, \n",
    "                                                               batch_size = batch_size_gen, \n",
    "                                                               dtype = dtype, \n",
    "                                                               seed = seed_test, \n",
    "                                                               mirror_strategy = mirror_strategy)\n",
    "        print(f\"X_data_test shape: {X_data_test.shape}.\")\n",
    "        #print(f\"X_data_test first sample: {X_data_test[0]}.\") # type: ignore\n",
    "        X_data_nf: tf.Tensor = Utils.generate_and_clean_data(dist = NFObject.nf_dist, # type: ignore\n",
    "                                                             n_samples = n_samples, \n",
    "                                                             batch_size = batch_size_gen, \n",
    "                                                             dtype = dtype, \n",
    "                                                             seed = seed_test, \n",
    "                                                             mirror_strategy = mirror_strategy)\n",
    "        print(f\"X_data_nf shape: {X_data_nf.shape}.\")\n",
    "        #print(f\"X_data_nf first sample: {X_data_nf[0]}.\") # type: ignore\n",
    "        end = timer()\n",
    "        print(f\"Samples generated in {end - start:.2f} s.\")\n",
    "        print(\"Computing metrics...\")\n",
    "        start = timer()\n",
    "        X_data_test = tf.cast(X_data_test, dtype = dtype) # type: ignore\n",
    "        X_data_nf = tf.cast(X_data_nf, dtype = dtype) # type: ignore\n",
    "        DataInputs: GMetrics.TwoSampleTestInputs = GMetrics.TwoSampleTestInputs(dist_1_input = X_data_test,\n",
    "                                                                                dist_2_input = X_data_nf,\n",
    "                                                                                niter = n_iter,\n",
    "                                                                                batch_size = nsamples_test,\n",
    "                                                                                dtype_input = dtype,\n",
    "                                                                                seed_input = seed_metrics,\n",
    "                                                                                use_tf = True,\n",
    "                                                                                verbose = True)\n",
    "        tmp1, tmp2, tmp3, tmp4, tmp5 = GMetrics.utils.compute_lik_ratio_statistic(dist_ref = targ_dist,\n",
    "                                                                                  dist_alt = NFObject.nf_dist, # type: ignore\n",
    "                                                                                  sample_ref = X_data_test,\n",
    "                                                                                  sample_alt = X_data_nf,\n",
    "                                                                                  batch_size = nsamples_test)\n",
    "        logprob_ref_ref_sum_list = tmp1.numpy() # type: ignore\n",
    "        logprob_ref_alt_sum_list = tmp2.numpy() # type: ignore\n",
    "        logprob_alt_alt_sum_list = tmp3.numpy() # type: ignore\n",
    "        lik_ratio_list = tmp4.numpy() # type: ignore\n",
    "        lik_ratio_norm_list = tmp5.numpy() # type: ignore\n",
    "        KSTest: GMetrics.KSTest = GMetrics.KSTest(data_input = DataInputs,\n",
    "                                                  verbose = True)\n",
    "        SWDMetric: GMetrics.SWDMetric = GMetrics.SWDMetric(data_input = DataInputs,\n",
    "                                                           verbose = True)\n",
    "        FNMetric: GMetrics.FNMetric = GMetrics.FNMetric(data_input = DataInputs,\n",
    "                                                        verbose = True)\n",
    "        KSTest.compute()\n",
    "        SWDMetric.compute(nslices = n_slices_factor*ndims)\n",
    "        FNMetric.compute()\n",
    "        ks_result: Dict[str, np.ndarray] = KSTest.Results[-1].result_value\n",
    "        ks_lists: List[List[float]] = ks_result[\"pvalue_lists\"].tolist()\n",
    "        ks_means: List[float] = ks_result[\"pvalue_means\"].tolist()\n",
    "        ks_stds: List[float] = ks_result[\"pvalue_stds\"].tolist()\n",
    "        swd_result: Dict[str, np.ndarray] = SWDMetric.Results[-1].result_value\n",
    "        swd_lists: List[List[float]] = swd_result[\"metric_lists\"].tolist()\n",
    "        swd_means: List[float] = swd_result[\"metric_means\"].tolist()\n",
    "        swd_stds: List[float] = swd_result[\"metric_stds\"].tolist()\n",
    "        fn_result: Dict[str, np.ndarray] = FNMetric.Results[-1].result_value\n",
    "        fn_list: List[float] = fn_result[\"metric_list\"].tolist()\n",
    "        ad_lists: Optional[List[List[float]]] = None\n",
    "        ad_means: Optional[List[float]] = None\n",
    "        ad_stds: Optional[List[float]] = None\n",
    "        wd_lists: Optional[List[List[float]]] = None\n",
    "        wd_means: Optional[List[float]] = None\n",
    "        wd_stds: Optional[List[float]] = None\n",
    "        end = timer()\n",
    "        metrics_time = end - start\n",
    "        print(f\"Metrics computed in {metrics_time:.2f} s.\")\n",
    "    except:\n",
    "        try:\n",
    "            print(\"===========\\nFailed on GPU, re-trying on CPU\\n===========\\n\")\n",
    "            with tf.device('/device:CPU:0'): # type: ignore\n",
    "                start = timer()\n",
    "                n_samples = nsamples_test*n_iter\n",
    "                X_data_test = Utils.generate_and_clean_data(targ_dist = targ_dist, \n",
    "                                                            n_samples = n_samples, \n",
    "                                                            batch_size = batch_size_gen, \n",
    "                                                            dtype = dtype, \n",
    "                                                            seed = seed_test, \n",
    "                                                            mirror_strategy = False)\n",
    "                print(f\"X_data_test shape: {X_data_test.shape}.\")\n",
    "                #print(f\"X_data_test first sample: {X_data_test[0]}.\") # type: ignore\n",
    "                X_data_nf = Utils.generate_and_clean_data(nf_dist = NFObject.nf_dist, # type: ignore\n",
    "                                                          n_samples = n_samples, \n",
    "                                                          batch_size = batch_size_gen, \n",
    "                                                          dtype = dtype, \n",
    "                                                          seed = seed_test, \n",
    "                                                          mirror_strategy = False)\n",
    "                print(f\"X_data_nf shape: {X_data_nf.shape}.\")\n",
    "                #print(f\"X_data_nf first sample: {X_data_nf[0]}.\") # type: ignore\n",
    "                end = timer()\n",
    "                print(f\"Samples generated in {end - start:.2f} s.\")\n",
    "                print(\"Computing metrics...\")\n",
    "                start = timer()\n",
    "                X_data_test = tf.cast(X_data_test, dtype = dtype) # type: ignore\n",
    "                X_data_nf = tf.cast(X_data_nf, dtype = dtype) # type: ignore\n",
    "                DataInputs = GMetrics.TwoSampleTestInputs(dist_1_input = X_data_test,\n",
    "                                                          dist_2_input = X_data_nf,\n",
    "                                                          niter = n_iter,\n",
    "                                                          batch_size = nsamples_test,\n",
    "                                                          dtype_input = dtype,\n",
    "                                                          seed_input = seed_metrics,\n",
    "                                                          use_tf = True,\n",
    "                                                          verbose = True)\n",
    "                tmp1, tmp2, tmp3, tmp4, tmp5 = GMetrics.utils.compute_lik_ratio_statistic(dist_ref = targ_dist,\n",
    "                                                                                          dist_alt = NFObject.nf_dist, # type: ignore\n",
    "                                                                                          sample_ref = X_data_test,\n",
    "                                                                                          sample_alt = X_data_nf,\n",
    "                                                                                          batch_size = nsamples_test)\n",
    "                logprob_ref_ref_sum_list = tmp1.numpy() # type: ignore\n",
    "                logprob_ref_alt_sum_list = tmp2.numpy() # type: ignore\n",
    "                logprob_alt_alt_sum_list = tmp3.numpy() # type: ignore\n",
    "                lik_ratio_list = tmp4.numpy() # type: ignore\n",
    "                lik_ratio_norm_list = tmp5.numpy() # type: ignore\n",
    "                KSTest = GMetrics.KSTest(data_inputs = DataInputs,\n",
    "                                         verbose = True)\n",
    "                SWDMetric = GMetrics.SWDMetric(data_inputs = DataInputs,\n",
    "                                               verbose = True)\n",
    "                FNMetric = GMetrics.FNMetric(data_inputs = DataInputs,\n",
    "                                             verbose = True)\n",
    "                KSTest.compute()\n",
    "                SWDMetric.compute(nslices = n_slices_factor*ndims)\n",
    "                FNMetric.compute()\n",
    "                ks_result = KSTest.Results[-1].result_value\n",
    "                ks_lists = ks_result[\"pvalue_lists\"].tolist()\n",
    "                ks_means = ks_result[\"pvalue_means\"].tolist()\n",
    "                ks_stds = ks_result[\"pvalue_stds\"].tolist()\n",
    "                swd_result = SWDMetric.Results[-1].result_value\n",
    "                swd_lists = swd_result[\"metric_lists\"].tolist()\n",
    "                swd_means = swd_result[\"metric_means\"].tolist()\n",
    "                swd_stds = swd_result[\"metric_stds\"].tolist()\n",
    "                fn_result = FNMetric.Results[-1].result_value\n",
    "                fn_list = fn_result[\"metric_list\"].tolist()\n",
    "                ad_lists = None\n",
    "                ad_means = None\n",
    "                ad_stds = None\n",
    "                wd_lists = None\n",
    "                wd_means = None\n",
    "                wd_stds = None\n",
    "                end = timer()\n",
    "                metrics_time = end - start\n",
    "                print(f\"Metrics computed in {metrics_time:.2f} s.\")\n",
    "        except:\n",
    "            print(\"===========\\nFailed computing metrics\\n===========\\n\")\n",
    "            logprob_ref_ref_sum_list = []\n",
    "            logprob_ref_alt_sum_list = []\n",
    "            logprob_alt_alt_sum_list = []\n",
    "            lik_ratio_list = []\n",
    "            lik_ratio_norm_list = []\n",
    "            ks_means = []\n",
    "            ks_stds = []\n",
    "            ks_lists = []\n",
    "            ad_means = []\n",
    "            ad_stds = []\n",
    "            ad_lists = []\n",
    "            fn_list = []\n",
    "            wd_means = []\n",
    "            wd_stds = []\n",
    "            wd_lists = []\n",
    "            swd_means = []\n",
    "            swd_stds = []\n",
    "            swd_lists = []\n",
    "            metrics_time = 0.\n",
    "    if make_plots:\n",
    "        try:\n",
    "            start = timer()\n",
    "            Plotters.train_plotter(t_losses_all,v_losses_all,path_to_results)\n",
    "            Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore\n",
    "            Plotters.marginal_plot(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims) # type: ignore\n",
    "            #Plotters.sample_plotter(X_data_test,nf_dist,path_to_results)\n",
    "            end = timer()\n",
    "            plots_time: float = end - start\n",
    "            print(f\"Plots done in {plots_time:.2f} s.\")\n",
    "        except:\n",
    "            print(\"===========\\nFailed to plot\\n===========\\n\")\n",
    "    end_pred: float = timer()\n",
    "    prediction_time: float = end_pred - start_pred\n",
    "    total_time: float = training_time + prediction_time\n",
    "    results_dict = Utils.update_results_dict(results_dict = results_dict,\n",
    "                                             hyperparams_dict = hyperparams_dict,\n",
    "                                             train_loss_history = t_losses_all,\n",
    "                                             val_loss_history = v_losses_all,\n",
    "                                             lr_history = lr_all,\n",
    "                                             epochs_output = epochs_output,\n",
    "                                             training_time = training_time,\n",
    "                                             prediction_time = prediction_time,\n",
    "                                             total_time = total_time,\n",
    "                                             logprob_ref_ref_sum_list = logprob_ref_ref_sum_list,\n",
    "                                             logprob_ref_alt_sum_list = logprob_ref_alt_sum_list,\n",
    "                                             logprob_alt_alt_sum_list = logprob_alt_alt_sum_list,\n",
    "                                             lik_ratio_list = lik_ratio_list,\n",
    "                                             lik_ratio_norm_list = lik_ratio_norm_list,\n",
    "                                             ks_means = ks_means,\n",
    "                                             ks_stds = ks_stds,\n",
    "                                             ks_lists = ks_lists,\n",
    "                                             ad_means = ad_means,\n",
    "                                             ad_stds = ad_stds,\n",
    "                                             ad_lists = ad_lists,\n",
    "                                             fn_list = fn_list,\n",
    "                                             wd_means = wd_means,\n",
    "                                             wd_stds = wd_stds,\n",
    "                                             wd_lists = wd_lists,\n",
    "                                             swd_means = swd_means,\n",
    "                                             swd_stds = swd_stds,\n",
    "                                             swd_lists = swd_lists\n",
    "                                             )\n",
    "    return results_dict, prediction_time, total_time\n",
    "\n",
    "##############################################################################################\n",
    "################################## Parameters initialization #################################\n",
    "##############################################################################################\n",
    "\n",
    "### Initialize number of components ###\n",
    "ncomp: int = 3\n",
    "\n",
    "### Initialize hyperparameters lists ###\n",
    "ndims_list: List[int] = [4, 8, 16, 32, 64, 100, 200, 400, 1000]\n",
    "nbijectors_list: List[int] = [5, 10]\n",
    "hidden_layers_list: List[List[int]] = [[128, 128, 128], [256, 256, 256]]\n",
    "seeds_list: List[int] = [0, 187, 377, 440, 520, 541, 721, 869, 926, 933]\n",
    "\n",
    "### Initialize nsamples inputs ###\n",
    "nsamples_train: int = 100000\n",
    "nsamples_val: int = 30000\n",
    "nsamples_test: int = 100000\n",
    "\n",
    "### Initialize seeds inputs ###\n",
    "seed_test: int = 0 # overwritten in the loop by seed_train + 1\n",
    "seed_dist: int = 0\n",
    "seed_metrics: int = seed_test\n",
    "\n",
    "### Initialize bijector inputs ###\n",
    "bijector_name: str = 'MAFN'\n",
    "range_min: int = -5\n",
    "spline_knots_list: List[Union[int,str]] = [\"--\"] # Only relevant for the neural spline\n",
    "\n",
    "### Initialize NN hyperparameters ###\n",
    "activation: str = 'relu'\n",
    "regulariser: Optional[str] = None\n",
    "eps_regulariser: float = 0.\n",
    "\n",
    "### Initialzie training hyperparameters ###\n",
    "epochs_input: int = 1000\n",
    "batch_size: int = 512\n",
    "max_retry: int = 10\n",
    "debug_print_mode: bool = True\n",
    "\n",
    "### Initialize optimizer hyperparameters ###\n",
    "lr_orig: float = 0.1\n",
    "\n",
    "### Initialize callbacks hyperparameters ###\n",
    "es_min_delta: float = .0001\n",
    "es_patience: int = 100\n",
    "lr_min_delta: float = .0001\n",
    "lr_patience: int = 50\n",
    "lr_reduce_factor: float = .5\n",
    "lr_reduce_factor_on_nan: float = .5\n",
    "min_lr: float = 1e-6\n",
    "\n",
    "### Initialize parameters for inference ###\n",
    "n_iter: int = 10\n",
    "batch_size_gen: int = 10000\n",
    "n_slices_factor: int = 2\n",
    "dtype: type = tf.float32\n",
    "mirror_strategy = False\n",
    "make_plots = True\n",
    "\n",
    "### Initialize old variables for backward compatibility\n",
    "corr: Optional[str] = None\n",
    "\n",
    "### Initialize dictionaries ###\n",
    "results_dict: Dict[str, Any] = Utils.init_results_dict()\n",
    "hyperparams_dict: Dict[str, Any] = Utils.init_hyperparams_dict()\n",
    "\n",
    "### Initialize output dir ###\n",
    "mother_output_dir: str = Utils.define_dir('../../results/MAFN_new/')\n",
    "\n",
    "### Create 'log' file ####\n",
    "log_file_name: str = Utils.create_log_file(mother_output_dir, results_dict)\n",
    "\n",
    "##############################################################################################\n",
    "####################################### Training loop ########################################\n",
    "##############################################################################################\n",
    "\n",
    "run_number: int = 0\n",
    "n_runs: int = len(ndims_list) * len(seeds_list) * len(nbijectors_list) * len(spline_knots_list) * len(hidden_layers_list)\n",
    "start_global: float = timer()\n",
    "for ndims in ndims_list:\n",
    "    targ_dist: tfp.distributions.Distribution = MixtureGaussian(ncomp = ncomp, ndims = ndims, seed = seed_dist)\n",
    "    base_dist: tfp.distributions.Distribution = Distributions.gaussians(ndims)\n",
    "    for seed_train in seeds_list:\n",
    "        for nbijectors in nbijectors_list:\n",
    "            for spline_knots in spline_knots_list:\n",
    "                for hidden_layers in hidden_layers_list:\n",
    "                    if run_number > 10:\n",
    "                        raise Exception(\"Passed run 11.\")\n",
    "                    start_run: float = timer()\n",
    "                    hllabel: str = '-'.join(str(e) for e in hidden_layers)\n",
    "                    run_number = run_number + 1\n",
    "                    results_dict_txt_saved: bool = False\n",
    "                    results_dict_json_saved: bool = False\n",
    "                    results_log_saved: bool = False\n",
    "                    path_to_results: str\n",
    "                    to_run: bool\n",
    "                    path_to_results, to_run = Utils.define_run_dir(mother_output_dir+'run_'+str(run_number)+'/',\n",
    "                                                                   force = \"skip\",\n",
    "                                                                   bkp = False)\n",
    "                    if to_run:\n",
    "                        try:\n",
    "                            path_to_weights: str = Utils.define_dir(os.path.join(path_to_results, 'weights'))\n",
    "                            checkpoint_path: str = os.path.join(path_to_weights, 'best_weights.h5')\n",
    "                            ########### Model train ###########\n",
    "                            NFObject: Trainer.Trainer\n",
    "                            hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],\n",
    "                                                                                                   nsamples = [nsamples_train, nsamples_val, nsamples_test],\n",
    "                                                                                                   run_number = run_number,\n",
    "                                                                                                   base_dist = base_dist,\n",
    "                                                                                                   targ_dist = targ_dist,\n",
    "                                                                                                   hyperparams_dict = hyperparams_dict,\n",
    "                                                                                                   n_runs = n_runs,\n",
    "                                                                                                   ndims = ndims,\n",
    "                                                                                                   bijector_name = bijector_name,\n",
    "                                                                                                   nbijectors = nbijectors,\n",
    "                                                                                                   spline_knots = spline_knots,\n",
    "                                                                                                   range_min = range_min,\n",
    "                                                                                                   hidden_layers = hidden_layers,\n",
    "                                                                                                   batch_size = batch_size,\n",
    "                                                                                                   epochs_input = epochs_input,\n",
    "                                                                                                   lr_orig = lr_orig,\n",
    "                                                                                                   es_min_delta = es_min_delta,\n",
    "                                                                                                   es_patience = es_patience,\n",
    "                                                                                                   lr_reduce_factor = lr_reduce_factor,\n",
    "                                                                                                   lr_min_delta = lr_min_delta,\n",
    "                                                                                                   lr_patience = lr_patience,\n",
    "                                                                                                   min_lr = min_lr,\n",
    "                                                                                                   activation = activation,\n",
    "                                                                                                   regulariser = regulariser,\n",
    "                                                                                                   eps_regulariser = eps_regulariser,\n",
    "                                                                                                   training_device = training_device,\n",
    "                                                                                                   path_to_results = path_to_results,\n",
    "                                                                                                   checkpoint_path = checkpoint_path,\n",
    "                                                                                                   max_retry = max_retry,\n",
    "                                                                                                   debug_print_mode = debug_print_mode)\n",
    "                            \n",
    "                            print(f\"Model trained in {training_time:.2f} s.\\n\") # type: ignore\n",
    "                            ########### Model prediction ###########\n",
    "                            results_dict, prediction_time, total_time = prediction_function(results_dict = results_dict,\n",
    "                                                                                            gpu_models = gpu_models,\n",
    "                                                                                            NFObject = NFObject, # type: ignore\n",
    "                                                                                            targ_dist = targ_dist,\n",
    "                                                                                            seed_test = seed_test,\n",
    "                                                                                            seed_metrics = seed_metrics,\n",
    "                                                                                            n_iter = n_iter,\n",
    "                                                                                            nsamples_test = nsamples_test,\n",
    "                                                                                            batch_size_gen = batch_size_gen,\n",
    "                                                                                            n_slices_factor = n_slices_factor,\n",
    "                                                                                            dtype = dtype,\n",
    "                                                                                            mirror_strategy = mirror_strategy)\n",
    "                            ########### Save results ###########\n",
    "                            Utils.save_results_current_run_txt(path_to_results, results_dict)\n",
    "                            results_dict_txt_saved = True\n",
    "                            print(\"results.txt saved\")\n",
    "                            Utils.save_results_current_run_json(path_to_results, results_dict)\n",
    "                            results_dict_json_saved = True\n",
    "                            print(\"results.json saved\")\n",
    "                            Utils.save_results_log(log_file_name, results_dict)\n",
    "                            results_log_saved = True\n",
    "                            print(\"Results log saved\")\n",
    "                            print(f\"Model predictions computed in {prediction_time:.2f} s.\")\n",
    "                            dummy_file_path: str = os.path.join(path_to_results,'done.txt')\n",
    "                            with open(dummy_file_path, 'w') as f:\n",
    "                                pass\n",
    "                            end_run: float = timer()\n",
    "                            total_time_run=end_run-start_run\n",
    "                            print(textwrap.dedent(f\"\"\"\\\n",
    "                                ===========\n",
    "                                Run {run_number}/{n_runs} done in {total_time_run:.2f} s.\n",
    "                                ===========\n",
    "                                \"\"\"))\n",
    "                        except Exception as ex:\n",
    "                            # Get current system exception\n",
    "                            ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "                            # Extract unformatter stack traces as tuples\n",
    "                            trace_back = traceback.extract_tb(ex_traceback) # type: ignore\n",
    "                            # Format stacktrace\n",
    "                            stack_trace = list()\n",
    "                            for trace in trace_back:\n",
    "                                stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "                            if not results_dict_txt_saved:\n",
    "                                results_dict = Utils.update_results_dict(results_dict = results_dict,\n",
    "                                                                         hyperparams_dict = hyperparams_dict)\n",
    "                                Utils.save_results_current_run_txt(path_to_results, results_dict)\n",
    "                            if not results_dict_json_saved:\n",
    "                                Utils.save_results_current_run_json(path_to_results, results_dict)\n",
    "                            if not results_log_saved:\n",
    "                                Utils.save_results_log(log_file_name, results_dict)\n",
    "                            ex_type_str = f\"Exception type: {ex_type.__name__}\" # type: ignore\n",
    "                            print(textwrap.dedent(f\"\"\"\\\n",
    "                                ===========\n",
    "                                Run {run_number}/{n_runs} failed.\n",
    "                                {ex_type_str}\n",
    "                                Exception message: {ex_value}\n",
    "                                Stack trace: {stack_trace}\n",
    "                                ===========\n",
    "                                \"\"\"))\n",
    "                    else:\n",
    "                        print(textwrap.dedent(f\"\"\"\\\n",
    "                            ===========\n",
    "                            Run {run_number}/{n_runs} already exists. Skipping it.\n",
    "                            ===========\n",
    "                            \"\"\"))\n",
    "keys_to_remove = ['ks_lists', 'ad_lists', 'fn_list', 'wd_lists', 'swd_lists', 'train_loss_history', 'val_loss_history', 'lr_history']\n",
    "dict_copy: Dict[str, Any] = {k: v for k, v in results_dict.items() if k not in keys_to_remove}\n",
    "results_frame: pd.DataFrame = pd.DataFrame(dict_copy)\n",
    "results_last_run_file: str = os.path.join(mother_output_dir,'results_last_run.txt')\n",
    "results_frame.to_csv(results_last_run_file,index=False)\n",
    "end_global: float = timer()\n",
    "print(f\"Everything done in {end_global-start_global:.2f} s.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
