{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to tf2_12 (Python 3.10.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-12 16:09:07.760990: Importing os...\n",
      "2023-12-12 16:09:07.761429: Importing sys...\n",
      "2023-12-12 16:09:07.761511: Importing and initializing argparse...\n",
      "Visible devices: [1]\n",
      "2023-12-12 16:09:07.761780: Importing timer from timeit...\n",
      "2023-12-12 16:09:07.761852: Setting env variables for tf import (only device [1] will be available)...\n",
      "2023-12-12 16:09:07.761983: Importing numpy...\n",
      "2023-12-12 16:09:07.846804: Importing pandas...\n",
      "2023-12-12 16:09:08.103839: Importing shutil...\n",
      "2023-12-12 16:09:08.103997: Importing subprocess...\n",
      "2023-12-12 16:09:08.104069: Importing tensorflow...\n",
      "Tensorflow version: 2.12.0\n",
      "2023-12-12 16:09:10.543510: Importing tensorflow_probability...\n",
      "Tensorflow probability version: 0.20.1\n",
      "2023-12-12 16:09:10.968767: Importing textwrap...\n",
      "2023-12-12 16:09:10.968869: Importing timeit...\n",
      "2023-12-12 16:09:10.968944: Importing traceback...\n",
      "2023-12-12 16:09:10.969001: Importing typing...\n",
      "2023-12-12 16:09:10.969075: Setting tf configs...\n",
      "2023-12-12 16:09:11.352497: Importing custom module...\n",
      "Successfully loaded GPU model: NVIDIA A40\n",
      "2023-12-12 16:09:15.202426: All modues imported successfully.\n",
      "Directory ../../results/CsplineN_test/ already exists.\n",
      "Directory ../../results/CsplineN_test/run_1/ already exists.\n",
      "Deleting old run and running again.\n",
      "Old run deleted.\n",
      "===========\n",
      "Generating train data for run 1.\n",
      "===========\n",
      "Train data generated in 0.25 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[64], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization\", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization])\n",
      "x_data_train shape: (100000, 64)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/CsplineN_test/run_1/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 869}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_test/run_1/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 10, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,\n",
      "        2.0475917 ],\n",
      "       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,\n",
      "        1.3282784 ],\n",
      "       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,\n",
      "        1.3179724 ],\n",
      "       ...,\n",
      "       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,\n",
      "        4.308157  ],\n",
      "       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,\n",
      "        1.439661  ],\n",
      "       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,\n",
      "        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[64], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization\", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization, Permute, Cspline, BatchNormalization])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalizationSampleNormal\", batch_shape=[], event_shape=[64], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/CsplineN_test/run_1/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/CsplineN_test/run_1\n",
      "self.data_kwargs: {'seed': 869}\n",
      "self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651\n",
      "   2.1592255 ]\n",
      " [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129\n",
      "   1.6233965 ]\n",
      " [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744\n",
      "   1.3646827 ]\n",
      " ...\n",
      " [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484\n",
      "   2.5814815 ]\n",
      " [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772\n",
      "   1.5695707 ]\n",
      " [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962\n",
      "   1.7694858 ]]\n",
      "self.y_data: []\n",
      "self.ndims: 64\n",
      "Model defined.\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 64)]              0         \n",
      "                                                                 \n",
      " log_prob_layer (LogProbLaye  (None,)                  1324480   \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,324,480\n",
      "Trainable params: 1,323,200\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalization_of_permute_of_cspline_of_batch_normalizationSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7f9c007835e0>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7f9c001596f0>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9c001596f0>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7f9c001e7340>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f9b8076cb80>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_test/run_1/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f9b8076cfd0>, <keras.callbacks.ModelCheckpoint object at 0x7f9b8076d150>, <keras.callbacks.EarlyStopping object at 0x7f9b8076d360>, <keras.callbacks.ReduceLROnPlateau object at 0x7f9b8076d390>, <keras.callbacks.TerminateOnNaN object at 0x7f9b8076d120>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 10, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,\n",
      "        2.0475917 ],\n",
      "       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,\n",
      "        1.3282784 ],\n",
      "       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,\n",
      "        1.3179724 ],\n",
      "       ...,\n",
      "       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,\n",
      "        4.308157  ],\n",
      "       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,\n",
      "        1.439661  ],\n",
      "       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,\n",
      "        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/CsplineN_test/run_1/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 1/1 with hyperparameters:\n",
      "timestamp = 2023-12-12 16:09:21.391496\n",
      "ndims = 64\n",
      "seed_train = 869\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = CsplineN\n",
      "nbijectors = 10\n",
      "spline_knots = 8\n",
      "range_min = -16\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 1323200\n",
      "epochs_input = 10\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model with initial learning rate 0.001...\n",
      "Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035\n",
      "  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786\n",
      "  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087\n",
      "  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921\n",
      "  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684\n",
      "  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578\n",
      "  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881\n",
      "  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217\n",
      "  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273\n",
      "  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867\n",
      "  7.1426215   3.6510358   4.425651    2.1592255 ]\n",
      "Epoch 1/10\n",
      "2023-12-12 16:12:27.195 \n",
      "Epoch 1/10 \n",
      "\t loss: 68.6511, MinusLogProbMetric: 68.6511, val_loss: 122.7405, val_MinusLogProbMetric: 122.7405\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 122.74046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_test/run_1/weights/best_weights.h5\n",
      "196/196 - 187s - loss: 68.6511 - MinusLogProbMetric: 68.6511 - val_loss: 122.7405 - val_MinusLogProbMetric: 122.7405 - lr: 0.0010 - 187s/epoch - 952ms/step\n",
      "Epoch 2/10\n",
      "2023-12-12 16:13:35.724 \n",
      "Epoch 2/10 \n",
      "\t loss: 121.4505, MinusLogProbMetric: 121.4505, val_loss: 140.0454, val_MinusLogProbMetric: 140.0454\n",
      "\n",
      "Epoch 2: val_loss did not improve from 122.74046\n",
      "196/196 - 67s - loss: 121.4505 - MinusLogProbMetric: 121.4505 - val_loss: 140.0454 - val_MinusLogProbMetric: 140.0454 - lr: 0.0010 - 67s/epoch - 343ms/step\n",
      "Epoch 3/10\n",
      "2023-12-12 16:14:42.523 \n",
      "Epoch 3/10 \n",
      "\t loss: 122.4182, MinusLogProbMetric: 122.4182, val_loss: 118.1878, val_MinusLogProbMetric: 118.1878\n",
      "\n",
      "Epoch 3: val_loss improved from 122.74046 to 118.18784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_test/run_1/weights/best_weights.h5\n",
      "196/196 - 68s - loss: 122.4182 - MinusLogProbMetric: 122.4182 - val_loss: 118.1878 - val_MinusLogProbMetric: 118.1878 - lr: 0.0010 - 68s/epoch - 346ms/step\n",
      "Epoch 4/10\n",
      "2023-12-12 16:15:51.939 \n",
      "Epoch 4/10 \n",
      "\t loss: 177.4001, MinusLogProbMetric: 177.4001, val_loss: 137.5905, val_MinusLogProbMetric: 137.5905\n",
      "\n",
      "Epoch 4: val_loss did not improve from 118.18784\n",
      "196/196 - 68s - loss: 177.4001 - MinusLogProbMetric: 177.4001 - val_loss: 137.5905 - val_MinusLogProbMetric: 137.5905 - lr: 0.0010 - 68s/epoch - 349ms/step\n",
      "Epoch 5/10\n",
      "2023-12-12 16:17:00.769 \n",
      "Epoch 5/10 \n",
      "\t loss: 111.0245, MinusLogProbMetric: 111.0245, val_loss: 115.5948, val_MinusLogProbMetric: 115.5948\n",
      "\n",
      "Epoch 5: val_loss improved from 118.18784 to 115.59483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_test/run_1/weights/best_weights.h5\n",
      "196/196 - 70s - loss: 111.0245 - MinusLogProbMetric: 111.0245 - val_loss: 115.5948 - val_MinusLogProbMetric: 115.5948 - lr: 0.0010 - 70s/epoch - 356ms/step\n",
      "Epoch 6/10\n",
      "2023-12-12 16:18:10.866 \n",
      "Epoch 6/10 \n",
      "\t loss: 99.9250, MinusLogProbMetric: 99.9250, val_loss: 123.0867, val_MinusLogProbMetric: 123.0867\n",
      "\n",
      "Epoch 6: val_loss did not improve from 115.59483\n",
      "196/196 - 69s - loss: 99.9250 - MinusLogProbMetric: 99.9250 - val_loss: 123.0867 - val_MinusLogProbMetric: 123.0867 - lr: 0.0010 - 69s/epoch - 352ms/step\n",
      "Epoch 7/10\n",
      "2023-12-12 16:19:17.445 \n",
      "Epoch 7/10 \n",
      "\t loss: 94.4478, MinusLogProbMetric: 94.4478, val_loss: 88.2000, val_MinusLogProbMetric: 88.2000\n",
      "\n",
      "Epoch 7: val_loss improved from 115.59483 to 88.20004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_test/run_1/weights/best_weights.h5\n",
      "196/196 - 68s - loss: 94.4478 - MinusLogProbMetric: 94.4478 - val_loss: 88.2000 - val_MinusLogProbMetric: 88.2000 - lr: 0.0010 - 68s/epoch - 345ms/step\n",
      "Epoch 8/10\n",
      "2023-12-12 16:20:21.720 \n",
      "Epoch 8/10 \n",
      "\t loss: 101.8468, MinusLogProbMetric: 101.8468, val_loss: 86.3912, val_MinusLogProbMetric: 86.3912\n",
      "\n",
      "Epoch 8: val_loss improved from 88.20004 to 86.39124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_test/run_1/weights/best_weights.h5\n",
      "196/196 - 64s - loss: 101.8468 - MinusLogProbMetric: 101.8468 - val_loss: 86.3912 - val_MinusLogProbMetric: 86.3912 - lr: 0.0010 - 64s/epoch - 327ms/step\n",
      "Epoch 9/10\n",
      "2023-12-12 16:21:27.586 \n",
      "Epoch 9/10 \n",
      "\t loss: 90.3985, MinusLogProbMetric: 90.3985, val_loss: 85.8572, val_MinusLogProbMetric: 85.8572\n",
      "\n",
      "Epoch 9: val_loss improved from 86.39124 to 85.85717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_test/run_1/weights/best_weights.h5\n",
      "196/196 - 66s - loss: 90.3985 - MinusLogProbMetric: 90.3985 - val_loss: 85.8572 - val_MinusLogProbMetric: 85.8572 - lr: 0.0010 - 66s/epoch - 336ms/step\n",
      "Epoch 10/10\n",
      "2023-12-12 16:22:33.165 \n",
      "Epoch 10/10 \n",
      "\t loss: 80.6625, MinusLogProbMetric: 80.6625, val_loss: 95.4545, val_MinusLogProbMetric: 95.4545\n",
      "\n",
      "Epoch 10: val_loss did not improve from 85.85717\n",
      "196/196 - 65s - loss: 80.6625 - MinusLogProbMetric: 80.6625 - val_loss: 95.4545 - val_MinusLogProbMetric: 95.4545 - lr: 0.0010 - 65s/epoch - 330ms/step\n",
      "Training succeeded with seed 869.\n",
      "Model trained in 791.97 s.\n",
      "\n",
      "===========\n",
      "Computing predictions\n",
      "===========\n",
      "\n",
      "Computing metrics...\n",
      "Parsing input distribution...\n",
      "Input distribution is a tfp.distributions.Distribution object.\n",
      "Parsing input distribution...\n",
      "Input distribution is a tfp.distributions.Distribution object.\n",
      "Checking and setting numerical distributions.\n",
      "Resetting dist_num.\n",
      "Resetting dist_num.\n",
      "Generating random directions based on nslices, ndims, and seed_slicing.\n",
      "\n",
      "------------------------------------------\n",
      "Starting KS tests calculation...\n",
      "Running TF KS tests...\n",
      "niter = 10\n",
      "batch_size = 100000\n",
      "The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.\n",
      "The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.\n",
      "nchunks = 10\n",
      "Iterating from 0 to 1 out of 10 .\n",
      "Iterating from 1 to 2 out of 10 .\n",
      "Iterating from 2 to 3 out of 10 .\n",
      "Iterating from 3 to 4 out of 10 .\n",
      "Iterating from 4 to 5 out of 10 .\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7fa0dffe7f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Iterating from 5 to 6 out of 10 .\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7fa0dff59f30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Iterating from 6 to 7 out of 10 .\n",
      "Iterating from 7 to 8 out of 10 .\n",
      "Iterating from 8 to 9 out of 10 .\n",
      "Iterating from 9 to 10 out of 10 .\n",
      "KS tests calculation completed in 81.2425018614158 seconds.\n",
      "\n",
      "------------------------------------------\n",
      "Starting SWD metric calculation...\n",
      "Running TF SWD calculation...\n",
      "niter = 10\n",
      "batch_size = 100000\n",
      "The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.\n",
      "The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.\n",
      "SWD metric calculation completed in 63.252035703510046 seconds.\n",
      "\n",
      "------------------------------------------\n",
      "Starting FN metric calculation...\n",
      "Running TF FN calculation...\n",
      "niter = 10\n",
      "batch_size = 100000\n",
      "The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.\n",
      "The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.\n",
      "WARNING:tensorflow:5 out of the last 23 calls to <function generate_and_clean_data_simple_1 at 0x7f9d2c369990> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "FN metric calculation completed in 58.70969663280994 seconds.\n",
      "Metrics computed in 204.89 s.\n",
      "                ===========\n",
      "                print(\"===========\n",
      "Failed to plot\n",
      "===========\n",
      "\")\n",
      "                Exception type: ValueError\n",
      "                Exception message: Number of rows must be a positive integer, not 0\n",
      "                Stack trace: ['File : <ipython-input-1-cafc0acde8f7> , Line : 488, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\\'red\\',bins=n_bins,labels=[r\"%s\" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']\n",
      "                ===========\n",
      "\n",
      "results.txt saved\n",
      "results.json saved\n",
      "Results log saved\n",
      "Model predictions computed in 205.22 s.\n",
      "===========\n",
      "Run 1/1 done in 1002.61 s.\n",
      "===========\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py\u001b[0m in \u001b[0;36mline 721\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=714'>715</a>\u001b[0m total_time_run\u001b[39m=\u001b[39mend_run\u001b[39m-\u001b[39mstart_run\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=715'>716</a>\u001b[0m \u001b[39mprint\u001b[39m(textwrap\u001b[39m.\u001b[39mdedent(\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=716'>717</a>\u001b[0m \u001b[39m    ===========\u001b[39m\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=717'>718</a>\u001b[0m \u001b[39m    Run \u001b[39m\u001b[39m{\u001b[39;00mrun_number\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_runs\u001b[39m}\u001b[39;00m\u001b[39m done in \u001b[39m\u001b[39m{\u001b[39;00mtotal_time_run\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m s.\u001b[39m\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=718'>719</a>\u001b[0m \u001b[39m    ===========\u001b[39m\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=719'>720</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m))\n\u001b[0;32m--> <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=720'>721</a>\u001b[0m run \u001b[39m=\u001b[39m run \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=721'>722</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/rtorre/teo_fs/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/Cspline_checks_teogpu02.py?line=722'>723</a>\u001b[0m     os\u001b[39m.\u001b[39mremove(dummy_file_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 130x130 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################################\n",
    "######################################### Initialize #########################################\n",
    "##############################################################################################\n",
    "\n",
    "visible_devices = [1]\n",
    "import datetime\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing os...\")\n",
    "import os\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing sys...\")\n",
    "import sys\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing and initializing argparse...\")\n",
    "if not any(\"ipykernel\" in arg for arg in sys.argv):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-v\", \"--visible_devices\", help=\"Set visible devices\", nargs='*', type=list, default=visible_devices)\n",
    "    args = parser.parse_args()\n",
    "    visible_devices = args.visible_devices if args.visible_devices else visible_devices\n",
    "    if len(visible_devices) == 0:\n",
    "        visible_devices = int(visible_devices)\n",
    "    elif len(visible_devices) == 1:\n",
    "        if len(visible_devices[0]) == 0:\n",
    "            visible_devices = int(visible_devices[0])\n",
    "        else:\n",
    "            visible_devices = [int(i) for i in visible_devices[0]]\n",
    "    else:\n",
    "        visible_devices = [int(i) for i in visible_devices]\n",
    "print(\"Visible devices:\", visible_devices)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing timer from timeit...\")\n",
    "from timeit import default_timer as timer\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Setting env variables for tf import (only device\", visible_devices, \"will be available)...\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([str(i) for i in visible_devices])\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing numpy...\")\n",
    "import numpy as np\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing pandas...\")\n",
    "import pandas as pd\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing shutil...\")\n",
    "import shutil\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing subprocess...\")\n",
    "import subprocess\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing tensorflow...\")\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing tensorflow_probability...\")\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "print(\"Tensorflow probability version:\", tfp.__version__)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing textwrap...\")\n",
    "import textwrap\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing timeit...\")\n",
    "from timeit import default_timer as timer\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing traceback...\")\n",
    "import traceback\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing typing...\")\n",
    "from typing import List, Tuple, Dict, Union, Optional, Any\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Setting tf configs...\")\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu_device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu_device, True)\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing custom module...\")\n",
    "\n",
    "sys.path.append('../../../code')\n",
    "import Bijectors, Distributions, MixtureDistributions, Plotters, Trainer, Utils # type: ignore\n",
    "import GenerativeModelsMetrics as GMetrics # type: ignore\n",
    "\n",
    "def get_gpu_info() -> Optional[List[str]]:\n",
    "    try:\n",
    "        gpu_info: str = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv,noheader\"]).decode('utf-8')\n",
    "        return gpu_info.strip().split('\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "gpu_models: Optional[List[str]] = get_gpu_info()\n",
    "if gpu_models:\n",
    "    training_device: str = gpu_models[0]\n",
    "    print(\"Successfully loaded GPU model: {}\".format(training_device))\n",
    "else:\n",
    "    training_device = 'undetermined'\n",
    "    print(\"Failed to load GPU model. Defaulting to 'undetermined'.\")\n",
    "    \n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"All modues imported successfully.\")\n",
    "\n",
    "##############################################################################################\n",
    "####################################### Helper functions #####################################\n",
    "##############################################################################################\n",
    "\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def MixtureGaussian(ncomp: int,\n",
    "                    ndims: int,\n",
    "                    seed: int = 0) -> tfp.distributions.Mixture:\n",
    "    targ_dist: tfp.distributions.Mixture = MixtureDistributions.MixMultiNormal1(ncomp,ndims,seed=seed)\n",
    "    return targ_dist\n",
    "\n",
    "def get_io_kwargs(path_to_results: str) -> Dict[str,Any]:\n",
    "    return {'results_path': path_to_results,\n",
    "            'load_weights': True,\n",
    "            'load_results': True}\n",
    "    \n",
    "def get_data_kwargs(seed: int = 0) -> Dict[str,Any]:\n",
    "    return {'seed': seed}\n",
    "\n",
    "def get_compiler_kwargs(lr: float,\n",
    "                        ignore_nans: bool,\n",
    "                        nan_threshold: float\n",
    "                       ) -> Dict[str,Any]:\n",
    "    compiler_kwargs = {'optimizer': {'class_name': 'Custom>Adam', # this gives the new Adam optimizer\n",
    "    #compiler_kwargs = {'optimizer': {'class_name': 'Adam', # this gives the new Adam optimizer\n",
    "                                     'config': {'learning_rate': lr,\n",
    "                                                'beta_1': 0.9,\n",
    "                                                'beta_2': 0.999,\n",
    "                                                'epsilon': 1e-07,\n",
    "                                                #'clipnorm': 1.,\n",
    "                                                'amsgrad': True}},\n",
    "                       'metrics': [{'class_name': 'MinusLogProbMetric',\n",
    "                                    'config': {'ignore_nans': ignore_nans, \n",
    "                                               'debug_print_mode': False}}],\n",
    "                       'loss': {'class_name': 'MinusLogProbLoss', \n",
    "                                'config': {'name': \"MLP\", \n",
    "                                           'ignore_nans': ignore_nans, \n",
    "                                           'nan_threshold': nan_threshold, \n",
    "                                           'debug_print_mode': False}}}\n",
    "    return compiler_kwargs\n",
    "    \n",
    "def get_callbacks_kwargs(checkpoint_path: str,\n",
    "                         es_min_delta: float,\n",
    "                         es_patience: int,\n",
    "                         lr_reduce_factor: float,\n",
    "                         lr_min_delta: float,\n",
    "                         lr_patience: int,\n",
    "                         min_lr: float\n",
    "                         ) -> List[Dict[str,Any]]:\n",
    "    callbacks_kwargs = [{'class_name': 'PrintEpochInfo',\n",
    "                         'config': {}},\n",
    "                        #{'class_name': 'HandleNaNCallback',\n",
    "                        # 'config': {'checkpoint_path': checkpoint_path,\n",
    "                        #            'lr_reduction_factor': lr_reduce_factor_on_nan,\n",
    "                        #            'random_seed_var': np.random.randint(1000000)}},\n",
    "                        #{'class_name': 'TerminateOnNaNFractionCallback',\n",
    "                        # 'config': {'threshold': 0.1,\n",
    "                        #            'validation_data': X_data_val}},\n",
    "                        {'class_name': 'ModelCheckpoint',\n",
    "                         'config': {'filepath': checkpoint_path,\n",
    "                                    'monitor': 'val_loss',\n",
    "                                    'save_best_only': True,\n",
    "                                    'save_weights_only': True,\n",
    "                                    'verbose': 1,\n",
    "                                    'mode': 'auto',\n",
    "                                    'save_freq': 'epoch'}},\n",
    "                        {'class_name': 'EarlyStopping',\n",
    "                         'config': {'monitor': 'val_loss', \n",
    "                                    'min_delta': es_min_delta, \n",
    "                                    'patience': es_patience, \n",
    "                                    'verbose': 1,\n",
    "                                    'mode': 'auto', \n",
    "                                    'baseline': None, \n",
    "                                    'restore_best_weights': True}},\n",
    "                        {'class_name': 'ReduceLROnPlateau', \n",
    "                         'config': {'monitor': 'val_loss', \n",
    "                                    'factor': lr_reduce_factor, \n",
    "                                    'min_delta': lr_min_delta, \n",
    "                                    'patience': lr_patience, \n",
    "                                    'min_lr': min_lr}},\n",
    "                        {'class_name': 'TerminateOnNaN', 'config': {}}\n",
    "                        ]\n",
    "    return callbacks_kwargs\n",
    "\n",
    "def get_fit_kwargs(batch_size: int,\n",
    "                   epochs_input: int,\n",
    "                   validation_data: Tuple[Union[np.ndarray,tf.Tensor],Union[np.ndarray,tf.Tensor]],\n",
    "                   shuffle: bool,\n",
    "                   verbose: int\n",
    "                  ) -> Dict[str,Any]:\n",
    "    fit_kwargs = {'batch_size': batch_size, \n",
    "                  'epochs': epochs_input, \n",
    "                  'validation_data': validation_data,\n",
    "                  'shuffle': shuffle, \n",
    "                  'verbose': verbose}\n",
    "    return fit_kwargs\n",
    "\n",
    "def train_function(seeds: List[int],\n",
    "                   nsamples: List[int],\n",
    "                   run_number: int,\n",
    "                   base_dist: tfp.distributions.Distribution,\n",
    "                   targ_dist: tfp.distributions.Distribution,\n",
    "                   hyperparams_dict: Dict[str, Any],\n",
    "                   n_runs: int,\n",
    "                   ndims: int,\n",
    "                   bijector_name: str,\n",
    "                   nbijectors: int,\n",
    "                   spline_knots: Union[int,str],\n",
    "                   range_min: int,\n",
    "                   hidden_layers: List[int],\n",
    "                   batch_size: int,\n",
    "                   epochs_input: int,\n",
    "                   lr_orig: float,\n",
    "                   es_min_delta: float,\n",
    "                   es_patience: int,\n",
    "                   lr_reduce_factor: float,\n",
    "                   lr_min_delta: float,\n",
    "                   lr_patience: int,\n",
    "                   min_lr: float,\n",
    "                   activation: str,\n",
    "                   regulariser: Optional[str],\n",
    "                   eps_regulariser: float,\n",
    "                   use_batch_norm: bool,\n",
    "                   training_device: str,\n",
    "                   path_to_results: str,\n",
    "                   checkpoint_path: str,\n",
    "                   max_retry: int,\n",
    "                   debug_print_mode: bool,\n",
    "                   nan_threshold: float,\n",
    "                  ) -> Tuple[Dict[str, Any], Trainer.Trainer, int, float]:\n",
    "    succeeded = False\n",
    "    retry: int = 0\n",
    "    lr: float = lr_orig\n",
    "    while not succeeded:\n",
    "        seed_train: int\n",
    "        seed_test: int\n",
    "        seed_dist: int\n",
    "        seed_metrics: int\n",
    "        seed_train, seed_test, seed_dist, seed_metrics = seeds\n",
    "        seed_test = seed_train + 1                     \n",
    "        Utils.reset_random_seeds(seed = seed_train)\n",
    "        nsamples_train: int\n",
    "        nsamples_val: int\n",
    "        nsamples_test: int\n",
    "        nsamples_train, nsamples_val, nsamples_test = nsamples\n",
    "        X_data_train: tf.Tensor\n",
    "        X_data_val: tf.Tensor\n",
    "        Y_data_train: tf.Tensor\n",
    "        Y_data_val: tf.Tensor\n",
    "        X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,\n",
    "                                                                                       targ_dist = targ_dist,\n",
    "                                                                                       nsamples_train = nsamples_train,\n",
    "                                                                                       nsamples_val = nsamples_val,\n",
    "                                                                                       seed_train = seed_train)\n",
    "        bijector: tfp.bijectors.Bijector = Bijectors.ChooseBijector(bijector_name = bijector_name,\n",
    "                                                                    ndims = ndims,\n",
    "                                                                    spline_knots = spline_knots,\n",
    "                                                                    nbijectors = nbijectors,\n",
    "                                                                    range_min = range_min,\n",
    "                                                                    hidden_layers = hidden_layers,\n",
    "                                                                    activation = activation,\n",
    "                                                                    regulariser = regulariser,\n",
    "                                                                    eps_regulariser = eps_regulariser,\n",
    "                                                                    use_batch_norm = use_batch_norm)\n",
    "        Utils.save_bijector_info(path_to_results, bijector)\n",
    "        print(\"Building Trainer NFObject.\\n\")\n",
    "        NFObject: Trainer.Trainer = Trainer.Trainer(base_distribution = base_dist,\n",
    "                                                    flow = bijector, \n",
    "                                                    x_data_train = X_data_train,\n",
    "                                                    y_data_train = Y_data_train,\n",
    "                                                    io_kwargs = get_io_kwargs(path_to_results = path_to_results),\n",
    "                                                    data_kwargs = get_data_kwargs(seed = seed_train),\n",
    "                                                    compiler_kwargs = get_compiler_kwargs(lr = lr,\n",
    "                                                                                          ignore_nans = True,\n",
    "                                                                                          nan_threshold = nan_threshold),\n",
    "                                                    callbacks_kwargs = get_callbacks_kwargs(checkpoint_path = checkpoint_path,\n",
    "                                                                                            es_min_delta = es_min_delta,\n",
    "                                                                                            es_patience = es_patience,\n",
    "                                                                                            lr_reduce_factor = lr_reduce_factor,\n",
    "                                                                                            lr_min_delta = lr_min_delta,\n",
    "                                                                                            lr_patience = lr_patience,\n",
    "                                                                                            min_lr = min_lr),\n",
    "                                                    fit_kwargs = get_fit_kwargs(batch_size = batch_size,\n",
    "                                                                                epochs_input = epochs_input,\n",
    "                                                                                validation_data = (X_data_val, Y_data_val),\n",
    "                                                                                shuffle = True,\n",
    "                                                                                verbose = 2),\n",
    "                                                    debug_print_mode = debug_print_mode)\n",
    "        trainable_params: int = NFObject.trainable_params\n",
    "        non_trainable_params: int = NFObject.non_trainable_params\n",
    "        hyperparams_dict = Utils.update_hyperparams_dict(hyperparams_dict = hyperparams_dict,\n",
    "                                                         run_number = run_number,\n",
    "                                                         n_runs = n_runs,\n",
    "                                                         seeds = [seed_train, seed_test, seed_dist, seed_metrics],\n",
    "                                                         nsamples = [nsamples_train, nsamples_val, nsamples_test],\n",
    "                                                         ndims = ndims,\n",
    "                                                         corr = None,\n",
    "                                                         bijector_name = bijector_name,\n",
    "                                                         nbijectors = nbijectors,\n",
    "                                                         spline_knots = spline_knots,\n",
    "                                                         range_min = range_min,\n",
    "                                                         hllabel = '-'.join(str(e) for e in hidden_layers),\n",
    "                                                         trainable_parameters = trainable_params,\n",
    "                                                         non_trainable_parameters = non_trainable_params,\n",
    "                                                         batch_size = batch_size,\n",
    "                                                         epochs_input = epochs_input,\n",
    "                                                         activation = activation,\n",
    "                                                         regulariser = regulariser,\n",
    "                                                         eps_regulariser = eps_regulariser,\n",
    "                                                         training_device = training_device)\n",
    "        Utils.save_hyperparams_dict(path_to_results, hyperparams_dict)\n",
    "        print(f\"Training model with initial learning rate {lr}...\")\n",
    "        print(\"Train first sample:\", X_data_train[0]) # type: ignore\n",
    "        NFObject.train()\n",
    "        training_time: float = NFObject.training_time # type: ignore\n",
    "        loss_history = list(NFObject.history['loss'])\n",
    "        if np.isnan(loss_history).any():\n",
    "            print(\"The loss history contains NaN values.\")\n",
    "\n",
    "        if np.isinf(loss_history).any():\n",
    "            print(\"The loss history contains Inf values.\")\n",
    "\n",
    "        if len(loss_history) > 0:\n",
    "            if np.isnan(loss_history).any() or np.isinf(loss_history).any():\n",
    "                succeeded = False\n",
    "                seed_train = np.random.randint(1000000)\n",
    "                lr = lr * lr_reduce_factor_on_nan\n",
    "                retry = retry + 1\n",
    "                print(f\"Training failed: trying again with seed {seed_train} and lr {lr}.\")\n",
    "            else:\n",
    "                succeeded = True\n",
    "                print(f\"Training succeeded with seed {seed_train}.\")\n",
    "        else:\n",
    "            succeeded = False\n",
    "            seed_train = np.random.randint(1000000)\n",
    "            lr = lr * lr_reduce_factor_on_nan\n",
    "            retry = retry + 1\n",
    "            print(f\"Training failed: trying again with seed {seed_train} and lr {lr}.\")\n",
    "            \n",
    "        if retry > max_retry:\n",
    "            raise Exception(\"Training failed for the maximum number of retry.\")\n",
    "        \n",
    "    return hyperparams_dict, NFObject, seed_train, training_time # type: ignore\n",
    "    \n",
    "\n",
    "def prediction_function(hyperparams_dict: Dict[str, Any],\n",
    "                        results_dict: Dict[str, Any],\n",
    "                        gpu_models: Optional[List[str]],\n",
    "                        NFObject: Trainer.Trainer,\n",
    "                        ndims: int,\n",
    "                        targ_dist: tfp.distributions.Distribution,\n",
    "                        seed_test: int,\n",
    "                        seed_metrics: int,\n",
    "                        n_iter: int,\n",
    "                        nsamples_test: int,\n",
    "                        n_slices_factor: int,\n",
    "                        dtype: type,\n",
    "                        max_vectorize: int,\n",
    "                        mirror_strategy: bool,\n",
    "                        make_plots: bool,\n",
    "                        path_to_results: str\n",
    "                       ) -> Tuple[Dict[str, Any], GMetrics.TwoSampleTestInputs, int, float]:\n",
    "    start_pred: float = timer()\n",
    "    t_losses_all: list = list(NFObject.history['loss']) # type: ignore\n",
    "    v_losses_all: list = list(NFObject.history['val_loss']) # type: ignore\n",
    "    lr_all: list = list(NFObject.history['lr']) # type: ignore\n",
    "    epochs_output: int = len(t_losses_all)\n",
    "    training_time: float = NFObject.training_time # type: ignore\n",
    "    try:\n",
    "        print(\"===========\\nComputing predictions\\n===========\\n\")\n",
    "        print(\"Computing metrics...\")\n",
    "        start = timer()\n",
    "        DataInputs: GMetrics.TwoSampleTestInputs = GMetrics.TwoSampleTestInputs(dist_1_input = targ_dist,\n",
    "                                                                                dist_2_input = NFObject.nf_dist,\n",
    "                                                                                niter = n_iter,\n",
    "                                                                                batch_size_test = nsamples_test,\n",
    "                                                                                batch_size_gen = 100,\n",
    "                                                                                small_sample_threshold = 1e7,\n",
    "                                                                                dtype_input = dtype,\n",
    "                                                                                seed_input = seed_metrics,\n",
    "                                                                                use_tf = True,\n",
    "                                                                                mirror_strategy = mirror_strategy,\n",
    "                                                                                verbose = True)\n",
    "        #LRMetric: GMetrics.LRMetric = GMetrics.LRMetric(data_input = DataInputs,\n",
    "        #                                                verbose = True)\n",
    "        KSTest: GMetrics.KSTest = GMetrics.KSTest(data_input = DataInputs,\n",
    "                                                  verbose = True)\n",
    "        SWDMetric: GMetrics.SWDMetric = GMetrics.SWDMetric(data_input = DataInputs,\n",
    "                                                           nslices = n_slices_factor*ndims,\n",
    "                                                           seed_slicing = 0,\n",
    "                                                           verbose = True)\n",
    "        FNMetric: GMetrics.FNMetric = GMetrics.FNMetric(data_input = DataInputs,\n",
    "                                                        verbose = True)\n",
    "        #LRMetric.compute()\n",
    "        KSTest.compute(max_vectorize = max_vectorize)\n",
    "        SWDMetric.compute(max_vectorize = max_vectorize)\n",
    "        FNMetric.compute(max_vectorize = max_vectorize)\n",
    "        #lr_result: Dict[str, np.ndarray] = LRMetric.Results[-1].result_value\n",
    "        logprob_ref_ref_sum_list = None#lr_result[\"logprob_ref_ref_sum_list\"].tolist()\n",
    "        logprob_ref_alt_sum_list = None#lr_result[\"logprob_ref_alt_sum_list\"].tolist()\n",
    "        logprob_alt_alt_sum_list = None#lr_result[\"logprob_alt_alt_sum_list\"].tolist()\n",
    "        lik_ratio_list = None#lr_result[\"lik_ratio_list\"].tolist()\n",
    "        lik_ratio_norm_list = None#lr_result[\"lik_ratio_norm_list\"].tolist()\n",
    "        ks_result: Dict[str, np.ndarray] = None#KSTest.Results[-1].result_value\n",
    "        ks_lists: List[List[float]] = None#ks_result[\"statistic_lists\"].tolist()\n",
    "        ks_means: List[float] = None#ks_result[\"statistic_means\"].tolist()\n",
    "        ks_stds: List[float] = None#ks_result[\"statistic_stds\"].tolist()\n",
    "        swd_result: Dict[str, np.ndarray] = None#SWDMetric.Results[-1].result_value\n",
    "        swd_lists: List[List[float]] = None#swd_result[\"metric_lists\"].tolist()\n",
    "        swd_means: List[float] = None#swd_result[\"metric_means\"].tolist()\n",
    "        swd_stds: List[float] = None#swd_result[\"metric_stds\"].tolist()\n",
    "        fn_result: Dict[str, np.ndarray] = None#FNMetric.Results[-1].result_value\n",
    "        fn_list: List[float] = None#fn_result[\"metric_list\"].tolist()\n",
    "        ad_lists: Optional[List[List[float]]] = None\n",
    "        ad_means: Optional[List[float]] = None\n",
    "        ad_stds: Optional[List[float]] = None\n",
    "        wd_lists: Optional[List[List[float]]] = None\n",
    "        wd_means: Optional[List[float]] = None\n",
    "        wd_stds: Optional[List[float]] = None\n",
    "        end = timer()\n",
    "        metrics_time = end - start\n",
    "        print(f\"Metrics computed in {metrics_time:.2f} s.\")\n",
    "    except:\n",
    "        raise Exception(\"Failed computing metrics\")\n",
    "        try:\n",
    "            print(\"===========\\nFailed on GPU, re-trying on CPU\\n===========\\n\")\n",
    "            with tf.device('/device:CPU:0'): # type: ignore\n",
    "                print(\"Computing metrics...\")\n",
    "                start = timer()\n",
    "                DataInputs = GMetrics.TwoSampleTestInputs(dist_1_input = targ_dist,\n",
    "                                                          dist_2_input = NFObject.nf_dist,\n",
    "                                                          niter = n_iter,\n",
    "                                                          batch_size = nsamples_test,\n",
    "                                                          dtype_input = dtype,\n",
    "                                                          seed_input = seed_metrics,\n",
    "                                                          use_tf = True,\n",
    "                                                          verbose = True)\n",
    "                LRMetric = GMetrics.LRMetric(data_input = DataInputs,\n",
    "                                             verbose = True)\n",
    "                KSTest = GMetrics.KSTest(data_input = DataInputs,\n",
    "                                         verbose = True)\n",
    "                SWDMetric = GMetrics.SWDMetric(data_input = DataInputs,\n",
    "                                               verbose = True)\n",
    "                FNMetric = GMetrics.FNMetric(data_input = DataInputs,\n",
    "                                             verbose = True)\n",
    "                LRMetric.compute()\n",
    "                KSTest.compute(max_vectorize = max_vectorize)\n",
    "                SWDMetric.compute(nslices = n_slices_factor*ndims)\n",
    "                FNMetric.compute(max_vectorize = max_vectorize)\n",
    "                lr_result = LRMetric.Results[-1].result_value\n",
    "                logprob_ref_ref_sum_list = lr_result[\"logprob_ref_ref_sum_list\"].tolist()\n",
    "                logprob_ref_alt_sum_list = lr_result[\"logprob_ref_alt_sum_list\"].tolist()\n",
    "                logprob_alt_alt_sum_list = lr_result[\"logprob_alt_alt_sum_list\"].tolist()\n",
    "                lik_ratio_list = lr_result[\"lik_ratio_list\"].tolist()\n",
    "                lik_ratio_norm_list = lr_result[\"lik_ratio_norm_list\"].tolist()\n",
    "                ks_result = KSTest.Results[-1].result_value\n",
    "                ks_lists = ks_result[\"statistic_lists\"].tolist()\n",
    "                ks_means = ks_result[\"statistic_means\"].tolist()\n",
    "                ks_stds = ks_result[\"statistic_stds\"].tolist()\n",
    "                swd_result = SWDMetric.Results[-1].result_value\n",
    "                swd_lists = swd_result[\"metric_lists\"].tolist()\n",
    "                swd_means = swd_result[\"metric_means\"].tolist()\n",
    "                swd_stds = swd_result[\"metric_stds\"].tolist()\n",
    "                fn_result = FNMetric.Results[-1].result_value\n",
    "                fn_list = fn_result[\"metric_list\"].tolist()\n",
    "                ad_lists = None\n",
    "                ad_means = None\n",
    "                ad_stds = None\n",
    "                wd_lists = None\n",
    "                wd_means = None\n",
    "                wd_stds = None\n",
    "                end = timer()\n",
    "                metrics_time = end - start\n",
    "                print(f\"Metrics computed in {metrics_time:.2f} s.\")\n",
    "        except:\n",
    "            print(\"===========\\nFailed computing metrics\\n===========\\n\")\n",
    "            logprob_ref_ref_sum_list = []\n",
    "            logprob_ref_alt_sum_list = []\n",
    "            logprob_alt_alt_sum_list = []\n",
    "            lik_ratio_list = []\n",
    "            lik_ratio_norm_list = []\n",
    "            ks_means = []\n",
    "            ks_stds = []\n",
    "            ks_lists = []\n",
    "            ad_means = []\n",
    "            ad_stds = []\n",
    "            ad_lists = []\n",
    "            fn_list = []\n",
    "            wd_means = []\n",
    "            wd_stds = []\n",
    "            wd_lists = []\n",
    "            swd_means = []\n",
    "            swd_stds = []\n",
    "            swd_lists = []\n",
    "            metrics_time = 0.\n",
    "    if make_plots:\n",
    "        try:\n",
    "            start = timer()\n",
    "            Plotters.train_plotter(t_losses_all,v_losses_all,path_to_results)\n",
    "            X_data_test: tf.Tensor = DataInputs.dist_1_num[:nsamples_test] # type: ignore\n",
    "            X_data_nf: tf.Tensor = DataInputs.dist_2_num[:nsamples_test] # type: ignore\n",
    "            Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore\n",
    "            Plotters.marginal_plot(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims) # type: ignore\n",
    "            #Plotters.sample_plotter(X_data_test,nf_dist,path_to_results)\n",
    "            end = timer()\n",
    "            plots_time: float = end - start\n",
    "            print(f\"Plots done in {plots_time:.2f} s.\")\n",
    "        except Exception as ex:\n",
    "            # Get current system exception\n",
    "            ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "            # Extract unformatter stack traces as tuples\n",
    "            trace_back = traceback.extract_tb(ex_traceback) # type: ignore\n",
    "            # Format stacktrace\n",
    "            stack_trace = list()\n",
    "            for trace in trace_back:\n",
    "                stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "            ex_type_str = f\"Exception type: {ex_type.__name__}\" # type: ignore\n",
    "            print(textwrap.dedent(f\"\"\"\\\n",
    "                ===========\n",
    "                print(\"===========\\nFailed to plot\\n===========\\n\")\n",
    "                {ex_type_str}\n",
    "                Exception message: {ex_value}\n",
    "                Stack trace: {stack_trace}\n",
    "                ===========\n",
    "                \"\"\"))\n",
    "        \n",
    "    end_pred: float = timer()\n",
    "    prediction_time: float = end_pred - start_pred\n",
    "    total_time: float = training_time + prediction_time\n",
    "    results_dict = Utils.update_results_dict(results_dict = results_dict,\n",
    "                                             hyperparams_dict = hyperparams_dict,\n",
    "                                             train_loss_history = t_losses_all,\n",
    "                                             val_loss_history = v_losses_all,\n",
    "                                             lr_history = lr_all,\n",
    "                                             epochs_output = epochs_output,\n",
    "                                             training_time = training_time,\n",
    "                                             prediction_time = prediction_time,\n",
    "                                             total_time = total_time,\n",
    "                                             logprob_ref_ref_sum_list = logprob_ref_ref_sum_list,\n",
    "                                             logprob_ref_alt_sum_list = logprob_ref_alt_sum_list,\n",
    "                                             logprob_alt_alt_sum_list = logprob_alt_alt_sum_list,\n",
    "                                             lik_ratio_list = lik_ratio_list,\n",
    "                                             lik_ratio_norm_list = lik_ratio_norm_list,\n",
    "                                             ks_means = ks_means,\n",
    "                                             ks_stds = ks_stds,\n",
    "                                             ks_lists = ks_lists,\n",
    "                                             ad_means = ad_means,\n",
    "                                             ad_stds = ad_stds,\n",
    "                                             ad_lists = ad_lists,\n",
    "                                             fn_list = fn_list,\n",
    "                                             wd_means = wd_means,\n",
    "                                             wd_stds = wd_stds,\n",
    "                                             wd_lists = wd_lists,\n",
    "                                             swd_means = swd_means,\n",
    "                                             swd_stds = swd_stds,\n",
    "                                             swd_lists = swd_lists\n",
    "                                             )\n",
    "    return results_dict, DataInputs, prediction_time, total_time # type: ignore\n",
    "\n",
    "##############################################################################################\n",
    "################################## Parameters initialization #################################\n",
    "##############################################################################################\n",
    "\n",
    "### Initialize number of components ###\n",
    "ncomp: int = 3\n",
    "\n",
    "### Initialize hyperparameters lists ###\n",
    "ndims: int = 64\n",
    "nbijectors: int = 10\n",
    "hidden_layers: List[int] = [128, 128, 128]\n",
    "seed_train: int = 869\n",
    "\n",
    "### Initialize nsamples inputs ###\n",
    "nsamples_train: int = 100000\n",
    "nsamples_val: int = 30000\n",
    "nsamples_test: int = 100000\n",
    "\n",
    "### Initialize seeds inputs ###\n",
    "seed_test: int = 0 # overwritten in the loop by seed_train + 1\n",
    "seed_dist: int = 0\n",
    "seed_metrics: int = seed_test\n",
    "\n",
    "### Initialize bijector inputs ###\n",
    "bijector_name: str = 'CsplineN'\n",
    "range_min: int = -16\n",
    "spline_knots = 8\n",
    "\n",
    "### Initialize NN hyperparameters ###\n",
    "activation: str = 'relu'\n",
    "regulariser: Optional[str] = None\n",
    "eps_regulariser: float = 0.\n",
    "use_batch_norm: bool = True\n",
    "\n",
    "### Initialzie training hyperparameters ###\n",
    "epochs_input: int = 10\n",
    "batch_size: int = 512\n",
    "nan_threshold: float = 0.01\n",
    "max_retry: int = 10\n",
    "debug_print_mode: bool = True\n",
    "\n",
    "### Initialize optimizer hyperparameters ###\n",
    "lr_orig: float = 0.001\n",
    "\n",
    "### Initialize callbacks hyperparameters ###\n",
    "es_min_delta: float = .0001\n",
    "es_patience: int = 100\n",
    "lr_min_delta: float = .0001\n",
    "lr_patience: int = 50\n",
    "lr_reduce_factor: float = .5\n",
    "lr_reduce_factor_on_nan: float = float(1/3)\n",
    "min_lr: float = 1e-6\n",
    "\n",
    "### Initialize parameters for inference ###\n",
    "n_iter: int = 10\n",
    "n_slices_factor: int = 2\n",
    "dtype: type = tf.float32\n",
    "max_vectorize: int = 10\n",
    "mirror_strategy = False\n",
    "make_plots = True\n",
    "\n",
    "### Initialize old variables for backward compatibility\n",
    "corr: Optional[str] = None\n",
    "\n",
    "### Initialize old variables for backward compatibility\n",
    "corr: Optional[str] = None\n",
    "\n",
    "### Initialize dictionaries ###\n",
    "results_dict: Dict[str, Any] = Utils.init_results_dict()\n",
    "hyperparams_dict: Dict[str, Any] = Utils.init_hyperparams_dict()\n",
    "\n",
    "### Initialize output dir ###\n",
    "mother_output_dir: str = Utils.define_dir('../../results/CsplineN_test/')\n",
    "\n",
    "### Create 'log' file ####\n",
    "log_file_name: str = Utils.create_log_file(mother_output_dir, results_dict)\n",
    "\n",
    "##############################################################################################\n",
    "####################################### Training loop ########################################\n",
    "##############################################################################################\n",
    "run_number: int = 0\n",
    "n_runs = 1\n",
    "start_global: float = timer()\n",
    "targ_dist: tfp.distributions.Distribution = MixtureGaussian(ncomp = ncomp, ndims = ndims, seed = seed_dist)\n",
    "base_dist: tfp.distributions.Distribution = Distributions.gaussians(ndims)\n",
    "start_run: float = timer()\n",
    "hllabel: str = '-'.join(str(e) for e in hidden_layers)\n",
    "run_number = run_number + 1\n",
    "results_dict_txt_saved: bool = False\n",
    "results_dict_json_saved: bool = False\n",
    "results_log_saved: bool = False\n",
    "path_to_results: str\n",
    "to_run: bool\n",
    "path_to_results, to_run = Utils.define_run_dir(mother_output_dir+'run_'+str(run_number)+'/',\n",
    "                                               force = \"delete\",\n",
    "                                               bkp = False)\n",
    "if to_run:\n",
    "    #try:\n",
    "    dummy_file_path: str = os.path.join(path_to_results,'running.txt')\n",
    "    with open(dummy_file_path, 'w') as f:\n",
    "        pass\n",
    "    path_to_weights: str = Utils.define_dir(os.path.join(path_to_results, 'weights'))\n",
    "    checkpoint_path: str = os.path.join(path_to_weights, 'best_weights.h5')\n",
    "    ########### Model train ###########\n",
    "    NFObject: Trainer.Trainer\n",
    "    #tf.data.experimental.enable_debug_mode()\n",
    "    hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],\n",
    "                                                                           nsamples = [nsamples_train, nsamples_val, nsamples_test],\n",
    "                                                                           run_number = run_number,\n",
    "                                                                           base_dist = base_dist,\n",
    "                                                                           targ_dist = targ_dist,\n",
    "                                                                           hyperparams_dict = hyperparams_dict,\n",
    "                                                                           n_runs = n_runs,\n",
    "                                                                           ndims = ndims,\n",
    "                                                                           bijector_name = bijector_name,\n",
    "                                                                           nbijectors = nbijectors,\n",
    "                                                                           spline_knots = spline_knots,\n",
    "                                                                           range_min = range_min,\n",
    "                                                                           hidden_layers = hidden_layers,\n",
    "                                                                           batch_size = batch_size,\n",
    "                                                                           epochs_input = epochs_input,\n",
    "                                                                           lr_orig = lr_orig,\n",
    "                                                                           es_min_delta = es_min_delta,\n",
    "                                                                           es_patience = es_patience,\n",
    "                                                                           lr_reduce_factor = lr_reduce_factor,\n",
    "                                                                           lr_min_delta = lr_min_delta,\n",
    "                                                                           lr_patience = lr_patience,\n",
    "                                                                           min_lr = min_lr,\n",
    "                                                                           activation = activation,\n",
    "                                                                           regulariser = regulariser,\n",
    "                                                                           eps_regulariser = eps_regulariser,\n",
    "                                                                           use_batch_norm = use_batch_norm,\n",
    "                                                                           training_device = training_device,\n",
    "                                                                           path_to_results = path_to_results,\n",
    "                                                                           checkpoint_path = checkpoint_path,\n",
    "                                                                           max_retry = max_retry,\n",
    "                                                                           debug_print_mode = debug_print_mode,\n",
    "                                                                           nan_threshold = nan_threshold)\n",
    "     \n",
    "    print(f\"Model trained in {training_time:.2f} s.\\n\") # type: ignore\n",
    "    ########## Model prediction ###########\n",
    "    results_dict, DataInputs, prediction_time, total_time = prediction_function(hyperparams_dict = hyperparams_dict,\n",
    "                                                                                results_dict = results_dict,\n",
    "                                                                                gpu_models = gpu_models,\n",
    "                                                                                NFObject = NFObject, # type: ignore\n",
    "                                                                                ndims = ndims,\n",
    "                                                                                targ_dist = targ_dist,\n",
    "                                                                                seed_test = seed_test,\n",
    "                                                                                seed_metrics = seed_metrics,\n",
    "                                                                                n_iter = n_iter,\n",
    "                                                                                nsamples_test = nsamples_test,\n",
    "                                                                                n_slices_factor = n_slices_factor,\n",
    "                                                                                dtype = dtype,\n",
    "                                                                                max_vectorize = max_vectorize,\n",
    "                                                                                mirror_strategy = mirror_strategy,\n",
    "                                                                                make_plots = make_plots,\n",
    "                                                                                path_to_results = path_to_results)\n",
    "    ########### Save results ###########\n",
    "    Utils.save_results_current_run_txt(path_to_results, results_dict)\n",
    "    results_dict_txt_saved = True\n",
    "    print(\"results.txt saved\")\n",
    "    Utils.save_results_current_run_json(path_to_results, results_dict)\n",
    "    results_dict_json_saved = True\n",
    "    print(\"results.json saved\")\n",
    "    Utils.save_results_log(log_file_name, results_dict)\n",
    "    results_log_saved = True\n",
    "    print(\"Results log saved\")\n",
    "    print(f\"Model predictions computed in {prediction_time:.2f} s.\")\n",
    "    end_run: float = timer()\n",
    "    total_time_run=end_run-start_run\n",
    "    print(textwrap.dedent(f\"\"\"\\\n",
    "        ===========\n",
    "        Run {run_number}/{n_runs} done in {total_time_run:.2f} s.\n",
    "        ===========\n",
    "        \"\"\"))\n",
    "    run = run + 1\n",
    "    try:\n",
    "        os.remove(dummy_file_path)\n",
    "    except:\n",
    "        pass\n",
    "    dummy_file_path = os.path.join(path_to_results,'done.txt')\n",
    "    with open(dummy_file_path, 'w') as f:\n",
    "        pass\n",
    "    #except Exception as ex:\n",
    "    #    try:\n",
    "    #        os.remove(dummy_file_path)\n",
    "    #    except:\n",
    "    #        pass\n",
    "    #    # Get current system exception\n",
    "    #    ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "    #    # Extract unformatter stack traces as tuples\n",
    "    #    trace_back = traceback.extract_tb(ex_traceback) # type: ignore\n",
    "    #    # Format stacktrace\n",
    "    #    stack_trace = list()\n",
    "    #    for trace in trace_back:\n",
    "    #        stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "    #    if not results_dict_txt_saved:\n",
    "    #        results_dict = Utils.update_results_dict(results_dict = results_dict,\n",
    "    #                                                 hyperparams_dict = hyperparams_dict)\n",
    "    #        Utils.save_results_current_run_txt(path_to_results, results_dict)\n",
    "    #    if not results_dict_json_saved:\n",
    "    #        Utils.save_results_current_run_json(path_to_results, results_dict)\n",
    "    #    if not results_log_saved:\n",
    "    #        Utils.save_results_log(log_file_name, results_dict)\n",
    "    #    ex_type_str = f\"Exception type: {ex_type.__name__}\" # type: ignore\n",
    "    #    print(textwrap.dedent(f\"\"\"\\\n",
    "    #        ===========\n",
    "    #        Run {run_number}/{n_runs} failed.\n",
    "    #        {ex_type_str}\n",
    "    #        Exception message: {ex_value}\n",
    "    #        Stack trace: {stack_trace}\n",
    "    #        ===========\n",
    "    #        \"\"\"))\n",
    "else:\n",
    "    print(textwrap.dedent(f\"\"\"\\\n",
    "        ===========\n",
    "        Run {run_number}/{n_runs} already exists. Skipping it.\n",
    "        ===========\n",
    "        \"\"\"))\n",
    "keys_to_remove = ['ks_lists', 'ad_lists', 'fn_list', 'wd_lists', 'swd_lists', 'train_loss_history', 'val_loss_history', 'lr_history']\n",
    "dict_copy: Dict[str, Any] = {k: v for k, v in results_dict.items() if k not in keys_to_remove}\n",
    "results_frame: pd.DataFrame = pd.DataFrame(dict_copy)\n",
    "results_last_run_file: str = os.path.join(mother_output_dir,'results_last_run.txt')\n",
    "results_frame.to_csv(results_last_run_file,index=False)\n",
    "end_global: float = timer()\n",
    "print(f\"Everything done in {end_global-start_global:.2f} s.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
