import sklearn
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
import numpy as np
import random
tfd = tfp.distributions
tfb = tfp.bijectors

def RandCorr(matrixSize,seed):
    np.random.seed(0)
    V = sklearn.datasets.make_spd_matrix(matrixSize,random_state=seed)
    D = np.sqrt(np.diag(np.diag(V)))
    Dinv = np.linalg.inv(D)
    Vnorm = np.matmul(np.matmul(Dinv,V),Dinv)
    return Vnorm

def is_pos_def(x):
    return np.all(np.linalg.eigvals(x) > 0)

def RandCov(std,seed):
    matrixSize = len(std)
    corr = RandCorr(matrixSize,seed)
    D = np.diag(std)
    V = np.matmul(np.matmul(D,corr),D)
    return V

def plot_corr_matrix(X):
    df = pd.DataFrame(X)
    f = plt.figure(figsize=(18, 18))
    plt.matshow(df.corr(), fignum=f.number)
    cb = plt.colorbar()
    plt.grid(False)
    plt.show()
    plt.close()

def MixNormal1(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to that of 'loc' and 'scale'). This means that each component in each
    dimension can be assigned a different probability.

    The resulting multivariate distribution has small correlation.

    Note: The functions 'MixNormal1' and 'MixNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixNormal2' and 'MixNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.random.sample([n_components,n_dimensions])*10
    #loc = [[1.,4.,7.,10.],[2.,5.,8.,11.],[3.,6.,9.,12.]]
    scale = np.random.sample([n_components,n_dimensions])
    #scale = np.full([n_components,n_dimensions],0.1)
    probs = np.random.sample([n_dimensions,n_components])
    #probs = np.full([n_dimensions,n_components],1.)
    components = []
    for i in range(n_components):
        components.append(tfd.Normal(loc=loc[i],scale=scale[i]))
    mix_gauss=tfd.Mixture(
        cat=tfd.Categorical(probs=probs),
        components=components,
        validate_args=True)
    return mix_gauss
    
def MixNormal2(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to 'n_components'). This means that each component in all
    dimension is assigned a single probability.

    The resulting multivariate distribution has small correlation.

    Note: The functions 'MixNormal1' and 'MixNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixNormal2' and 'MixNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.transpose(np.random.sample([n_components,n_dimensions])*10)
    #loc = np.transpose([[1.,4.,7.,10.],[2.,5.,8.,11.],[3.,6.,9.,12.]])
    scale = np.transpose(np.random.sample([n_components,n_dimensions]))
    #scale = np.transpose(np.full([n_components,n_dimensions],0.1))
    probs = np.random.sample(n_components)
    #probs = np.full(n_components,1.)
    mix_gauss=tfd.MixtureSameFamily(
        mixture_distribution=tfd.Categorical(probs=probs),
        components_distribution=tfd.Normal(loc=loc,scale=scale),
        validate_args=True)
    return mix_gauss

def MixNormal1_indep(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to that of 'loc' and 'scale'). This means that each component in each
    dimension can be assigned a different probability.

    The resulting multivariate distribution has small correlation.

    Note: The functions 'MixNormal1' and 'MixNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixNormal2' and 'MixNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.random.sample([n_components,n_dimensions])*10
    #loc = [[1.,4.,7.,10.],[2.,5.,8.,11.],[3.,6.,9.,12.]]
    scale = np.random.sample([n_components,n_dimensions])
    #scale = np.full([n_components,n_dimensions],0.1)
    probs = np.random.sample([n_dimensions,n_components])
    #probs = np.full([n_dimensions,n_components],1.)
    components = []
    for i in range(n_components):
        components.append(tfd.Normal(loc=loc[i],scale=scale[i]))
    mix_gauss=tfd.Independent(
    distribution=tfd.Mixture(
        cat=tfd.Categorical(probs=probs),
        components=components,
        validate_args=True),
    reinterpreted_batch_ndims=0)
    return mix_gauss
    
def MixNormal2_indep(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to 'n_components'). This means that each component in all
    dimension is assigned a single probability.

    The resulting multivariate distribution has small correlation.

    Note: The functions 'MixNormal1' and 'MixNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixNormal2' and 'MixNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.transpose(np.random.sample([n_components,n_dimensions])*10)
    #loc = np.transpose([[1.,4.,7.,10.],[2.,5.,8.,11.],[3.,6.,9.,12.]])
    scale = np.transpose(np.random.sample([n_components,n_dimensions]))
    #scale = np.transpose(np.full([n_components,n_dimensions],0.1))
    probs = np.random.sample(n_components)
    #probs = np.full(n_components,1.)
    mix_gauss=tfd.Independent(
        distribution=tfd.MixtureSameFamily(
            mixture_distribution=tfd.Categorical(probs=probs),
            components_distribution=tfd.Normal(loc=loc,scale=scale),
            validate_args=True),
        reinterpreted_batch_ndims=0)
    return mix_gauss

def MixMultiNormal1(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Multivariate Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to 'n_components'). This means that each Multivariate distribution 
    is assigned a single probability.

    The resulting multivariate distribution has large (random) correlation.

    Note: The functions 'MixMultiNormal1' and 'MixMultiNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixMultiNormal2' and 'MixMultiNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.random.sample([n_components,n_dimensions])*10
    scale = np.random.sample([n_components,n_dimensions])
    probs = np.random.sample(n_components)
    components = []
    for i in range(n_components):
        components.append(tfd.MultivariateNormalDiag(loc=loc[i],scale_diag=scale[i]))
    mix_gauss=tfd.Mixture(
        cat=tfd.Categorical(probs=probs),
        components=components,
        validate_args=True)
    return mix_gauss
    
def MixMultiNormal2(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Multivariate Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to 'n_components'). This means that each Multivariate distribution 
    is assigned a single probability.

    The resulting multivariate distribution has large (random) correlation.

    Note: The functions 'MixMultiNormal1' and 'MixMultiNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixMultiNormal2' and 'MixMultiNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.random.sample([n_components,n_dimensions])*10
    scale = np.random.sample([n_components,n_dimensions])
    probs = np.random.sample(n_components)
    mix_gauss=tfd.MixtureSameFamily(
        mixture_distribution=tfd.Categorical(probs=probs),
        components_distribution=tfd.MultivariateNormalDiag(loc=loc,scale_diag=scale),
        validate_args=True)
    return mix_gauss

def MixMultiNormal1_indep(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Multivariate Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to 'n_components'). This means that each Multivariate distribution 
    is assigned a single probability.

    The resulting multivariate distribution has large (random) correlation.

    Note: The functions 'MixMultiNormal1' and 'MixMultiNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixMultiNormal2' and 'MixMultiNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.random.sample([n_components,n_dimensions])*10
    scale = np.random.sample([n_components,n_dimensions])
    probs = np.random.sample(n_components)
    components = []
    for i in range(n_components):
        components.append(tfd.MultivariateNormalDiag(loc=loc[i],scale_diag=scale[i]))
    mix_gauss=tfd.Independent(
    distribution=tfd.Mixture(
        cat=tfd.Categorical(probs=probs),
        components=components,
        validate_args=True),
    reinterpreted_batch_ndims=0)
    return mix_gauss
    
def MixMultiNormal2_indep(n_components=3,n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Multivariate Normal distributions in 'n_dimensions' dimensions 
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes 
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to 'n_components'). This means that each Multivariate distribution 
    is assigned a single probability.

    The resulting multivariate distribution has large (random) correlation.

    Note: The functions 'MixMultiNormal1' and 'MixMultiNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixMultiNormal2' and 'MixMultiNormal2_indep' (also identical).
    """
    np.random.seed(seed)
    loc = np.random.sample([n_components,n_dimensions])*10
    scale = np.random.sample([n_components,n_dimensions])
    probs = np.random.sample(n_components)
    mix_gauss=tfd.Independent(
        distribution=tfd.MixtureSameFamily(
            mixture_distribution=tfd.Categorical(probs=probs),
            components_distribution=tfd.MultivariateNormalDiag(loc=loc,scale_diag=scale),
            validate_args=True),
        reinterpreted_batch_ndims=0)
    return mix_gauss



def TruncateDistribution(Distribution,n_dimensions,hinge=10e-5,seed=0):
    "Transforms a continuous distribution into a truncated one."


    sample=Distribution.sample(10000).numpy()
    means=list(np.mean(sample,axis=0))
    mins=list(np.amin(sample, axis=0))
    maxs=list(np.max(sample, axis=0))
    
    np.random.seed(seed)
    coin_toss=int(np.random.choice([0,1]))
    
    min_conditions=[]
    max_conditions=[]
    for j in range(n_dimensions):
        
        if coin_toss==1:
            min_conditions.append(means[j])
            max_conditions.append(maxs[j])
        else:
            min_conditions.append(mins[j])
            max_conditions.append(means[j])
        
    hinge=.5
    SoftClipBijector=tfb.SoftClip(low=min_conditions, high=max_conditions,hinge_softness=hinge)
    TruncatedDistribution=tfd.TransformedDistribution(Distribution,SoftClipBijector)

    return TruncatedDistribution, SoftClipBijector

def TruncatedCreator(locs,scales,lows,highs,ndims):


        trunc_list=[]
        
        for dim in range(ndims):
            trunc=tfd.TruncatedNormal(locs[dim],scales[dim],lows[dim],highs[dim])
            trunc_list.append(trunc)
        

        ndim_trunc = tfd.Blockwise(tfd.JointDistributionSequential(trunc_list))
    
        return ndim_trunc


def UnimodalTruncatedMixMultiNormal1(n_dimensions=4,seed=0):
    """
    Defines a mixture of 'n_components' Multivariate Normal distributions in 'n_dimensions' dimensions
    with means and stddevs given by the tensors 'loc' and 'scale' with shapes
    '(n_components,n_dimensions)'.
    The components are mixed according to the categorical distribution with probabilities
    'probs' (with shape equal to 'n_components'). This means that each Multivariate distribution
    is assigned a single probability.

    The resulting multivariate distribution has large (random) correlation.

    Note: The functions 'MixMultiNormal1' and 'MixMultiNormal1_indep'
    generate identical samples, different from the samples generated by
    'MixMultiNormal2' and 'MixMultiNormal2_indep' (also identical).
    """

    
    loc = np.random.sample(n_dimensions)*10
    scale = np.random.sample(n_dimensions)
    choice=int(np.random.choice([0,1,2]))
    
    trunc_list=[]
    max_conds=[]
    min_conds=[]
    
    for dim in range(n_dimensions):
        choice=int(np.random.choice([0,1,2]))
        
        if choice==0:
            trunc=tfd.TruncatedNormal(loc[dim],scale[dim],low=loc[dim],high=30)
            min_conds.append(tf.cast(loc[dim],dtype=tf.float32))
            max_conds.append(30)
        elif choice==1:
            trunc=tfd.TruncatedNormal(loc[dim],scale[dim],low=-30,high=loc[dim])
            min_conds.append(-30)
            max_conds.append(tf.cast(loc[dim],dtype=tf.float32))
        elif choice==2:
            trunc=tfd.TruncatedNormal(loc[dim],scale[dim],low=loc[dim]-scale[dim],high=loc[dim]+scale[dim])
            min_conds.append(tf.cast(loc[dim],dtype=tf.float32)-tf.cast(scale[dim],dtype=tf.float32))
            max_conds.append(tf.cast(loc[dim],dtype=tf.float32)+tf.cast(scale[dim],dtype=tf.float32))
            
        trunc_list.append(trunc)

    Truncated_Dist = tfd.Blockwise(tfd.JointDistributionSequential(trunc_list))
    
    
    return Truncated_Dist,min_conds,max_conds


def TruncatedMixMultiNormal1(n_components=3,n_dimensions=4,seed=0):
    """

    """

    
    np.random.seed(seed)
    choice=int(np.random.choice([0,1,2]))


            
    n_mid_components=n_components-2
    loc = np.random.sample([n_mid_components,n_dimensions])*10
    scale = np.random.sample([n_mid_components,n_dimensions])
    probs = np.random.sample(n_components)
    
    loc_edges = np.random.sample([2,n_dimensions])*10
    scale_edges = np.random.sample([2,n_dimensions])

    choice=0
    if choice==0:
    
        component_first=TruncatedCreator(np.amin(loc,axis=0)-10*scale_edges[0],scale_edges[0],np.amin(loc,axis=0),np.ones(n_dimensions)*10000,n_dimensions)
        #component_first=tfd.TruncatedNormal(loc=loc_edges[0],scale=scale_edges[0],low=loc_edges[0],high=1000)
        print(component_first)
        
        component_last=tfd.MultivariateNormalDiag(loc=loc_edges[1],scale_diag=scale_edges[1])
        print(component_last)
    elif choice==1:
        component_first=tfd.MultivariateNormalDiag(loc=loc_edges[0],scale_diag=scale_edges[0])
        component_last=tfd.TruncatedNormal(loc=loc_edges[1],scale=scale_edges[1],low=-10000,high=loc_edges[1])
    elif choice==2:
        component_first=tfd.TruncatedNormal(loc=loc_edges[0],scale=scale_edges[0],low=loc_edges[0],high=1000)
        component_last=tfd.TruncatedNormal(loc=loc_edges[1],scale=scale_edges[1],low=-10000,high=loc_edges[1])
        


    n_mid_components=n_components-2
    loc = np.random.sample([n_mid_components,n_dimensions])*10
    scale = np.random.sample([n_mid_components,n_dimensions])
    probs = np.random.sample(n_components)
    
    
    components = []
    components.append(component_first)
    for i in range(n_mid_components):
        print(loc[i])
        components.append(tfd.MultivariateNormalDiag(loc=loc[i],scale_diag=scale[i]))
        print(tfd.MultivariateNormalDiag(loc=loc[i],scale_diag=scale[i]))
    components.append(component_last)
    
    print(probs)
    mix_gauss=tfd.Mixture(
        cat=tfd.Categorical(probs=probs),
        components=components,
        validate_args=True)
    return mix_gauss

def describe_distributions(distributions):
    """
    Describes a 'tfp.distributions' object.
    """
    print('\n'.join([str(d) for d in distributions]))

def rot_matrix(data):
    cov_matrix = np.cov(data, rowvar=False)
    w, V = np.linalg.eig(cov_matrix)
    return V

def transform_data(data,rotation):
    data_new = np.dot(data,rotation)
    return data_new

def inverse_transform_data(data,rotation):
    data_new = np.dot(data,np.transpose(rotation))
    return data_new

def random():

    random=1* np.random.random_sample((200, 3)) -0
    
    print(list(random))
    
    for r in random:
        print(str(list(r))+',')
     
    
    return
    
#random()
