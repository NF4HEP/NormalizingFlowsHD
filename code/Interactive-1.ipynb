{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to tf2_12 (Python 3.10.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible devices: [0, 1, 2, 3]\n",
      "2023-08-23 01:31:41.474248: Importing os...\n",
      "2023-08-23 01:31:41.474393: Importing timer from timeit...\n",
      "2023-08-23 01:31:41.474512: Setting env variables for tf import (only device [0, 1, 2, 3] will be available)...\n",
      "2023-08-23 01:31:41.474738: Importing numpy...\n",
      "2023-08-23 01:31:41.635184: Importing sys...\n",
      "2023-08-23 01:31:41.635365: Importing pandas...\n",
      "2023-08-23 01:31:41.839937: Importing shutil...\n",
      "2023-08-23 01:31:41.840160: Importing subprocess...\n",
      "2023-08-23 01:31:41.840233: Importing tensorflow...\n",
      "Tensorflow version: 2.12.0\n",
      "2023-08-23 01:31:44.512409: Importing tensorflow_probability...\n",
      "Tensorflow probability version: 0.20.1\n",
      "2023-08-23 01:31:45.144407: Importing textwrap...\n",
      "2023-08-23 01:31:45.144525: Importing timeit...\n",
      "2023-08-23 01:31:45.144607: Importing traceback...\n",
      "2023-08-23 01:31:45.144673: Importing typing...\n",
      "2023-08-23 01:31:45.144757: Setting tf configs...\n",
      "2023-08-23 01:31:45.349967: Importing custom module...\n",
      "Successfully loaded GPU model: NVIDIA A40\n",
      "2023-08-23 01:31:46.383026: All modues imported successfully.\n",
      "Directory ../../results/MAFN_final/ already exists.\n",
      "===========\n",
      "Generating train data for run 1.\n",
      "===========\n",
      "Train data generated in 0.22 s.\n",
      "\n",
      "Building Trainer NFObject.\n",
      "\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Initializing Trainer with following parameters:\n",
      "base_distribution: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "x_data_train shape: (100000, 4)\n",
      "y_data_train shape: (100000, 0)\n",
      "io_kwargs: {'results_path': '../../results/MAFN_final/run_1/', 'load_weights': True, 'load_results': True}\n",
      "data_kwargs: {'seed': 0}\n",
      "compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'debug_print_mode': False}}}\n",
      "callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'TerminateOnNaNFractionCallback', 'config': {'threshold': 0.1, 'validation_data': array([[6.5709734, 6.8908386, 5.9732676, 5.608854 ],\n",
      "       [4.2167983, 4.049984 , 4.3976784, 8.770138 ],\n",
      "       [9.040854 , 2.6368966, 8.529587 , 4.7631216],\n",
      "       ...,\n",
      "       [4.202516 , 5.2170463, 4.0922155, 8.889091 ],\n",
      "       [5.248318 , 7.001233 , 6.1389027, 5.426161 ],\n",
      "       [6.221549 , 7.0797963, 5.9922805, 5.422195 ]], dtype=float32)}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/MAFN_final/run_1/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}]\n",
      "fit_kwargs: {'batch_size': 512, 'epochs': 2, 'validation_data': (array([[6.5709734, 6.8908386, 5.9732676, 5.608854 ],\n",
      "       [4.2167983, 4.049984 , 4.3976784, 8.770138 ],\n",
      "       [9.040854 , 2.6368966, 8.529587 , 4.7631216],\n",
      "       ...,\n",
      "       [4.202516 , 5.2170463, 4.0922155, 8.889091 ],\n",
      "       [5.248318 , 7.001233 , 6.1389027, 5.426161 ],\n",
      "       [6.221549 , 7.0797963, 5.9922805, 5.422195 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "\n",
      "--------------- Debub info ---------------\n",
      "Defined attributes:\n",
      "self.base_dist: tfp.distributions.Sample(\"SampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.flow: tfp.bijectors._Chain(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow\", batch_shape=[], min_event_ndims=1, bijectors=[MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow, Permute, MaskedAutoregressiveFlow])\n",
      "self.nf_dist: tfp.distributions._TransformedDistribution(\"chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal\", batch_shape=[], event_shape=[4], dtype=float32)\n",
      "self.io_kwargs: {'results_path': '../../results/MAFN_final/run_1/', 'load_weights': True, 'load_results': True}\n",
      "self.results_path: ../../results/MAFN_final/run_1\n",
      "self.data_kwargs: {'seed': 0}\n",
      "self.x_data: [[ 4.249943   5.1000476  4.9857802  9.668455 ]\n",
      " [ 4.248929   6.814287   4.9319386  8.396124 ]\n",
      " [ 4.7694697  6.4568024  6.1531773  5.442369 ]\n",
      " ...\n",
      " [ 4.2374673  6.3307285  3.225162   8.281889 ]\n",
      " [ 4.2109528  5.5085325  3.9714446  7.607838 ]\n",
      " [ 4.2538686  7.1307273  3.6821642 10.31261  ]]\n",
      "self.y_data: []\n",
      "self.ndims: 4\n",
      "Model defined.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " log_prob_layer (LogProbLaye  (None,)                  173480    \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,480\n",
      "Trainable params: 173,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary:  None\n",
      "self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flow_of_permute_of_masked_autoregressive_flowSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description=\"created by layer 'log_prob_layer'\")\n",
      "self.model: <keras.engine.functional.Functional object at 0x7faa8c085630>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "optimizer: <keras.optimizers.adam.Adam object at 0x7faa642416f0>\n",
      "type(optimizer): <class 'keras.optimizers.adam.Adam'>\n",
      "self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}\n",
      "self.optimizer: <keras.optimizers.adam.Adam object at 0x7faa642416f0>\n",
      "self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'debug_print_mode': False}}\n",
      "self.loss: <Trainer.MinusLogProbLoss object at 0x7faa3c721360>\n",
      "self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'debug_print_mode': False}}]\n",
      "self.metrics: [<Trainer.MinusLogProbMetric object at 0x7faa3c7219c0>]\n",
      "self.compile_kwargs: {}\n",
      "self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'TerminateOnNaNFractionCallback', 'config': {'threshold': 0.1, 'validation_data': array([[6.5709734, 6.8908386, 5.9732676, 5.608854 ],\n",
      "       [4.2167983, 4.049984 , 4.3976784, 8.770138 ],\n",
      "       [9.040854 , 2.6368966, 8.529587 , 4.7631216],\n",
      "       ...,\n",
      "       [4.202516 , 5.2170463, 4.0922155, 8.889091 ],\n",
      "       [5.248318 , 7.001233 , 6.1389027, 5.426161 ],\n",
      "       [6.221549 , 7.0797963, 5.9922805, 5.422195 ]], dtype=float32)}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_final/run_1/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}]\n",
      "self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7faa3c721f30>, <Trainer.TerminateOnNaNFractionCallback object at 0x7faa3c721ed0>, <keras.callbacks.ModelCheckpoint object at 0x7faa3c7221a0>, <keras.callbacks.EarlyStopping object at 0x7faa3c7222c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7faa3c7222f0>]\n",
      "self.fit_kwargs: {'batch_size': 512, 'epochs': 2, 'validation_data': (array([[6.5709734, 6.8908386, 5.9732676, 5.608854 ],\n",
      "       [4.2167983, 4.049984 , 4.3976784, 8.770138 ],\n",
      "       [9.040854 , 2.6368966, 8.529587 , 4.7631216],\n",
      "       ...,\n",
      "       [4.202516 , 5.2170463, 4.0922155, 8.889091 ],\n",
      "       [5.248318 , 7.001233 , 6.1389027, 5.426161 ],\n",
      "       [6.221549 , 7.0797963, 5.9922805, 5.422195 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}\n",
      "self.is_compiled: False\n",
      "self.training_time: 0.0\n",
      "self.history: {}\n",
      "Model successfully compiled.\n",
      "No weights found in ../../results/MAFN_final/run_1/weights/best_weights.h5. Training from scratch.\n",
      "No history found. Generating new history.\n",
      "===============\n",
      "Running 1/360 with hyperparameters:\n",
      "timestamp = 2023-08-23 01:31:53.760383\n",
      "ndims = 4\n",
      "seed_train = 0\n",
      "nsamples_train = 100000\n",
      "nsamples_val = 30000\n",
      "nsamples_test = 100000\n",
      "bijector = MAFN\n",
      "nbijectors = 5\n",
      "spline_knots = --\n",
      "range_min = -5\n",
      "hidden_layers = 128-128-128\n",
      "trainable_parameters = 173480\n",
      "epochs_input = 2\n",
      "batch_size = 512\n",
      "activation = relu\n",
      "training_device = NVIDIA A40\n",
      "===============\n",
      "\n",
      "Training model.\n",
      "\n",
      "Train first sample: [4.249943  5.1000476 4.9857802 9.668455 ]\n",
      "Epoch 1/2\n",
      "2023-08-23 01:32:16.622 \n",
      "Epoch 1/2 \n",
      "\t loss: 9.7319, MinusLogProbMetric: 9.7319, val_loss: 5.4972, val_MinusLogProbMetric: 5.4972\n",
      "938/938 [==============================] - 8s 7ms/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 5.49717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_final/run_1/weights/best_weights.h5\n",
      "196/196 - 31s - loss: 9.7319 - MinusLogProbMetric: 9.7319 - val_loss: 5.4972 - val_MinusLogProbMetric: 5.4972 - lr: 0.0010 - 31s/epoch - 160ms/step\n",
      "Epoch 2/2\n",
      "2023-08-23 01:32:31.102 \n",
      "Epoch 2/2 \n",
      "\t loss: 4.5077, MinusLogProbMetric: 4.5077, val_loss: 3.9516, val_MinusLogProbMetric: 3.9516\n",
      "938/938 [==============================] - 7s 7ms/step\n",
      "\n",
      "Epoch 2: val_loss improved from 5.49717 to 3.95157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_final/run_1/weights/best_weights.h5\n",
      "196/196 - 13s - loss: 4.5077 - MinusLogProbMetric: 4.5077 - val_loss: 3.9516 - val_MinusLogProbMetric: 3.9516 - lr: 0.0010 - 13s/epoch - 68ms/step\n",
      "succeeded: True\n",
      "Model trained in 44.97 s.\n",
      "\n",
      "===========\n",
      "Computing predictions\n",
      "===========\n",
      "\n",
      "Generating samples on 4 GPUs...\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "X_data_test shape: (1000000, 4).\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "X_data_nf shape: (1000000, 4).\n",
      "Samples generated in 10.80 s.\n",
      "Computing metrics...\n",
      "Parsing input distribution...\n",
      "Input distribution is a numeric numpy array or tf.Tensor.\n",
      "Parsing input distribution...\n",
      "Input distribution is a numeric numpy array or tf.Tensor.\n",
      "\n",
      "------------------------------------------\n",
      "Starting KS tests calculation...\n",
      "Running TF KS tests...\n",
      "niter = 10\n",
      "batch_size = 100000\n",
      "KS tests calculation completed in 4.466453265398741 seconds.\n",
      "\n",
      "------------------------------------------\n",
      "Starting SWD metric calculation...\n",
      "Running TF SWD calculation...\n",
      "niter = 10\n",
      "batch_size = 100000\n",
      "SWD metric calculation completed in 1.113512403331697 seconds.\n",
      "\n",
      "------------------------------------------\n",
      "Starting FN metric calculation...\n",
      "Running TF FN calculation...\n",
      "niter = 10\n",
      "batch_size = 100000\n",
      "FN metric calculation completed in 1.9066746532917023 seconds.\n",
      "Metrics computed in 7.96 s.\n",
      "Plots done in 9.06 s.\n",
      "results.txt saved\n",
      "results.json saved\n",
      "Results log saved\n",
      "Model predictions computed in 27.82 s.\n",
      "===========\n",
      "Run 1/360 done in 74.50 s.\n",
      "===========\n",
      "\n",
      "Everything done in 80.81 s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################################\n",
    "######################################### Initialize #########################################\n",
    "##############################################################################################\n",
    "\n",
    "visible_devices = [0,1,2,3]\n",
    "print(\"Visible devices:\", visible_devices)\n",
    "import datetime\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing os...\")\n",
    "import os\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing timer from timeit...\")\n",
    "from timeit import default_timer as timer\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Setting env variables for tf import (only device\", visible_devices, \"will be available)...\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([str(i) for i in visible_devices])\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing numpy...\")\n",
    "import numpy as np\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing sys...\")\n",
    "import sys\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing pandas...\")\n",
    "import pandas as pd\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing shutil...\")\n",
    "import shutil\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing subprocess...\")\n",
    "import subprocess\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing tensorflow...\")\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing tensorflow_probability...\")\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "print(\"Tensorflow probability version:\", tfp.__version__)\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing textwrap...\")\n",
    "import textwrap\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing timeit...\")\n",
    "from timeit import default_timer as timer\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing traceback...\")\n",
    "import traceback\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing typing...\")\n",
    "from typing import List, Tuple, Dict, Union, Optional, Any\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Setting tf configs...\")\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu_device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu_device, True)\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"Importing custom module...\")\n",
    "\n",
    "sys.path.append('../../../code')\n",
    "import Bijectors, Distributions, MixtureDistributions, Plotters, Trainer, Utils # type: ignore\n",
    "\n",
    "sys.path.insert(0,'../../../../')\n",
    "import GenerativeModelsMetrics as GMetrics # type: ignore\n",
    "\n",
    "def get_gpu_info() -> Optional[List[str]]:\n",
    "    try:\n",
    "        gpu_info: str = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=gpu_name\", \"--format=csv,noheader\"]).decode('utf-8')\n",
    "        return gpu_info.strip().split('\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "gpu_models: Optional[List[str]] = get_gpu_info()\n",
    "if gpu_models:\n",
    "    training_device: str = gpu_models[0]\n",
    "    print(\"Successfully loaded GPU model: {}\".format(training_device))\n",
    "else:\n",
    "    training_device = 'undetermined'\n",
    "    print(\"Failed to load GPU model. Defaulting to 'undetermined'.\")\n",
    "    \n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")+\":\", \"All modues imported successfully.\")\n",
    "\n",
    "##############################################################################################\n",
    "####################################### Helper functions #####################################\n",
    "##############################################################################################\n",
    "\n",
    "def MixtureGaussian(ncomp: int,\n",
    "                    ndims: int,\n",
    "                    seed: int = 0) -> tfp.distributions.Mixture:\n",
    "    targ_dist: tfp.distributions.Mixture = MixtureDistributions.MixMultiNormal1(ncomp,ndims,seed=seed)\n",
    "    return targ_dist\n",
    "\n",
    "def get_io_kwargs(path_to_results: str) -> Dict[str,Any]:\n",
    "    return {'results_path': path_to_results,\n",
    "            'load_weights': True,\n",
    "            'load_results': True}\n",
    "    \n",
    "def get_data_kwargs(seed: int = 0) -> Dict[str,Any]:\n",
    "    return {'seed': seed}\n",
    "\n",
    "def get_compiler_kwargs(lr: float) -> Dict[str,Any]:\n",
    "    compiler_kwargs = {'optimizer': {'class_name': 'Custom>Adam', # this gives the new Adam optimizer\n",
    "                                     'config': {'learning_rate': lr,\n",
    "                                                'beta_1': 0.9,\n",
    "                                                'beta_2': 0.999,\n",
    "                                                'epsilon': 1e-07,\n",
    "                                                'amsgrad': True}},\n",
    "                       'metrics': [{'class_name': 'MinusLogProbMetric', \n",
    "                                    'config': {'debug_print_mode': False}}],\n",
    "                       'loss': {'class_name': 'MinusLogProbLoss', \n",
    "                                'config': {'name': \"MLP\", 'ignore_nans': True, 'debug_print_mode': False}}}\n",
    "    return compiler_kwargs\n",
    "    \n",
    "def get_callbacks_kwargs(X_data_val: Union[np.ndarray,tf.Tensor],\n",
    "                         checkpoint_path: str,\n",
    "                         lr_reduce_factor_on_nan: float,\n",
    "                         es_min_delta: float,\n",
    "                         es_patience: int,\n",
    "                         lr_reduce_factor: float,\n",
    "                         lr_min_delta: float,\n",
    "                         lr_patience: int,\n",
    "                         min_lr: float\n",
    "                         ) -> List[Dict[str,Any]]:\n",
    "    callbacks_kwargs = [{'class_name': 'PrintEpochInfo',\n",
    "                         'config': {}},\n",
    "                        #{'class_name': 'HandleNaNCallback',\n",
    "                        # 'config': {'checkpoint_path': checkpoint_path,\n",
    "                        #            'lr_reduction_factor': lr_reduce_factor_on_nan,\n",
    "                        #            'random_seed_var': np.random.randint(1000000)}},\n",
    "                        {'class_name': 'TerminateOnNaNFractionCallback',\n",
    "                         'config': {'threshold': 0.1,\n",
    "                                    'validation_data': X_data_val}},\n",
    "                        {'class_name': 'ModelCheckpoint',\n",
    "                         'config': {'filepath': checkpoint_path,\n",
    "                                    'monitor': 'val_loss',\n",
    "                                    'save_best_only': True,\n",
    "                                    'save_weights_only': True,\n",
    "                                    'verbose': 1,\n",
    "                                    'mode': 'auto',\n",
    "                                    'save_freq': 'epoch'}},\n",
    "                        {'class_name': 'EarlyStopping',\n",
    "                         'config': {'monitor': 'val_loss', \n",
    "                                    'min_delta': es_min_delta, \n",
    "                                    'patience': es_patience, \n",
    "                                    'verbose': 1,\n",
    "                                    'mode': 'auto', \n",
    "                                    'baseline': None, \n",
    "                                    'restore_best_weights': True}},\n",
    "                        {'class_name': 'ReduceLROnPlateau', \n",
    "                         'config': {'monitor': 'val_loss', \n",
    "                                    'factor': lr_reduce_factor, \n",
    "                                    'min_delta': lr_min_delta, \n",
    "                                    'patience': lr_patience, \n",
    "                                    'min_lr': min_lr}}#,\n",
    "                        #{'class_name': 'TerminateOnNaN', 'config': {}}\n",
    "                        ]\n",
    "    return callbacks_kwargs\n",
    "\n",
    "def get_fit_kwargs(batch_size: int,\n",
    "                   epochs_input: int,\n",
    "                   validation_data: Tuple[Union[np.ndarray,tf.Tensor],Union[np.ndarray,tf.Tensor]],\n",
    "                   shuffle: bool,\n",
    "                   verbose: int\n",
    "                  ) -> Dict[str,Any]:\n",
    "    fit_kwargs = {'batch_size': batch_size, \n",
    "                  'epochs': epochs_input, \n",
    "                  'validation_data': validation_data,\n",
    "                  'shuffle': shuffle, \n",
    "                  'verbose': verbose}\n",
    "    return fit_kwargs\n",
    "\n",
    "def train_function(seeds: List[int],\n",
    "                   nsamples: List[int],\n",
    "                   run_number: int,\n",
    "                   targ_dist: tfp.distributions.Distribution,\n",
    "                   hyperparams_dict: Dict[str, Any],\n",
    "                   n_runs: int,\n",
    "                   ndims: int,\n",
    "                   bijector_name: str,\n",
    "                   nbijectors: int,\n",
    "                   spline_knots: Union[int,str],\n",
    "                   range_min: int,\n",
    "                   hllabel: str,\n",
    "                   batch_size: int,\n",
    "                   epochs_input: int,\n",
    "                   activation: str,\n",
    "                   regulariser: Optional[str],\n",
    "                   eps_regulariser: float,\n",
    "                   training_device: str\n",
    "                  ) -> Tuple[Dict[str, Any], Trainer.Trainer, bool, int, float]:\n",
    "    seed_train: int\n",
    "    seed_test: int\n",
    "    seed_dist: int\n",
    "    seed_metrics: int\n",
    "    seed_train, seed_test, seed_dist, seed_metrics = seeds\n",
    "    seed_test = seed_train + 1                     \n",
    "    Utils.reset_random_seeds(seed = seed_train)\n",
    "    nsamples_train: int\n",
    "    nsamples_val: int\n",
    "    try:\n",
    "        nsamples_train, nsamples_val = nsamples\n",
    "    except:\n",
    "        nsamples_train, nsamples_val, _ = nsamples\n",
    "    X_data_train: tf.Tensor\n",
    "    X_data_val: tf.Tensor\n",
    "    Y_data_train: tf.Tensor\n",
    "    Y_data_val: tf.Tensor\n",
    "    X_data_train, X_data_val, Y_data_train, Y_data_val = Utils.generate_train_data(run_number = run_number,\n",
    "                                                                                   targ_dist = targ_dist,\n",
    "                                                                                   nsamples_train = nsamples_train,\n",
    "                                                                                   nsamples_val = nsamples_val,\n",
    "                                                                                   seed_train = seed_train)\n",
    "    bijector: tfp.bijectors.Bijector = Bijectors.ChooseBijector(bijector_name = bijector_name,\n",
    "                                                                ndims = ndims,\n",
    "                                                                spline_knots = spline_knots,\n",
    "                                                                nbijectors = nbijectors,\n",
    "                                                                range_min = range_min,\n",
    "                                                                hidden_layers = hidden_layers,\n",
    "                                                                activation = activation,\n",
    "                                                                regulariser = regulariser,\n",
    "                                                                eps_regulariser = eps_regulariser)\n",
    "    Utils.save_bijector_info(path_to_results, bijector)\n",
    "    print(\"Building Trainer NFObject.\\n\")\n",
    "    NFObject: Trainer.Trainer = Trainer.Trainer(base_distribution = base_dist,\n",
    "                                                flow = bijector, \n",
    "                                                x_data_train = X_data_train,\n",
    "                                                y_data_train = Y_data_train,\n",
    "                                                io_kwargs = get_io_kwargs(path_to_results = path_to_results),\n",
    "                                                data_kwargs = get_data_kwargs(seed = seed_train),\n",
    "                                                compiler_kwargs = get_compiler_kwargs(lr = lr_orig),\n",
    "                                                callbacks_kwargs = get_callbacks_kwargs(X_data_val = X_data_val,\n",
    "                                                                                        checkpoint_path = checkpoint_path,\n",
    "                                                                                        lr_reduce_factor_on_nan = lr_reduce_factor_on_nan,\n",
    "                                                                                        es_min_delta = es_min_delta,\n",
    "                                                                                        es_patience = es_patience,\n",
    "                                                                                        lr_reduce_factor = lr_reduce_factor,\n",
    "                                                                                        lr_min_delta = lr_min_delta,\n",
    "                                                                                        lr_patience = lr_patience,\n",
    "                                                                                        min_lr = min_lr),\n",
    "                                                fit_kwargs = get_fit_kwargs(batch_size = batch_size,\n",
    "                                                                            epochs_input = epochs_input,\n",
    "                                                                            validation_data = (X_data_val, Y_data_val),\n",
    "                                                                            shuffle = True,\n",
    "                                                                            verbose = 2),\n",
    "                                                debug_print_mode = debug_print_mode)\n",
    "    trainable_params: int = NFObject.trainable_params\n",
    "    non_trainable_params: int = NFObject.non_trainable_params\n",
    "    hyperparams_dict = Utils.update_hyperparams_dict(hyperparams_dict = hyperparams_dict,\n",
    "                                                     run_number = run_number,\n",
    "                                                     n_runs = n_runs,\n",
    "                                                     seeds = [seed_train, seed_test, seed_dist, seed_metrics],\n",
    "                                                     nsamples = [nsamples_train, nsamples_val, nsamples_test],\n",
    "                                                     ndims = ndims,\n",
    "                                                     corr = None,\n",
    "                                                     bijector_name = bijector_name,\n",
    "                                                     nbijectors = nbijectors,\n",
    "                                                     spline_knots = spline_knots,\n",
    "                                                     range_min = range_min,\n",
    "                                                     hllabel = hllabel,\n",
    "                                                     trainable_parameters = trainable_params,\n",
    "                                                     non_trainable_parameters = non_trainable_params,\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     epochs_input = epochs_input,\n",
    "                                                     activation = activation,\n",
    "                                                     regulariser = regulariser,\n",
    "                                                     eps_regulariser = eps_regulariser,\n",
    "                                                     training_device = training_device)\n",
    "    Utils.save_hyperparams_dict(path_to_results, hyperparams_dict)\n",
    "    print(\"Training model.\\n\")\n",
    "    print(\"Train first sample:\", X_data_train[0]) # type: ignore\n",
    "    NFObject.train()\n",
    "    training_time: float = NFObject.training_time # type: ignore\n",
    "    if len(list(NFObject.history['loss'])) > 0:\n",
    "        succeeded = True\n",
    "        print(f\"succeeded: {succeeded}\")\n",
    "    else:\n",
    "        succeeded = False\n",
    "        seed_train = np.random.randint(1000000)\n",
    "        print(\"Training failed: trying again with seed\", seed_train, \".\")\n",
    "    return hyperparams_dict, NFObject, succeeded, seed_train, training_time\n",
    "\n",
    "def prediction_function(results_dict: Dict[str, Any],\n",
    "                        gpu_models: Optional[List[str]],\n",
    "                        NFObject: Trainer.Trainer,\n",
    "                        targ_dist: tfp.distributions.Distribution,\n",
    "                        seed_test: int,\n",
    "                        seed_metrics: int,\n",
    "                        n_iter: int,\n",
    "                        nsamples_test: int,\n",
    "                        batch_size_gen: int,\n",
    "                        n_slices_factor: int,\n",
    "                        dtype: type\n",
    "                       ) -> Tuple[Dict[str, Any], float, float]:\n",
    "    start_pred: float = timer()\n",
    "    t_losses_all: list = list(NFObject.history['loss']) # type: ignore\n",
    "    v_losses_all: list = list(NFObject.history['val_loss']) # type: ignore\n",
    "    lr_all: list = list(NFObject.history['lr']) # type: ignore\n",
    "    epochs_output: int = len(t_losses_all)\n",
    "    training_time: float = NFObject.training_time # type: ignore\n",
    "    try:\n",
    "        print(\"===========\\nComputing predictions\\n===========\\n\")\n",
    "        if gpu_models:\n",
    "            print(f\"Generating samples on {len(gpu_models)} GPUs...\")\n",
    "        start = timer()\n",
    "        n_samples: int = nsamples_test*n_iter\n",
    "        X_data_test: tf.Tensor = Utils.generate_and_clean_data(dist = targ_dist, \n",
    "                                                               n_samples = n_samples, \n",
    "                                                               batch_size = batch_size_gen, \n",
    "                                                               dtype = dtype, \n",
    "                                                               seed = seed_test, \n",
    "                                                               mirror_strategy = True)\n",
    "        print(f\"X_data_test shape: {X_data_test.shape}.\")\n",
    "        #print(f\"X_data_test first sample: {X_data_test[0]}.\") # type: ignore\n",
    "        X_data_nf: tf.Tensor = Utils.generate_and_clean_data(dist = NFObject.nf_dist, # type: ignore\n",
    "                                                             n_samples = n_samples, \n",
    "                                                             batch_size = batch_size_gen, \n",
    "                                                             dtype = dtype, \n",
    "                                                             seed = seed_test, \n",
    "                                                             mirror_strategy = True)\n",
    "        print(f\"X_data_nf shape: {X_data_nf.shape}.\")\n",
    "        #print(f\"X_data_nf first sample: {X_data_nf[0]}.\") # type: ignore\n",
    "        end = timer()\n",
    "        print(f\"Samples generated in {end - start:.2f} s.\")\n",
    "        print(\"Computing metrics...\")\n",
    "        start = timer()\n",
    "        X_data_test = tf.cast(X_data_test, dtype = dtype) # type: ignore\n",
    "        X_data_nf = tf.cast(X_data_nf, dtype = dtype) # type: ignore\n",
    "        DataInputs: GMetrics.TwoSampleTestInputs = GMetrics.TwoSampleTestInputs(dist_1_input = X_data_test,\n",
    "                                                                                dist_2_input = X_data_nf,\n",
    "                                                                                niter = n_iter,\n",
    "                                                                                batch_size = nsamples_test,\n",
    "                                                                                dtype_input = dtype,\n",
    "                                                                                seed_input = seed_metrics,\n",
    "                                                                                use_tf = True,\n",
    "                                                                                verbose = True)\n",
    "        KSTest: GMetrics.KSTest = GMetrics.KSTest(data_input = DataInputs,\n",
    "                                                  verbose = True)\n",
    "        SWDMetric: GMetrics.SWDMetric = GMetrics.SWDMetric(data_input = DataInputs,\n",
    "                                                           verbose = True)\n",
    "        FNMetric: GMetrics.FNMetric = GMetrics.FNMetric(data_input = DataInputs,\n",
    "                                                        verbose = True)\n",
    "        KSTest.compute()\n",
    "        SWDMetric.compute(nslices = n_slices_factor*ndims)\n",
    "        FNMetric.compute()\n",
    "        ks_result: Dict[str, np.ndarray] = KSTest.Results[-1].result_value\n",
    "        ks_lists: List[List[float]] = ks_result[\"pvalue_lists\"].tolist()\n",
    "        ks_means: List[float] = ks_result[\"pvalue_means\"].tolist()\n",
    "        ks_stds: List[float] = ks_result[\"pvalue_stds\"].tolist()\n",
    "        swd_result: Dict[str, np.ndarray] = SWDMetric.Results[-1].result_value\n",
    "        swd_lists: List[List[float]] = swd_result[\"metric_lists\"].tolist()\n",
    "        swd_means: List[float] = swd_result[\"metric_means\"].tolist()\n",
    "        swd_stds: List[float] = swd_result[\"metric_stds\"].tolist()\n",
    "        fn_result: Dict[str, np.ndarray] = FNMetric.Results[-1].result_value\n",
    "        fn_list: List[float] = fn_result[\"metric_list\"].tolist()\n",
    "        ad_lists: Optional[List[List[float]]] = None\n",
    "        ad_means: Optional[List[float]] = None\n",
    "        ad_stds: Optional[List[float]] = None\n",
    "        wd_lists: Optional[List[List[float]]] = None\n",
    "        wd_means: Optional[List[float]] = None\n",
    "        wd_stds: Optional[List[float]] = None\n",
    "        end = timer()\n",
    "        metrics_time = end - start\n",
    "        print(f\"Metrics computed in {metrics_time:.2f} s.\")\n",
    "    except:\n",
    "        print(\"===========\\nFailed on GPU, re-trying on CPU\\n===========\\n\")\n",
    "        with tf.device('/device:CPU:0'): # type: ignore\n",
    "            start = timer()\n",
    "            n_samples = nsamples_test*n_iter\n",
    "            X_data_test = Utils.generate_and_clean_data(targ_dist = targ_dist, \n",
    "                                                        n_samples = n_samples, \n",
    "                                                        batch_size = batch_size_gen, \n",
    "                                                        dtype = dtype, \n",
    "                                                        seed = seed_test, \n",
    "                                                        mirror_strategy = False)\n",
    "            print(f\"X_data_test shape: {X_data_test.shape}.\")\n",
    "            #print(f\"X_data_test first sample: {X_data_test[0]}.\") # type: ignore\n",
    "            X_data_nf = Utils.generate_and_clean_data(nf_dist = NFObject.nf_dist, # type: ignore\n",
    "                                                      n_samples = n_samples, \n",
    "                                                      batch_size = batch_size_gen, \n",
    "                                                      dtype = dtype, \n",
    "                                                      seed = seed_test, \n",
    "                                                      mirror_strategy = False)\n",
    "            print(f\"X_data_nf shape: {X_data_nf.shape}.\")\n",
    "            #print(f\"X_data_nf first sample: {X_data_nf[0]}.\") # type: ignore\n",
    "            end = timer()\n",
    "            print(f\"Samples generated in {end - start:.2f} s.\")\n",
    "            print(\"Computing metrics...\")\n",
    "            start = timer()\n",
    "            X_data_test = tf.cast(X_data_test, dtype = dtype) # type: ignore\n",
    "            X_data_nf = tf.cast(X_data_nf, dtype = dtype) # type: ignore\n",
    "            DataInputs = GMetrics.TwoSampleTestInputs(dist_1_input = X_data_test,\n",
    "                                                      dist_2_input = X_data_nf,\n",
    "                                                      niter = n_iter,\n",
    "                                                      batch_size = nsamples_test,\n",
    "                                                      dtype_input = dtype,\n",
    "                                                      seed_input = seed_metrics,\n",
    "                                                      use_tf = True,\n",
    "                                                      verbose = True)\n",
    "            KSTest = GMetrics.KSTest(data_inputs = DataInputs,\n",
    "                                     verbose = True)\n",
    "            SWDMetric = GMetrics.SWDMetric(data_inputs = DataInputs,\n",
    "                                           verbose = True)\n",
    "            FNMetric = GMetrics.FNMetric(data_inputs = DataInputs,\n",
    "                                         verbose = True)\n",
    "            KSTest.compute()\n",
    "            SWDMetric.compute(nslices = n_slices_factor*ndims)\n",
    "            FNMetric.compute()\n",
    "            ks_result = KSTest.Results[-1].result_value\n",
    "            ks_lists = ks_result[\"pvalue_lists\"].tolist()\n",
    "            ks_means = ks_result[\"pvalue_means\"].tolist()\n",
    "            ks_stds = ks_result[\"pvalue_stds\"].tolist()\n",
    "            swd_result = SWDMetric.Results[-1].result_value\n",
    "            swd_lists = swd_result[\"metric_lists\"].tolist()\n",
    "            swd_means = swd_result[\"metric_means\"].tolist()\n",
    "            swd_stds = swd_result[\"metric_stds\"].tolist()\n",
    "            fn_result = FNMetric.Results[-1].result_value\n",
    "            fn_list = fn_result[\"metric_list\"].tolist()\n",
    "            ad_lists = None\n",
    "            ad_means = None\n",
    "            ad_stds = None\n",
    "            wd_lists = None\n",
    "            wd_means = None\n",
    "            wd_stds = None\n",
    "            end = timer()\n",
    "            metrics_time = end - start\n",
    "            print(f\"Metrics computed in {metrics_time:.2f} s.\")\n",
    "    if make_plots:\n",
    "        try:\n",
    "            start = timer()\n",
    "            Plotters.train_plotter(t_losses_all,v_losses_all,path_to_results)\n",
    "            Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore\n",
    "            Plotters.marginal_plot(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims) # type: ignore\n",
    "            #Plotters.sample_plotter(X_data_test,nf_dist,path_to_results)\n",
    "            end = timer()\n",
    "            plots_time: float = end - start\n",
    "            print(f\"Plots done in {plots_time:.2f} s.\")\n",
    "        except:\n",
    "            print(\"===========\\nFailed to plot\\n===========\\n\")\n",
    "    end_pred: float = timer()\n",
    "    prediction_time: float = end_pred - start_pred\n",
    "    total_time: float = training_time + prediction_time\n",
    "    results_dict = Utils.update_results_dict(results_dict = results_dict,\n",
    "                                             hyperparams_dict = hyperparams_dict,\n",
    "                                             train_loss_history = t_losses_all,\n",
    "                                             val_loss_history = v_losses_all,\n",
    "                                             lr_history = lr_all,\n",
    "                                             epochs_output = epochs_output,\n",
    "                                             training_time = training_time,\n",
    "                                             prediction_time = prediction_time,\n",
    "                                             total_time = total_time,\n",
    "                                             ks_means = ks_means,\n",
    "                                             ks_stds = ks_stds,\n",
    "                                             ks_lists = ks_lists,\n",
    "                                             ad_means = ad_means,\n",
    "                                             ad_stds = ad_stds,\n",
    "                                             ad_lists = ad_lists,\n",
    "                                             fn_list = fn_list,\n",
    "                                             wd_means = wd_means,\n",
    "                                             wd_stds = wd_stds,\n",
    "                                             wd_lists = wd_lists,\n",
    "                                             swd_means = swd_means,\n",
    "                                             swd_stds = swd_stds,\n",
    "                                             swd_lists = swd_lists\n",
    "                                             )\n",
    "    return results_dict, prediction_time, total_time\n",
    "\n",
    "##############################################################################################\n",
    "################################## Parameters initialization #################################\n",
    "##############################################################################################\n",
    "\n",
    "### Initialize number of components ###\n",
    "ncomp: int = 3\n",
    "\n",
    "### Initialize hyperparameters lists ###\n",
    "ndims_list: List[int] = [4, 8, 16, 32, 64, 100, 200, 400, 1000]\n",
    "nbijectors_list: List[int] = [5, 10]\n",
    "hidden_layers_list: List[List[int]] = [[128, 128, 128], [256, 256, 256]]\n",
    "seeds_list: List[int] = [0, 187, 377, 440, 520, 541, 721, 869, 926, 933]\n",
    "\n",
    "### Initialize nsamples inputs ###\n",
    "nsamples_train: int = 100000\n",
    "nsamples_val: int = 30000\n",
    "nsamples_test: int = 100000\n",
    "\n",
    "### Initialize seeds inputs ###\n",
    "seed_test: int = 0 # overwritten in the loop by seed_train + 1\n",
    "seed_dist: int = 0\n",
    "seed_metrics: int = seed_test\n",
    "\n",
    "### Initialize bijector inputs ###\n",
    "bijector_name: str = 'MAFN'\n",
    "range_min: int = -5\n",
    "spline_knots_list: List[Union[int,str]] = [\"--\"] # Only relevant for the neural spline\n",
    "\n",
    "### Initialize NN hyperparameters ###\n",
    "activation: str = 'relu'\n",
    "regulariser: Optional[str] = None\n",
    "eps_regulariser: float = 0.\n",
    "\n",
    "### Initialzie training hyperparameters ###\n",
    "epochs_input: int = 2\n",
    "batch_size: int = 512\n",
    "debug_print_mode: bool = True\n",
    "\n",
    "### Initialize optimizer hyperparameters ###\n",
    "lr_orig: float = 0.001\n",
    "\n",
    "### Initialize callbacks hyperparameters ###\n",
    "es_min_delta: float = .0001\n",
    "es_patience: int = 100\n",
    "lr_min_delta: float = .0001\n",
    "lr_patience: int = 50\n",
    "lr_reduce_factor: float = .5\n",
    "lr_reduce_factor_on_nan: float = .5\n",
    "min_lr: float = 1e-6\n",
    "\n",
    "### Initialize parameters for inference ###\n",
    "n_iter: int = 10\n",
    "batch_size_gen: int = 10000\n",
    "n_slices_factor: int = 2\n",
    "dtype: type = tf.float32\n",
    "make_plots = True\n",
    "\n",
    "### Initialize old variables for backward compatibility\n",
    "corr: Optional[str] = None\n",
    "\n",
    "### Initialize dictionaries ###\n",
    "results_dict: Dict[str, Any] = Utils.init_results_dict()\n",
    "hyperparams_dict: Dict[str, Any] = Utils.init_hyperparams_dict()\n",
    "\n",
    "### Initialize output dir ###\n",
    "mother_output_dir: str = Utils.define_dir('../../results/MAFN_final/')\n",
    "\n",
    "### Create 'log' file ####\n",
    "log_file_name: str = Utils.create_log_file(mother_output_dir, results_dict)\n",
    "\n",
    "##############################################################################################\n",
    "####################################### Training loop ########################################\n",
    "##############################################################################################\n",
    "\n",
    "run_number: int = 0\n",
    "n_runs: int = len(ndims_list) * len(seeds_list) * len(nbijectors_list) * len(spline_knots_list) * len(hidden_layers_list)\n",
    "start_global: float = timer()\n",
    "for ndims in ndims_list:\n",
    "    targ_dist: tfp.distributions.Distribution = MixtureGaussian(ncomp = ncomp, ndims = ndims, seed = seed_dist)\n",
    "    base_dist: tfp.distributions.Distribution = Distributions.gaussians(ndims)\n",
    "    for seed_train in seeds_list:\n",
    "        for nbijectors in nbijectors_list:\n",
    "            for spline_knots in spline_knots_list:\n",
    "                for hidden_layers in hidden_layers_list:\n",
    "                    while run_number < 1:\n",
    "                        start_run: float = timer()\n",
    "                        hllabel: str = '-'.join(str(e) for e in hidden_layers)\n",
    "                        run_number = run_number + 1\n",
    "                        results_dict_txt_saved: bool = False\n",
    "                        results_dict_json_saved: bool = False\n",
    "                        results_log_saved: bool = False\n",
    "                        path_to_results: str\n",
    "                        to_run: bool\n",
    "                        path_to_results, to_run = Utils.define_run_dir(mother_output_dir+'run_'+str(run_number)+'/',\n",
    "                                                                       bkp = True,\n",
    "                                                                       force = \"delete\")\n",
    "                        if to_run:\n",
    "                            try:\n",
    "                                succeeded: bool = False\n",
    "                                path_to_weights: str = Utils.define_dir(os.path.join(path_to_results, 'weights'))\n",
    "                                checkpoint_path: str = os.path.join(path_to_weights, 'best_weights.h5')\n",
    "\n",
    "                                ########### Model train ###########\n",
    "\n",
    "                                NFObject: Trainer.Trainer\n",
    "                                while not succeeded:\n",
    "                                    hyperparams_dict, NFObject, succeeded, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],\n",
    "                                                                                                                      nsamples = [nsamples_train, nsamples_val],\n",
    "                                                                                                                      run_number = run_number,\n",
    "                                                                                                                      targ_dist = targ_dist,\n",
    "                                                                                                                      hyperparams_dict = hyperparams_dict,\n",
    "                                                                                                                      n_runs = n_runs,\n",
    "                                                                                                                      ndims = ndims,\n",
    "                                                                                                                      bijector_name = bijector_name,\n",
    "                                                                                                                      nbijectors = nbijectors,\n",
    "                                                                                                                      spline_knots = spline_knots,\n",
    "                                                                                                                      range_min = range_min,\n",
    "                                                                                                                      hllabel = hllabel,\n",
    "                                                                                                                      batch_size = batch_size,\n",
    "                                                                                                                      epochs_input = epochs_input,\n",
    "                                                                                                                      activation = activation,\n",
    "                                                                                                                      regulariser = regulariser,\n",
    "                                                                                                                      eps_regulariser = eps_regulariser,\n",
    "                                                                                                                      training_device = training_device)\n",
    "\n",
    "                                print(f\"Model trained in {training_time:.2f} s.\\n\") # type: ignore\n",
    "\n",
    "                                ########### Model prediction ###########\n",
    "\n",
    "                                results_dict, prediction_time, total_time = prediction_function(results_dict = results_dict,\n",
    "                                                                                                gpu_models = gpu_models,\n",
    "                                                                                                NFObject = NFObject, # type: ignore\n",
    "                                                                                                targ_dist = targ_dist,\n",
    "                                                                                                seed_test = seed_test,\n",
    "                                                                                                seed_metrics = seed_metrics,\n",
    "                                                                                                n_iter = n_iter,\n",
    "                                                                                                nsamples_test = nsamples_test,\n",
    "                                                                                                batch_size_gen = batch_size_gen,\n",
    "                                                                                                n_slices_factor = n_slices_factor,\n",
    "                                                                                                dtype = dtype)\n",
    "\n",
    "                                ########### Save results ###########\n",
    "\n",
    "                                Utils.save_results_current_run_txt(path_to_results, results_dict)\n",
    "                                results_dict_txt_saved = True\n",
    "                                print(\"results.txt saved\")\n",
    "                                Utils.save_results_current_run_json(path_to_results, results_dict)\n",
    "                                results_dict_json_saved = True\n",
    "                                print(\"results.json saved\")\n",
    "                                Utils.save_results_log(log_file_name, results_dict)\n",
    "                                results_log_saved = True\n",
    "                                print(\"Results log saved\")\n",
    "                                print(f\"Model predictions computed in {prediction_time:.2f} s.\")\n",
    "                                dummy_file_path: str = os.path.join(path_to_results,'done.txt')\n",
    "                                with open(dummy_file_path, 'w') as f:\n",
    "                                    pass\n",
    "                                end_run: float = timer()\n",
    "                                total_time_run=end_run-start_run\n",
    "                                print(textwrap.dedent(f\"\"\"\\\n",
    "                                    ===========\n",
    "                                    Run {run_number}/{n_runs} done in {total_time_run:.2f} s.\n",
    "                                    ===========\n",
    "                                    \"\"\"))\n",
    "                            except Exception as ex:\n",
    "                                # Get current system exception\n",
    "                                ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "                                # Extract unformatter stack traces as tuples\n",
    "                                trace_back = traceback.extract_tb(ex_traceback) # type: ignore\n",
    "                                # Format stacktrace\n",
    "                                stack_trace = list()\n",
    "                                for trace in trace_back:\n",
    "                                    stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "                                if not results_dict_txt_saved:\n",
    "                                    results_dict = Utils.update_results_dict(results_dict = results_dict,\n",
    "                                                                             hyperparams_dict = hyperparams_dict)\n",
    "                                    Utils.save_results_current_run_txt(path_to_results, results_dict)\n",
    "                                if not results_dict_json_saved:\n",
    "                                    Utils.save_results_current_run_json(path_to_results, results_dict)\n",
    "                                if not results_log_saved:\n",
    "                                    Utils.save_results_log(log_file_name, results_dict)\n",
    "                                ex_type_str = f\"Exception type: {ex_type.__name__}\" # type: ignore\n",
    "                                print(textwrap.dedent(f\"\"\"\\\n",
    "                                    ===========\n",
    "                                    Run {run_number}/{n_runs} failed.\n",
    "                                    {ex_type_str}\n",
    "                                    Exception message: {ex_value}\n",
    "                                    Stack trace: {stack_trace}\n",
    "                                    ===========\n",
    "                                    \"\"\"))\n",
    "                        else:\n",
    "                            print(textwrap.dedent(f\"\"\"\\\n",
    "                                ===========\n",
    "                                Run {run_number}/{n_runs} already exists. Skipping it.\n",
    "                                ===========\n",
    "                                \"\"\"))\n",
    "keys_to_remove = ['ks_lists', 'ad_lists', 'fn_list', 'wd_lists', 'swd_lists', 'train_loss_history', 'val_loss_history', 'lr_history']\n",
    "dict_copy: Dict[str, Any] = {k: v for k, v in results_dict.items() if k not in keys_to_remove}\n",
    "results_frame: pd.DataFrame = pd.DataFrame(dict_copy)\n",
    "results_last_run_file: str = os.path.join(mother_output_dir,'results_last_run.txt')\n",
    "results_frame.to_csv(results_last_run_file,index=False)\n",
    "end_global: float = timer()\n",
    "print(f\"Everything done in {end_global-start_global:.2f} s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512,\n",
       " 'epochs': 2,\n",
       " 'validation_data': (array([[6.5709734, 6.8908386, 5.9732676, 5.608854 ],\n",
       "         [4.2167983, 4.049984 , 4.3976784, 8.770138 ],\n",
       "         [9.040854 , 2.6368966, 8.529587 , 4.7631216],\n",
       "         ...,\n",
       "         [4.202516 , 5.2170463, 4.0922155, 8.889091 ],\n",
       "         [5.248318 , 7.001233 , 6.1389027, 5.426161 ],\n",
       "         [6.221549 , 7.0797963, 5.9922805, 5.422195 ]], dtype=float32),\n",
       "  <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>),\n",
       " 'shuffle': True,\n",
       " 'verbose': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFObject.fit_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = NFObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-f782314fcfc8>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "self.model.fit(x=self.x_data,\n",
    "               y=self.y_data,\n",
    "               callbacks=self.callbacks,\n",
    "               **self.fit_kwargs)\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2023-08-23 01:35:28.883 \n",
      "Epoch 1/2 \n",
      "\t loss: 4.0390, MinusLogProbMetric: 4.0390, val_loss: 3.9697, val_MinusLogProbMetric: 3.9697\n",
      "938/938 [==============================] - 7s 7ms/step\n",
      "\n",
      "Epoch 1: val_loss did not improve from 3.95157\n",
      "196/196 - 16s - loss: 4.0390 - MinusLogProbMetric: 4.0390 - val_loss: 3.9697 - val_MinusLogProbMetric: 3.9697 - lr: 0.0010 - 16s/epoch - 79ms/step\n",
      "Epoch 2/2\n",
      "2023-08-23 01:35:42.828 \n",
      "Epoch 2/2 \n",
      "\t loss: 3.4527, MinusLogProbMetric: 3.4527, val_loss: 3.3138, val_MinusLogProbMetric: 3.3138\n",
      "938/938 [==============================] - 7s 7ms/step\n",
      "\n",
      "Epoch 2: val_loss improved from 3.95157 to 3.31382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_final/run_1/weights/best_weights.h5\n",
      "196/196 - 13s - loss: 3.4527 - MinusLogProbMetric: 3.4527 - val_loss: 3.3138 - val_MinusLogProbMetric: 3.3138 - lr: 0.0010 - 13s/epoch - 67ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae50fb6d40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model.fit(x=self.x_data,\n",
    "               y=self.y_data,\n",
    "               callbacks=self.callbacks,\n",
    "               **self.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512,\n",
       " 'epochs': 2,\n",
       " 'validation_data': (array([[6.5709734, 6.8908386, 5.9732676, 5.608854 ],\n",
       "         [4.2167983, 4.049984 , 4.3976784, 8.770138 ],\n",
       "         [9.040854 , 2.6368966, 8.529587 , 4.7631216],\n",
       "         ...,\n",
       "         [4.202516 , 5.2170463, 4.0922155, 8.889091 ],\n",
       "         [5.248318 , 7.001233 , 6.1389027, 5.426161 ],\n",
       "         [6.221549 , 7.0797963, 5.9922805, 5.422195 ]], dtype=float32),\n",
       "  <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>),\n",
       " 'shuffle': True,\n",
       " 'verbose': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.fit_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_data,\n\u001b[1;32m      2\u001b[0m                y\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_data,\n\u001b[1;32m      3\u001b[0m                callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks,\n\u001b[1;32m      4\u001b[0m                verbose \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1676'>1677</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1677'>1678</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1678'>1679</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1681'>1682</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1682'>1683</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1683'>1684</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1684'>1685</a>\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1685'>1686</a>\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/keras/engine/training.py?line=1686'>1687</a>\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=890'>891</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=892'>893</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=893'>894</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=895'>896</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=896'>897</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=922'>923</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=923'>924</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=924'>925</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=925'>926</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=926'>927</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=927'>928</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=928'>929</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?line=929'>930</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py?line=139'>140</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py?line=140'>141</a>\u001b[0m   (concrete_function,\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py?line=141'>142</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py?line=142'>143</a>\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py?line=143'>144</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1752'>1753</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1753'>1754</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1754'>1755</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1755'>1756</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1756'>1757</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1757'>1758</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1758'>1759</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1759'>1760</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1760'>1761</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1761'>1762</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=1762'>1763</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=378'>379</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=379'>380</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=380'>381</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=381'>382</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=382'>383</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=383'>384</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=384'>385</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=385'>386</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=386'>387</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=387'>388</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=388'>389</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=389'>390</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=392'>393</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py?line=393'>394</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=49'>50</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=50'>51</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "self.model.fit(x=self.x_data,\n",
    "               y=self.y_data,\n",
    "               callbacks=self.callbacks,\n",
    "               verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2023-08-23 01:39:08.192 \n",
      "Epoch 1/2 \n",
      "\t loss: 2.7320, MinusLogProbMetric: 2.7320, val_loss: 2.7957, val_MinusLogProbMetric: 2.7957\n",
      "938/938 [==============================] - 7s 7ms/step\n",
      "\n",
      "Epoch 1: val_loss improved from 3.31382 to 2.79568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_final/run_1/weights/best_weights.h5\n",
      "196/196 - 14s - loss: 2.7320 - MinusLogProbMetric: 2.7320 - val_loss: 2.7957 - val_MinusLogProbMetric: 2.7957 - lr: 0.0010 - 14s/epoch - 69ms/step\n",
      "Epoch 2/2\n",
      "2023-08-23 01:39:21.681 \n",
      "Epoch 2/2 \n",
      "\t loss: 2.6859, MinusLogProbMetric: 2.6859, val_loss: 2.6676, val_MinusLogProbMetric: 2.6676\n",
      "938/938 [==============================] - 7s 7ms/step\n",
      "\n",
      "Epoch 2: val_loss improved from 2.79568 to 2.66755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/MAFN_final/run_1/weights/best_weights.h5\n",
      "196/196 - 13s - loss: 2.6859 - MinusLogProbMetric: 2.6859 - val_loss: 2.6676 - val_MinusLogProbMetric: 2.6676 - lr: 0.0010 - 13s/epoch - 68ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae39165150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model.fit(x=self.x_data,\n",
    "               y=self.y_data,\n",
    "               callbacks=self.callbacks,\n",
    "               batch_size=512,\n",
    "               epochs=2,\n",
    "               validation_data = self.fit_kwargs['validation_data'],\n",
    "               shuffle=True,\n",
    "               verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 6s - loss: 2.6406 - MinusLogProbMetric: 2.6406 - val_loss: 2.7587 - val_MinusLogProbMetric: 2.7587 - 6s/epoch - 30ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 6s - loss: 2.6625 - MinusLogProbMetric: 2.6625 - val_loss: 2.6403 - val_MinusLogProbMetric: 2.6403 - 6s/epoch - 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa99c6a7ac0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model.fit(x=self.x_data,\n",
    "               y=self.y_data,\n",
    "               #callbacks=self.callbacks,\n",
    "               batch_size=512,\n",
    "               epochs=2,\n",
    "               validation_data = self.fit_kwargs['validation_data'],\n",
    "               shuffle=True,\n",
    "               verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(self.callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.LambdaCallback at 0x7faa3c721f30>,\n",
       " <Trainer.TerminateOnNaNFractionCallback at 0x7faa3c721ed0>,\n",
       " <keras.callbacks.ModelCheckpoint at 0x7faa3c7221a0>,\n",
       " <keras.callbacks.EarlyStopping at 0x7faa3c7222c0>,\n",
       " <keras.callbacks.ReduceLROnPlateau at 0x7faa3c7222f0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "self.model.fit(x=self.x_data,\n",
    "               y=self.y_data,\n",
    "               callbacks=self.callbacks[1:],\n",
    "               batch_size=512,\n",
    "               epochs=2,\n",
    "               validation_data = self.fit_kwargs['validation_data'],\n",
    "               shuffle=True,\n",
    "               verbose = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
